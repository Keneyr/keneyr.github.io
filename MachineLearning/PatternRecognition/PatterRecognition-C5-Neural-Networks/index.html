<!DOCTYPE html>
<html  lang="en">
<head>
    <meta charset="utf-8" />

<meta name="generator" content="Hexo 4.2.1" />

<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

<title>PatterRecognition-C5-Neural-Networks - QinranY&#39;s Homepage</title>


    <meta name="description" content="Keywords: Gradient descent optimization, Error backpropagation, Hessian Matrix, Jacobian Matrix, Regularization, Mixture Density Network, Bayesian Neural Network, Python">
<meta property="og:type" content="article">
<meta property="og:title" content="PatterRecognition-C5-Neural-Networks">
<meta property="og:url" content="https://keneyr.com/MachineLearning/PatternRecognition/PatterRecognition-C5-Neural-Networks/index.html">
<meta property="og:site_name" content="QinranY&#39;s Homepage">
<meta property="og:description" content="Keywords: Gradient descent optimization, Error backpropagation, Hessian Matrix, Jacobian Matrix, Regularization, Mixture Density Network, Bayesian Neural Network, Python">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://keneyr.com/images/og_image.png">
<meta property="article:published_time" content="2023-10-16T08:49:29.000Z">
<meta property="article:modified_time" content="2024-01-10T11:31:50.737Z">
<meta property="article:author" content="Keneyr">
<meta property="article:tag" content="MachineLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://keneyr.com/images/og_image.png">








<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.7.2/css/bulma.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Ubuntu:400,600|Source+Code+Pro">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/vs2015.css">


    
    
<style>body>.footer,body>.navbar,body>.section{opacity:0}</style>

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css">

    
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.css">

    
    
    
    
<link rel="stylesheet" href="/css/back-to-top.css">

    
    
    
    
    
    
    
    <link rel="stylesheet" href="/css/progressbar.css">
<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
    
    <script async="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    


<link rel="stylesheet" href="/css/style.css">
</head>
<body class="is-2-column">
    <nav class="navbar navbar-main">
    <div class="container">
        <div class="navbar-brand is-flex-center">
            <a class="navbar-item navbar-logo" href="/">
            
                <img src="/images/beauty.jpg" alt="PatterRecognition-C5-Neural-Networks" height="28">
            
            </a>
        </div>
        <div class="navbar-menu">
            
            <div class="navbar-start">
                
                <a class="navbar-item"
                href="/">Home</a>
                
                <a class="navbar-item"
                href="/categories">Categories</a>
                
                <a class="navbar-item"
                href="/tags">Tags</a>
                
                <a class="navbar-item"
                href="/about">About</a>
                
                <a class="navbar-item"
                href="/publication">Publication</a>
                
                <a class="navbar-item"
                href="/projects">Projects</a>
                
                <a class="navbar-item"
                href="/life">Life</a>
                
                <a class="navbar-item"
                href="/work">Work</a>
                
                <a class="navbar-item"
                href="/Animation">Animation</a>
                
                <a class="navbar-item"
                href="/archives">Archives</a>
                
                <a class="navbar-item"
                href="/CV">CV</a>
                
                <a class="navbar-item"
                href="/extra">Extra</a>
                
            </div>
            
            <div class="navbar-end">
                
                    
                    <a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/Keneyr">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                    
                
                
                <a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;">
                    <i class="fas fa-list-ul"></i>
                </a>
                
                
                <a class="navbar-item search" title="Search" href="javascript:;">
                    <i class="fas fa-search"></i>
                </a>
                
            </div>
        </div>
    </div>
</nav>
    
    <section class="section">
        <div class="container">
            <div class="columns">
                <div class="column is-8-tablet is-8-desktop is-9-widescreen has-order-2 column-main">
<div class="card">
    
    <div class="card-content article ">
        
        <div class="level article-meta is-size-7 is-uppercase is-mobile is-overflow-x-auto">
            <div class="level-left">
                <time class="level-item has-text-grey" datetime="2023-10-16T08:49:29.000Z">2023-10-16</time>
                
                <div class="level-item">
                <a class="has-link-grey -link" href="/categories/MachineLearning/">MachineLearning</a>&nbsp;/&nbsp;<a class="has-link-grey -link" href="/categories/MachineLearning/PatternRecognition/">PatternRecognition</a>
                </div>
                
                
                <span class="level-item has-text-grey">
                    
                    
                    39 minutes read (About 5878 words)
                </span>
                
                
                <span class="level-item has-text-grey" id="busuanzi_container_page_pv">
                    <i class="far fa-eye"></i>
                    <span id="busuanzi_value_page_pv">0</span> visits
                </span>
                
            </div>
        </div>
        
        <h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal">
            
                PatterRecognition-C5-Neural-Networks
            
        </h1>
        <div class="content">
            <p><strong><em>Keywords</em></strong>: Gradient descent optimization, Error backpropagation, Hessian Matrix, Jacobian Matrix, Regularization, Mixture Density Network, Bayesian Neural Network, Python</p>
<a id="more"></a>

<p>This is the Chapter5 ReadingNotes from book <em>Bishop-Pattern-Recognition-and-Machine-Learning-2006</em>. <a href="https://github.com/Keneyr/PRML">[Code_Python]</a></p>
<p>In &#x1F449;<a href="">Linear Models for Regression &gt;&gt;</a> and &#x1F449;<a href="">Linear Models for Classification &gt;&gt;</a> that comprised <strong><em>linear combinations of fixed basis functions</em></strong>. We saw that such models have useful analytical and computational properties but that their practical applicability was limited by the <u>curse of dimensionality</u>.</p>
<p>In order to apply such models to largescale problems, it is necessary to adapt the basis functions to the data.</p>
<h2 id="Feed-forward-Network-Functions"><a href="#Feed-forward-Network-Functions" class="headerlink" title="Feed-forward Network Functions"></a>Feed-forward Network Functions</h2><p>The general form of linear combinations of fixed nonlinear basis functions $\phi_j(x)$ is<br>$$<br>y(\pmb{x,w}) = f(\sum_{j=1}^M w_j \phi_j(x))<br>\tag{5.1}<br>$$<br><strong><em>Our goal</em></strong> is to extend this model by making the basis functions $\phi_j(x)$ depend on parameters and then to allow these parameters to be adjusted, along with the coefficients $\lbrace w_j \rbrace$, during training.</p>
<p>We can give the overall network function that, for sigmoidal output unit activation functions, takes the form<br>$$<br>y_k(x,\pmb{w}) = \sigma(\sum_{j=0}^{M} w_{kj}^{(2)}h(\sum_{i=0}^{D}w_{ji}^{(1)}x_i))<br>\tag{5.9}<br>$$<br>A key difference compared to the perceptron, however, is that the neural network uses <em>continuous sigmoidal nonlinearities</em> in the hidden units, whereas the perceptron uses <em>step-function nonlinearities</em>. This means that the neural network function is <em>differentiable</em> with respect to the network parameters, and this property will play a central role in network training.</p>
<p><img src="/images/PatternRecognition/c5/forward-propagation.png" alt="forward propagation"></p>
<h2 id="Network-Training"><a href="#Network-Training" class="headerlink" title="Network Training"></a>Network Training</h2><p>In summary, there is a natural choice of both output unit activation function and matching error function, according to the type of problem being solved. </p>
<ul>
<li><strong>For regression</strong>. we use linear outputs and a sum-of-squares error.<br>$$<br>E(\pmb{w}) = \frac{1}{2}\sum_{n=1}^{N}\lbrace y(x_n, \pmb{w}) - t_n\rbrace^2<br>\tag{5.14}<br>$$</li>
<li><strong>for (multiple independent) binary classifications</strong>. we use logistic sigmoid outputs and a cross-entropy error function.<br>$$<br>y = \sigma(a) = \frac{1}{1 + exp(-a)}<br>\tag{5.19}<br>$$<br>$$<br>E(\pmb{w}) = -\sum_{n=1}^{N}\lbrace t_n\ln y_n + (1-t_n)\ln(1-y_n)\rbrace, t_n = 0,1<br>\tag{5.21}<br>$$<br>If we have $K$ separate binary classifications to perform, then we can use a network having $K$ outputs each of which has a logistic sigmoid activation function. Associated with each output is a binary class label $t_k \in {0, 1}$, where $k = 1, . . . , K$.<br>$$<br>E(\pmb{w}) = -\sum_{n=1}^{N}\sum_{k=1}^{K} \lbrace t_{nk}\ln y_{nk} + (1 - t_{nk})\ln (1-y_{nk})\rbrace<br>\tag{5.23}<br>$$</li>
<li><strong>for multiclass classification</strong>. we use softmax outputs with the corresponding multiclass cross-entropy error function.<br>$$<br>y_k(x,\pmb{w}) = \frac{exp(a_k(\pmb{x, w}))}{\sum_j exp(a_j(\pmb{x,w}))}<br>\tag{5.25}<br>$$<br>$$<br>E(\pmb{w}) = -\sum_{n=1}^{N}\sum_{k=1}^{K} t_{nk} \ln y_k(x_n, \pmb{w})<br>\tag{5.24}<br>$$</li>
<li><strong>For classification problems involving two classes</strong>. we can use a single logistic sigmoid output, or alternatively we can use a network with two outputs having a softmax output activation function.</li>
</ul>
<h3 id="Parameter-optimization"><a href="#Parameter-optimization" class="headerlink" title="Parameter optimization"></a>Parameter optimization</h3><p><img src="/images/PatternRecognition/c5/error-surface.png" alt="error surface"></p>
<p>if we make a small step in weight space from $\pmb{w}$ to $\pmb{w + \sigma w}$ then the change in the error function is $\sigma E \simeq \sigma \pmb{w^T} \nabla E(\pmb{w})$, where the vector $\nabla E(\pmb{w})$ points in the direction of greatest rate of increase of the error function.</p>
<p>However, the error function typically has a highly nonlinear dependence on the weights and bias parameters, and so there will be many points in weight space at which the gradient vanishes (or is numerically very small).<br>$$<br>\nabla E(\pmb{w}) = 0 \tag{5.26}<br>$$</p>
<p>For a successful application of neural networks, it may not be necessary to find the global minimum (and in general it will not be known whether the global minimum has been found) but it may be necessary to compare several local minima in order to find a sufficiently good solution.</p>
<p>Because there is clearly no hope of finding an analytical solution to the equation $\nabla E(\pmb{w}) = 0$ we resort to <strong><em>iterative numerical procedures</em></strong>.</p>
<p>Most techniques involve choosing some initial value $\pmb{w^{(0)}}$ for the weight vector and then moving through weight space in a succession of steps of the form<br>$$<br>\pmb{w}^{(\tau + 1)} = \pmb{w}^{(\tau)} + \Delta \pmb{w^{(\tau)}}<br>\tag{5.27}<br>$$</p>
<p>&#x1F449;<a href="/Math/Calculus/Calculus-C14-Partial-Derivatives/#Directional-Derivatives-and-Gradient-Vectors">More about Gradient in Calculus &gt;&gt;</a></p>
<h3 id="Local-quadratic-approximation"><a href="#Local-quadratic-approximation" class="headerlink" title="Local quadratic approximation"></a>Local quadratic approximation</h3><p>Consider the &#x1F449;<a href="/Math/Calculus/Calculus-C10-Infinite-Sequences-and-Series/#Taylor-Polynomials">Taylor expansion &gt;&gt;</a> of $E(\pmb{w})$ around some point $\pmb{\hat{w}}$ in weight space<br>$$<br>E(\pmb{w}) \simeq E(\pmb{\hat{w}}) + (\pmb{w - \pmb{\hat{w}}})^T \pmb{b} + \frac{1}{2}(\pmb{w - \pmb{\hat{w}}})^T \pmb{H}(\pmb{w - \pmb{\hat{w}}})<br>\tag{5.28}<br>$$<br>where cubic and higher terms have been omitted. Here $\pmb{b}$ is defined to be the gradient of $E$ evaluated at $\pmb{\hat{w}}$<br>$$<br>\pmb{b} = \nabla E|_{\pmb{w} = \pmb{\hat{w}}}<br>\tag{5.29}<br>$$</p>
<p>and the Hessian matrix $\pmb{H} = \nabla \nabla E$ has elements($\pmb{H}$ is symmetric)</p>
<p>$$<br>(\pmb{H})_{ij} = \frac{\partial E}{\partial w_i \partial w_j} |_{\pmb{w = \hat{w}}}<br>\tag{5.30}<br>$$<br>From (5.28), the corresponding local approximation to the gradient is given by<br>$$<br>\nabla E \simeq \pmb{b} + \pmb{H}(\pmb{w - \hat{w}})<br>\tag{5.31}<br>$$</p>
<p>Consider the particular case of a local quadratic approximation around a point $\pmb{w^{\ast}}$ that is a minimum of the error function. In this case there is no linear term, because $\nabla E = 0$ at $\pmb{w^{\ast}}$, and (5.28) becomes<br>$$<br>E(\pmb{w}) = E(\pmb{w\ast}) + \frac{1}{2}(\pmb{w-w^{\ast}})^T\pmb{H}(\pmb{w-w^{\ast}})<br>\tag{5.32}<br>$$<br>where the Hessian $\pmb{H}$ is evaluated at $\pmb{w^{\ast}}$.</p>
<p>Consider the eigenvalue equation for Hessian Matrix:</p>
<p>$$<br>\pmb{Hu_i} = \lambda_i \pmb{u_i}<br>\tag{5.33}<br>$$<br>where the eigenvectors $\pmb{u_i}$ form a complete orthonormal set so that<br>$$<br>\pmb{u_i^Tu_j} = \sigma_{ij}<br>\tag{5.34}<br>$$<br>We now expand $(\pmb{w − w^{\ast}})$ as a linear combination of the eigenvectors in the form<br>$$<br>\pmb{w-w^{\ast}} = \sum_i \alpha_i \pmb{u_i}<br>\tag{5.35}<br>$$</p>
<blockquote>
<p>Recall $x = Py$, $x^T A x = y^T D y$, columns of $P$ are the eigen vectors of $A$, $D$ is a diagonal matrix with eigen values.</p>
</blockquote>
<p>This can be regarded as a transformation of the coordinate system in which the origin is translated to the point $\pmb{w^{\ast}}$, and the axes are rotated to align with the eigenvectors.</p>
<p>Substituting (5.35) into (5.32), and using (5.33) and (5.34), allows the error function to be written in the form<br>$$<br>E(\pmb{w}) = E(\pmb{w^{\ast}}) + \frac{1}{2}\sum_i \lambda_i \alpha_i^2<br>\tag{5.36}<br>$$<br>A matrix $\pmb{H}$ is said to be positive definite if, and only if,<br>$$<br>\pmb{v^THv} &gt; 0, for-all-\pmb{v}<br>\tag{5.37}<br>$$<br>Because the eigenvectors $\lbrace \pmb{u_i} \rbrace$ form a complete set, an arbitrary vector $\pmb{v}$ can be written in the form</p>
<p>$$<br>\pmb{v} = \sum_i c_i \pmb{u_i}<br>\tag{5.38}<br>$$</p>
<p>From (5.33) and (5.34), we then have<br>$$<br>\pmb{v^THv} = \sum_i c_i^2 \lambda_i<br>\tag{5.39}<br>$$</p>
<blockquote>
<p>and so $\pmb{H}$ will be positive definite if, and only if, all of its eigenvalues are positive.</p>
</blockquote>
<p><img src="/images/PatternRecognition/c5/quadratic-approximaton.png" alt="quadratic approximaton of the error fucntion"></p>
<p>For a one-dimensional weight space, a stationary point $\pmb{w^{\ast}}$ will be a minimum if<br>$$<br>\frac{\partial^2 E}{\partial \pmb{w^2}}|_{\pmb{w^{\ast}}} &gt; 0<br>\tag{5.40}<br>$$<br>The corresponding result in $D$-dimensions is that the Hessian matrix, evaluated at $\pmb{w^{\ast}}$, should be positive definite.</p>
<p>&#x1F449;<a href="/Math/Linear-Algebra/Algebra-C5-EigenValues-And-EigenVectors/">More about Eigenvalues in Algebra &gt;&gt;</a></p>
<p>&#x1F449;<a href="/Math/Linear-Algebra/Algebra-C7-Symmetric-Matrices-And-Quadratic-Forms/">More about Positive Definite in Algebra &gt;&gt;</a></p>
<h3 id="Use-of-gradient-information"><a href="#Use-of-gradient-information" class="headerlink" title="Use of gradient information"></a>Use of gradient information</h3><h3 id="Gradient-descent-optimization"><a href="#Gradient-descent-optimization" class="headerlink" title="Gradient descent optimization"></a>Gradient descent optimization</h3><p>The simplest approach to using gradient information is to choose the weight update in (5.27) to comprise a small step in the direction of the negative gradient, so that</p>
<p>$$<br>\pmb{w}^{\tau + 1} = \pmb{w}^{(\tau)} - \eta \nabla E(\pmb{w}^{(\tau)})<br>\tag{5.41}<br>$$<br>where the parameter $\eta &gt; 0$ is known as the <strong><em>learning rate</em></strong>.</p>
<p><strong>For batch optimization (Techniques that use the whole data set at once are called batch methods)</strong>, there are more efficient methods, such as <u>conjugate gradients</u> and <u>quasi-Newton methods</u>, which are much more robust and much faster than simple gradient descent.<br>Unlike gradient descent, these algorithms have the property that the error function always decreases at each iteration unless the weight vector has arrived at a local or global minimum.</p>
<p>There is, however, an <strong>on-line version</strong> of gradient descent that has proved useful in practice for training neural networks on large data sets. On-line gradient descent, also known as <strong><em>sequential gradient descent</em></strong> or <strong><em>stochastic gradient descent</em></strong>, makes an update to the weight vector based on one data point at a time, so that</p>
<p>$$<br>\pmb{w}^{\tau + 1} = \pmb{w}^{\tau} - \eta \nabla E_n(\pmb{w}^{(\tau)})<br>\tag{5.43}<br>$$</p>
<blockquote>
<ol>
<li>One advantage of on-line methods compared to batch methods is that the former handle redundancy in the data much more efficiently.</li>
<li>Another property of on-line gradient descent is the possibility of escaping from local minima, since a stationary point with respect to the error function for the whole data set will generally not be a stationary point for each data point individually.</li>
</ol>
</blockquote>
<h2 id="Error-Backpropagation"><a href="#Error-Backpropagation" class="headerlink" title="Error Backpropagation"></a>Error Backpropagation</h2><h3 id="Evaluation-of-error-function-derivatives"><a href="#Evaluation-of-error-function-derivatives" class="headerlink" title="Evaluation of error-function derivatives"></a>Evaluation of error-function derivatives</h3><p>Consider first a simple linear model in which the outputs $y_k$ are linear combinations of the input variables $x_i$ so that<br>$$<br>y_k = \sum_i w_{ki}x_i<br>\tag{5.45}<br>$$<br>together with an error function that, for a particular input pattern $n$, takes the form<br>$$<br>E_n = \frac{1}{2}\sum_k (y_{nk} - t_{nk})^2, y_{nk} = y_k(x_n, \pmb{w})<br>\tag{5.46}<br>$$<br>The gradient of this error function with respect to a weight $w_{ji}$ is given by<br>$$<br>\frac{\partial E_n}{\partial w_{ji}} = (y_{nj}-t_{nj})x_{ni}<br>\tag{5.47}<br>$$<br>which can be interpreted as a ‘local’ computation involving the product of an ‘error signal’ $y_{nj} − t_{nj}$ associated with the output end of the link $w_{ji}$ and the variable $x_{ni}$ associated with the input end of the link.</p>
<p>In a general feed-forward network, each unit computes a weighted sum of its inputs of the form<br>$$<br>a_j = \sum_i w_{ji}z_i<br>\tag{5.48}<br>$$<br>where $z_i$ is the activation of a unit, or input, that sends a connection to unit $j$, and $w_{ji}$ is the weight associated with that connection.</p>
<p>The sum in (5.48) is transformed by a nonlinear activation function $h(\cdot)$ to give the activation $z_j$ of unit $j$ in the form<br>$$<br>z_j  = h(a_j)<br>\tag{5.49}<br>$$<br>We can therefore apply the &#x1F449;<a href="/Math/Calculus/Calculus-C3-Derivatives/#The-Chaine-Rule">chain rule &gt;&gt;</a> for partial derivatives to give<br>$$<br>\frac{\partial E_n}{\partial w_{ji}} = \frac{\partial E_n}{\partial a_j} \frac{\partial a_j}{\partial w_{ji}}<br>\tag{5.50}<br>$$<br>let $\delta_j$ be errors,<br>$$<br>\delta_j \equiv \frac{\partial E_n}{\partial a_j}<br>\tag{5.51}<br>$$<br>By equation(5.48), we get<br>$$<br>\frac{\partial a_j}{\partial w_{ji}} = z_i<br>\tag{5.52}<br>$$<br>Substituting (5.51) and (5.52) into(5.50), we obtain<br>$$<br>\frac{\partial E_n}{\partial w_{ji}} = \delta_j z_i<br>\tag{5.53}<br>$$<br>Equation (5.53) has the same form with equation(5.47), right?</p>
<p>As we have seen already, for the output units, we have<br>$$<br>\delta_k = y_k - t_k<br>\tag{5.54}<br>$$<br>To evaluate the $\sigma$’s for hidden units, we again make use of the chain rule for partial derivatives,<br>$$<br>\delta_j \equiv \frac{\partial E_n}{\partial a_j} = \sum_k \frac{\partial E_n}{\partial a_k} \frac{\partial a_k}{\partial a_j}<br>\tag{5.55}<br>$$<br>where the sum runs over all units $k$ to which unit $j$ sends connections.</p>
<p><img src="/images/PatternRecognition/c5/backpropagation.png" alt="backpropagation"></p>
<p>If we now substitute the definition of $\sigma$ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the following backpropagation formula<br>$$<br>\delta_j = h’(a_j)\sum_k w_{kj} \delta_k<br>\tag{5.56}<br>$$<br>which tells us that the value of $\sigma$ for a particular hidden unit can be obtained by propagating the $\sigma$’s backwards from units higher up in the network, as illustrated in Figure 5.7.</p>
<p><img src="/images/PatternRecognition/c5/error-propagation.png" alt="error propagation process"></p>
<h3 id="A-simple-example"><a href="#A-simple-example" class="headerlink" title="A simple example"></a>A simple example</h3><p>Consider a two-layer network of the form illustrated in Figure 5.1, together with a sum-of-squares error, in which the output units have linear activation functions, so that $y_k = a_k$, while the hidden units have logistic sigmoid activation functions given by</p>
<p>$$<br>h(a) \equiv tanh(a)<br>\tag{5.58}<br>$$<br>where<br>$$<br>tanh(a) = \frac{e^a - e^{-a}}{e^a + e^{-a}}<br>\tag{5.59}<br>$$</p>
<p>this activation function’s derivative is,<br>$$<br>h(a) = 1 - h(a)^2<br>\tag{5.60}<br>$$</p>
<p>for pattern n the sum-of-squares error function is given by</p>
<p>$$<br>E_n = \frac{1}{2} \sum_{k=1}^K (y_k - t_k)^2<br>\tag{5.61}<br>$$</p>
<p>where $y_k$ is the activation of output unit $k$, and $t_k$ is the corresponding target, for a particular input pattern $x_n$.</p>
<p>For each pattern in the training set in turn, we first perform a forward propagation using<br>$$<br>a_j  = \sum_{i = 0}^D w_{ji}^{(1)} x_i<br>\tag{5.62}<br>$$</p>
<p>$$<br>z_j = tanh(a_j)<br>\tag{5.63}<br>$$</p>
<p>$$<br>y_k = \sum_{j=1}^M w_{kj}^{(2)} z_j<br>\tag{5.64}<br>$$</p>
<p><img src="/images/PatternRecognition/c5/two-layer-network.png" alt="two-layer-network"></p>
<p>Next we compute the $\sigma$’s for each output unit using</p>
<p>$$<br>\sigma_k = y_k - t_k<br>\tag{5.65}<br>$$<br>Then we backpropagate these to obtain $\sigma s$ for the hidden units using<br>$$<br>\sigma_j = (1 - z_j^2) \sum_{k=1}^K w_{kj} \sigma_k<br>\tag{5.66}<br>$$<br>Finally, the derivatives with respect to the first-layer and second-layer weights are given by<br>$$<br>\frac{\partial E_n}{\partial w_{ji}^{(1)}} = \sigma_j x_i<br>$$<br>$$<br>\frac{\partial E_n}{\partial w_{kj}^{(2)}} = \sigma_k z_j<br>\tag{5.67}<br>$$</p>
<h3 id="Efficiency-of-backpropagation"><a href="#Efficiency-of-backpropagation" class="headerlink" title="Efficiency of backpropagation"></a>Efficiency of backpropagation</h3><h3 id="The-Jacobian-matrix"><a href="#The-Jacobian-matrix" class="headerlink" title="The Jacobian matrix"></a>The Jacobian matrix</h3><p>The technique of backpropagation can also be applied to the calculation of other derivatives.</p>
<p>Here we consider the evaluation of the Jacobian matrix, whose elements are given by the derivatives of the network outputs with respect to the inputs<br>$$<br>J_{ki} \equiv \frac{\partial y_k}{\partial x_i}<br>\tag{5.70}<br>$$<br>Jacobian matrices play a useful role in systems built from a number of distinct modules, as illustrated in Figure 5.8.</p>
<p><img src="/images/PatternRecognition/c5/jacobian-matrix.png" alt="jacobian matrix"></p>
<p>Suppose we wish to minimize an error function $E$ with respect to the parameter $w$ in Figure 5.8. The derivative of the error function is given by<br>$$<br>\frac{\partial E}{\partial w} = \sum_{k,j} \frac{\partial E}{\partial y_k}\frac{\partial y_k}{\partial z_j}\frac{\partial z_j}{\partial w}<br>\tag{5.71}<br>$$</p>
<p>In general, the network mapping represented by a trained neural network will be nonlinear, and so the elements of the Jacobian matrix will not be constants but will depend on the particular input vector used.</p>
<p>$$<br>\begin{aligned}<br>J_{ki} &amp;= \frac{\partial y_k}{\partial x_i}\\<br>&amp;= \sum_j \frac{\partial y_k}{\partial a_j}\frac{\partial a_j}{\partial x_i}\\<br>&amp;= \sum_j w_{ji}\frac{\partial y_k}{\partial a_j}<br>\end{aligned}<br>\tag{5.73}<br>$$</p>
<p>We now write down a <u>recursive backpropagation formula</u> to determine the derivatives $\frac{\partial y_k}{\partial a_j}$<br>$$<br>\begin{aligned}<br>\frac{\partial y_k}{\partial a_j} &amp;= \sum_l \frac{\partial y_k}{\partial a_l} \frac{\partial a_l}{\partial a_j}\\<br>&amp;=  h’(a_j)\sum_l w_{lj}\frac{\partial y_k}{\partial a_l}<br>\end{aligned}<br>\tag{5.74}<br>$$</p>
<p>&#x1F449;<a href="">Jacobian matrix in Numberical Analysis &gt;&gt;</a></p>
<h2 id="The-Hessian-Matrix"><a href="#The-Hessian-Matrix" class="headerlink" title="The Hessian Matrix"></a>The Hessian Matrix</h2><p>The Hessian plays an important role in many aspects of neural computing, including the following:</p>
<ol>
<li>Several nonlinear optimization algorithms used for training neural networks are based on considerations of the second-order properties of the error surface, which are controlled by the Hessian matrix (Bishop and Nabney, 2008).</li>
<li>The Hessian forms the basis of a fast procedure for re-training a feed-forward network following a small change in the training data (Bishop, 1991).</li>
<li>The inverse of the Hessian has been used to identify the least significant weights in a network as part of network ‘pruning’ algorithms (Le Cun et al., 1990).</li>
<li>The Hessian plays a central role in the Laplace approximation for a Bayesian neural network (see Section 5.7). Its inverse is used to determine the predictive distribution for a trained network, its eigenvalues determine the values of hyperparameters, and its determinant is used to evaluate the model evidence.</li>
</ol>
<h3 id="Diagonal-approximation"><a href="#Diagonal-approximation" class="headerlink" title="Diagonal approximation"></a>Diagonal approximation</h3><p>From (5.48), the diagonal elements of the Hessian, for pattern $n$, can be written<br>$$<br>\frac{\partial^2 E_n}{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_j} z^2_i<br>\tag{5.79}<br>$$<br>recursively using the chain rule of differential calculus to give a backpropagation equation of the form<br>$$<br>\frac{\partial^2 E_n}{\partial a^2_j} = h’(a_j)^2 \sum_k \sum_{k’} w_{kj}w_{k’j} \frac{\partial^2 E_n}{\partial a_k \partial a_{k’}} + h’’(a_j) \sum_k w_{kj} \frac{\partial^2 E_n}{\partial a_k}<br>\tag{5.80}<br>$$<br>If we now neglect off-diagonal elements in the second-derivative terms,<br>$$<br>\frac{\partial^2 E_n}{\partial a^2_j} = h’(a_j)^2 \sum_k  w^2_{kj} \frac{\partial^2 E_n}{\partial^2 a_k} + h’’(a_j) \sum_k w_{kj} \frac{\partial E_n}{\partial a_k}<br>\tag{5.81}<br>$$<br>The major roblem with diagonal approximations, however, is that in practice the Hessian is typically found to be strongly nondiagonal, and so these approximations, which are driven mainly be computational convenience, must be treated with care.</p>
<h3 id="Outer-product-approximation"><a href="#Outer-product-approximation" class="headerlink" title="Outer product approximation"></a>Outer product approximation</h3><p>When neural networks are applied to regression problems, it is common to use a sum-of-squares error function of the form<br>$$<br>E = \frac{1}{2} \sum_{n=1}^N(y_n - t_n)^2<br>\tag{5.82}<br>$$<br>We can then write the Hessian matrix in the form<br>$$<br>\pmb{H} = \nabla \nabla E = \sum_{n=1}^{N} \nabla y_n \nabla y_n + \sum_{n=1}^N (y_n - t_n)\nabla \nabla y_n<br>\tag{5.83}<br>$$<br>By neglecting the second term in (5.83), we arrive at the <strong><em>Levenberg–Marquardt approximation</em></strong> or outer product approximation (because the Hessian matrix is built up from a sum of outer products of vectors), given by<br>$$<br>\pmb{H} \simeq \sum_{n=1}^N \pmb{b_n b_n^T}, \pmb{b_n} = \nabla y_n = \nabla a_n<br>\tag{5.84}<br>$$<br>Evaluation of the outer product approximation for the Hessian is straightforward as it only involves first derivatives of the error function,which can be evaluated efficiently in $O(W)$ steps using standard backpropagation.</p>
<blockquote>
<p>It is important to emphasize that this approximation is only likely to be valid for a network that has been trained appropriately.</p>
</blockquote>
<h3 id="Inverse-Hessian"><a href="#Inverse-Hessian" class="headerlink" title="Inverse Hessian"></a>Inverse Hessian</h3><p>First we write the outer-product approximation in matrix notation as<br>$$<br>\pmb{H_N} = \sum_{n=1}^N \pmb{b_n b_n^T}<br>\tag{5.86}<br>$$<br>where $\pmb{b_n} \equiv ∇_{\pmb{w}}a_n$ is the contribution to the gradient of the output unit activation arising from data point $n$.</p>
<p>Suppose we have already obtained the inverse Hessian using the first $L$ data points. By separating off the contribution from data point $L + 1$, we obtain</p>
<p>$$<br>\pmb{H} _{L+1} = \pmb{H} _L + \pmb{b} _{L+1} \pmb{b^T} _{L+1}<br>\tag{5.87}<br>$$</p>
<p>In order to evaluate the inverse of the Hessian, we now consider the matrix identity</p>
<p>$$<br>(\pmb{M+vv^T})^{-1} = \pmb{M}^{-1}- \frac{(\pmb{M^{-1}v})(\pmb{v^TM^{-1}})}{1+\pmb{v^TM^{-1}v}}<br>\tag{5.88}<br>$$</p>
<p>If we now identify $\pmb{H_{L}}$ with $\pmb{M}$ and $\pmb{b}_{L+1}$ with $\pmb{v}$, we obtain</p>
<p>$$<br>\pmb{H_{L+1}}^{-1} = \pmb{H_L}^{-1}- \frac{(\pmb{H_L^{-1}b_{L+1}})(\pmb{b_{L+1}^TH_L^{-1}})}{1+\pmb{b_{L+1}^TH_L^{-1}b_{L+1}}}<br>\tag{5.89}<br>$$<br>In this way, data points are sequentially absorbed until $L+1 = N$ and the whole data set has been processed.</p>
<p>In particular, <em>quasi-Newton nonlinear optimization algorithms</em> gradually build up an approximation to the inverse of the Hessian during training.</p>
<h3 id="Finite-differences"><a href="#Finite-differences" class="headerlink" title="Finite differences"></a>Finite differences</h3><h3 id="Exact-evaluation-of-the-Hessian"><a href="#Exact-evaluation-of-the-Hessian" class="headerlink" title="Exact evaluation of the Hessian"></a>Exact evaluation of the Hessian</h3><h3 id="Fast-multiplication-by-the-Hessian"><a href="#Fast-multiplication-by-the-Hessian" class="headerlink" title="Fast multiplication by the Hessian"></a>Fast multiplication by the Hessian</h3><p>For many applications of the Hessian, the quantity of interest is not the Hessian matrix $\pmb{H}$ itself but the product of $\pmb{H}$ with some vector $\pmb{v}$.</p>
<h2 id="Regularization-in-Neural-Networks"><a href="#Regularization-in-Neural-Networks" class="headerlink" title="Regularization in Neural Networks"></a>Regularization in Neural Networks</h2><p>Note that $M$ controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting there will be an optimum value of $M$ that gives the best generalization performance, corresponding to the optimum balance between under-fitting and over-fitting.</p>
<p><img src="/images/PatternRecognition/c5/different-M.png" alt="different M effect"></p>
<h3 id="Consistent-Gaussian-priors"><a href="#Consistent-Gaussian-priors" class="headerlink" title="Consistent Gaussian priors"></a>Consistent Gaussian priors</h3><p>Remember in Chapter1, we see that an alternative approach is to choose a relatively large value for $M$ and then to control complexity by the addition of a regularization term to the error function. The simplest regularizer is the quadratic, giving a regularized error of the form</p>
<p>$$<br>\widetilde{E}(\pmb{w}) = E(\pmb{w}) + \frac{\lambda}{2}\pmb{w^Tw}<br>\tag{5.112}<br>$$<br>This regularizer is also known as <strong><em>weight decay</em></strong>. One limitation of this form is <u>inconsistent with certain scaling properties of network mappings</u>.</p>
<p>Consider a multilayer perceptron network having two layers of weights and linear output units. The activations of the hidden units in the first hidden layer take the form</p>
<p>$$<br>z_j = h(\sum_i w_{ji}x_i + w_{j0})<br>\tag{5.113}<br>$$</p>
<p>while the activations of the output units are given by</p>
<p>$$<br>y_k = \sum_j w_{kj}z_j + w_{k0}<br>\tag{5.114}<br>$$</p>
<p>Suppose we perform a linear transformation of the input data of the form</p>
<p>$$<br>x_i \rightarrow \widetilde{x_i} = ax_i + b<br>\tag{5.115}<br>$$</p>
<p>$$<br>y_k \rightarrow \widetilde{y_k} = cy_k + d<br>\tag{5.118}<br>$$<br>also, we can perform a corresponding linear transformation of output data and the weights parameters.</p>
<p>If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then <u>consistency</u> requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. </p>
<blockquote>
<p>Any regularizer should be consistent with this property.</p>
</blockquote>
<p>These require that the regularizer should be invariant to re-scaling of the weights and to shifts of the biases. Such a regularizer is given by<br>$$<br>\frac{\lambda_1}{2} \sum_{w \in \mathcal{W_1}} w^2 + \frac{\lambda_2}{2} \sum_{w \in \mathcal{W_2}} w^2<br>\tag{5.121}<br>$$<br>where $\mathcal{W_1}$ denotes the set of weights in the first layer, $\mathcal{W_2}$ denotes the set of weights in the second layer, and biases are excluded from the summations.</p>
<p>This regularizer will remain unchanged under the weight transformations provided the regularization parameters are re-scaled using </p>
<p>$$<br>\lambda_1 \rightarrow a^{1/2}\lambda_1\\<br>\lambda_2 \rightarrow c^{-1/2}\lambda_2<br>$$</p>
<p>The regularizer (5.121) corresponds to a prior of the form</p>
<p>$$<br>p(\pmb{w}|\alpha_1, \alpha_2) \propto exp(-\frac{\alpha_1}{2}\sum_{w \in \mathcal{W_1}} w^2 - \frac{\alpha_2}{2}\sum_{w \in \mathcal{W_2}} w^2)<br>\tag{5.122}<br>$$</p>
<p>We can illustrate the effect of the resulting four hyperparameters by drawing samples from the prior and plotting the corresponding network functions, as shown in Figure 5.11.</p>
<p><img src="/images/PatternRecognition/c5/hyper-parameters.png" alt="hyper parameters"></p>
<p>More generally, we can consider priors in which the weights are divided into any number of groups $\mathcal{W_k}$ so that</p>
<p>$$<br>p(\pmb{w}) \propto exp(-\frac{1}{2}\sum_k \alpha_k ||\pmb{w}||_k^2)<br>\tag{5.123}<br>$$</p>
<p>where,</p>
<p>$$<br>||\pmb{w}|| _k^2 = \sum_{j \in \mathcal{W_k}} w_j^2<br>\tag{5.124}<br>$$</p>
<h3 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h3><p>An alternative to regularization as a way of controlling the effective complexity of a network is the procedure of early stopping.</p>
<p><img src="/images/PatternRecognition/c5/early-stopping.png" alt="early stopping"></p>
<h3 id="Invariances"><a href="#Invariances" class="headerlink" title="Invariances"></a>Invariances</h3><p>For example, in the classification of objects in two-dimensional images, such as handwritten digits, a particular object should be assigned the same classification irrespective of its position within the image (translation invariance) or of its size (scale invariance). </p>
<p>Such transformations produce significant changes in the raw data, expressed in terms of the intensities at each of the pixels in the image, and yet should give rise to the same output from the classification system.</p>
<p>We therefore seek alternative approaches for encouraging an adaptive model to exhibit the required invariances. These can broadly be divided into four categories:</p>
<ol>
<li>The training set is augmented using replicas of the training patterns, transformed according to the desired invariances. For instance, in our digit recognition example, we could make multiple copies of each example in which the digit is shifted to a different position in each image.</li>
</ol>
<p><img src="/images/PatternRecognition/c5/augumented-data.png" alt="augumented data"></p>
<ol start="2">
<li>A regularization term is added to the error function that penalizes changes in the model output when the input is transformed. This leads to the technique of <u>tangent propagation</u>, discussed in Section 5.5.4.</li>
</ol>
<ol start="3">
<li>Invariance is built into the pre-processing by extracting features that are invariant under the required transformations. Any subsequent regression or classification system that uses such features as inputs will necessarily also respect these invariances.</li>
</ol>
<ol start="4">
<li>The final option is to build the invariance properties into the structure of a neural network (or into the definition of a kernel function in the case of techniques such as the relevance vector machine). One way to achieve this is through the use of local receptive fields and shared weights, as discussed in the context of <u>convolutional neural networks</u> in Section 5.5.6.</li>
</ol>
<h3 id="Tangent-propagation"><a href="#Tangent-propagation" class="headerlink" title="Tangent propagation"></a>Tangent propagation</h3><p>Suppose the transformation is governed by a single parameter $\xi$ (which might be rotation angle for instance). Then the subspace $\mathcal{M}$ <strong><em>swept out</em></strong> by $x_n$ will be one-dimensional, and will be parameterized by $\xi$.</p>
<p>Let the vector that results from acting on $x_n$ by this transformation be denoted by $s(x_n, \xi)$, which is defined so that $s(x_n, 0) = x$. Then the tangent to the curve $M$ is given by the &#x1F449;<a href="/Math/Calculus/Calculus-C14-Partial-Derivatives/#Directional-Derivatives-and-Gradient-Vectors">directional derivative &gt;&gt;</a> $\tau = \frac{\partial s}{\partial \xi}$, and the tangent vector at the point $x_n$ is given by</p>
<p>$$<br>\tau_n = \frac{\partial s(x_n, \xi)}{\partial \xi}|_{\xi = 0}<br>\tag{5.125}<br>$$</p>
<p><img src="/images/PatternRecognition/c5/tangent-vector.png" alt="tangent vector"></p>
<blockquote>
<p>Image a function $z = f(x,y)$, the directional derivative is $D_{\pmb{u}}f|P_0$, is the slope of the trace curve on the surface at point $P_0$.</p>
</blockquote>
<p>The derivative of output $k$ with respect to $\xi$ is given by<br>$$<br>\frac{\partial y_k}{\partial \xi} |_{\xi = 0} = \sum_{i=1}^D \frac{\partial y_k}{\partial x_i}\frac{\partial x_i}{\partial \xi}|_{\xi = 0} = \sum_{i=1}^D J_{ki}\tau_i<br>\tag{5.126}<br>$$<br>where $J_{ki}$ is the $(k, i)$ element of the Jacobian matrix $J$.</p>
<p>The result (5.126) can be used to modify the standard error function, so as to encourage local invariance in the neighbourhood of the data points, by the addition to the original error function $E$ of a regularization function $\Omega$ to give a total error function of the form</p>
<p>$$<br>\widetilde{E} = E + \lambda \Omega<br>\tag{5.127}<br>$$<br>where $\lambda$ is a regularization coefficient and</p>
<p>$$<br>\begin{aligned}<br>\Omega &amp;= \frac{1}{2} \sum_n \sum_k (\frac{\partial y_{nk}}{\partial \xi}|_{\xi = 0})^2 \\<br>&amp;= \frac{1}{2} \sum_n \sum_k (\sum_{i=1}^D J_{nki}\tau _{ni})^2<br>\end{aligned}<br>\tag{5.128}<br>$$<br>The regularization function will be zero when the network mapping function is invariant under the transformation in the neighbourhood of each pattern vector, and the value of the parameter $\lambda$ determines the balance between fitting the training data and learning the invariance property.</p>
<p><img src="/images/PatternRecognition/c5/digital-tangent-vector.png" alt="tangent vector rotation"></p>
<p>A related technique, called tangent distance, can be used to build invariance properties into distance-based methods such as nearest-neighbour classifiers (Simardet al., 1993).</p>
<h3 id="Training-with-transformed-data"><a href="#Training-with-transformed-data" class="headerlink" title="Training with transformed data"></a>Training with transformed data</h3><p>Encourage invariance of a model to a set of transformations is to expand the training set using transformed versions of the original<br>input patterns, this approach is closely related to the technique of tangent propagation.</p>
<h3 id="Convolutional-networks"><a href="#Convolutional-networks" class="headerlink" title="Convolutional networks"></a>Convolutional networks</h3><p>Another approach to creating models that are invariant to certain transformation of the inputs is to build the invariance properties into the structure of a neural network. This is the basis for the convolutional neural network (Le Cun et al., 1989; LeCun et al., 1998), which has been widely applied to image data.</p>
<p><img src="/images/PatternRecognition/c5/convolutional-networks.png" alt="convolutional network"></p>
<p>&#x1F449;<a href="https://www.geeksforgeeks.org/introduction-convolution-neural-network/">First Understanding CNN &gt;&gt; </a></p>
<h3 id="Soft-weight-sharing"><a href="#Soft-weight-sharing" class="headerlink" title="Soft weight sharing"></a>Soft weight sharing</h3><p>Recall that the simple weight decay regularizer, given in (5.112), can be viewed as the negative log of a Gaussian prior distribution over the weights.</p>
<p>We can encourage the weight values to form several groups, rather than just one group, by considering instead a probability distribution that is a mixture of Gaussians. </p>
<p>The centres and variances of the Gaussian components, as well as the mixing coefficients, will be considered as adjustable parameters to be determined as part of the learning process. Thus, we have a probability density of the form</p>
<p>$$<br>p(\pmb{w}) = \prod_i p(w_i)<br>\tag{5.136}<br>$$<br>where<br>$$<br>p(w_i) = \sum_{j=1}^M \pi_j N(w_i | \mu_j, \sigma_j^2)<br>\tag{5.137}<br>$$<br>Taking the negative logarithm then leads to a regularization function of the form<br>$$<br>\Omega (\pmb{w}) = - \sum_i \ln (\sum_{j=1}^M \pi_j N(w_j|\mu_j, \sigma_j^2))<br>\tag{5.138}<br>$$<br>The total error function is then given by<br>$$<br>\widetilde{E}(\pmb{w}) = E(\pmb{w}) + \lambda \Omega (\pmb{w})<br>\tag{5.139}<br>$$</p>
<h2 id="Mixture-Density-Networks"><a href="#Mixture-Density-Networks" class="headerlink" title="Mixture Density Networks"></a>Mixture Density Networks</h2><p>The goal of supervised learning is to <strong><em>model a conditional distribution</em></strong> $p(T|X)$, which for many simple regression problems is chosen to be <em>Gaussian</em>.（maximum likelihood function, posterior distribution, predictive distribution…）</p>
<p>However, practical machine learning problems can often have significantly non-Gaussian distributions. These can arise, for example, with <strong><em>inverse problems</em></strong> in which the distribution can be multimodal(多模态), in which case the Gaussian assumption can lead to very poor predictions.</p>
<p><img src="/images/PatternRecognition/c5/kinametics.png" alt="kinametics"></p>
<p>For example, Data for this problem is generated by sampling a variable $x$ uniformly over the interval $(0, 1)$, to give a set of values ${x_n}$, and the corresponding target values tn are obtained by computing the function $x_n + 0.3 \sin(2\pi x_n)$ and then adding uniform noise over the interval $(−0.1, 0.1)$.</p>
<p>The inverse problem is then obtained by keeping the same data points but exchanging the roles of $x$ and $t$.</p>
<p><u>Least squares corresponds to maximum likelihood under a Gaussian assumption</u>. We see that this leads to a very poor model for the highly non-Gaussian inverse problem.</p>
<p><img src="/images/PatternRecognition/c5/forward-inverse-problem.png" alt="forward inverse problem by neural network"></p>
<p>We therefore seek a general framework for modelling conditional probability distributions. This can be achieved by using a mixture model for $p(\pmb{t|x})$ in which both the <u>mixing coefficients</u> as well as the <u>component densities</u> are flexible functions of the input vector $\pmb{x}$, giving rise to the mixture density network. </p>
<p>For any given value of $\pmb{x}$, the mixture model provides a general formalism for modelling an arbitrary conditional density function $p(\pmb{t|x})$. Provided we consider a sufficiently flexible network, we then have a framework for approximating arbitrary conditional distributions. Here we shall develop the model explicitly for Gaussian components, so that<br>$$<br>p(\pmb{t|x}) = \sum_{k=1}^K \pi_k(\pmb{x}) N(\pmb{t}|\pmb{\mu_k}, \sigma_k^2(\pmb{x}))<br>\tag{5.148}<br>$$</p>
<p><img src="/images/PatternRecognition/c5/mixture-density-network.png" alt="mixture density network"></p>
<p>The neural network in Figure 5.20 can, for example, be a two-layer network having sigmoidal (‘tanh’) hidden units. If there are $L$(e.g. L = 1, only Gaussian) components in the mixture model (5.148), and if $t$ has $K$(e.g. K = 3, 3 gaussian distribution mixture) components. Then the network will have $L$ output unit activations denoted by $a_k^{\pi}$ that determine the mixing coefficients $\pi_k(x)$, K outputs denoted by $a_k^{\sigma}$ that determine the kernel widths $\sigma_k(x)$, and $L \times K$ outputs denoted by $a_{kj}^{\mu}$ that determine the components $\mu_{kj}(x)$ of the kernel centres $\mu_k(x)$. ???</p>
<blockquote>
<p>The mixing coefficients must satisfy the constraints<br>$$<br>\sum_{k=1}^K \pi_k(x) = 1<br>\tag{5.149}<br>$$<br>which can be achieved using a set of softmax outputs<br>$$<br>\pi_k(x) = \frac{exp (a_k^\pi)}{\sum_{l=1}^K exp(a_l^\pi)}<br>\tag{5.150}<br>$$<br>Similarly, the variances must satisfy $\sigma_k^2(x) \geq 0$ and so can be represented in terms of the exponentials of the corresponding network activations using<br>$$<br>\sigma_k(x) = exp(a_k^{\sigma})<br>\tag{5.151}<br>$$<br>Finally, because the means $\mu_k(x)$ have real components, they can be represented directly by the network output activations<br>$$<br>\mu_{kj}(x) = a_{kj}^{\mu}<br>$$</p>
</blockquote>
<p><img src="/images/PatternRecognition/c5/mixture-results.png" alt="results"></p>
<h2 id="Bayesian-Neural-Networks"><a href="#Bayesian-Neural-Networks" class="headerlink" title="Bayesian Neural Networks"></a>Bayesian Neural Networks</h2><h3 id="Posterior-parameter-distribution"><a href="#Posterior-parameter-distribution" class="headerlink" title="Posterior parameter distribution"></a>Posterior parameter distribution</h3><p>Consider the problem of predicting a single continuous target variable $t$ from a vector $\pmb{x}$ of inputs (the extension to multiple targets is straightforward). We shall suppose that the conditional distribution $p(t|\pmb{x})$ is Gaussian, with an $x$-dependent mean given by the output of a neural network model $y(\pmb{x},\pmb{w})$, and with precision (inverse variance) $\beta$.</p>
<p>We can find a Gaussian approximation to the posterior distribution by using the &#x1F449;<a href="/MachineLearning/PatternRecognition/PatterRecognition-C4-Linear-Models-For-Classification/#The-Laplace-Approximation">Laplace approximation &gt;&gt; </a>.</p>
<p>$$<br>p(\pmb{w} | D, \alpha, \beta) \propto p(\pmb{w} | \alpha) p(D | \pmb{w}, \beta)<br>\tag{5.164}<br>$$</p>
<p>$$<br>q(\pmb{w} | D) = N( \pmb{w} | \pmb{w_{MAP}}, \pmb{A}^{-1})<br>\tag{5.167}<br>$$</p>
<blockquote>
<p>However, even with the Gaussian approximation to the posterior, this integration is still analytically intractable due to the <strong><em>nonlinearity</em></strong> of the network function $y(\pmb{x}, \pmb{w})$ as a function of $\pmb{w}$.<br>$$<br>p(t|\pmb{x}, D) = \int p(t|\pmb{x, w}) q(\pmb{w}|D) d\pmb{w}<br>\tag{5.168}<br>$$</p>
</blockquote>
<p>To make progress, we now assume that the posterior distribution has small variance compared with the characteristic scales of $\pmb{w}$ over which $y(\pmb{x}, \pmb{w})$ is varying. This allows us to make a <strong><em>Taylor series expansion of the network function</em></strong> around $\pmb{w_{MAP}}$ and retain only the linear terms</p>
<p>$$<br>y(\pmb{x}, \pmb{w}) \simeq y(\pmb{x}, \pmb{w_{MAP}}) + \pmb{g^T(w - w_{MAP})}<br>\tag{5.169}<br>$$<br>where we have defined<br>$$<br>\pmb{g} = \nabla _w y(\pmb{x, w})|_wMAP<br>\tag{5.170}<br>$$</p>
<p>With this approximation, we now have a linear-Gaussian model with a Gaussian distribution for $p(\pmb{w})$ and a Gaussian for $p(t|\pmb{w})$ whose mean is a linear function of $\pmb{w}$ of the form</p>
<p>$$<br>p(t | \pmb{x},\pmb{w}, \beta) \simeq N(t | y(\pmb{x, w_{MAP}} + \pmb{g^T(w-w_{MAP})}), \beta^{-1})<br>\tag{5.171}<br>$$<br>We can therefore make use of the general result (2.115) for the marginal $p(t)$ to give<br>&#x1F449;<a href="/MachineLearning/PatternRecognition/PatterRecognition-C2-Probability-Distributions/#Bayes’-theorem-for-Gaussian-variables">2.115 &gt;&gt; </a></p>
<p>$$<br>p(t | \pmb{x}, D, \alpha, \beta) = N(t | y(\pmb{x} , \pmb{w_{MAP}}), \sigma^2(\pmb{x}))<br>\tag{5.172}<br>$$<br>where<br>$$<br>\sigma^2{\pmb{x}} = \beta^{-1} + \pmb{g^TA^{-1}g}<br>\tag{5.173}<br>$$</p>
<h3 id="Hyperparameter-optimization"><a href="#Hyperparameter-optimization" class="headerlink" title="Hyperparameter optimization"></a>Hyperparameter optimization</h3><p>So far, we have assumed that the hyperparameters $\alpha$ and $\beta$ are fixed and known. We can make use of the <u>evidence framework</u>, discussed in Section 3.5, together with the Gaussian approximation to the posterior obtained using the <u>Laplace approximation</u>, to obtain a practical procedure for choosing the values of such <u>hyperparameters</u>.</p>
<h3 id="Bayesian-neural-networks-for-classification"><a href="#Bayesian-neural-networks-for-classification" class="headerlink" title="Bayesian neural networks for classification"></a>Bayesian neural networks for classification</h3>
        </div>
        
        <div class="level is-size-7 is-uppercase">
            <div class="level-start">
                <div class="level-item">
                    <span class="is-size-6 has-text-grey has-mr-7">#</span>
                    <a class="has-link-grey -link" href="/tags/MachineLearning/" rel="tag">MachineLearning</a>
                </div>
            </div>
        </div>
        
        
        
        
<div class="sharethis-inline-share-buttons"></div>
<script type='text/javascript' src='//platform-api.sharethis.com/js/sharethis.js#property=5e58d41ecdbda60012fcd87d&amp;product=inline-share-buttons&amp;cms=sop' async='async'></script>

        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="menu-label has-text-centered">Like this article? Support the author with</h3>
        <div class="buttons is-centered">
            
                
<a class="button is-info donate">
    <span class="icon is-small">
        <i class="fab fa-alipay"></i>
    </span>
    <span>Alipay</span>
    <div class="qrcode"><img src="/images/Alipay.jpg" alt="Alipay"></div>
</a>

                
                
<a class="button is-success donate">
    <span class="icon is-small">
        <i class="fab fa-weixin"></i>
    </span>
    <span>Wechat</span>
    <div class="qrcode"><img src="/images/Wechatpay.jpg" alt="Wechat"></div>
</a>

                
        </div>
    </div>
</div>



<div class="card card-transparent">
    <div class="level post-navigation is-flex-wrap is-mobile">
        
        <div class="level-start">
            <a class="level level-item has-link-grey  article-nav-prev" href="/MachineLearning/PatternRecognition/PatterRecognition-C6-Kernel-Methods/">
                <i class="level-item fas fa-chevron-left"></i>
                <span class="level-item">PatterRecognition-C6-Kernel-Methods</span>
            </a>
        </div>
        
        
        <div class="level-end">
            <a class="level level-item has-link-grey  article-nav-next" href="/MachineLearning/PatternRecognition/PatterRecognition-C4-Linear-Models-For-Classification/">
                <span class="level-item">PatterRecognition-C4-Linear-Models-For-Classification</span>
                <i class="level-item fas fa-chevron-right"></i>
            </a>
        </div>
        
    </div>
</div>



<div class="card">
    <div class="card-content">
        <h3 class="title is-5 has-text-weight-normal">Comments</h3>
        
<div id="comment-container"></div>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.4.1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1.4.1/dist/gitalk.min.js"></script>
<script>
    var gitalk = new Gitalk({
        clientID: 'f7d6d05297c7a1b94ee1',
        clientSecret: 'f3c964e1d94edd347ea044c1bc1ecdadac432326',
        id: 'd0beaa2d28a6386228597674b87497dd',
        repo: 'keneyr.github.io',
        owner: 'Keneyr',
        admin: "Keneyr",
        createIssueManually: false,
        distractionFreeMode: false
    })
    gitalk.render('comment-container')
</script>

    </div>
</div>
</div>
                




<div class="column is-4-tablet is-2-desktop is-3-widescreen  has-order-1 column-left ">
    
        
<div class="card widget">
    <div class="card-content">
        <nav class="level">
            <div class="level-item has-text-centered" style="flex-shrink: 1">
                <div>
                    
                    <figure class="image is-128x128 has-mb-6">
                        <img class="is-rounded" src="/images/anna.jpg" alt="Anna">
                    </figure>
                    
                    <p class="is-size-4 is-block">
                        Anna
                    </p>
                    
                    
                    <p class="is-size-6 is-block">
                        Game Programmer
                    </p>
                    
                    
                    <p class="is-size-6 is-flex is-flex-center has-text-grey">
                        <i class="fas fa-map-marker-alt has-mr-7"></i>
                        <span>Shanghai,China</span>
                    </p>
                    
                </div>
            </div>
        </nav>
        <nav class="level is-mobile">
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Posts
                    </p>
                    <a href="/archives">
                        <p class="title has-text-weight-normal">
                            91
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Categories
                    </p>
                    <a href="/categories">
                        <p class="title has-text-weight-normal">
                            21
                        </p>
                    </a>
                </div>
            </div>
            <div class="level-item has-text-centered is-marginless">
                <div>
                    <p class="heading">
                        Tags
                    </p>
                    <a href="/tags">
                        <p class="title has-text-weight-normal">
                            16
                        </p>
                    </a>
                </div>
            </div>
        </nav>
        
        <div class="level">
            <a class="level-item button is-link is-rounded" href="https://github.com/Keneyr" target="_blank" rel="noopener">
                Follow</a>
        </div>
        
        
        
    </div>
</div>
    
        

    <div class="card widget" id="toc">
        <div class="card-content">
            <div class="menu">
                <h3 class="menu-label">
                    Catalogue
                </h3>
                <ul class="menu-list"><li>
        <a class="is-flex" href="#Feed-forward-Network-Functions">
        <span class="has-mr-6">1</span>
        <span>Feed-forward Network Functions</span>
        </a></li><li>
        <a class="is-flex" href="#Network-Training">
        <span class="has-mr-6">2</span>
        <span>Network Training</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Parameter-optimization">
        <span class="has-mr-6">2.1</span>
        <span>Parameter optimization</span>
        </a></li><li>
        <a class="is-flex" href="#Local-quadratic-approximation">
        <span class="has-mr-6">2.2</span>
        <span>Local quadratic approximation</span>
        </a></li><li>
        <a class="is-flex" href="#Use-of-gradient-information">
        <span class="has-mr-6">2.3</span>
        <span>Use of gradient information</span>
        </a></li><li>
        <a class="is-flex" href="#Gradient-descent-optimization">
        <span class="has-mr-6">2.4</span>
        <span>Gradient descent optimization</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Error-Backpropagation">
        <span class="has-mr-6">3</span>
        <span>Error Backpropagation</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Evaluation-of-error-function-derivatives">
        <span class="has-mr-6">3.1</span>
        <span>Evaluation of error-function derivatives</span>
        </a></li><li>
        <a class="is-flex" href="#A-simple-example">
        <span class="has-mr-6">3.2</span>
        <span>A simple example</span>
        </a></li><li>
        <a class="is-flex" href="#Efficiency-of-backpropagation">
        <span class="has-mr-6">3.3</span>
        <span>Efficiency of backpropagation</span>
        </a></li><li>
        <a class="is-flex" href="#The-Jacobian-matrix">
        <span class="has-mr-6">3.4</span>
        <span>The Jacobian matrix</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#The-Hessian-Matrix">
        <span class="has-mr-6">4</span>
        <span>The Hessian Matrix</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Diagonal-approximation">
        <span class="has-mr-6">4.1</span>
        <span>Diagonal approximation</span>
        </a></li><li>
        <a class="is-flex" href="#Outer-product-approximation">
        <span class="has-mr-6">4.2</span>
        <span>Outer product approximation</span>
        </a></li><li>
        <a class="is-flex" href="#Inverse-Hessian">
        <span class="has-mr-6">4.3</span>
        <span>Inverse Hessian</span>
        </a></li><li>
        <a class="is-flex" href="#Finite-differences">
        <span class="has-mr-6">4.4</span>
        <span>Finite differences</span>
        </a></li><li>
        <a class="is-flex" href="#Exact-evaluation-of-the-Hessian">
        <span class="has-mr-6">4.5</span>
        <span>Exact evaluation of the Hessian</span>
        </a></li><li>
        <a class="is-flex" href="#Fast-multiplication-by-the-Hessian">
        <span class="has-mr-6">4.6</span>
        <span>Fast multiplication by the Hessian</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Regularization-in-Neural-Networks">
        <span class="has-mr-6">5</span>
        <span>Regularization in Neural Networks</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Consistent-Gaussian-priors">
        <span class="has-mr-6">5.1</span>
        <span>Consistent Gaussian priors</span>
        </a></li><li>
        <a class="is-flex" href="#Early-stopping">
        <span class="has-mr-6">5.2</span>
        <span>Early stopping</span>
        </a></li><li>
        <a class="is-flex" href="#Invariances">
        <span class="has-mr-6">5.3</span>
        <span>Invariances</span>
        </a></li><li>
        <a class="is-flex" href="#Tangent-propagation">
        <span class="has-mr-6">5.4</span>
        <span>Tangent propagation</span>
        </a></li><li>
        <a class="is-flex" href="#Training-with-transformed-data">
        <span class="has-mr-6">5.5</span>
        <span>Training with transformed data</span>
        </a></li><li>
        <a class="is-flex" href="#Convolutional-networks">
        <span class="has-mr-6">5.6</span>
        <span>Convolutional networks</span>
        </a></li><li>
        <a class="is-flex" href="#Soft-weight-sharing">
        <span class="has-mr-6">5.7</span>
        <span>Soft weight sharing</span>
        </a></li></ul></li><li>
        <a class="is-flex" href="#Mixture-Density-Networks">
        <span class="has-mr-6">6</span>
        <span>Mixture Density Networks</span>
        </a></li><li>
        <a class="is-flex" href="#Bayesian-Neural-Networks">
        <span class="has-mr-6">7</span>
        <span>Bayesian Neural Networks</span>
        </a><ul class="menu-list"><li>
        <a class="is-flex" href="#Posterior-parameter-distribution">
        <span class="has-mr-6">7.1</span>
        <span>Posterior parameter distribution</span>
        </a></li><li>
        <a class="is-flex" href="#Hyperparameter-optimization">
        <span class="has-mr-6">7.2</span>
        <span>Hyperparameter optimization</span>
        </a></li><li>
        <a class="is-flex" href="#Bayesian-neural-networks-for-classification">
        <span class="has-mr-6">7.3</span>
        <span>Bayesian neural networks for classification</span>
        </a></li></ul></li></ul>
            </div>
        </div>
    </div>

    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Links
        </h3>
        <ul class="menu-list">
        
            <li>
                <a class="level is-mobile" href="https://www.youtube.com/@Gdconf/videos" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">GDC</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.youtube.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://www.esports.net/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">Esports</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.esports.net</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://www.blender.org/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">blender</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.blender.org</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://dl.acm.org/topic/ccs2012/10010147.10010371.10010352" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">acmLibrary</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">dl.acm.org</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://www.physicsbasedanimation.com/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">physicalAnimation</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.physicsbasedanimation.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://graphics.pixar.com/research/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">Pixar Studio</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">graphics.pixar.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://www.ubisoft.com/en-us/studio/laforge" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">Ubisoft Studio</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.ubisoft.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://theorangeduck.com/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">Daniel Holden</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">theorangeduck.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://artanim.ch/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">artanim</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">artanim.ch</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://www.ode.org/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">ODE</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.ode.org</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://github.com/NVIDIAGameWorks/PhysX/tree/4.1" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">PhysX</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">github.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://github.com/google-deepmind/mujoco" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">mujoco</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">github.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://rodolphe-vaillant.fr/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">rodolphe</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">rodolphe-vaillant.fr</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://arrowinmyknee.com/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">arrowinmyknee</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">arrowinmyknee.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://www.3dgep.com/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">3dgep</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.3dgep.com</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="http://www.library.fudan.edu.cn/wjzx/2019njybgjskt/list1.htm" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">excellentMathBooks</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.library.fudan.edu.cn</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="https://www.deeplearningbook.org/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">DeepLearningBook</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">www.deeplearningbook.org</span>
                    </span>
                </a>
            </li>
        
            <li>
                <a class="level is-mobile" href="http://dpkingma.com/" target="_blank" rel="noopener">
                    <span class="level-left">
                        <span class="level-item">Probability Model</span>
                    </span>
                    <span class="level-right">
                        <span class="level-item tag">dpkingma.com</span>
                    </span>
                </a>
            </li>
        
        </ul>
        </div>
    </div>
</div>

    
        
<div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Categories
            </h3>
            <ul class="menu-list">
            <li>
        <a class="level is-marginless" href="/categories/CS/">
            <span class="level-start">
                <span class="level-item">CS</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/CS/Advanced-CPP/">
            <span class="level-start">
                <span class="level-item">Advanced CPP</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">3</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/CS/Algorithms/">
            <span class="level-start">
                <span class="level-item">Algorithms</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/GameProgramming/">
            <span class="level-start">
                <span class="level-item">GameProgramming</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/GameProgramming/System/">
            <span class="level-start">
                <span class="level-item">System</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">4</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Graphics/">
            <span class="level-start">
                <span class="level-item">Graphics</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">19</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Graphics/Animation/">
            <span class="level-start">
                <span class="level-item">Animation</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">13</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Graphics/Geometry/">
            <span class="level-start">
                <span class="level-item">Geometry</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">3</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Graphics/Rendering/">
            <span class="level-start">
                <span class="level-item">Rendering</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">3</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/MachineLearning/">
            <span class="level-start">
                <span class="level-item">MachineLearning</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">14</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/MachineLearning/PatternRecognition/">
            <span class="level-start">
                <span class="level-item">PatternRecognition</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">14</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Math/">
            <span class="level-start">
                <span class="level-item">Math</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">41</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Math/Calculus/">
            <span class="level-start">
                <span class="level-item">Calculus</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">17</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Linear-Algebra/">
            <span class="level-start">
                <span class="level-item">Linear Algebra</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">9</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Numerical-Analysis/">
            <span class="level-start">
                <span class="level-item">Numerical Analysis</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">14</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Math/Variation/">
            <span class="level-start">
                <span class="level-item">Variation</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li></ul></li><li>
        <a class="level is-marginless" href="/categories/Paper/">
            <span class="level-start">
                <span class="level-item">Paper</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">9</span>
            </span>
        </a><ul><li>
        <a class="level is-marginless" href="/categories/Paper/Human-Pose/">
            <span class="level-start">
                <span class="level-item">Human Pose</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">2</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Paper/Motion-Control/">
            <span class="level-start">
                <span class="level-item">Motion Control</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Paper/Motion-Generation/">
            <span class="level-start">
                <span class="level-item">Motion Generation</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">1</span>
            </span>
        </a></li><li>
        <a class="level is-marginless" href="/categories/Paper/Sketch/">
            <span class="level-start">
                <span class="level-item">Sketch</span>
            </span>
            <span class="level-end">
                <span class="level-item tag">5</span>
            </span>
        </a></li></ul></li>
            </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Tag Cloud
        </h3>
        <a href="/tags/Algorithms/" style="font-size: 10px;">Algorithms</a> <a href="/tags/Animation/" style="font-size: 17.14px;">Animation</a> <a href="/tags/CPP/" style="font-size: 12.86px;">CPP</a> <a href="/tags/Calculus/" style="font-size: 20px;">Calculus</a> <a href="/tags/GameProgramming/" style="font-size: 14.29px;">GameProgramming</a> <a href="/tags/Geometry/" style="font-size: 12.86px;">Geometry</a> <a href="/tags/Human-Pose/" style="font-size: 11.43px;">Human Pose</a> <a href="/tags/Linear-Algebra/" style="font-size: 15.71px;">Linear Algebra</a> <a href="/tags/MachineLearning/" style="font-size: 18.57px;">MachineLearning</a> <a href="/tags/Motion-Control/" style="font-size: 10px;">Motion Control</a> <a href="/tags/Motion-Generation/" style="font-size: 10px;">Motion Generation</a> <a href="/tags/Numerical-Analysis/" style="font-size: 18.57px;">Numerical Analysis</a> <a href="/tags/Rendering/" style="font-size: 12.86px;">Rendering</a> <a href="/tags/Sketch/" style="font-size: 12.86px;">Sketch</a> <a href="/tags/Sketh/" style="font-size: 11.43px;">Sketh</a> <a href="/tags/Variation/" style="font-size: 10px;">Variation</a>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <h3 class="menu-label">
            Recent
        </h3>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2024-02-21T07:49:31.000Z">2024-02-21</time></div>
                    <a href="/Paper/Motion-Control/Paper-Learned-Motion-Matching/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Paper-Learned-Motion-Matching</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Paper/">Paper</a> / <a class="has-link-grey -link" href="/categories/Paper/Motion-Control/">Motion Control</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2024-01-22T04:29:46.000Z">2024-01-22</time></div>
                    <a href="/Paper/Human-Pose/Paper-MAPConNet-SelfSupervised-3D-Pose-Transfer-with-Mesh-and-Point-ConstrastiveLearning/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Paper-MAPConNet-SelfSupervised-3D-Pose-Transfer-with-Mesh-and-Point-ConstrastiveLearning</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Paper/">Paper</a> / <a class="has-link-grey -link" href="/categories/Paper/Human-Pose/">Human Pose</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2023-12-28T04:27:50.000Z">2023-12-28</time></div>
                    <a href="/CS/Algorithms/Algorithms-C1-Foundations/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Algorithms-C1-Foundations</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/CS/">CS</a> / <a class="has-link-grey -link" href="/categories/CS/Algorithms/">Algorithms</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2023-11-15T07:01:37.000Z">2023-11-15</time></div>
                    <a href="/Graphics/Animation/Animation-C14-Skinning/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Animation-C14-Skinning</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Graphics/">Graphics</a> / <a class="has-link-grey -link" href="/categories/Graphics/Animation/">Animation</a>
                    </p>
                </div>
            </div>
        </article>
        
        <article class="media">
            
            <div class="media-content">
                <div class="content">
                    <div><time class="has-text-grey is-size-7 is-uppercase" datetime="2023-11-14T07:38:58.000Z">2023-11-14</time></div>
                    <a href="/Graphics/Animation/Animation-C13-Learning-based-Animation/" class="title has-link-black-ter is-size-6 has-text-weight-normal">Animation-C13-Learning-based-Animation</a>
                    <p class="is-size-7 is-uppercase">
                        <a class="has-link-grey -link" href="/categories/Graphics/">Graphics</a> / <a class="has-link-grey -link" href="/categories/Graphics/Animation/">Animation</a>
                    </p>
                </div>
            </div>
        </article>
        
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
        <h3 class="menu-label">
            Archives
        </h3>
        <ul class="menu-list">
        
        <li>
            <a class="level is-marginless" href="/archives/2024/02/">
                <span class="level-start">
                    <span class="level-item">February 2024</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2024/01/">
                <span class="level-start">
                    <span class="level-item">January 2024</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2023/12/">
                <span class="level-start">
                    <span class="level-item">December 2023</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2023/11/">
                <span class="level-start">
                    <span class="level-item">November 2023</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">18</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2023/10/">
                <span class="level-start">
                    <span class="level-item">October 2023</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">51</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2023/09/">
                <span class="level-start">
                    <span class="level-item">September 2023</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">12</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2023/04/">
                <span class="level-start">
                    <span class="level-item">April 2023</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2020/04/">
                <span class="level-start">
                    <span class="level-item">April 2020</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">1</span>
                </span>
            </a>
        </li>
        
        <li>
            <a class="level is-marginless" href="/archives/2020/03/">
                <span class="level-start">
                    <span class="level-item">March 2020</span>
                </span>
                <span class="level-end">
                    <span class="level-item tag">5</span>
                </span>
            </a>
        </li>
        
        </ul>
        </div>
    </div>
</div>
    
        <div class="card widget">
    <div class="card-content">
        <div class="menu">
            <h3 class="menu-label">
                Tags
            </h3>
            <div class="field is-grouped is-grouped-multiline">
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Algorithms/">
                        <span class="tag">Algorithms</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Animation/">
                        <span class="tag">Animation</span>
                        <span class="tag is-grey">13</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/CPP/">
                        <span class="tag">CPP</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Calculus/">
                        <span class="tag">Calculus</span>
                        <span class="tag is-grey">17</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/GameProgramming/">
                        <span class="tag">GameProgramming</span>
                        <span class="tag is-grey">4</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Geometry/">
                        <span class="tag">Geometry</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Human-Pose/">
                        <span class="tag">Human Pose</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Linear-Algebra/">
                        <span class="tag">Linear Algebra</span>
                        <span class="tag is-grey">9</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/MachineLearning/">
                        <span class="tag">MachineLearning</span>
                        <span class="tag is-grey">14</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Motion-Control/">
                        <span class="tag">Motion Control</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Motion-Generation/">
                        <span class="tag">Motion Generation</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Numerical-Analysis/">
                        <span class="tag">Numerical Analysis</span>
                        <span class="tag is-grey">14</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Rendering/">
                        <span class="tag">Rendering</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Sketch/">
                        <span class="tag">Sketch</span>
                        <span class="tag is-grey">3</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Sketh/">
                        <span class="tag">Sketh</span>
                        <span class="tag is-grey">2</span>
                    </a>
                </div>
                
                <div class="control">
                    <a class="tags has-addons" href="/tags/Variation/">
                        <span class="tag">Variation</span>
                        <span class="tag is-grey">1</span>
                    </a>
                </div>
                
            </div>
        </div>
    </div>
</div>
    
    
        <div class="column-right-shadow is-hidden-widescreen ">
        
        </div>
    
</div>

                
            </div>
        </div>
    </section>
    <footer class="footer">
    <div class="container">
        <div class="level">
            <div class="level-start has-text-centered-mobile">
                <a class="footer-logo is-block has-mb-6" href="/">
                
                    <img src="/images/beauty.jpg" alt="PatterRecognition-C5-Neural-Networks" height="28">
                
                </a>
                <p class="is-size-7">
                &copy; 2024 Keneyr&nbsp;
                Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> & <a
                        href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a>
                
                <br>
                <span id="busuanzi_container_site_uv">
                Visited by <span id="busuanzi_value_site_uv">0</span> users
                </span>

                <span id="busuanzi_container_site_pv" class="theme-info">
                    | PageView: <span id="busuanzi_value_site_pv">span>
                    </span>

                
            <br><span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
                </p>
            </div>
            <div class="level-end">
            
                <div class="field has-addons is-flex-center-mobile has-mt-5-mobile is-flex-wrap is-flex-middle">
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/">
                        
                        <i class="fab fa-creative-commons"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/">
                        
                        <i class="fab fa-creative-commons-by"></i>
                        
                    </a>
                </p>
                
                <p class="control">
                    <a class="button is-white is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus">
                        
                        <i class="fab fa-github"></i>
                        
                    </a>
                </p>
                
                </div>
            
            </div>
        </div>
    </div>
</footer>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("2/28/2020 12:49:00");//此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "The site has been running "+dnum+" days "; 
        document.getElementById("times").innerHTML = hnum + " hours " + mnum + " minutes " + snum + " seconds"; 
    } 
setInterval("createtime()",250);
</script>

    <script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script>
<script>moment.locale("en");</script>


<script>
var IcarusThemeSettings = {
    site: {
        url: 'https://keneyr.com',
        external_link: {"enable":true,"exclude":[]}
    },
    article: {
        highlight: {
            clipboard: true,
            fold: 'unfolded'
        }
    }
};
</script>


<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script>





<script src="/js/animation.js"></script>



<script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script>
<script src="/js/gallery.js" defer></script>



<div id="outdated">
    <h6>Your browser is out-of-date!</h6>
    <p>Update your browser to view this website correctly. <a id="btnUpdateBrowser" href="http://outdatedbrowser.com/">Update
            my browser now </a></p>
    <p class="last"><a href="#" id="btnCloseUpdateBrowser" title="Close">&times;</a></p>
</div>
<script src="https://cdn.jsdelivr.net/npm/outdatedbrowser@1.1.5/outdatedbrowser/outdatedbrowser.min.js" defer></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        outdatedBrowser({
            bgColor: '#f25648',
            color: '#ffffff',
            lowerThan: 'flex'
        });
    });
</script>


<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
    MathJax.Hub.Config({
        'HTML-CSS': {
            matchFontHeight: false
        },
        SVG: {
            matchFontHeight: false
        },
        CommonHTML: {
            matchFontHeight: false
        },
        tex2jax: {
            inlineMath: [
                ['$','$'],
                ['\\(','\\)']
            ]
        }
    });
});
</script>


<a id="back-to-top" title="Back to Top" href="javascript:;">
    <i class="fas fa-chevron-up"></i>
</a>
<script src="/js/back-to-top.js" defer></script>














<script src="/js/main.js" defer></script>

    
    <div class="searchbox ins-search">
    <div class="searchbox-container ins-search-container">
        <div class="searchbox-input-wrapper">
            <input type="text" class="searchbox-input ins-search-input" placeholder="Type something..." />
            <span class="searchbox-close ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="searchbox-result-wrapper ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
    (function (window) {
        var INSIGHT_CONFIG = {
            TRANSLATION: {
                POSTS: 'Posts',
                PAGES: 'Pages',
                CATEGORIES: 'Categories',
                TAGS: 'Tags',
                UNTITLED: '(Untitled)',
            },
            CONTENT_URL: '/content.json',
        };
        window.INSIGHT_CONFIG = INSIGHT_CONFIG;
    })(window);
</script>
<script src="/js/insight.js" defer></script>
<link rel="stylesheet" href="/css/search.css">
<link rel="stylesheet" href="/css/insight.css">
    
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
<script type="text/javascript">
    var windowWidth = $(window).width();
    if (windowWidth > 480) {
      document.write('<script type="text/javascript" src="/js/src/snow.js"><\/script>');
    }
</script>  
</html>