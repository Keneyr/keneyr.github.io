{"pages":[{"title":"","text":"","link":"/CV/index.html"},{"title":"About Me","text":"Currently I’m a Game Client Programmer. I have worked in Garena, participating in developing FreeFire(2021.11-2022.9) and Tencent, participating in developing Arena of Valor(2020.9-2021.1,Intern). I have received my Master’s degree in Computer Science and Technology from Beijing Forestry University. My research interest includes Computer Animation, HCI, Machine Learning. Now I’m persuing a phd position on Computer Animation or HCI, please contact me if you’re interested in me. You can find my CV here: Anna’s Curriculum Vitae. Email / Github / CSDN","link":"/about/index.html"},{"title":"","text":"Computer Animation Groups: Peking University, China ETH Zurich, Swiss The University of British Columbia, Canada University of Victoria, Canada University of Waterloo, Canada University of Montreal, Canada Useful Animation Videos or Materials: Books: 3D Math Primer for Graphics and Game Development 2nd Edition &gt;&gt;Reading Notes Computer Animation Algorithms and Techniques &gt;&gt;Reading Notes Useful MachineLearning Materials: Books: A First Course in Probability &gt;&gt;Reading Notes Linear Algebra and Its Application &gt;&gt;Reading Notes Pattern Recognition and Machine Learning &gt;&gt;Reading Notes Useful CS Materials: Books: C++ Primer &gt;&gt;Reading Notes Introduction to Algorithms &gt;&gt;Reading Notes Useful CG Websites: https://www.3dgep.com/ Useful Latex Websites: https://garsia.math.yorku.ca/MPWP/LATEXmath/latexsym.html","link":"/extra/index.html"},{"title":"","text":"Pattern Recognition and Machine Learning C1:","link":"/reading/index.html"},{"title":"","text":"Study of Video-driven 2D Character Animation Generation Method Abstract Video-driven animation has always been a hot and challenging topic in the field of computer animation. We propose a method of mapping a sequence of human skeletal keypoints in a video onto a two-dimensional character to generate 2D character animation. For a given two-dimensional character picture, we extract the motion of real human in video data, driving the character deformation. We analyze common two-dimensional human body movements, classify the basic posture of the human body, realize the recognition of skeleton posture based on back propagation network, capture human body motion by automatically tracking the position of the human skeleton keypoints coordinates in the video and redirect the motion data to a 2D character. Compared with the traditional method, our work is less affected by video data illumination and background complexity. We calibrate human body motion in videos to a 2D character according to the skeleton topology to avoid motion distortion caused by the difference in skeleton size and ratio. The experimental results show that the proposed algorithm can generate the motion of two-dimensional characters based on the motion of human characters in video data. The animation is natural and smooth, and the algorithm has strong robustness. Conclution and Future work This paper proposes a method of mapping human motion data in a video onto a two-dimensional character to generate character animation. We statistically analyze common two-dimensional human body movements, classify the basic posture of the human body; design and implement the method of human body posture recognition based on the skeleton information on images and videos; propose a geometric calibration method based on the tree structure to correct motion reorientation of the bones, obtaining a good skeleton-driven deformation effect, and generating high quality animation in the same posture. This method can be used to auto-produced animation, which need fewer user interaction. Although our method maps the sequence of human skeletal postures in the video onto a two-dimensional character, resulting in a high quality animation, there is some room for improvement: 1) Scope of application. Our method is only for 2D human characters, cannot be extended to other types of images such as animals and plants; 2) Physical simulation. We didn’t take some phenomena such as hair, clothes into consideration, thus these deformation was not natural enough, as shown in Fig.14(e), the man’s hair deforms in a stiff way. In the future, we will try to improve the issues discussed above, an approach for animals and plants animation will be adapted by this method, physical simulation will be used to get higher quality results. On the other hand, we will also integrate all methods into one interactive animation system.","link":"/projects/index.html"},{"title":"","text":"","link":"/work/index.html"},{"title":"","text":"Video-Driven 2D Character Animation Qinran YIN & Weiqun CAO Chinese Journal of Electronics, vol. 30, no. 6, pp. 1038-1048, 2021 [Project page][Paper] [Code]","link":"/publication/index.html"},{"title":"Life Photo","text":"Cooking garlic fried chicken korean fried chicken spicy boston lobster braised crucian carp spicy crayfish beer braised duck sweet and sour pork(guo bao rou) fried rice with egg golden enoki mushroom beef soup fried fensi durian melaleuca bean sprouts(get-lean diet)","link":"/life/index.html"}],"posts":[{"title":"Algebra-C3-Determinants","text":"Keywords: Determinants, Linear Transformation, Area or Volume, Cramer’s Rule This is the Chapter3 ReadingNotes from book Linear Algebra and its Application. Introduction to Determinants If $A$ is a triangular matrix, then det $A$ is the product of the entries on the main diagonal of $A$. The determinant of an $n \\times n$ matrix $A$ can be computed by a cofactor expansion across any row or down any column. The expansion across the $i-th$ row using the cofactors is $$\\begin{aligned}det A &amp;= a_{i1}C_{i1} + a_{i2}C_{i2} + \\cdots + a_{in}C_{in}\\\\C_{ij} &amp;= (-1)^{i+j}detA_{i,j}\\end{aligned}$$ 在计算机中是怎么计算行列式的？Yet it would be impossible to calculate a $25 \\times 25$ determinant by cofactor expansion.In general, a cofactor expansion requires more than $n!$ multiplications, and $25!$ is approximately $1.5 * 10 ^ {25}$ If a computer performs one trillion multiplications per second, it would have to run for more than $500,000 years$ to compute a $25 \\times 25$ determinant by this method. Fortunately, there are faster methods, see the next chapter. Properties of Deternimants Let $A$ be a square matrix.a. If a multiple of one row of $A$ is added to another row to produce a matrix $B$, then $det B = det A$.b. If two rows of $A$ are interchanged to produce $B$, then $det B = - det A$.c. If one row of $A$ is multiplied by $k$ to produce $B$, then $det B = k * det A$. For example: Compute $det A$, where $A = \\begin{bmatrix}2 &amp; -8 &amp; 6 &amp; 8\\\\ 3 &amp; -9 &amp; 5 &amp; 10\\\\ -3 &amp; 0 &amp; 1 &amp; -2\\\\1 &amp; -4 &amp; 0 &amp; 6\\end{bmatrix}$ Solution: 求行列式如下：$$det A = 2\\left|\\begin{array}{} 1 &amp; -4 &amp; 3 &amp; 4 \\\\ 3 &amp; -9 &amp; 5 &amp; 10\\\\ -3 &amp; 0 &amp; 1 &amp; -2\\\\ 1 &amp; -4 &amp; 0 &amp; 6\\end{array}\\right|=2\\left|\\begin{array}{} 1 &amp; -4 &amp; 3 &amp; 4 \\\\ 0 &amp; 3 &amp; -4 &amp; -2\\\\ 0 &amp; 0 &amp; -6 &amp; 2\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right|=2\\cdot (1)\\cdot(3)\\cdot(-6)\\cdot(1)= -36$$ A square matrix $A$ is invertible if and only if $det A \\neq$ 0 (因为阶梯式下，对角线上如果有0，矩阵肯定不可逆) $$det A = \\begin{cases}(-1)^r \\cdot (product\\space of \\space pivots\\space in \\space U) &amp; when \\space A \\space is\\space invertible\\\\0 &amp; when \\space A \\space is\\space not \\space invertible\\end{cases}\\tag{1}\\\\U = \\begin{bmatrix}\\blacksquare &amp; * &amp; * &amp; * \\\\0 &amp; \\blacksquare &amp; * &amp; * \\\\0 &amp; 0 &amp; \\blacksquare &amp; * \\\\0 &amp; 0 &amp; 0 &amp; \\blacksquare\\end{bmatrix}$$ $r$: interchange operations in the process of matrix $A \\rightarrow U$ . Most computer programs that compute $det A$ for a general matrix $A$ use the method of formula (1) above. It can be shown that evaluation of an $n \\times n$ determinant using row operations requires about $2n^3 / 3$ arithmetic operations. Any modern microcomputer can calculate a $25 \\times 25$ determinant in a fraction of a second, since only about $10,000$ operations are required. Column Operations If $A$ is an $n \\times n$ matrix, then $det A^T = det A$ Determinants and Matrix Products If $A$ and $B$ are $n \\times n$ matrices, then $det AB = detA \\cdot detB$. A Linearity Property of the Determinant Function$$A = \\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{a_{j-1}} &amp; \\vec{x}&amp; \\vec{a_{j+1}} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}\\\\T(\\vec{x}) = det \\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{a_{j-1}} &amp; \\vec{x}&amp; \\vec{a_{j+1}} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}\\\\T(c\\vec{x}) = cT(\\vec{x})\\\\T(\\vec{u}+\\vec{v}) = T(\\vec{u}) + T(\\vec{v})$$ Cramer’s Rule, Volume, Linear TransformationCramer’s Rule（克拉默法则）Let A be an invertible $n \\times n$ matrix. For any $\\vec{b}$ in $R^n$, the unique solution $\\vec{x}$ of $A\\vec{x} = \\vec{b}$ has entries given by $$\\vec{x_i} = \\frac{det A_i(\\vec{b})}{det A}, i = 1, 2, \\cdots, n$$$$det A_i(\\vec{b}) =\\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{b} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}第i列的向量改成了\\vec{b}向量$$ Application to EngineeringLaplace transforms: This approach converts an appropriate system of linear differential equations into a system of linear algebraic equations whose coefficients involve a parameter.（只是提一嘴拉普拉斯变换） For example: Consider the following system in which $s$ is an unspecified parameter. Determine the values of $s$ for which the system has a unique solution, and use Cramer’s rule to describe the solution. $$\\begin{cases}3s x_1 - 2x_1 = 4\\\\-6x_1 + sx_2 = 1\\end{cases}$$ Solution：$$A = \\begin{bmatrix}3s &amp; -2\\\\-6 &amp; s\\end{bmatrix},A_1(\\vec{b}) = \\begin{bmatrix}4 &amp; -2\\\\1 &amp; s\\end{bmatrix},A_2(\\vec{b}) = \\begin{bmatrix}3s &amp; 4\\\\-6 &amp; 1\\end{bmatrix}$$Since,$$det A = 3s^2-12 = 3(s+2)(s-2)$$so,$$unique-solution: s \\neq \\pm2\\\\x_1 = \\frac{det A_1(\\vec{b})}{det A} = \\frac{4s+2}{3(s+2)(s-2)},x_1 = \\frac{det A_2(\\vec{b})}{det A} = \\frac{s+8}{(s+2)(s-2)}$$ A Formula for $A^{–1}$let $A$ be an invertible $n \\times n$ matrix. Then$$A^{-1} = \\frac{1}{det A} adj A$$The adjugate matrix is the transpose of the matrix of cofactors For example: Find the inverse matrix of $A = \\begin{bmatrix}2 &amp; 1 &amp; 3\\\\1 &amp; -1 &amp; 1\\\\1 &amp; 4 &amp; 2\\end{bmatrix}$ Solution: $$C_{11} = + \\left|\\begin{array}{} -1 &amp; 1 \\\\ 4 &amp; -2\\end{array}\\right| = -2 , C_{12} = 3, C_{13} = 5,C_{21} = 14, C_{22} = -7,C_{23} = -7$$$$adj A =\\begin{bmatrix}-2 &amp; 14 &amp; 4\\\\3 &amp; -7 &amp; 1\\\\5 &amp; -7 &amp; 3\\end{bmatrix}接下来套公式即可$$ 在计算机中，For a larger $n \\times n$ matrix (real or complex), Cramer’s rule is hopelessly inefficient.Computing just one determinant takes about as much work as solving $Ax = b$ by row reduction. Determinants as Area or Volume If $A$ is a $2 \\times 2$ matrix, the area of the parallelogram（平行四边形） determined by the columns of $A$ is $|detA|$.If $A$ is a $3 \\times 3$ matrix, the volume of the parallelepiped（平行六面体） determined by the columns of $A$ is $|detA|$. $$\\left|{}det\\begin{bmatrix}a &amp; 0 \\\\ 0 &amp; d\\end{bmatrix}\\right|=\\left|ad\\right| = {area \\space of \\space rectangle}$$ For example: Calculate the area of the parallelogram determined by the points $(-2,-2),(0,3),(4,-1)and (6,4)$ Solution: 求平行四边形面积如下：$$1、平移(如下图),新点：(0,0), (2,5),(6,1),(8,6)$$$$2、建立矩阵列向量,A =\\begin{bmatrix}2 &amp; 5\\\\5 &amp; 1\\end{bmatrix},\\left|detA\\right| = \\left|-28\\right|，面积是28$$ Linear TransformationsLet $T:R^2\\rightarrow R^2$ be the linear transformation determined by a $2 \\times 2$ matrix $A$. If $S$ is a parallelogram(or other finite area) in $R^2$, then $$(area-of-T(S)) = \\left|detA\\right| \\cdot (area-of-S)$$ If $T$ is determined by a $3 \\times 3$ matrix $A$, and if $S$ is a parallelepiped(or other finite volume) in $R^3$, then $$(volume-of-T(S)) = \\left|detA\\right| \\cdot (volume-of-S)$$ for example: Let $a$ and $b$ be positive numbers. Find the area of the region $E$ bounded by the ellipse whose equation is $$\\frac{x_1^2}{a^2} + \\frac{x_2^2}{b^2} = 1$$We claim that $E$ is the image of the unit disk $D$ under the linear transformation $T$ determined by the matrix $A = \\begin{bmatrix}a &amp; 0\\\\ 0 &amp; b\\end{bmatrix}$, because if $\\vec{u} =\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}$ ,$\\vec{x} = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$,and $\\vec{x} = A\\vec{u}$, then $$u_1 = \\frac{x_1}{a}, u_2 = \\frac{x_2}{b}$$ it follows that $\\vec{u}$ is in the unit disk, with $u_1^2 + u_2^2 \\leqslant 1$, if and only if $\\vec{x}$ is in E, with $(x_1/a)^2 + (x_2/b)^2 \\leqslant 1$, so the area of ellipse is :$$\\begin{aligned}area \\space of \\space ellipse &amp;= area \\space of \\space T(D)\\\\&amp;= \\left|detA\\right| \\cdot area \\space of \\space D\\\\&amp;= ab \\cdot \\pi (1)^2 \\\\&amp;= \\pi ab\\end{aligned}$$","link":"/Math/Linear-Algebra/Algebra-C3-Determinants/"},{"title":"3DMath-Transformation","text":"Keywords: Coordinate Space, Transformation, Matrix, Quaternions, Polar Coordinate System Attetion: left-hand rule, row vectors, right multiply. Deduce the Projection and Rotation MatrixPerspective Projection Matrix from the picture above, the camera is at origin $(0,0,0)$, the projection plane is $z = d$ (also, the Focus distance is $d$), by similar triangles, we can see that $$\\frac{p_y’}{d} = \\frac{p_y}{z}\\Rightarrow p_y’ = \\frac{dp_y}{z}, also \\space p_x’ = \\frac{dp_x}{z}$$ so $$p = \\begin{bmatrix} x &amp; y &amp; z \\end{bmatrix}\\longmapsto\\begin{aligned}p’ &amp;= \\begin{bmatrix} \\frac{dx}{z} &amp; \\frac{dy}{z} &amp; \\frac{dz}{z} \\end{bmatrix}\\\\&amp;= \\frac{\\begin{bmatrix} x &amp; y &amp; z \\end{bmatrix}}{z/d}\\end{aligned}$$ So we need a $4 \\times 4$ matrix that multiplies a homogeneous vector $[x, y, z, 1]$ to produce $[x, y, z, z/d]$. The matrix that does this is $$\\begin{bmatrix}x &amp; y &amp; z &amp; 1\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 1/d\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}=\\begin{bmatrix}x &amp; y &amp; z &amp; z/d\\end{bmatrix}$$ Multiplication by this matrix doesn’t actually perform the perspective transform, it just computes the proper denominator into $w(w = z/d)$. Remember that the perspective division actually occurs when we convert from $4D$ to $3D$ by dividing by $w$. Orthogonal Projection MatrixRotation Matrix3D Rotation about Cardinal Axes matrix to rotate about the $x-axis$:$$R_x(\\theta) =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; \\cos\\theta &amp; \\sin \\theta\\\\0 &amp; -\\sin \\theta &amp; \\cos \\theta\\end{bmatrix}$$ matrix to rotate about the $y-axis$:$$R_y(\\theta) =\\begin{bmatrix}\\cos\\theta &amp; 0 &amp; -\\sin\\theta \\\\0 &amp; 1 &amp; 0\\\\\\sin\\theta &amp; 0 &amp; \\cos\\theta\\end{bmatrix}$$ matrix to rotate about the $z-axis$:$$R_z(\\theta) =\\begin{bmatrix}\\cos\\theta &amp; \\sin\\theta &amp; 0\\\\-\\sin\\theta &amp; \\cos\\theta &amp; 0\\\\0 &amp; 1 &amp; 0\\end{bmatrix}$$ 3D Rotation about Arbitrary AxisLet’s derive a matrix to rotate about $\\widehat{\\vec{n}}$ by the angle $\\theta$. that means: $$\\vec{v}’ = \\vec{v}R(\\widehat{\\vec{n}},\\theta)$$ The basic idea is to solve the problem in the plane perpendicular to $\\widehat{\\vec{n}}$. we separate $\\vec{v}$ into two vectors, $\\vec{v_{\\parallel}}$ and $\\vec{v_{\\perp}}$, which are parallel and perpendicular to $\\vec{v}$, respectively, such that $\\vec{v} = \\vec{v_{\\parallel}} + \\vec{v_{\\perp}}$. By rotating each of these components individually, we can rotate the vector as a whole. In other words, $\\vec{v’} = \\vec{v_{\\parallel}’} + \\vec{v_{\\perp}’}$. Since $\\vec{v_{\\parallel}}$ is parallel to $\\widehat{\\vec{n}}$, it will not be affected by the rotation about $\\widehat{\\vec{n}}$. In other words, $\\vec{v_{\\parallel}’} = \\vec{v_{\\parallel}}$. So all we need to do is compute $\\vec{v_{\\perp}’}$, and then we have $\\vec{v’} = \\vec{v_{\\parallel}’} + \\vec{v_{\\perp}’}$. To compute $\\vec{v_{\\perp}’}$ , we construct the vectors $\\vec{v_{\\parallel}}$ and $\\vec{v_{\\perp}}$ and an intermediate vector $\\vec{w}$, The vector $\\vec{w}$ is mutually perpendicular to $\\vec{v_{\\parallel}}$ and $\\vec{v_{\\perp}}$ and has the same length as $\\vec{v_{\\perp}}$: $$\\begin{aligned}\\vec{v_{\\parallel}} &amp;= (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}\\\\\\vec{v_{\\perp}} &amp;= \\vec{v} - \\vec{v_{\\parallel}} \\\\&amp;= \\vec{v} - (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}} \\\\\\vec{w} &amp;= \\widehat{\\vec{n}} \\times \\vec{v_{\\perp}} \\\\&amp;= \\widehat{\\vec{n}} \\times (\\vec{v} - \\vec{v_{\\parallel}})\\\\&amp;= \\widehat{\\vec{n}} \\times \\vec{v} - \\widehat{\\vec{n}} \\times \\vec{v_{\\parallel}}\\\\&amp;= \\widehat{\\vec{n}} \\times \\vec{v}\\end{aligned}$$ How do these vectors help us compute $\\vec{v_{\\perp}’}$? Notice that $\\vec{w}$ and $\\vec{v_{\\perp}}$ form a 2D coordinate space, with $\\vec{v_{\\perp}}$ as the $“x-axis”$ and $\\vec{w}$ as the $“y-axis”$. (Note that the two vectors don’t necessarily have unit length.) $\\vec{v_{\\perp}’}$ is the result of rotating $\\vec{v’}$ in this plane by the angle $θ$. Remember the endpoints of a unit ray rotated by an angle $\\theta$ are $\\cos\\theta$ and $\\sin\\theta$? $$\\begin{aligned}\\vec{p’} &amp;= \\cos \\theta \\vec{e_1} + \\sin \\theta \\vec{e_1} \\\\&amp;= \\cos\\theta \\begin{bmatrix} 1 &amp; 0\\end{bmatrix} + \\sin\\theta \\begin{bmatrix} 0 &amp; 1\\end{bmatrix}\\\\&amp;= \\begin{bmatrix} \\cos\\theta &amp; 0\\end{bmatrix} + \\begin{bmatrix} 0 &amp; \\sin\\theta\\end{bmatrix} \\\\&amp;= \\begin{bmatrix} \\cos\\theta &amp; \\sin\\theta \\end{bmatrix}\\end{aligned}$$The only difference here is that our ray is not a unit ray, and we are using $\\vec{v_{\\perp}}$ and $\\vec{w}$ as our basis vectors. Thus, $\\vec{v_{\\perp}’}$ can be computed as $$\\begin{aligned}\\vec{v_{\\perp}’} &amp;= \\cos\\theta \\vec{v_{\\perp}} + \\sin\\theta \\vec{w}\\\\&amp;= \\cos\\theta (\\vec{v} - (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}) + \\sin\\theta (\\widehat{\\vec{n}} \\times \\vec{v})\\end{aligned}$$ Thus, $$\\begin{aligned}\\vec{v’} &amp;= \\vec{v_{\\parallel}’} + \\vec{v_{\\perp}’} \\\\&amp;= \\cos\\theta (\\vec{v} - (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}) + \\sin\\theta (\\widehat{\\vec{n}} \\times \\vec{v}) + (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}\\end{aligned}\\tag{5.1}$$ the remaining arithmetic is essentially a notational change that expresses Equation (5.1) as a matrix multiplication. Now that we have expressed $\\vec{v’}$ in terms of $\\vec{v}, \\widehat{\\vec{n}}$ and $θ$. we can compute what the basis vectors are after transformation and construct our matrix.$$\\vec{e_1} = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\end{bmatrix}\\longmapsto\\vec{e_1’} =\\begin{bmatrix}n_x^2(1-\\cos\\theta) + \\cos\\theta\\\\n_x n_y(1-\\cos\\theta) + n_z \\sin\\theta\\\\n_x n_z(1-\\cos\\theta) - n_y\\sin\\theta\\end{bmatrix}^T$$$$\\vec{e_2} = \\begin{bmatrix} 0 &amp; 1 &amp; 0\\end{bmatrix}\\longmapsto\\vec{e_2’} =\\begin{bmatrix}n_x n_y(1-\\cos\\theta) - n_z \\sin\\theta\\\\n_y^2(1-\\cos\\theta) + \\cos\\theta\\\\n_y n_z(1-\\cos\\theta) + n_x\\sin\\theta\\end{bmatrix}^T$$$$\\vec{e_3} = \\begin{bmatrix} 0 &amp; 0 &amp; 1\\end{bmatrix}\\longmapsto\\vec{e_3’} =\\begin{bmatrix}n_x n_z(1-\\cos\\theta) + n_y\\sin\\theta\\\\n_y n_z(1-\\cos\\theta) - n_x \\sin\\theta\\\\n_z^2(1-\\cos\\theta) +\\cos\\theta\\end{bmatrix}^T$$Thus $$\\begin{aligned}R(\\widehat{\\vec{n}},\\theta) &amp;=\\begin{bmatrix}R(\\vec{e_1}) &amp; R(\\vec{e_2}) &amp; R(\\vec{e_3’})\\end{bmatrix}^T\\\\&amp;=\\begin{bmatrix}\\vec{e_1’} &amp; \\vec{e_2’} &amp; \\vec{e_3’}\\end{bmatrix}^T\\\\&amp;=\\begin{bmatrix}n_x^2(1-\\cos\\theta) + \\cos\\theta &amp; n_x n_y(1-\\cos\\theta) + n_z \\sin\\theta &amp; n_x n_z(1-\\cos\\theta) - n_y\\sin\\theta\\\\n_x n_y(1-\\cos\\theta) - n_z \\sin\\theta &amp; n_y^2(1-\\cos\\theta) + \\cos\\theta &amp; n_y n_z(1-\\cos\\theta) + n_x\\sin\\theta\\\\n_x n_z(1-\\cos\\theta) + n_y\\sin\\theta &amp; n_y n_z(1-\\cos\\theta) - n_x \\sin\\theta &amp; n_z^2(1-\\cos\\theta) +\\cos\\theta\\end{bmatrix}\\end{aligned}$$ How to understand Basis in Linear Algebra &gt;&gt; How to understand Basis and Coordinate Systems in Linear Algebra &gt;&gt; How to understand Basis and Coordinate Systems and Coordinate Mapping in Linear Algebra &gt;&gt; QuaternionDeduce Quaternion we can say that if we multiply a complex number by $i$, we can rotate the complex number through the complex plane at 90° increments. $$\\begin{aligned}p &amp;= 2 + i\\\\q &amp;= pi\\\\&amp;= -1+2i\\\\r &amp;= qi\\\\&amp;= -2-i\\\\s &amp;= ri\\\\&amp;= 1-2i\\\\t &amp;= si\\\\&amp;= 2 + i\\end{aligned}$$ Rotate a point through the 2D complex plane as$$q = \\cos\\theta + i \\sin\\theta$$ Then,$$\\begin{aligned}p &amp;= a + bi\\\\pq &amp;= (a+bi)(\\cos\\theta + i \\sin\\theta)\\\\ &amp;= a\\cos\\theta - b\\sin\\theta + (a\\sin\\theta + b\\cos\\theta)i\\end{aligned}$$Written in matrix form:$$\\begin{bmatrix}a’ &amp; -b’\\\\b’ &amp; a’\\end{bmatrix}=\\begin{bmatrix}\\cos\\theta &amp; -\\sin\\theta\\\\\\sin\\theta &amp; \\cos\\theta\\end{bmatrix}\\begin{bmatrix}a &amp; -b\\\\b &amp; a\\end{bmatrix}$$Which is the method to rotate an arbitrary point in the complex plane counter-clockwise about the origin. The general form to express quaternions is$$\\begin{aligned}\\pmb{q} &amp;= s + xi + yj + zk, s,x,y,z\\in R\\\\&amp;= [s,\\vec{v}]\\end{aligned}$$ Let express a quaternion that can be used to rotate a point in 3D-space as such:$$\\pmb{q} = [\\cos\\theta, \\sin\\theta\\vec{v}]$$ let $\\pmb{p}$ as a Pure quaternion in the form, $\\pmb{q}$ is a unit-norm quaternion:$$\\pmb{p} = [0,\\vec{p}], \\pmb{q} = [s, \\lambda \\hat{\\vec{v}}]$$Then,$$\\begin{aligned}\\pmb{p’} &amp;= \\pmb{qp}\\\\&amp;= [s, \\lambda \\hat{\\vec{v}}][0,\\vec{p}]\\\\&amp;= [-\\lambda \\hat{\\vec{v}} \\cdot \\vec{p}, s\\vec{p}+ \\lambda \\hat{\\vec{v}} \\times \\vec{p}]\\end{aligned}$$ First, think special case $\\vec{p} \\perp \\hat{\\vec{v}}$, so the result becomes Pure quaternion:$$\\pmb{p’} = [0, s\\vec{p}+ \\lambda \\hat{\\vec{v}} \\times \\vec{p}]$$ In this case, to rotate $\\vec{p}$ about $\\hat{\\vec{v}}$ we just substitute $s = \\cos\\theta$ and $\\lambda = \\sin\\theta$.$$\\pmb{p’} = [0, \\cos\\theta \\vec{p}+ \\sin \\theta \\hat{\\vec{v}} \\times \\vec{p}]$$ For example: let’s rotate a vector $\\vec{p}$ 45° about the $z-axis$. our quaternion $\\pmb{q}$ is: $$\\begin{aligned}\\pmb{q} &amp;= [\\cos\\theta, \\sin\\theta \\vec{k}]\\\\&amp;= [\\frac{\\sqrt 2}{2}, \\frac{\\sqrt 2}{2} \\vec{k}]\\end{aligned}$$ let $\\vec{p} \\perp \\vec{k}$, so $$\\pmb{p} = [0, 2\\vec{i}]$$Thus, $$\\begin{aligned}\\pmb{p’} &amp;= \\pmb{qp}\\\\&amp;= [\\frac{\\sqrt 2}{2}, \\frac{\\sqrt 2}{2} \\vec{k}][0, 2\\vec{i}]\\\\&amp;= [0, \\frac{\\sqrt 2}{2} \\vec{i} + \\frac{\\sqrt 2}{2} \\vec{j}]\\end{aligned}$$$$|\\pmb{p’}| = 2$$ For example let’s consider a quaternion that is not orthogonal to $\\vec{p}$, but still 45°. $$\\begin{aligned}&amp;\\hat{\\vec{v}} = \\frac{\\sqrt 2}{2}\\vec{i} + \\frac{\\sqrt 2}{2}\\vec{k}\\\\&amp;\\vec{p} = 2\\vec{i}\\\\&amp;\\pmb{q} = [\\cos\\theta, \\sin\\theta \\hat{\\vec{v}}]\\\\&amp;\\pmb{p} = [0,\\vec{p}]\\end{aligned}$$ Thus,$$\\begin{aligned}\\pmb{p’} &amp;= \\pmb{qp}\\\\&amp;= [\\cos\\theta, \\sin\\theta \\hat{\\vec{v}}][0,\\vec{p}]\\\\&amp;= [-1, \\sqrt 2 \\vec{i} + \\vec{j}]\\end{aligned}$$But, the norm has changed, that’s not we want.$$|\\pmb{p’}| = \\sqrt 3$$ Hamilton recognized (but didn’t publish) that if we post-multiply the result of $\\pmb{qp}$ by the inverse of $\\pmb{q}$ then the result is a pure quaternion and the norm of the vector component is maintained.so,$$\\begin{aligned}\\pmb{q} &amp;= [\\cos\\theta, \\sin\\theta \\hat{\\vec{v}}]\\\\&amp;= [\\cos\\theta, \\sin\\theta (\\frac{\\sqrt 2}{2}\\vec{i} + \\frac{\\sqrt 2}{2}\\vec{k})]\\\\\\pmb{q}^{-1} &amp;= [\\cos\\theta, -\\sin\\theta \\hat{\\vec{v}}]\\\\&amp;= [\\cos\\theta, -\\sin\\theta (\\frac{\\sqrt 2}{2}\\vec{i} + \\frac{\\sqrt 2}{2}\\vec{k})]\\\\\\end{aligned}$$ Thus,$$\\begin{aligned}\\pmb{qp} &amp;= [-1, \\sqrt 2 \\vec{i} + \\vec{j}]\\\\\\pmb{qpq^{-1}} &amp;= [0, \\vec{i} + \\sqrt 2 \\vec{j} + \\vec{k}]\\end{aligned}$$ Which is a pure quaternion and the norm of the result is:$$|\\pmb{p’}| = 2$$ but the vector has been rotated 90° rather than 45° which is twice as much as desired! So in order to correctly rotate a vector $\\vec{p}$ by an angle $\\theta$ about an arbitrary axis $\\widehat{\\vec{v}}$ , we must consider the half-angle and construct the following quaternion: $$\\pmb{q} = [\\cos \\frac{1}{2} \\theta, \\sin \\frac{1}{2} \\theta \\widehat{\\vec{v}}]$$ Quaternion InterpolationAppendix$$OpenGL-perspective-matrix:\\begin{bmatrix} \\frac{1}{aspect _ ratio \\cdot tan\\frac{\\theta}{2}} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{zFar+zNear}{zNear-zFar}&amp; \\frac{2 \\cdot zNear \\cdot zFar}{zNear-zFar} \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\\\end{bmatrix} $$ $$OpenGL-perspective-matrix:\\begin{bmatrix} \\frac{2|n|}{r-l} &amp; 0 &amp; \\frac{r+l}{r-l} &amp; 0 \\\\ 0 &amp; \\frac{2|n|}{t-b} &amp; \\frac{t+b}{t-b} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{|n|+|f|}{|n|-|f|} &amp; \\frac{2|f||n|}{|n|-|f|} \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\\\end{bmatrix} $$ $$OpenGL-orthographic-matrix:\\begin{bmatrix} \\frac{1}{aspect _ ratio*tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{2}{zNear-zFar} &amp; \\frac{zNear+zFar}{zNear-zFar} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\\\end{bmatrix} $$ so,for the OpenGL conventions, we can tell whether a projection matrix is perspective or orthographic based on the bottom row.$$OpenGL-Perspective\\begin{bmatrix} 0 &amp; 0 &amp; -1 &amp; 0 \\end{bmatrix}$$$$penGL-Orthographic\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}$$ $$Dx-orthographic-matrix:\\begin{bmatrix} \\frac{2}{w} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{2}{h} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{1}{zF-zN} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{zn}{zN-zF} &amp; 1 \\\\\\end{bmatrix}$$ $$Dx-perspective-matrix:\\begin{bmatrix} \\frac{1}{aspect _ ratio*tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{zF}{zF-zN} &amp; 1 \\\\ 0 &amp; 0 &amp; \\frac{zN \\cdot zF}{zN-zF} &amp; 1 \\\\\\end{bmatrix}$$ OpenGL use column vectors, Projection_Matrix * View_Matrix * Model_Matrix * VectorDirectx use row vectors, Vector * Model_Matrx * View_Matrx * Projection_Matrix https://www.qiujiawei.com/understanding-quaternions/","link":"/Math/Geometry/3DMath-Transformation/"},{"title":"Algebra-C2-Matrix-Algebra","text":"Keywords: Inverse, LU Factorization, Homogeneous Coordinates, Perspective Projections, Rank This is the Chapter2 ReadingNotes from book Linear Algebra and its Application. Matrix Operation$$\\begin{aligned}&amp;AB \\neq BA\\\\&amp;AB = AC,it’s \\space not \\space true \\space B = C\\\\&amp;AB = 0, it’s \\space not \\space true \\space A = 0 \\space or \\space B = 0\\\\&amp;(A^T)^T = A\\\\&amp;(A+B)^T = A^T+B^T\\\\&amp;(AB)^T = B^TA^T\\\\&amp;A(BC) = (AB)C\\end{aligned}$$ The fastest way to obtain $AB$ on a computer depends on the way in which the computer stores matrices in its memory. The standard high-performance algorithms, such as in LAPACK, calculate $AB$ by columns, as in our definition of the product. (A version of LAPACK written in C++ calculates $AB$ by rows.) The definition of $AB$ lends itself well to parallel processing on a computer. The columns of $B$ are assigned individually or in groups to different processors, which independently and hence simultaneously compute the corresponding columns of $AB$. The Inverse of MatrixAn $n \\times n$ matrix $A$ is said to be invertible if there is an $n \\times n$ matrix $C$ such that $$CA = I \\space and \\space AC = I$$ A matrix that is not invertible is sometimes called a singular matrix, and an invertible matrix is called a nonsingular matrix.一个不可逆的矩阵有时被称为奇异矩阵，一个可逆的矩阵被称为非奇异矩阵 $$\\begin{aligned}&amp; A = \\begin{bmatrix}a &amp; b \\\\ c &amp; d\\end{bmatrix}. \\spaceif \\space ad - bc \\neq 0, A \\space is\\space invertible\\\\&amp; A^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix}d &amp; -b \\\\ -c &amp; a\\end{bmatrix}\\\\a&amp; d - bc = 0 , A\\space is \\space not \\space invertible\\\\&amp; det A = ad - bc(行列式)\\end{aligned}$$ If $A$ is an invertible $n \\times n$ matrix, then for each $\\vec{b}$ in $R^n$, the equation $A\\vec{x} = \\vec{b}$ has the unique solution $\\vec{x} = A^{-1}\\vec{b}$. $$(A^{-1})^{-1} = A\\\\(AB)^{-1} = B^{-1}A^{-1}\\\\(A^T)^{-1} = (A^{-1})^T\\\\$$ An $n \\times n$ matrix $A$ is invertible if and only if $A$ is row equivalent to $I_n$, and in this case, any sequence of elementary row operations that reduces A to $I_n$ also transforms $I_n$ into $A^{-1}$. For example: transform $E_1$ to $I$, get $I$ to $E_1^{-1}$:$$E_1 = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -4 &amp; 0 &amp; 1\\end{bmatrix},E_1^{-1} = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 4 &amp; 0 &amp; 1\\end{bmatrix}$$ An algorithm for Finding $A^{-1}$Row reduce the augmented matrix $\\begin{bmatrix}A &amp; I\\end{bmatrix}$. If $A$ is row equivalent to $I$, then $\\begin{bmatrix}A &amp; I\\end{bmatrix}$ is row equivalent to $\\begin{bmatrix}I &amp; A^{-1}\\end{bmatrix}$. Other wise, A does not have an inverse. For example: Find the Inverse of the matrix A = $\\begin{bmatrix}0 &amp; 1 &amp; 2\\\\1 &amp; 0 &amp; 3\\\\4 &amp; -3 &amp; 8\\end{bmatrix}$, if it exists. $$\\begin{bmatrix}A &amp; I\\end{bmatrix} =\\begin{bmatrix}0 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; 3 &amp; 0 &amp; 1 &amp; 0\\\\4 &amp; -3 &amp; 8 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; \\frac{-9}{2} &amp; 7 &amp; \\frac{-3}{2}\\\\0 &amp; 1 &amp; 0 &amp; -2 &amp; 4 &amp; -1\\\\0 &amp; 0 &amp; 1 &amp; \\frac{3}{2} &amp; -2 &amp; \\frac{1}{2}\\end{bmatrix}\\longrightarrowA^{-1} = \\begin{bmatrix}\\frac{-9}{2} &amp; 7 &amp; \\frac{-3}{2}\\\\ -2 &amp; 4 &amp; -1\\\\ \\frac{3}{2} &amp; -2 &amp; \\frac{1}{2}\\end{bmatrix}$$ Characterizations of Inversible Matrix注意，这些性质的应用必须是n $\\times$ n的矩阵 Let $A$ be a square $n \\times n$ matrix. Then the following statements are equivalent. That is, for a given $A$, the statements are either all true or all false.a. $A$ is an invertible matrix.b. $A$ is row equivalent to the $n \\times n$ identity matrix.c. $A$ has $n$ pivot positions.d. The equation $Ax = 0$ has only the trivial solution.e. The columns of $A$ form a linearly independent set.f. The linear transformation $x \\rightarrow Ax$ is one-to-one.g. The equation $Ax = b$ has at least one solution for each $b$ in $R^n$.h. The columns of $A$ span $R^n$.i. The linear transformation $x \\rightarrow Ax$ maps $R^n$ onto $R^n$.j. There is an $n \\times n$ matrix $C$ such that $CA = I$.k. There is an $n \\times n$ matrix $D$ such that $AD = I$.l. $A^T$ is an invertible matrix. Invertible Linear TransformationsLet $T :R^n\\rightarrow R^n$ be a linear transformation and let $A$ be the standard matrix for $T$. Then $T$ is invertible if and only if $A$ is an invertible matrix. In that case, the linear transformation $S$ given by $S(x) = A^{-1}x $ is the unique function satisfying equations (1) and (2) $$S(T(x)) = x, x \\in R^n\\tag{1}$$$$T(S(x)) = x, x \\in R^n\\tag{2}$$ 由于计算机的精度问题，you might occasionally encounter a “nearly singular” or illconditioned matrix—an invertible matrix, that can become singular if some of its entries are changed ever so slightly.In this case, row reduction may produce fewer than $n$ pivot positions, as a result of roundoff error.Also, roundoff error can sometimes make a singular matrix appear to be invertible. Partitioned Matrices$$A =\\left[ \\begin{array}{ccc|cc|c} 3 &amp; 0 &amp; -1 &amp; 5 &amp; 9 &amp; -2 \\\\ -5 &amp; 2 &amp; 4 &amp; 0 &amp; -3 &amp; 1 \\\\ \\hline -8 &amp; -6 &amp; 3 &amp; 1 &amp; 7 &amp; -4 \\end{array}\\right]$$can also be written as the $2 \\times 3$ partitioned (or block) matrix $$A = \\begin{bmatrix}A_{11} &amp; A_{12} &amp; A_{13} \\\\A_{21} &amp; A_{22} &amp; A_{23}\\end{bmatrix}$$ Multiplication of Partitioned Matrices$$A : m \\times n, B : n \\times p\\\\AB = \\begin{bmatrix}col_1(A) &amp; col_2(A) &amp; \\cdots &amp; col_n(A)\\end{bmatrix}\\begin{bmatrix}row_1(B) \\\\ row_1(B) \\\\ \\cdots \\\\ row_n(B)\\end{bmatrix}\\tag{1}$$ Inverse of Partitioned Matrices$$assume\\space A_{11}: p\\times p, A_{22}: q\\times q, A \\space invertible.A = \\begin{bmatrix}A_{11} &amp; A_{12}\\\\0 &amp; A_{22}\\end{bmatrix}\\tag{bolck upper triangular}$$ $$求逆过程如下：\\\\A_{11}B_{11} + A_{12}B_{21} = I_p\\tag{1}$$$$A_{11}B_{12} + A_{12}B_{22} = 0\\tag{2}$$$$A_{22}B_{21} = 0\\tag{3}$$$$A_{22}B_{22} = I_{q}\\tag{4}$$$$(1)(2)(3)(4)\\rightarrowA^{-1} = \\begin{bmatrix}A_{11} &amp; A_{12}\\\\0 &amp; A_{22}\\end{bmatrix} ^ {-1}=\\begin{bmatrix}A_{11}^{-1} &amp;-A_{11}^{-1}A_{12}A_{22}^{-1}\\\\0 &amp; A_{22}^{-1}\\end{bmatrix}$$ 分块矩阵提高计算机计算效率 When matrices are too large to fit in a computer’s high-speed memory, partitioning permits the computer to work with only two or three submatrices at a time. Matrix Factorizations 矩阵分解The LU FactorizationAsume that $A$ is an $m \\times n$ matrix that can be row reduced to echelon form, without row interchanges. Then $A$ can be written in the form $A = LU$ , where $L$ is an $m \\times m$ lower triangular matrix with $1’s$ on the diagonal and $U$ is an $m \\times n$ echelon form of $A$. The matrix $L$ is invertible and is called a unit lower triangular matrix. 单位下三角矩阵$$A = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ * &amp; 1 &amp; 0 &amp; 0\\\\ * &amp; * &amp; 1 &amp; 0\\\\ * &amp; * &amp; * &amp; 1\\end{bmatrix}\\begin{bmatrix}\\blacksquare &amp; * &amp; * &amp; * &amp; *\\\\0 &amp; \\blacksquare &amp; * &amp; * &amp; *\\\\0 &amp; 0 &amp; 0 &amp; \\blacksquare &amp; *\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ 思考，这样分解有什么好处？ $$A\\vec{x} = \\vec{b}\\longrightarrowL(U\\vec{x}) = \\vec{b},\\\\let\\space \\vec{y} = U\\vec{x}\\Rightarrow\\\\\\begin{cases}L\\vec{y} = \\vec{b}\\\\U\\vec{x} = \\vec{y}\\tag{2}\\end{cases}$$ 分解后好求解，因为LU都是三角矩阵 For example： $$A =\\begin{bmatrix}3 &amp; -7 &amp; -2 &amp; 2\\\\-3 &amp; 5 &amp; 1 &amp; 0\\\\6 &amp; -4 &amp; 0 &amp; -5\\\\-9 &amp; 5 &amp; -5 &amp; 12\\end{bmatrix}=\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\-1 &amp; 1 &amp; 0 &amp; 0\\\\2 &amp; -5 &amp; 1 &amp; 0\\\\-3 &amp; 8 &amp; 3 &amp; 1\\end{bmatrix}\\begin{bmatrix}3 &amp; -7 &amp; -2 &amp; 2\\\\0 &amp; -2 &amp; -1 &amp; 2\\\\0 &amp; 0 &amp; -1 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; -1\\end{bmatrix}= LU,Solve A\\vec{x} = \\vec{b}, where\\spaceb =\\begin{bmatrix}-9 \\\\5 \\\\7 \\\\11\\end{bmatrix}$$Solution：$$\\begin{bmatrix}L &amp; \\vec{b}\\end{bmatrix} =\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; -9\\\\-1 &amp; 1 &amp; 0 &amp; 0 &amp; 5\\\\2 &amp; -5 &amp; 1 &amp; 0 &amp; 7\\\\-3 &amp; 8 &amp; 3 &amp; 1 &amp; 11\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; -9\\\\0 &amp; 1 &amp; 0 &amp; 0 &amp; -4\\\\0 &amp; 0 &amp; 1 &amp; 0 &amp; 5\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 1\\end{bmatrix} =\\begin{bmatrix}I &amp; \\vec{y}\\end{bmatrix}\\\\\\begin{bmatrix}U &amp; \\vec{y}\\end{bmatrix} =\\begin{bmatrix}3 &amp; -7 &amp; -2 &amp; 2 &amp; -9\\\\0 &amp; -2 &amp; -1 &amp; 2 &amp; -4\\\\0 &amp; 0 &amp; -1 &amp; 1 &amp; 5\\\\0 &amp; 0 &amp; 0 &amp; -1 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 3\\\\0 &amp; 1 &amp; 0 &amp; 0 &amp; 4\\\\0 &amp; 0 &amp; 1 &amp; 0 &amp; -6\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; -1\\end{bmatrix} =\\begin{bmatrix}I &amp; \\vec{x}\\end{bmatrix}$$ The LU Factorization AlgorithmSuppose $A$ can be reduced to an echelon form $U$ using only row replacements that add a multiple of one row to another row below it. In this case, there exist unit lower triangular elementary matrices $E_1,\\cdots,E_p$ such that: $$E_p\\cdots E_1A = U\\tag{3}$$Then$$A = (E_p\\cdots E_1)^{-1}U = LU$$where$$L = (E_p\\cdots E_1)^{-1}\\tag{4}$$ 从推导过程可以看出，$A$经历了哪些初等行变换，$L$也同时经历，最终$A$变成了$U$，$L$变成了$I$ It can be shown that products and inverses of unit lower triangular matrices are also unit lower triangular.Thus $L$ is unit lower triangular. For example:Find the $LU$ factorization of $A$.$$A =\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\-4 &amp; -5 &amp; 3 &amp; -8 &amp; 1\\\\2 &amp; -5 &amp; -4 &amp; 1 &amp; 8\\\\-6 &amp; 0 &amp; 7 &amp; -3 &amp; 1\\end{bmatrix}$$ Solution：$$A = \\begin{bmatrix}\\bbox[border:2px solid red]{2} &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\\\bbox[border:2px solid red]{-4} &amp; -5 &amp; 3 &amp; -8 &amp; 1\\\\\\bbox[border:2px solid red]2 &amp; -5 &amp; -4 &amp; 1 &amp; 8\\\\\\bbox[border:2px solid red]{-6} &amp; 0 &amp; 7 &amp; -3 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\0 &amp; \\bbox[border:2px solid red]3 &amp; 1 &amp; 2 &amp; -3\\\\0 &amp; \\bbox[border:2px solid red]{-9} &amp; -3 &amp; -4 &amp; 10\\\\0 &amp; \\bbox[border:2px solid red]{12} &amp; 4 &amp; 12 &amp; -5\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\0 &amp; 3 &amp; 1 &amp; 2 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; \\bbox[border:2px solid red]2 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; \\bbox[border:2px solid red]4 &amp; 7\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\0 &amp; 3 &amp; 1 &amp; 2 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; 2 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp;\\bbox[border:2px solid red]5\\end{bmatrix} = U$$$$\\begin{bmatrix}2\\\\-4\\\\2\\\\6\\end{bmatrix}\\begin{bmatrix}\\\\3\\\\-9\\\\12\\end{bmatrix}\\begin{bmatrix}\\\\\\\\2\\\\4\\end{bmatrix}\\begin{bmatrix}\\\\\\\\\\\\5\\end{bmatrix}\\\\\\div 2\\downarrow\\div 3\\downarrow\\div 2\\downarrow\\div 5\\downarrow\\\\\\begin{bmatrix}1 &amp; &amp; &amp; \\\\-2 &amp; 1 &amp; &amp; \\\\1 &amp; -3 &amp; 1 &amp; \\\\3 &amp; 4 &amp; 2 &amp; 1\\end{bmatrix} = L$$ More About QR Factorization &gt;&gt; The Leontief Input–Output Model(Omit)Applications to Computer GraphicsHomogeneous CoordinatesThe mathematics of computer graphics is intimately connected with matrix multiplication. Unfortunately, translating an object on a screen does not correspond directly to matrix multiplication because translation is not a linear transformation. The standard way to avoid this difficulty is to introduce what are called homogeneous coordinates. $$translation : (x,y)\\mapsto(x + h, y + k)\\\\homogeneous-translation: (x,y,1)\\mapsto(x + h, y + k, 1)\\\\matrix-multiplication:\\begin{bmatrix}1 &amp; 0 &amp; h\\\\0 &amp; 1 &amp; k\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix} x\\\\y\\\\1\\end{bmatrix}=\\begin{bmatrix}x + h\\\\y+k\\\\1\\end{bmatrix}$$ Any linear transformation on $R^2$ is represented with respect to homogeneous coordinates by a partitioned matrix of the form $\\begin{bmatrix}A &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix}$, where $A$ is a $2 \\times 2$ matrix. Typical examples are: $$\\begin{bmatrix}\\cos\\psi &amp; -\\sin\\psi &amp; 0\\\\\\sin\\psi &amp; \\cos\\psi &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix},\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix},\\begin{bmatrix}s &amp; 0 &amp; 0\\\\0 &amp; t &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}$$ Homogeneous 3D Coordinates$(x,y,z,1)$ are homogeneous coordinates for the point $(x,y,z)$ in $R^3$. In general, $(X,Y,Z,H)$ are homogeneous coordinates for $(x,y,z)$ if $H \\neq 0$ $$x = \\frac{X}{H}, y = \\frac{Y}{H}, z = \\frac{Z}{H}$$ For example: Rotation about the y-axis through an angle of 30&deg; Solution: First, construct the $3\\times 3$ matrix for the rotation. The vector $\\vec{e_1}$ rotates down toward the negative $z-axis$, stopping at $(\\cos 30, 0, -\\sin 30) = (\\frac{\\sqrt{3}}{2}, 0 , -0.5)$. The vector $\\vec{e_2}$ on the y-axis does not move. The vector $\\vec{e_3}$ on the $z-axis$ rotates down toward the positive $x-axis$, stopping at $(\\sin 30, 0, \\cos 30) = (0.5, 0, \\frac{\\sqrt{3}}{2})$. So the rotation matrix is$$\\begin{bmatrix}\\frac{\\sqrt{3}}{2} &amp; 0 &amp; 0.5\\\\0 &amp; 1 &amp; 0\\\\-0.5 &amp; 0 &amp; \\frac{\\sqrt{3}}{2}\\end{bmatrix}$$ Perspective ProjectionsFor example: let $xy-plane$ represent the computer screen, and imagine that the eye of a viewer is along the positive $z-axis$, at point $(0,0,d)$.A perspective projection maps each point $(x,y,z)$ onto an image point $(x^{\\ast}, y^{\\ast}, 0)$. Solution: induce projection-matrix as follows： $$\\frac{x^{\\ast}}{d} = \\frac{x}{d-z}\\rightarrowx^{\\ast} = \\frac{dx}{d-z} = \\frac{x}{1-\\frac{z}{d}}$$$$Similarly\\rightarrow y^{\\ast} = \\frac{y}{1-\\frac{z}{d}}$$$$(x,y,z,1)\\xrightarrow{projection} (\\frac{x}{1-\\frac{z}{d}}, \\frac{y}{1-\\frac{z}{d}}, 0 ,1)\\sim(x,y,0,1-\\frac{z}{d})$$$$p\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}=\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; -1/d &amp; 1\\end{bmatrix}\\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}=\\begin{bmatrix}x \\\\ y \\\\ 0 \\\\ 1-\\frac{z}{d}\\end{bmatrix}$$ More about Rotation matrix &amp; Perspective projection &gt;&gt; SubSpace of $R^n$ A subspace of $R^n$ is any set $H$ in $R^n$ that has three properties: The Zero Vector is in $H$. For each $\\vec{u}$ and $\\vec{v}$ in $H$, the sum $\\vec{u} + \\vec{v}$ is in $H$. For each $\\vec{u}$ in $H$ and each scalar $c$, the vector $c\\vec{u}$ is in $H$. Column Space and Null Space of a MatrixThe column space of a matrix $A$ is the set $ColA$ of all linear combinations of the columns of $A$. if $A = \\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}$, with the columns in $R^m$, then $Col A$ is the same as $Span \\set{\\vec{a_1} \\cdots \\vec{a_n}}$. For example: Let A = $\\begin{bmatrix}1 &amp; -3 &amp; -4 \\\\-4 &amp; 6 &amp; -2 \\\\-3 &amp; 7 &amp; 6\\end{bmatrix}$ and b = $\\begin{bmatrix}3 \\\\3 \\\\-4\\end{bmatrix}$. Determine whether $\\vec{b}$ is in the column space of $A$. Solution: $$\\begin{bmatrix}1 &amp; -3 &amp; -4 &amp; 3\\\\-4 &amp; 6 &amp; -2 &amp; 3\\\\-3 &amp; 7 &amp; 6 &amp; -4\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -3 &amp; -4 &amp; 3\\\\0 &amp; -6 &amp; -18 &amp; 15\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\\\有解，意味着b是A列向量的线性组合\\thereforeb \\subseteq Col A$$ Basis for a Subspace A basis for a subspace $H$ of $R^n$ is a linearly independent set in $H$ that spans $H$. $$\\vec{e_1} = \\begin{bmatrix}1 \\\\ 0 \\\\ \\cdots \\\\ 0\\end{bmatrix},\\vec{e_2} = \\begin{bmatrix}0 \\\\ 1 \\\\ \\cdots \\\\ 0\\end{bmatrix},\\cdots,\\vec{e_n} = \\begin{bmatrix}0 \\\\ 0 \\\\ \\cdots \\\\ 1\\end{bmatrix}$$ the set {$\\vec{e_1}, \\cdots, \\vec{e_n}$} is called the standard basis for $R^n$. For example: Find a basis for the null space of the matrix. $$A = \\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4\\end{bmatrix}$$ 找到矩阵零空间的一组基如下：$$\\begin{bmatrix}A &amp; \\vec{0}\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -2 &amp; 0 &amp; -1 &amp; 3 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix},\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\\\x_5\\end{bmatrix}=\\begin{bmatrix}2x_2 + x_4 - 3x_5\\\\x_2\\\\-2x_4 + 2x_5\\\\x_4\\\\x_5\\end{bmatrix}=x_2\\begin{bmatrix}2\\\\1\\\\0\\\\0\\\\0\\\\\\end{bmatrix} + x4\\begin{bmatrix}1\\\\0\\\\-2\\\\1\\\\0\\end{bmatrix} + x_5\\begin{bmatrix}-3\\\\0\\\\2\\\\0\\\\1\\end{bmatrix}= x_2\\vec{u} + x_4\\vec{v} + x_5\\vec{w}\\\\{\\vec{u}, \\vec{v}, \\vec{w}} 是基向量$$ Dimension and RankMore About Dimension and Rank is in C4 &gt;&gt; Coordinate SystemsSuppose the set $\\beta = \\set{\\vec{b_1},\\cdots\\cdots, \\vec{b_p}}$ is a basis for a SubSpace $H$. For each $\\vec{x}$ in $H$, the coordinates of $\\vec{x}$ relative to the basis $\\beta$ are the weights $c_1, \\cdots, c_p$ such that $\\vec{x} = c_1\\vec{b_1} + \\cdots + c_p\\vec{b_p}$, and the vector in $R^p$$$[\\vec{x}]_\\beta =\\begin{bmatrix}c_1\\\\\\cdots\\\\c_p\\end{bmatrix}$$is called the coordinate vector of $\\vec{x}$(relative to $\\beta$) or the $\\beta-$ coordinate vector of $\\vec{x}$ The Dimension of a Subspace The rank of a matrix $A$, denoted by $rank A$, is the dimension of the column space of $A$ Determin the rank of the matrix$$A\\sim\\begin{bmatrix}2 &amp; 5 &amp; -3 &amp; -4 &amp; 8\\\\0 &amp; -3 &amp; 2 &amp; 5 &amp; -7\\\\0 &amp; 0 &amp; 0 &amp; 4 &amp; -6\\\\\\end{bmatrix}矩阵有3个pivot-columns，所以rankA = 3$$ If a matrix $A$ has $n$ columns, then $rank A + dim Nul A = n$. Rank and the Invertible Matrix Theorem Let $A$ be an $n \\times n$ matrix. Then the following statements are each equivalent to the statement that $A$ is an invertible matrix.m. The columns of $A$ form a basis of $R^n$:n. $Col A = R^n$o. $dim Col A = n$p. $rank A = n$q. $Nul A = {0}$r. $dim Nul A = 0$","link":"/Math/Linear-Algebra/Algebra-C2-Matrix-Algebra/"},{"title":"Animation-C6-Motion-Capture","text":"","link":"/Graphics/Animation/Animation-C6-Motion-Capture/"},{"title":"Algebra-C5-EigenValues-And-EigenVectors","text":"Keywords: EigenVectors, Diagonalization, Characteristic Equation, Quaternion, Differential Equation This is the Chapter5 ReadingNotes from book Linear Algebra and its Application. EigenVectors And EigenValues An eigenvector of an $n \\times n$ matrix $A$ is a nonzero vector $\\vec{x}$ such that $A\\vec{x} = \\lambda\\vec{x}$ for some scalar $\\lambda$.A scalar $\\lambda$ is called an eigenvalue of $A$ if there is a nontrivial solution $\\vec{x}$ of $A\\vec{x} = \\lambda\\vec{x}$, such an $x$ is called an eigenvector corresponding to $\\lambda$. For Example: Let A = $\\begin{bmatrix}4 &amp; -1 &amp; 6\\\\ 2 &amp; 1 &amp; 6\\\\ 2 &amp; -1 &amp; 8 \\end{bmatrix}$.An eigenvalue of $A$ is 2. Find a basis for the corresponding eigenspace. $$A\\vec{x} = 2\\vec{x}\\rightarrow(A - 2I)\\vec{x} = \\vec{0}$$$$\\begin{bmatrix}2 &amp; -1 &amp; 6 &amp; 0\\\\2 &amp; -1 &amp; 6 &amp; 0\\\\2 &amp; -1 &amp; 6 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; -1 &amp; 6 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$general solution is:$$\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix} =x_2\\begin{bmatrix}1/2\\\\1\\\\0\\end{bmatrix} +x_3\\begin{bmatrix}-3\\\\0\\\\1\\end{bmatrix},x_2 and x_3 free$$ The eigenspace, shown in Figure 3, is a two-dimensional subspace of $R^3$. A basis is$$\\lbrace\\begin{bmatrix}1\\\\2\\\\0\\end{bmatrix}\\begin{bmatrix}-3\\\\0\\\\1\\end{bmatrix}\\rbrace$$ 个人理解就是所有特征向量组成的空间就是特征空间 The eigenvalues of a triangular matrix are the entries on its main diagonal.Let$$A = \\begin{bmatrix}3 &amp; 6 &amp; -8\\\\0 &amp; 0 &amp; 6\\\\0 &amp; 0 &amp; 2\\end{bmatrix},B = \\begin{bmatrix}4 &amp; 0 &amp; 0\\\\-2 &amp; 1 &amp; 0\\\\5 &amp; 3 &amp; 4\\end{bmatrix}$$The eigenvalues of $A$ are $3, 0, 2$. The eigenvalues of $B$ are $4, 1$. If $\\vec{v_1},\\cdots, \\vec{v_r}$ are eigenvectors that correspond to distinct eigenvalues $\\lambda_1, \\cdots, \\lambda_2$ of an $n \\times n$ matrix $A$, then the set $\\lbrace \\vec{v_1}, \\cdots , \\vec{v_r}\\rbrace$ is linearly independent. Eigenvectors and Difference Equations (差分方程)This section concludes by showing how to construct solutions of the first-order difference equation discussed in the chapter introductory example: $$\\vec{x_{k+1}} = A\\vec{x_{k}}\\tag{8}$$ If A is an $n \\times n$ matrix, then (8) is a recursive description of a sequence $\\lbrace \\vec{x_k}\\rbrace$ in $R^n$. A solution of (8) is an explicit description of $\\lbrace \\vec{x_k}\\rbrace$ whose formula for each $\\vec{x_k}$ does not depend directly on $A$ or on the preceding terms in the sequence other than the initial term $\\vec{x_0}$. The simplest way to build a solution of (8) is to take an eigenvector $\\vec{x_0}$ and its corresponding eigenvalue $\\lambda$and let $$\\vec{x_k} = \\lambda ^k \\vec{x_0}$$ This sequence is a solution because $$\\begin{aligned}A\\vec{x_k} &amp;= A(\\lambda ^k \\vec{x_0})\\\\&amp;=\\lambda ^k A\\vec{x_0} \\\\&amp;= \\lambda ^k (\\lambda\\vec{x_0})\\\\&amp;=\\vec{x_{k+1}}\\end{aligned}$$ The Characteristic Equation（特征方程）For Example: Find the eigenvalues of $A = \\begin{bmatrix}2 &amp; 3 \\\\ 3 &amp; -6 \\end{bmatrix}$. Solution: We must find all scalars $\\lambda$ such that the matrix equation$$(A - \\lambda I)\\vec{x} = \\vec{0}$$has a nontrivial solution（非零解）. This problem is equivalent to finding all$\\lambda$such that the matrix $A - \\lambda I$ is not invertible（不可逆）, where $$A - \\lambda I =\\begin{bmatrix}2 - \\lambda &amp; 3\\\\3 &amp; -6-\\lambda\\end{bmatrix}\\longrightarrowdet(A - \\lambda I) = (\\lambda - 3)(\\lambda + 7) = 0$$so the eigenvalues of $A$ are 3 and -7. Determinants Let $A$ be an $n\\times n$ matrix. Then A is invertible if and only if:s. The number $0$ is not an eigenvalue of $A$.t. The determinant of $A$ is not zero. Let $A$ and $B$ be $n \\times n$ matrices.a. $A$ is invertible if and only if det A 不等于 0.b. $det AB = (det A)(det B)$.c. $det A^T = det A$.d. If $A$ is triangular, then $det A$ is the product of the entries on the main diagonal of $A$.e. A row replacement operation on $A$ does not change the determinant.A row interchange changes the sign of the determinant.A row scaling also scales the determinant by the same scalar factor. The Characteristic EquationThe scalar equation $det(A - \\lambda I) = 0$ is called the characteristic equation of $A$. A scalar $\\lambda$ is an eigenvalue of an $n \\times n$ matrix $A$ if and only if $\\lambda$ satisfies the characteristic equation$$det(A - \\lambda I) = 0$$ For Example: Find the characteristic equation of$$A =\\begin{bmatrix}5 &amp; -2 &amp; 6 &amp; -1\\\\0 &amp; 3 &amp; -8 &amp; 0\\\\0 &amp; 0 &amp; 5 &amp; 4\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}$$ Solution: $$det(A - \\lambda I) = (5-\\lambda)(3-\\lambda)(5-\\lambda)(1-\\lambda)$$The characteristic equation is$$(5-\\lambda)^2(3-\\lambda)(1-\\lambda) = 0$$Expanding the product, we can also write $$\\lambda^4 - 14\\lambda^3 + 68\\lambda^2 - 130\\lambda + 75 = 0$$ then $det(A - \\lambda I)$ is a polynomial of degree $n$ called the characteristic polynomial（特征多项式） of $A$. The eigenvalue 5 in Example above is said to have multiplicity 2 because $\\lambda - 5$ occurs two times as a factor of the characteristic polynomial. Similarity$A$ is similar to $B$ if there is an invertible matrix $P$ such that $P^{-1}AP = B$, or, equivalently, $PBP^{-1} = A$. Changing $A$ into $P^{-1}AP = B$ is called a similarity transformation. If $n \\times n$ matrices $A$ and $B$ are similar, then they have the same characteristic polynomial and hence the same eigenvalues (with the same multiplicities). Warning: The matrices$$\\begin{bmatrix}2 &amp; 1\\\\0 &amp; 2\\end{bmatrix}and\\begin{bmatrix}2 &amp; 0\\\\0 &amp; 2\\end{bmatrix}$$are not similar even though they have the same eigenvalues. 2.Similarity is not the same as row equivalence. (If $A$ is row equivalent to $B$, then $B = EA$ for some invertible matrix $E$.) Row operations on a matrix usually change its eigenvalues. Application to Dynamical SystemsFor Example: Let $A = \\begin{bmatrix} 0.95 &amp; 0.03\\\\ 0.05 &amp; 0.97\\end{bmatrix}$. Analyze the long-term behavior of the dynamical system defined by $\\vec{x_{k+1}} = A\\vec{x_k}$, with $\\vec{x_0} = \\begin{bmatrix}0.6\\\\0.4\\end{bmatrix}$. Solution: The first step is to find the eigenvalues of $A$ and a basis for each eigenspace. The characteristic equation for $A$ is $$0 = det\\begin{bmatrix}0.95-\\lambda &amp; 0.03\\\\0.05 &amp; 0.97-\\lambda\\end{bmatrix} =\\lambda^2 - 1.92\\lambda + 0.92$$By the quadratic formula $$\\lambda = 1 or \\space 0.92$$It is readily checked that eigenvectors corresponding to $\\lambda = 1$ and $\\lambda = 0.92$ are multiples of $$\\vec{v_1} = \\begin{bmatrix}3\\\\5\\end{bmatrix}and \\space\\vec{v_2} = \\begin{bmatrix}1\\\\-1\\end{bmatrix}$$respectively.The next step is to write the given $\\vec{x_0}$ in terms of $\\vec{v_1}$ and $\\vec{v_2}$. This can be done because $\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ is obviously a basis for $R^2$: $$\\vec{x_0} = c_1\\vec{v_1} + c_2\\vec{v_2} =\\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2}\\end{bmatrix}\\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix}\\\\\\longrightarrow\\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix} =\\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2}\\end{bmatrix} ^ {-1}\\vec{x_0}=\\begin{bmatrix}0.125 \\\\ 0.225\\end{bmatrix}$$Because $\\vec{v_1}$ and $\\vec{v_2}$ are eigenvectors of $A$, with $A\\vec{v_1} = \\vec{v_1}$ and $A\\vec{v_2} = 0.92\\vec{v_2}$, we easily compute each $\\vec{x_k}$: $$\\vec{x_1} = A\\vec{x_0} = c_1\\vec{v_1}+c_2(0.92)\\vec{v_2}\\\\\\vec{x_2} = A\\vec{x_1} = c_1\\vec{v_1}+c_2(0.92)^2\\vec{v_2}\\\\\\cdots\\vec{x_k} = c_1\\vec{v_1}+c_2(0.92)^k\\vec{v_2}=0.125\\begin{bmatrix}3\\\\5\\end{bmatrix} + 0.225(0.92)^k\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\\\\\lim_{x \\to \\infty} \\vec{x_k} = 0.125\\vec{v_1}$$ DiagonalizationFor Example: Let $A = \\begin{bmatrix} 7 &amp; 2\\\\-4 &amp; 1\\end{bmatrix}$. Find a formula for $A^k$, given that $A = PDP^{-1}$, where$$P =\\begin{bmatrix}1 &amp; 1\\\\-1 &amp; -2\\end{bmatrix}andD =\\begin{bmatrix}5 &amp; 0\\\\0 &amp; 3\\end{bmatrix}$$ Solution: $$A^2 = PDP^{-1}PDP^{-1} = PD^2P^{-1}\\\\\\RightarrowA^k = PD^kP^{-1}$$A square matrix $A$ is said to be diagonalizable if $A$ is similar to a diagonal matrix, that is, if $A = PDP^{-1}$ for some invertible matrix $P$ and some diagonal matrix $D$. An $n \\times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors.In fact, $A = PDP^{-1}$, with $D$ a diagonal matrix, if and only if the columns of $P$ are $n$ linearly independent eigenvectors of $A$. In this case, the diagonal entries of $D$ are eigenvalues of $A$ that correspond, respectively, to the eigenvectors in $P$.In other words, $A$ is diagonalizable if and only if there are enough eigenvectors to form a basis of $R^n$. We call such a basis an eigenvector basis of $R^n$. Diagonalizing MatricesFor Example: Diagonalize the following matrix, if possible.$$A = \\begin{bmatrix}1 &amp; 3 &amp; 3\\\\-3 &amp; -5 &amp; -3\\\\3 &amp; 3 &amp; 1\\end{bmatrix}$$That is, find an invertible matrix $P$ and a diagonal matrix $D$ such that $A = PDP^{-1}$. Solution: Step1. Find the eigenvalues of A.$$det(A - \\lambda I) = -(\\lambda-1)(\\lambda+2)^2$$The eigenvalues are $\\lambda = 1 , \\space \\lambda = -2$ Step2. Find three linearly independent eigenvectors of A. $$Basis \\space for \\space \\lambda = 1: \\vec{v_1} = \\begin{bmatrix}1 \\\\ -1 \\\\ 1\\end{bmatrix}\\Basis \\space for \\space \\lambda = -2: \\vec{v_2} = \\begin{bmatrix}-1 \\\\ 1 \\\\ 0\\end{bmatrix} and \\space\\vec{v_3} = \\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}$$Step3. Construct P from the vectors in step 2.$$P = \\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2} &amp; \\vec{v_3}\\end{bmatrix} =\\begin{bmatrix}1 &amp; -1 &amp; -1\\\\-1 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 1\\end{bmatrix}$$Step4. Construct D from the corresponding eigenvalues.$$D =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; -2\\end{bmatrix}$$Step5. Verify.$$AP = PD$$ An $n \\times n$ matrix with $n$ distinct eigenvalues is diagonalizable. Matrices Whose Eigenvalues Are Not DistinctLet $A$ be an $n \\times n$ matrix whose distinct eigenvalues are $\\lambda_1, \\cdots, \\lambda_p$. a. For $1 \\le k \\le p$, the dimension of the eigenspace for $\\lambda_k$ is less than or equal to the multiplicity of the eigenvalue $\\lambda_k$. b. The matrix $A$ is diagonalizable if and only if the sum of the dimensions of the eigenspaces equals $n$, and this happens if and only if (i) the characteristic polynomial factors completely into linear factors and (ii) the dimension of the eigenspace for each $\\lambda_k$ equals the multiplicity of $\\lambda_k$. c. If $A$ is diagonalizable and $\\beta_k$ is a basis for the eigenspace corresponding to $\\lambda_k$ for each $k$, then the total collection of vectors in the sets $\\beta_1, \\cdots, \\beta_p$ forms an eigenvector basis for $R^n$. EigenVectors And Linear TransformationsThe Matrix of a Linear TransformationLet $V$ be an $n$-dimensional vector space, let $W$ be an $m$-dimensional vector space, and let $T$ be any linear transformation from $V$ to $W$ . To associate a matrix with $T$, choose (ordered) bases $\\beta$ and $\\gamma$ for $V$ and $W$ , respectively. Given any $\\vec{x}$ in $V$ , the coordinate vector $\\begin{bmatrix}\\vec{x}\\end{bmatrix}_\\beta$ is in $R^n$ and the coordinate vector of its image, $\\begin{bmatrix}T(\\vec{x})\\end{bmatrix}_\\gamma$, is in $R^m$, as shown in Figure 1. The connection between $\\begin{bmatrix}\\vec{x}\\end{bmatrix}_\\beta$ and $\\begin{bmatrix}T(\\vec{x})\\end{bmatrix}_\\gamma$ is easy to find. Let $\\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$ be the basis $\\beta$ for $V$ . If $\\vec{x} = r_1\\vec{b_1} + \\cdots + r_n\\vec{b_n}$, then$$\\begin{bmatrix} \\vec{x}_\\beta\\end{bmatrix}=\\begin{bmatrix}r_1 \\\\ \\cdots \\\\r_n\\end{bmatrix}$$and$$T(\\vec{x}) = T(r_1\\vec{b_1} + \\cdots + r_n\\vec{b_n})= r_1T(\\vec{b_1}) + \\cdots + r_nT(\\vec{b_n})\\tag{1}$$because $T$ is linear. Now, since the coordinate mapping from $W$ to $R^m$ is linear, equation (1) leads to$$\\begin{bmatrix} T(\\vec{x}) \\end{bmatrix}_\\gamma = r_1\\begin{bmatrix} T(\\vec{b_1}) \\end{bmatrix}_\\gamma\\ +\\cdots + r_n\\begin{bmatrix} T(\\vec{b_n}) \\end{bmatrix}_\\gamma\\tag{2}$$Since $C$-coordinate vectors are in $R^m$, the vector equation (2) can be written as a matrix equation, namely,$$\\begin{bmatrix}T(\\vec{x})\\end{bmatrix}_\\gamma =M\\begin{bmatrix}\\vec{x}\\end{bmatrix}_\\beta\\tag{3}$$where $$M = \\begin{bmatrix}\\begin{bmatrix}(T(\\vec{b_1}))\\end{bmatrix}_\\gamma&amp;\\begin{bmatrix}(T(\\vec{b_2}))\\end{bmatrix}_\\gamma&amp;\\cdots&amp;\\begin{bmatrix}(T(\\vec{b_n}))\\end{bmatrix}_\\gamma\\end{bmatrix}\\tag{4}$$The matrix $M$ is a matrix representation of $T$ , called the matrix for $T$ relative to the bases $\\beta$ and $\\gamma$. See Figure 2. For Example: Suppose $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$ is a basis for $V$ and $\\gamma = \\lbrace \\vec{c_1}, \\vec{c_2}, \\vec{c_3}\\rbrace$ is a basis for $W$. Let $T: V \\rightarrow W$ be a linear transformation with the property that $$T(\\vec{b_1}) = 3\\vec{c_1} - 2\\vec{c_2} + 5\\vec{c_3}\\\\T(\\vec{b_2}) = 4\\vec{c_1} + 7\\vec{c_2} - \\vec{c_3}$$Find the matrix $M$ for $T$ relative to $\\beta$ and $\\gamma$. Solution: The $\\gamma$-coordinate vectors of the images of $\\vec{b_1}$ and $\\vec{b_2}$ are $$\\begin{bmatrix}T(\\vec{b_1})\\end{bmatrix}_\\gamma =\\begin{bmatrix}3 \\\\ -2 \\\\ 5\\end{bmatrix}\\\\begin{bmatrix}T(\\vec{b_2})\\end{bmatrix}_\\gamma =\\begin{bmatrix}4 \\\\ 7 \\\\ -1\\end{bmatrix}$$Hence,$$M =\\begin{bmatrix}3 &amp; 4\\\\ -2 &amp; 7\\\\ 5 &amp; -1\\end{bmatrix}$$ Linear Transformations on $R^n$ Diagonal Matrix RepresentationSuppose $A = PDP^{-1}$, where $D$ is a diagonal $n \\times n$ matrix. If $\\beta$ is the basis for $R^n$ formed from the columns of $P$ , then $D$ is the $\\beta-matrix$ for the transformation $x \\mapsto Ax$. For example: Define $T: R^2\\mapsto R^2$ by $T(\\vec{x}) = A\\vec{x}$, where $A = \\begin{bmatrix} 7 &amp; 2\\\\-4 &amp; 1\\end{bmatrix}$. Find a basis $\\beta$ for $R^2$ with the property that the $\\beta-matrix$ for $T$ is a diagonal matrix. Solution: $$A = PDP^{-1}, P = \\begin{bmatrix}1 &amp; 1 \\\\ -1 &amp; -2\\end{bmatrix}, D = \\begin{bmatrix}5 &amp; 0 \\\\ 0 &amp; 3\\end{bmatrix}$$The columns of $P$ , call them $\\vec{b_1}$ and $\\vec{b_2}$, are eigenvectors of $A$. Thus, $D$ is the $B-matrix$ for $T$ when $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2} \\rbrace$. The mappings$\\vec{x} \\mapsto A\\vec{x}$ and $\\vec{u} \\mapsto D\\vec{u}$ describe the same linear transformation, relative to different bases. Complex EigenValues$C^n$: a space of $n$-tuples of complex numbers. For Example: If $A = \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix}$, then the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ on $R^2$ rotates the plane counterclockwise through a quarter-turn.The action of $A$ is periodic, since after four quarter-turns, a vector is back where it started. Obviously, no nonzero vector is mapped into a multiple of itself, so $A$ has no eigenvectors in $R^2$ and hence no real eigenvalues. In fact, the characteristic equation of $A$ is $$\\lambda^2 + 1 = 0\\rightarrow\\lambda = i, \\lambda = -i;$$If we permit $A$ to act on $C^2$, then $$\\begin{bmatrix}0 &amp; -1\\\\1 &amp; 0\\end{bmatrix}\\begin{bmatrix}1 \\\\ -i\\end{bmatrix}=i\\begin{bmatrix}1 \\\\ -i\\end{bmatrix}\\\\\\begin{bmatrix}0 &amp; -1\\\\1 &amp; 0\\end{bmatrix}\\begin{bmatrix}1 \\\\ i\\end{bmatrix}=-i\\begin{bmatrix}1 \\\\ i\\end{bmatrix}$$Thus, $i,-i$ are eigenvalues, with $\\begin{bmatrix}1 \\\\ -i\\end{bmatrix}$ and $\\begin{bmatrix}1 \\\\ i\\end{bmatrix}$ as corresponding eigenvectors. For Example: Let $A = \\begin{bmatrix} 0.5 &amp; -0.6 \\\\ 0.75 &amp; 1.1 \\end{bmatrix}$. Find the eigenvalues of $A$, and find a basis for each eigenspace. Solution: $$0 = det\\begin{bmatrix}0.5-\\lambda &amp; -0.6 \\\\ 0.75 &amp; 1.1-\\lambda\\end{bmatrix}=\\lambda^2 - 1.6\\lambda + 1\\\\\\Rightarrow\\lambda = 0.8 \\pm 0.6i$$for $\\lambda = 0.8 - 0.6i$, construct $$A - (0.8-0.6i)I =\\begin{bmatrix}-0.3 + 0.6i &amp; -0.6\\\\0.75 &amp; 0.3 + 0.6i\\end{bmatrix}\\tag{2}$$The second equation in (2) leads to $$0.75x_1 = (-0.3-0.6i)x_2\\\\x_1 = (-0.4-0.8i)x_2$$Choose $x_2 = 5$ to eliminate the decimals, and obtain $x_1 = -2 - 4i$. $$\\vec{v_1} = \\begin{bmatrix}-2-4i\\\\5\\end{bmatrix}$$Analogous calculations for $\\lambda = 0.8 + 0.6i$ produce the eigenvector $$\\vec{v_2} = \\begin{bmatrix}-2 + 4i\\\\5\\end{bmatrix}$$the matrix $A$ determines a transformation $\\vec{x} \\mapsto A\\vec{x}$ that is essentially a rotation. See Figure1. Real and Imaginary Parts of VectorsFor Example: If $\\vec{x} = \\begin{bmatrix}3-i\\\\i\\\\2+5i\\end{bmatrix}$ = $\\begin{bmatrix}3\\\\0\\\\2\\end{bmatrix}$ + i$\\begin{bmatrix}-1\\\\1\\\\5\\end{bmatrix}$, then $$Re(Real)\\vec{x} = \\begin{bmatrix}3 \\\\ 0 \\\\ 2\\end{bmatrix}\\\\Im(Imaginary)\\vec{x} = \\begin{bmatrix}-1 \\\\ 1 \\\\ 5\\end{bmatrix}, and \\overline{\\vec{x}} = \\begin{bmatrix}3\\\\0\\\\2\\end{bmatrix} - i\\begin{bmatrix}-1\\\\1\\\\5\\end{bmatrix}$$ Eigenvalues and Eigenvectors of a Real Matrix That Acts on $C^n$For Example: If $C = \\begin{bmatrix}a &amp; -b \\\\ b &amp; a\\end{bmatrix}$, where $a$ and $b$ are real and not both zero, then the eigenvalues of $C$ are $\\lambda = a \\pm bi$. Also, if $r = |\\lambda| = \\sqrt{a^2 + b^2}$, then $$C = r\\begin{bmatrix}a/r &amp; -b/r\\\\b/r &amp; a/r\\end{bmatrix} =\\begin{bmatrix}r &amp; 0\\\\0 &amp; r\\end{bmatrix}\\begin{bmatrix}\\cos\\psi &amp; -\\sin\\psi\\\\\\sin\\psi &amp; \\cos\\psi\\end{bmatrix}$$where $\\psi$ is the angle between the positive $x-axis$ and the ray from $(0,0)$ through $(a,b)$. The angle $\\psi$ is called the argument of $\\lambda = a + bi$. Thus the transformation $\\vec{x}\\mapsto C\\vec{x}$ may be viewed as the composition of a rotation through the angle $\\psi$ and a scaling by $|\\lambda|$ (see Figure 3). Let $A$ be a real $2 \\times 2$ matrix with a complex eigenvalue $\\lambda = a - bi, b \\neq 0$ and an associated eigenvector $\\vec{v}$ in $C^2$. Then $$A = PCP^{-1}, where \\space P = \\begin{bmatrix} Re\\vec{v} &amp; Im\\vec{v}\\end{bmatrix}and \\space C = \\begin{bmatrix}a &amp; -b\\\\ b &amp; a\\end{bmatrix}$$For Example: The matrix $A = \\begin{bmatrix}0.8 &amp; -0.6 &amp; 0\\\\0.6 &amp; 0.8 &amp; 0\\\\ 0 &amp; 0 &amp; 1.07\\end{bmatrix}$ has eigenvalues $0.8\\pm 0.6i$ and $1.07$. Any vector $\\vec{w_0}$ in the $x_1x_2-plane$ (with third coordinate 0) is rotated by $A$ into another point in the plane. Any vector $\\vec{x_0}$ not in the plane has its $x_3-coordinate$ multiplied by $1.07$. The iterates of the points $\\vec{w_0} = (2,0,0)$ and $\\vec{x_0} = (2,0,1)$ under multiplication by $A$ are shown in Figure 5. More about Complex EigenValues and Quaternions &gt;&gt; Discrete Dynamical Systems(Omitted)Applications to Differential Equations 微分方程$$\\vec{x(t)’} = A\\vec{x(t)}$$ where,$$\\vec{x(t)} =\\begin{bmatrix}x_1(t)\\\\\\cdots\\\\x_n(t)\\end{bmatrix},\\vec{x’(t)} =\\begin{bmatrix}x_1’(t)\\\\\\cdots\\\\x_n’(t)\\end{bmatrix},A =\\begin{bmatrix}a_{11} &amp; \\cdots &amp; a_{1n}\\\\\\cdots&amp; &amp; \\cdots\\\\a_{n1} &amp; \\cdots &amp; a_{nn}\\end{bmatrix}$$$x_1, x_2, \\cdots, x_n$ are differentiable functions of $t$. First-order Differential Equation &gt;&gt; For example : $$\\begin{bmatrix}x_1’(t)\\\\x_2’(t)\\end{bmatrix}=\\begin{bmatrix}3 &amp; 0\\\\0 &amp; -5\\end{bmatrix}\\begin{bmatrix}x_1(t)\\\\x_2(t)\\end{bmatrix}\\tag{2}$$ $$\\begin{cases}x_1’(t) = 3x_1(t)\\\\x_2’(t) = -5x_2(t)\\end{cases}\\tag{3}$$ The system (2) is said to be decoupled because each derivative of a function depends only on the function itself, not on some combination or “coupling” of both $x_1(t)$ and $x_2(t)$. From calculus, the solutions of (3) are $x_1(t) = c_1e^{3t}$ and $x_2(t) = c_2e^{-5t}$, for any constants $c_1$ and $c_2$. Each solution of equation (2) can be written in the form $$\\begin{bmatrix}x_1(t)\\\\x_2(t)\\end{bmatrix}=\\begin{bmatrix}c_1e^{3t}\\\\c_2e^{-5t}\\end{bmatrix}=c_1\\begin{bmatrix}1 \\\\0\\end{bmatrix}e^{3t}+c_2\\begin{bmatrix}0 \\\\1\\end{bmatrix}e^{-5t}$$This example suggests that for the general equation $\\vec{x}’ = A\\vec{x}$, a solution might be a linear combination of functions of the form $$\\vec{x}(t) = \\vec{v} e^{\\lambda t}\\tag{4}$$ Thus,$$\\begin{cases}\\vec{x’}(t) = \\lambda \\vec{v} e^{\\lambda t}\\\\A\\vec{x}(t) = A\\vec{v} e^{\\lambda t}\\end{cases}\\RightarrowA\\vec{v} = \\lambda\\vec{v}$$ that means, $\\lambda$ is an eigenvalue of $A$ and $\\vec{v}$ is a corresponding eigenvector. Eigenfunctions provide the key to solving systems of differential equations. For example: Suppose a particle is moving in a planar force field and its position vector $\\vec{x}$ satisfies $\\vec{x’} = A\\vec{x}$ and $\\vec{x}(0) = \\vec{x_0}$, where $$A =\\begin{bmatrix}4 &amp; -5\\\\-2 &amp; 1\\end{bmatrix},\\vec{x_0} =\\begin{bmatrix}2.9\\\\2.6\\end{bmatrix}$$Solve this initial value problem for $t \\geq 0$ and sketch the trajectory of the particle. Solution: the eigenvalues of A is $\\lambda_1 = 6, \\lambda_2 = -1$, with corresponding eigenvectors $\\vec{v_1} = (-5,2), \\vec{v_2} = (1,1)$. So the function is $$\\begin{aligned}\\vec{x}(t) &amp;= c_1\\vec{v_1}e^{\\lambda_1t} + c_2\\vec{v_2}e^{\\lambda_2t}\\\\&amp;= c_1\\begin{bmatrix}-5 \\\\ 2\\end{bmatrix} e^{6t} + c_2\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} e^{-t}\\end{aligned}$$is the solution of $\\vec{x’} = A\\vec{x}$. $$\\vec{x}(0) = \\vec{x_0} \\longrightarrowc_1\\begin{bmatrix}-5 \\\\ 2\\end{bmatrix} + c_2\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} =\\begin{bmatrix}2.9\\\\2.6\\end{bmatrix}$$Thus,$$\\vec{x}(t) = \\frac{-3}{70}\\begin{bmatrix}-5 \\\\ 2\\end{bmatrix} e^{6t} + \\frac{188}{70}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} e^{-t}$$ See Figure3, the origin is called a saddle point of the dynamical system because some trajectories approach the origin at first and then change direction and move away from the origin. A saddle point arises whenever the matrix $A$ has both positive and negative eigenvalues. The direction of greatest repulsion is the line through $\\vec{v_1}$ and $\\vec{0}$, corresponding to the positive eigenvalue. The direction of greatest attraction is the line through $\\vec{v_2}$ and $\\vec{0}$, corresponding to the negative eigenvalue. Iterative Estimates for Eigenvalues 迭代估计特征值(Numerical Analysis)The Power MethodThe power method applies to an $n \\times n$ matrix $A$ with a strictly dominant eigenvalue $\\lambda_1$, which means that $\\lambda_1$ must be larger in absolute value than all the other eigenvalues. The Power Method For Estimating A Strictly Dominant EigenValue Select an initial vector $\\vec{x_0}$ whose largest entry is 1. For k = 0, 1 …a. Compute $A\\vec{x_k}$.b. Let $\\mu_k$ be an entry in $A\\vec{x_k}$ whose absolute value is as large as possible.c. Compute $\\vec{x_{k+1}} = (\\frac{1}{\\mu_k})A\\vec{x_k}$. For almost all choices of $\\vec{x_0}$, the sequence $\\lbrace \\mu_k \\rbrace$ approaches the dominant eigenvalue, and the sequence $\\lbrace \\vec{x_k} \\rbrace$ approaches a corresponding eigenvector. The Inverse Power MethodThis method provides an approximation for any eigenvalue, provided a good initial estimate $\\alpha$ of the eigenvalue $\\lambda$ is known. The Inverse Power Method For Estimating An EigenValue $\\lambda$ of $A$ Select an initial estimate $\\alpha$ sufficiently close to $\\lambda$. Select an initial vector $\\vec{x_0}$ whose largest entry is 1. For k = 0, 1 …a. Solve $(A - \\alpha I)\\vec{y_k} = \\vec{x_k}$ for $\\vec{y_k}$.b. Let $\\mu_k$ be an entry in $\\vec{y_k}$ whose absolute value is as large as possible.c. Compute $v_{k} = \\alpha + (1/\\mu_k)$.d. Compute $\\vec{x_{k+1}} = (1/\\mu_k)\\vec{y_k}$ For almost all choices of $\\vec{x_0}$, the sequence $\\lbrace v_k \\rbrace$ approaches the eigenvalue $\\lambda$ of $A$, and the sequence $\\lbrace \\vec{x_k} \\rbrace$ approaches a corresponding eigenvector. More about Calculate EigenValues by Numerical Analysis &gt;&gt;","link":"/Math/Linear-Algebra/Algebra-C5-EigenValues-And-EigenVectors/"},{"title":"Calculus-C1-Functions","text":"Keywords: Orthogonal Projections, Gram-Schmidt Process, Least-Squares Problems, Linear Model This is the Chapter1 ReadingNotes from book Thomas Calculus 14th.","link":"/Math/Calculus/Calculus-C1-Functions/"},{"title":"Algebra-C7-Symmetric-Matrices-And-Quadratic-Forms","text":"Keywords: Symmetric Matrix, Quadratic Forms, Constrained Optimization, Singular Value Decomposition, ImageProcessing, Statistics This is the Chapter7 ReadingNotes from book Linear Algebra and its Application. Diagonalization of Symmetric Matrices（对称矩阵的对角化） If $A$ is symmetric, then any two eigenvectors from different eigenspaces are orthogonal. An $n \\times n$ matrix $A$ is orthogonally diagonalizable if and only if $A$ is a symmetric matrix. For Example: Orthogonally diagonalize the matrix $A = \\begin{bmatrix}3 &amp; -2 &amp; 4\\\\-2 &amp; 6 &amp; 2\\\\4 &amp; 2 &amp; 3\\end{bmatrix}$, whose characteristic equation is $$0 = -\\lambda^3 + 12\\lambda^2 -21\\lambda - 98 = -(\\lambda - 7)^2(\\lambda + 2)$$ Solution: $$\\lambda = 7, \\vec{v_1} = \\begin{bmatrix}1 \\\\ 0 \\\\ 1\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-1/2 \\\\ 1 \\\\ 0\\end{bmatrix};\\\\\\lambda = -2,\\vec{v_3} = \\begin{bmatrix}-1 \\\\ -1/2 \\\\ 1\\end{bmatrix}$$Although $\\vec{v_1}$ and $\\vec{v_2}$ are linearly independent, they are not orthogonal. the component of $\\vec{v_2}$ orthogonal to $\\vec{v_1}$ is $$\\vec{z_2} = \\vec{v_2} - \\frac{\\vec{v_2} \\cdot \\vec{v_1}}{\\vec{v_1} \\cdot \\vec{v_1}}\\vec{v_1}= \\begin{bmatrix}-1/4 \\\\ 1 \\\\ 1/4\\end{bmatrix}$$Then $\\lbrace \\vec{v_1}, \\vec{z_2}\\rbrace$ is an orthogonal set in the eigenspace for $\\lambda = 7$. Normalize $\\vec{v_1}$ and $\\vec{z_2}$ to obtain the following orthonormal basis for the eigenspace for$\\lambda = 7$: $$\\vec{u_1} = \\begin{bmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ 1/\\sqrt{2}\\end{bmatrix},\\vec{u_2} = \\begin{bmatrix} -1/\\sqrt{18} \\\\ 4/\\sqrt{18} \\\\ 1/\\sqrt{18}\\end{bmatrix},also, \\vec{u_3} = \\begin{bmatrix} -2/3 \\\\ -1/3 \\\\ 2/3\\end{bmatrix}$$Hence $\\vec{u_1},\\vec{u_2},\\vec{u_3}$ is an orthonormal set. Let $$P = \\begin{bmatrix}\\vec{u_1} &amp; \\vec{u_2} &amp; \\vec{u_3}\\end{bmatrix}=\\begin{bmatrix}1/\\sqrt{2} &amp; -1/\\sqrt{18} &amp; -2/3 \\\\ 0 &amp; 4/\\sqrt{18} &amp; -1/3 \\\\ 1/\\sqrt{2} &amp; 1/\\sqrt{18} &amp; 2/3\\end{bmatrix},D =\\begin{bmatrix}7 &amp; 0 &amp; 0 \\\\ 0 &amp; 7 &amp; 0 \\\\ 0 &amp; 0 &amp; -2\\end{bmatrix}$$Then $P$ orthogonally diagonalizes $A$, and $A = PDP^{-1}$. Spectral DecompositionSince $P$ is orthogonal matrix（正交矩阵, orthonormal columns）, $P^{-1} = P^T$. So $$A = PDP^T =\\begin{bmatrix}\\vec{u_1} &amp; \\cdots &amp; \\vec{u_n}\\end{bmatrix}\\begin{bmatrix}\\lambda_1 &amp; &amp; 0\\\\&amp;\\ddots\\\\0 &amp; &amp; \\lambda_n\\end{bmatrix}\\begin{bmatrix}\\vec{u_1}^T \\\\ \\cdots \\\\ \\vec{u_n}^T\\end{bmatrix}\\\\\\RightarrowA = \\lambda_1\\vec{u_1}\\vec{u_1}^T + \\lambda_2\\vec{u_2}\\vec{u_2}^T +\\cdots + \\lambda_n\\vec{u_n}\\vec{u_n}^T$$ Quadratic Forms（二次型）A quadratic form on $R^n$ is a function $Q$ defined on $R^n$ whose value at a vector $\\vec{x}$ in $R^n$ can be computed by an expression of the form $Q(\\vec{x}) = \\vec{x}^TA\\vec{x}$, where $A$ is an $n \\times n$ symmetric matrix. The matrix $A$ is called the matrix of the quadratic form. For Example: For $\\vec{x}$ in $R^3$, let $Q(\\vec{x}) = 5x_1^2 + 3x_2^2 + 2x_3^2 - x_1x_2 + 8x_2x_3$. Write this quadratic form as $\\vec{x}^TA\\vec{x}$. Solution: $$Q(\\vec{x}) = \\vec{x}^TA\\vec{x} =\\begin{bmatrix}x_1 &amp; x_2 &amp; x_3\\end{bmatrix}\\begin{bmatrix}5 &amp; -1/2 &amp; 0\\\\-1/2 &amp; 3 &amp; 4\\\\0 &amp; 4 &amp; 2\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix}$$ Change of Variable in a Quadratic FormLet A be an $n \\times n$ symmetric matrix. Then there is an orthogonal change of variable, $\\vec{x} = P\\vec{y}$, that transforms the quadratic form $\\vec{x}^TA\\vec{x}$ into a quadratic form $\\vec{y}^TD\\vec{y}$ with no cross-product term. The columns of $P$ in the theorem are called the principal axes of the quadratic form（二次型的主轴） $\\vec{x}^TA\\vec{x}$. The vector $\\vec{y}$ is the coordinate vector of $\\vec{x}$ relative to the orthonormal basis of $R^n$ given by these principal axes. For Example : Make a change of variable that transforms the quadratic form $Q(\\vec{x}) = x_1^2 - 8x_1x_2 - 5x_2^2$ into a quadratic form with no cross-product term. Solution: The matrix of the quadratic form is$$A =\\begin{bmatrix}1 &amp; -4\\\\-4 &amp; -5\\end{bmatrix}$$The first step is to orthogonally diagonalize $A$. $$\\lambda = 3 :\\begin{bmatrix}2/\\sqrt{5}\\\\-1/\\sqrt{5}\\end{bmatrix};$$$$\\lambda = -7 :\\begin{bmatrix}1/\\sqrt{5}\\\\2/\\sqrt{5}\\end{bmatrix}$$Let$$P = \\begin{bmatrix}2/\\sqrt{5} &amp; 1/\\sqrt{5}\\\\-1/\\sqrt{5} &amp; 2/\\sqrt{5}\\end{bmatrix},D = \\begin{bmatrix}3 &amp; 0\\\\0 &amp; -7\\end{bmatrix}$$Then $A = PDP^{-1}$ and $D = P^{-1}AP = P^TAP$, as pointed out earlier. A suitable change of variable is $$\\vec{x} = P\\vec{y},where \\space \\vec{x} =\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}and\\vec{y} =\\begin{bmatrix}y_1\\\\y_2\\end{bmatrix}$$Then, $$x_1^2 - 8x_1x_2 - 5x_2^2= \\vec{x}^TA\\vec{x}= (P\\vec{y})^TA(P\\vec{y})\\\\= \\vec{y}^TP^TAP\\vec{y}= \\vec{y}^TD\\vec{y}= 3y_1^2-7y_2^2$$ To illustrate the meaning of the equality of quadratic forms, we can compute $Q(\\vec{x})$ for $\\vec{x} = (2,-2)$ using the new quadratic form. First, since $\\vec{x} = P\\vec{y}$, $$\\vec{y} = P^{-1}\\vec{x} = P^T\\vec{x}$$so, $$\\vec{y} =\\begin{bmatrix}2/\\sqrt{5} &amp; -1/\\sqrt{5}\\\\1/\\sqrt{5} &amp; 2/\\sqrt{5}\\end{bmatrix}\\begin{bmatrix}2\\\\-2\\end{bmatrix}=\\begin{bmatrix}6/\\sqrt{5} \\\\-2/\\sqrt{5}\\end{bmatrix}$$Hence, $3y_1^2-7y_2^2 = 16$ A Geometric View of Principal Axes$$\\vec{x}^TA\\vec{x} = c, c : constant, A:symmetric \\tag{3}$$If $A$ is a diagonal matrix（对角矩阵）, the graph is in standard position, such as in Figure 2. If $A$ is not a diagonal matrix（不是对角矩阵）, the graph of equation (3) is rotated out of standard position, as in Figure 3. Finding the principal axes (determined by the eigenvectors of $A$) amounts to finding a new coordinate system with respect to which the graph is in standard position. The hyperbola in Figure 3(b) is the graph of the equation $\\vec{x}^TA\\vec{x} = 16$, where $A$ is the matrix in above Example$$A =\\begin{bmatrix}1 &amp; -4\\\\-4 &amp; -5\\end{bmatrix}$$The positive $y_1-axis$ in Figure 3(b) is in the direction of the first column of the matrix $P$, and the positive $y_2-axis$ is in the direction of the second column of $P$ .$$P =\\begin{bmatrix}2/\\sqrt{5} &amp; 1/\\sqrt{5}\\\\-1/\\sqrt{5} &amp; 2/\\sqrt{5}\\end{bmatrix}$$ Classifying Quadratic Forms A quadratic form $Q$ is:a. positive definite（正定） if $Q(\\vec{x}) &gt; 0$ for all $\\vec{x} \\neq \\vec{0}$b. negative definite（负定） if $Q(\\vec{x}) &lt; 0$ for all $\\vec{x} \\neq \\vec{0}$c. indefinite if $Q(\\vec{x})$ assumes both positive and negative values. $Q(\\vec{x})$ is said to be positive semidefinite if $Q(\\vec{x}) \\geq 0$ for all $\\vec{x}$, and to be negative semidefinite if$Q(\\vec{x}) \\leq 0$ for all $\\vec{x}$. Let $A$ be an $n \\times n$ symmetric matrix. Then a quadratic form $\\vec{x}^TA\\vec{x}$ is:a. positive definite if and only if the eigenvalues of $A$ are all positive,b. negative definite if and only if the eigenvalues of $A$ are all negative, orc. indefinite if and only if $A$ has both positive and negative eigenvalues. Constrained Optimization（有约束的优化）When a quadratic form $Q$ has no cross-product terms, it is easy to find the maximum and minimum of $Q(\\vec{x})$ for $\\vec{x}^T\\vec{x} = 1$. Example1:Find the maximum and minimum values of $Q(\\vec{x}) = 9x_1^2 + 4x_2^2 + 3x_3^2$ subject to the constraint $\\vec{x}^T\\vec{x} = 1$. Solution: $$\\vec{x}^T\\vec{x} = 1\\iff ||\\vec{x}|| = 1 \\iff ||\\vec{x}||^2 = 1 \\iff x:unit-vector$$ Since $x_2^2$ and $x_3^2$ are nonnegative, note that $$4x_2^2 \\leq 9x_2^2, and \\space 3x_3^2 \\leq 9x_3^2$$hence, $$\\begin{aligned}Q(\\vec{x}) &amp;= 9x_1^2 + 4x_2^2 + 3x_3^2 \\\\ &amp;\\leq 9x_1^2 + 9x_2^2 + 9x_3^2 \\\\ &amp;= 9\\end{aligned}$$So the maximum value of $Q(\\vec{x})$ cannot exceed $9$ when $\\vec{x}$ is a unit vector. To find the minimum value of $Q(\\vec{x})$, observe that $$9x_1^2 \\geq 3x_1^2, and \\space 4x_2^2 \\geq 3x_2^2$$hence, $$\\begin{aligned}Q(\\vec{x}) \\geq 3x_1^2 + 3x_2^2 + 3x_3^2 = 3\\end{aligned}$$ that the matrix of the quadratic form $Q(\\vec{x})$ has eigenvalues $9, 4, and 3$ and that the greatest and least eigenvalues equal, respectively, the (constrained) maximum and minimum of $Q(\\vec{x})$. The same holds true for any quadratic form, as we shall see.（最值和特征值相关） Example2:Let $A = \\begin{bmatrix} 3 &amp; 0 \\\\0 &amp; 7\\end{bmatrix}$, and let $Q(\\vec{x}) = \\vec{x}^TA\\vec{x}$ for $\\vec{x}$ in $R^2$. Figure 1 displays the graph of $Q$. Figure 2 shows only the portion of the graph inside a cylinder; the intersection of the cylinder with the surface is the set of points $(x_1,x_2,z)$such that $z = Q(x_1,x_2)$ and $x_1^2 + x_2^2 = 1$. Geometrically, the constrained optimization problem is to locate the highest and lowest points on the intersection curve. The two highest points on the curve are $7$ units above the $x_1x_2-plane$, occurring where $x_1 = 0$ and $x_2 = \\pm 1$. These points correspond to the eigenvalue $7$ of $A$ and the eigenvectors $\\vec{x} = (0, 1) and -\\vec{x} = (0,-1)$. Similarly, the two lowest points on the curve are $3$ units above the $x_1x_2-plane$. They correspond to the eigenvalue $3$ and the eigenvectors $(1, 0) and (-1,0)$. let$$m = min\\lbrace \\vec{x}^TA\\vec{x} : ||\\vec{x}|| = 1\\rbrace\\\\M = max\\lbrace \\vec{x}^TA\\vec{x} : ||\\vec{x}|| = 1\\rbrace\\tag{2}$$Let $A$ be a symmetric matrix, and define $m$ and $M$ as in (2). Then $M$ is the greatest eigenvalue $\\lambda_1$ of $A$ and $m$ is the least eigenvalue of $A$. The value of $\\vec{x}^TA\\vec{x}$ is $M$ when $\\vec{x}$ is a unit eigenvector $\\vec{u_1}$ corresponding to $M$. The value of $\\vec{x}^TA\\vec{x}$ is $m$ when $\\vec{x}$ is a unit eigenvector corresponding to $m$. Then the maximum value of $\\vec{x}^TA\\vec{x}$ subject to the constraints$$\\vec{x}^T\\vec{x} = 1, \\vec{x}^T\\vec{u_1} = 0$$is the second greatest eigenvalue, $\\lambda_2$, and this maximum is attained when $\\vec{x}$ is an eigenvector $\\vec{u_2}$ corresponding to $\\lambda_2$. Let $A$ be a symmetric $n \\times n$ matrix with an orthogonal diagonalization $A = PDP^{-1}$, where the entries on the diagonal of $D$ are arranged so that $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\lambda_n$ and where the columns of $P$ are corresponding unit eigenvectors $\\vec{u_1}, \\cdots, \\vec{u_n}$. Then for $k = 2, \\cdots, n$, the maximum value of $x^TAx$ subject to the constraints$$\\vec{x}^T\\vec{x} = 1, \\vec{x}^T\\vec{u_1} = 0, \\cdots, \\vec{x}^T\\vec{u_{k-1}} = 0$$is the eigenvalue $\\lambda_k$, and this maximum is attained at $\\vec{x} = \\vec{u_k}$. The Singular Value DecompositionUnfortunately, as we know, not all matrices can be factored as $A = PDP^{-1}$ with $D$ diagonal. However, a factorization $A = QDP^{-1}$ is possible for any $m \\times n$ matrix $A$. A special factorization of this type, called the singular value decomposition, is one of the most useful matrix factorizations in applied linear algebra. For a symmetric $A$, if $A\\vec{x} = \\lambda \\vec{x}$ and $||\\vec{x}|| = 1$, then $$||A\\vec{x}|| = ||\\lambda \\vec{x}|| = |\\lambda| ||\\vec{x}|| = |\\lambda|$$If $\\lambda_1$ is the eigenvalue with the greatest magnitude, then a corresponding unit eigenvector $\\vec{v_1}$ identifies a direction in which the stretching effect of $A$ is greatest. For example: If $A = \\begin{bmatrix}4 &amp; 11 &amp; 14\\\\8 &amp; 7 &amp; -2\\end{bmatrix}$, then the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ maps the unit sphere $\\lbrace \\vec{x} : ||\\vec{x}|| = 1\\rbrace$ in $R^3$ onto an ellipse in $R^2$, shown in Figure 1. Find a unit vector $\\vec{v}$ at which the length $||A\\vec{x}||$ is maximized, and compute this maximum length. Solution: $$||A\\vec{x}||^2 = (A\\vec{x})^T(A\\vec{x}) = \\vec{x}^T(A^TA)\\vec{x}$$$A^TA$ is a symmetric matrix, So the problem now is to maximize the quadratic form $\\vec{x}^T(A^TA)\\vec{x}$ subject to the constraint $||\\vec{x}|| = 1$. The eigenvalues of $A^TA$ are $\\lambda_1 = 360, \\lambda_2 = 90, \\lambda_3 = 0$. Corresponding unit eigenvectors are, respectively: $$\\vec{v_1} = \\begin{bmatrix}1/3 \\\\ 2/3 \\\\ 2/3\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-2/3 \\\\ -1/3 \\\\ 2/3\\end{bmatrix},\\vec{v_1} = \\begin{bmatrix}2/3 \\\\ -2/3 \\\\ 1/3\\end{bmatrix}$$The maximum value of $||A\\vec{x}||^2$ is 360, The vector $A\\vec{v_1}$ is a point on the ellipse in Figure 1 farthest from the origin, namely. $$A\\vec{v_1} = \\begin{bmatrix}18\\\\6\\end{bmatrix}$$the maximum value of $A||\\vec{x}||$ is $||A\\vec{v_1}|| = \\sqrt 360 = 6\\sqrt 10$. The Singular Values of an $m \\times n$ Matrix Let A be an $m \\times n$ matrix. Then $A^TA$ is symmetric and can be orthogonally diagonalized. Let $\\lbrace \\vec{v_1}, \\cdots, \\vec{v_n}\\rbrace$ be an orthonormal basis for $R^n$ consisting of eigenvectors of $A^TA$, and let $\\lambda_1, \\cdots, \\lambda_n$ be the associated eigenvalues of $A^TA$. Then, for $1 \\leq i \\leq n$,$$\\begin{aligned}||A\\vec{v_i}||^2 &amp;= (A\\vec{v_i})^TA\\vec{v_i} = \\vec{v_i}^TA^TA\\vec{v_i}\\\\&amp;= \\vec{v_i}^T(\\lambda_i\\vec{v_i})\\\\&amp;= \\lambda_i\\end{aligned}\\tag{2}$$The singular values of $A$ are the square roots of the eigenvalues of $A^TA$, the singular values of $A$ are the lengths of the vectors $A\\vec{v_1}, \\cdots, A\\vec{v_n}$. The Singular Value Decomposition（奇异值分解）The decomposition of $A$ involves an $m \\times n$ “diagonal” matrix $\\Sigma$ of the form$$\\Sigma = \\begin{bmatrix}D &amp; 0\\\\0 &amp; 0\\end{bmatrix}\\tag{3}$$where $D$ is an $r \\times r$ diagonal matrix. Let $A$ be an $m \\times n$ matrix with rank $r$. Then there exists an $m \\times n$ matrix $\\Sigma$ as in (3) for which the diagonal entries in $D$ are the first $r$ singular values of $A$, $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0$, and there exist an $m \\times m$ orthogonal matrix（正交矩阵） $U$ and an$n \\times n$ orthogonal matrix（正交矩阵） $V$ such that$$A = U \\Sigma V^T$$ but the diagonal entries of $\\Sigma$ are necessarily the singular values of $A$.The columns of $U$ in such a decomposition are called left singular vectors of $A$, and the columns of $V$ are called right singular vectors of A. For example: Construct a singular value decomposition of $A = \\begin{bmatrix}4 &amp; 11 &amp; 14\\\\8 &amp; 7 &amp; -2\\end{bmatrix}$ Solution: Step1. Find an orthogonal diagonalization of $A^TA$.（find the eigenvalues of $A^TA$ and a corresponding orthonormal set of eigenvectors.） Step2. Set up $V$ and $\\Sigma$. Arrange the eigenvalues of $A^TA$ in decreasing order. The corresponding unit eigenvectors, $\\vec{v_1}, \\vec{v_2}, \\vec{v_3}$, are the right singular vectors of $A$. $$V = \\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2} &amp; \\vec{v_3}\\end{bmatrix}=\\begin{bmatrix}1/3 &amp; -2/3 &amp; 2/3\\\\2/3 &amp; -1/3 &amp; -2/3\\\\2/3 &amp; 2/3 &amp; 1/3\\end{bmatrix}$$The square roots of the eigenvalues are the singular values. $$\\sigma_1 = 6\\sqrt{10},\\sigma_2 = 3\\sqrt{10},\\sigma_3 = 0$$ The nonzero singular values are the diagonal entries of $D$. $$D = \\begin{bmatrix}6\\sqrt 10 &amp; 0\\\\0 &amp; 3\\sqrt 10\\end{bmatrix}\\longrightarrow\\Sigma = \\begin{bmatrix} D &amp; 0\\end{bmatrix}= \\begin{bmatrix}6\\sqrt 10 &amp; 0 &amp; 0\\\\0 &amp; 3\\sqrt 10 &amp; 0\\end{bmatrix}$$ Step3. Construct $U$. When A has rank $r$, the first $r$ columns of $U$ are the normalized vectors obtained from $A\\vec{v_1}, \\cdots, A\\vec{v_r}$. In this example, $A$ has two nonzero singular values, so $rank A = 2$. Recall from equation (2), $||A\\vec{v_1}|| = \\sigma_1, ||A\\vec{v_2}|| = \\sigma_2$. Thus $$\\vec{u_1} = \\frac{1}{\\sigma_1}A\\vec{v_1} =\\begin{bmatrix}3/\\sqrt 10\\\\1/\\sqrt10\\end{bmatrix},\\vec{u_2} = \\frac{1}{\\sigma_2}A\\vec{v_2} =\\begin{bmatrix}1/\\sqrt 10\\\\-3/\\sqrt 10\\end{bmatrix}$$ Thus, $$A =\\begin{bmatrix}3/\\sqrt 10 &amp; 1/\\sqrt 10\\\\ 1/\\sqrt10 &amp; -3/\\sqrt 10\\end{bmatrix}\\begin{bmatrix}6\\sqrt 10 &amp; 0 &amp; 0\\\\0 &amp; 3\\sqrt 10 &amp; 0\\end{bmatrix}\\begin{bmatrix}1/3 &amp; -2/3 &amp; 2/3\\\\2/3 &amp; -1/3 &amp; -2/3\\\\2/3 &amp; 2/3 &amp; 1/3\\end{bmatrix}$$ Applications of the Singular Value Decomposition The Invertible Matrix Theorem (concluded)Let $A$ be an $n \\times n$ matrix. Then the following statements are each equivalent to the statement that $A$ is an invertible matrix.u. $(Col A)^\\perp = {0}$.v. $(Null A)^\\perp = R^n$.w. $RowA = R^n$.x. $A$ has $n$ nonzero singular values. More about SVD Decomposition by Numerical Analysis &gt;&gt; Applications to Image Processing and Statisticsfor example: An example of two-dimensional data is given by a set of weights and heights of $N$ college students. Let $X_j$ denote the observation vector in $R^2$ that lists the weight and height of the $j th$ student. If $w$ denotes weight and $h$ height, then the matrix of observations has the form $$\\begin{matrix}w_1 &amp; w_2 &amp; \\cdots &amp; w_N\\\\h_1 &amp; h_2 &amp; \\cdots &amp; h_N\\\\\\uparrow &amp; \\uparrow &amp; &amp;\\uparrow\\\\X_1 &amp; X_2 &amp; &amp; X_N\\end{matrix}$$The set of observation vectors can be visualized as a two-dimensional scatter plot. See Figure 1. Mean and Covariance（均值和协方差）let $\\begin{bmatrix} X_1 \\cdots X_N\\end{bmatrix}$ be a $p \\times N$ matrix of observations, such as described above. The sample mean, $M$, of the observation vectors $X_1 \\cdots X_N$ is given by $$M = \\frac{1}{N}(X_1 + \\cdots + X_n)$$ for $k = 1, \\cdots, N$, let$$\\hat{X_k} = X_k - M$$ The columns of the $p \\times N$ matrix$$B =\\begin{bmatrix}\\hat{X_1} \\hat{X_2} \\cdots \\hat{X_N}\\end{bmatrix}$$have a zero sample mean, and B is said to be in mean-deviation form. The (sample) covariance matrix is the $p \\times p$ matrix $S$ defined by$$S = \\frac{1}{N-1}BB^T$$Since any matrix of the form $BB^T$ is positive semidefinite, so is $S$. For example:Three measurements are made on each of four individuals in a random sample from a population. The observation vectors are$$X_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1\\end{bmatrix},X_2 = \\begin{bmatrix} 4 \\\\ 2 \\\\ 13\\end{bmatrix},X_3 = \\begin{bmatrix} 7 \\\\ 8 \\\\ 1\\end{bmatrix},X_4 = \\begin{bmatrix} 8 \\\\ 4 \\\\ 5\\end{bmatrix}$$Compute the sample mean and the covariance matrix. Solution:$$M = \\begin{bmatrix}5 \\\\ 4 \\\\ 5\\end{bmatrix},B = \\begin{bmatrix}4 &amp; -1 &amp; 2 &amp; 3 \\\\ -2 &amp; -2 &amp; 4 &amp; 0 \\\\ -4 &amp; 8 &amp; -4 &amp; 0\\end{bmatrix},S = \\begin{bmatrix}10 &amp; 6 &amp; 0 \\\\ 6 &amp; 8 &amp; -8 \\\\ 0 &amp; -8 &amp; 32\\end{bmatrix}$$To discuss the entries in $S = [S_{ij}]$, denote the coordinates of $X$ by $x_1, \\cdots, x_p$. For each $j = 1, \\cdots, p$, the diagonal entry $s_{jj}$ in $S$ is called the variance of $x_j$. The variance of $x_j$ measures the spread of the values of $x_j$ . The total variance of the data is the sum of the variances on the diagonal of $S$.$$\\lbrace{total \\space variance\\rbrace} = tr(S)$$ The entry $s_{ij}$ in $S$ for $i \\neq j$ is called the covariance of $x_i$ and $x_j$. (1,3)-entry in $S$ is $0$. Statisticians say that $x_1$ and $x_3$ are uncorrelated. Principal Component Analysis（主成分分析）Suppose the matrix $[X_1, \\cdots, X_N]$ is already in mean-deviation form. The goal of principal component analysis is to find an orthogonal $p \\times p$ matrix(正交矩阵) $P = [u_1 \\cdots u_p]$ that determines a change of variable, $X = PY$, or $$\\begin{bmatrix}x_1 \\\\x_2\\\\\\cdots \\\\x_p\\end{bmatrix} =\\begin{bmatrix}u_1 &amp; u_2 &amp; \\cdots &amp; u_p\\end{bmatrix}\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\cdots \\\\ y_p\\end{bmatrix}$$with the property that the new variables $y_1, \\cdots, y_p$ are uncorrelated and are arranged in order of decreasing variance. Recall the Quadratic form we learned above, $\\vec{x} = P\\vec{y}$, the columns of $P$ means the correspongding unit eigen vectors to eigen values(decreased) of $A$, so that, $Q = x^TAx = y^TDy, A = PDP^{-1} = PDP^T$, similar, Right? Notice that $Y_k$ is the coordinate vector of $X_k$ with respect to the columns of $P$, and $Y_k = P^{-1}X_k = P^T X_k$ for $k = 1, \\cdots, N$. Thus, the covariance matrix of $Y_1, \\cdots, Y_N$ is $D = P^TSP$. $D$ is a diagonal matrix. The unit eigenvectors $u_1, \\cdots, u_p$ of the covariance matrix $S$ are called the principal components of the data (in the matrix of observations). The first principal component is the eigenvector corresponding to the largest eigenvalue of $S$. Thus $y_1$ is a linear combination of the original variables $x_1, \\cdots, x_p$, using the entries in the eigenvector $u_1$ as weights. $$y_1 = u_1^TX = c_1x_1 + c_2x_2 + \\cdots + c_px_p$$ For example: The initial data for the multispectral image of Railroad Valley consisted of 4 million vectors in $R^3$. The associated covariance matrix is $$S = \\begin{bmatrix}2382.78 &amp; 2611.84 &amp; 2136.20\\\\2611.84 &amp; 3106.47 &amp; 2553.90\\\\2136.20 &amp; 2553.90 &amp; 2650.71\\end{bmatrix}$$Find the principal components of the data, and list the new variable determined by the first principal component. Solution: $$\\lambda_1 = 7614.23, \\lambda_1 = 427.63, \\lambda_1 = 98.10, \\\\$$$$\\vec{u_1} = \\begin{bmatrix} 0.5417 \\\\ 0.6295 \\\\ 0.5570\\end{bmatrix},\\vec{u_2} = \\begin{bmatrix} -0.4894 \\\\ -0.3026\\\\ 0.8179\\end{bmatrix},\\vec{u_3} = \\begin{bmatrix} 0.6834 \\\\ -0.7157 \\\\ 0.1441\\end{bmatrix}$$the variable for the first principal component is $$y_1 = 0.54x_1 + 0.63x_2 + 0.56x_3$$ This equation was used to create photograph (d) as follows. The variables $x_1, x_2, and x_3$ are the signal intensities in the three spectral bands. At each pixel in photograph (d), the gray scale value is computed from $y_1$, a weighted linear combination of $x_1, x_2, and x_3$. In this sense, photograph (d) “displays” the first principal component of the data. (Sensors aboard the satellite acquire seven simultaneous images of any region on earth to be studied. The sensors record energy from separate wavelength bands— three in the visible light spectrum and four in infrared and thermal bands.) As we shall see, this fact will permit us to view the data as essentially one-dimensional rather than three-dimensional. Reducing the Dimension of Multivariate DataIt can be shown that an orthogonal change of variables, $X = PY$, does not change the total variance of the data. (Roughly speaking, this is true because left-multiplication by $P$ does not change the lengths of vectors or the angles between them.)(正交矩阵的性质) $$\\lbrace total \\space variance \\space of \\space x_1, \\cdots, x_p\\rbrace=\\lbrace total \\space variance \\space of \\space y_1, \\cdots, y_p\\rbrace=\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_p$$ In a sense, 93.5%(7614.23/(7614.23+427.63+98.10)) of the information collected by Landsat for the Railroad Valley region is displayed in photograph (d), with 5.3% in (e) and only 1.2% remaining for (f). which means that the points lie approximately along a line, and the data are essentially one-dimensional. Characterizations of Principal Component VariablesThe singular value decomposition is the main tool for performing principal component analysis in practical applications. if $B$ is a $p \\times N$ matrix of observations in mean-deviation form, and if $A = (1/\\sqrt{N-1})B^T$, then $A^TA$ is the covariance matrix $S$. The squares of the singular values of $A$ are the $p$ eigenvalues of $S$. The right singular vectors of $A$ are the principal components of the data. Iterative calculation of the $SVD$ of $A$ is faster and more accurate than an eigenvalue decomposition of $S$.","link":"/Math/Linear-Algebra/Algebra-C7-Symmetric-Matrices-And-Quadratic-Forms/"},{"title":"Algebra-C1-Linear-Equation-in-Linear-Algebra","text":"Keywords: Aumented matrix, Linear Independence, Linear Transformation This is the Chapter1 ReadingNotes from book Linear Algebra and its Application. System of Linear EquationsLinear Equation:$$a_1 x_1 + a_2 x_2 + … + a_n x_n = b$$ NonLinear Equation:$$4 x_1 - 5 x_2 = x_1x_2$$ Linear System$$\\begin{cases}&amp;2 x_1 - x_2 + 1.5 x_3 = 8 \\\\&amp;x_1 - 4 x_3 = -7\\end{cases}\\tag{2}$$ Solution setThe set of all possible solutions is called the solution set of the linear system. e.g. (5, 6.5, 3) is a solution of system(2) Two linear systems are called equivalent if they have the same solution set.A system of linear equations has no solution, or exactly one solution, or infinitely many solutions Augmented Matrix$$\\begin{cases}x_1 - 2 x_2 + x_3 = 0\\\\2x_2 - 8x_3 = 8 \\\\5x_1 - 5x_3 = 10\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; -2 &amp; 1 &amp; 0 \\\\0 &amp; 2 &amp; -8 &amp; 8\\\\5 &amp; 0 &amp; -5 &amp; 10\\end{bmatrix}$$ Solving a Linear SystemThe procedure can also be called simplify the augmented matrix(化简增广矩阵的过程就是在求线性方程组的解) $$\\begin{cases}x_1 = 1, \\\\x_2 = 0, \\\\x_3 = -1\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 1 \\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; -1\\end{bmatrix}$$ ELEMENTARY ROW OPERATIONS (Replacement) Replace one row by the sum of itself and a multiple of another row (Interchange) Interchange two rows. (Scaling) Multiply all entries in a row by a nonzero constant Existence and Uniqueness Questionsthis system solution is unique, this system is consistent:$$\\begin{cases}x_1 - 2 x_ 2 + x_3 = 0\\\\x_2 - 4x_3 = 4\\\\x_3 = -1\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; -2 &amp; 1 &amp; 0 \\\\0 &amp; 1 &amp; -4 &amp; 4\\\\0 &amp; 0 &amp; 1 &amp; -1\\end{bmatrix}$$ Row Reduction and Echelon Formsa nonzero row or columna row or column that contains at least one nonzero entry leading entrya leading entry of a row refers to the leftmost nonzero entry (in a nonzero row) A rectangular matrix is in echelon form (or row echelon form) if it has the following three properties: All nonzero rows are above any rows of all zeros. Each leading entry of a row is in a column to the right of the leading entry of the row above it. All entries in a column below a leading entry are zeros.If a matrix in echelon form satisfies the following additional conditions, then it is in reduced echelon form (or reduced row echelon form): The leading entry in each nonzero row is 1. Each leading 1 is the only nonzero entry in its column. 行阶梯矩阵 row echelon form$$\\begin{bmatrix}2 &amp; -3 &amp; 2 &amp; 1 \\\\0 &amp; 1 &amp; -4 &amp; 8\\\\0 &amp; 0 &amp; 0 &amp; 5/2\\end{bmatrix}$$ 最简阶梯矩阵 reduced echelon form$$\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 29 \\\\0 &amp; 1 &amp; 0 &amp; 16\\\\0 &amp; 0 &amp; 1 &amp; 3\\end{bmatrix}$$ Pivot PositonsA pivot position in a matrix A is a location in A that corresponds to a leading 1in the reduced echelon form of A. A pivot column is a column of A that containsa pivot position I think the leading entry is pivot postion.只是pivot postions是在最简阶梯矩阵的语境下的 Solutions of Linear SystemsThe variables $x_1$ and $x_2$ corresponding to pivot columns in the matrix are called basic variables.The other variable, $x_3$, is called a free variable. $$\\begin{cases}x_1 - 5 x_ 3 = 1 \\\\x_2 + x_3 = 4\\\\0 = 0\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; 0 &amp; -5 &amp; 1 \\\\0 &amp; 1 &amp; 1 &amp; 4\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ Augumented matrix has solution Existence and Uniqueness TheoremA linear system is consistent if and only if the rightmost column of the augmented matrix is not a pivot column—that is, if and only if an echelon form of the augmented matrix has no row of the form$$\\begin{bmatrix}0 &amp; \\cdots &amp;0 &amp; b\\end{bmatrix} with \\space b\\space nonzero$$If a linear system is consistent, then the solution set contains either (i) a unique solution, when there are no free variables, or (ii) infinitely many solutions, when there is at least one free variable. Vector EquationsVector in $R^n$if $n$ is a positive integer, $R^n$ denotes the collection of all lists (or ordered n-tuples) of $n$ real numbers, usually written as $n \\times 1$ column matrics, such as $$\\vec{a} = \\begin{bmatrix} u_1\\\\u_2\\\\\\cdots\\\\u_3\\end{bmatrix}$$ Linear CombinationsGiven vectors $\\vec{v_1}, \\vec{v_2}, \\cdots, \\vec{v_p}$ in $R^n$ and given scalars $c_1, c_2, \\cdots, c_p$, the vector $\\vec{y}$ defined by $$\\vec{y} = c_1\\vec{v_1} + \\cdots + c_p\\vec{v_p}$$ is called a linear combination of $\\vec{v_1}, \\cdots, \\vec{v_p}$ with weights $c_1, \\cdots, c_p$ Linear Combinations &amp; MatricsA vector equation $$x_1\\vec{a_1} + x_2\\vec{a_2} + \\cdots + x_n\\vec{a_n} = \\vec{b}$$ has the same solution set as the linear system whose augmented matrix is $$\\begin{bmatrix}\\vec{a_1} &amp; \\vec{a_2} &amp; \\cdots &amp; \\vec{a_n} &amp; \\vec{b}\\end{bmatrix}\\tag{5}$$ In particular, $\\vec{b}$ can be generated by a linear combination of $\\vec{a_1}\\cdots\\vec{a_n}$ if and only if there exists a solution to the linear system corresponding to the matrix(5) Span{$\\vec{v_1}, \\cdots, \\vec{v_p}$}Asking whether a vector $\\vec{b}$ is in Span{$\\vec{v_1}, \\cdots, \\vec{v_p}$} amounts to asking whether the vector equation $$x_1\\vec{v_1} + \\cdots + x_p\\vec{v_p} = \\vec{b}$$ has a solution, or, equivalently, asking whether the linear system with augmented matrix $$\\begin{bmatrix}\\vec{v_1} &amp; \\cdots &amp; \\vec{v_p} &amp; \\vec{b}\\end{bmatrix}$$ has a solution A Geometric Description of Span{$\\vec{v}$} and Span{$\\vec{u}, \\vec{v}$}Let $\\vec{v}$ be a nonzero vector in $R^3$. Then Span{$\\vec{v}$} is the set of all scalar multiples of $\\vec{v}$, which is the set of points on the line in $R^3$ through $\\vec{v}$ and $\\vec{0}$. See Figure 10. If $\\vec{u}$ and $\\vec{v}$ are nonzero vectors in $R^3$, with $\\vec{v}$ not a multiple of $\\vec{u}$, then Span{$\\vec{u},\\vec{v}$}is the plane in $R^3$ that contains $\\vec{u}$, $\\vec{v}$, and $\\vec{0}$. In particular, Span{$\\vec{u},\\vec{v}$} contains the line in $R^3$ through $\\vec{u}$ and $\\vec{0}$ and the line through $\\vec{v}$ and $\\vec{0}$. See Figure 11. The Matrix Equation $Ax = b$if $A$ is an $m \\times n$ matrix, with columns $\\vec{a_1}, \\cdots, \\vec{a_n}$, and if $\\vec{x}$ is in $R^n$, then the product of $A$ and $\\vec{x}$, denoted by $A\\vec{x}$, is the linear combination of the columns of $A$ using the corresponding entries in $\\vec{x}$ as weights; that is, $$A\\vec{x} =\\begin{bmatrix}\\vec{a_1} &amp; \\vec{a_2} &amp; \\cdots \\vec{a_n}\\end{bmatrix}\\begin{bmatrix}x_1\\\\ \\cdots \\\\ x_n\\end{bmatrix}=x_1\\vec{a_1}+x_2\\vec{a_2}+\\cdots+x_n\\vec{a_n}$$ 以下四种写法是等价的，全部转化为求增广矩阵的Solution： $$\\begin{cases}x_1 + 2x_2 - x_3 = 4\\\\-5x_2 + 3x_3 = 1 \\tag{1}\\end{cases}$$ $$x_1\\begin{bmatrix}1\\\\0\\end{bmatrix} +x_2\\begin{bmatrix}2\\\\-5\\end{bmatrix} +x_3\\begin{bmatrix}-1\\\\3\\end{bmatrix} =\\begin{bmatrix}4\\\\1\\end{bmatrix} \\tag{2}$$ $$\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -5 &amp; 3\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}4 \\\\ 1\\end{bmatrix} \\tag{3}$$ $$\\begin{bmatrix}1 &amp; 2 &amp; -1 &amp; 4\\\\0 &amp; -5 &amp; 3 &amp; 1\\end{bmatrix} \\tag{4}$$ 计算机存储矩阵，用连续的空间存储提高效率To optimize a computer algorithm to compute $Ax$, the sequence of calculationsshould involve data stored in contiguous memory locations. The most widelyused professional algorithms for matrix computations are written in Fortran, alanguage that stores a matrix as a set of columns. Such algorithms compute $Ax$ asa linear combination of the columns of $A$. In contrast, if a program is written inthe popular language C, which stores matrices by rows, Ax should be computedvia the alternative rule that uses the rows of $A$ Solution sets of linear systemsHomogeneous Linear SystemsA system of linear equations is said to be homogeneous if it can be written in the form $A\\vec{x} = 0 $, where A is an $m \\times n$ matrix and $\\vec{0}$ is the zero vector in $R^m$. $\\vec{x} = \\vec{0}$ is a trivial solution. The homogeneous equation $A\\vec{x} = 0 $ has a nontrivial solution if and only if the equation has at least one free variable. For Example:$$\\begin{cases}3x_1 + 5x_2 - 4x_3 = 0\\\\-3x_1 - 2x_2 + 4x_3 = 0\\\\6x_1 + x_2 - 8x_3 = 0 \\tag{1}\\end{cases}$$Solution:$$\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 0\\\\-3 &amp; 2 &amp; 4 &amp; 0\\\\6 &amp; 1 &amp; -8 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 0\\\\0 &amp; 3 &amp; 0 &amp; 0\\\\0 &amp; -9 &amp; 0 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 0\\\\0 &amp; 3 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ Since $x_3$ is a free variable, $A\\vec{x} = \\vec{0}$ has nontrivial solutions, continue the row reduction of $\\begin{bmatrix}A &amp; \\vec{0}\\end{bmatrix}$ to reduced echelon form: $$\\begin{bmatrix}1 &amp; 0 &amp; -4/3 &amp; 0\\\\0 &amp; 3 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\rightarrow\\begin{matrix}x_1-\\frac{4}{3}x_3 = 0\\\\x_2 = 0\\\\0 = 0\\end{matrix}$$ the general solution of $A\\vec{x} = \\vec{0}$ has the form: $$\\vec{x} = \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}\\frac{4}{3}x_3\\\\0\\\\x_3\\end{bmatrix}=x_3\\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix}=x_3\\vec{v},where\\space \\vec{v} = \\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix}$$ Geometrically, the solution set is a line through $\\vec{0}$ in $R^3$. See Figure1. Also, form as $\\vec{x} = x_3\\vec{v}$, we say that the solution is in parametric vector form. Solutions of Nonhomogeneous Linear SystemsFor example:$$\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 7\\\\-3 &amp; -2 &amp; 4 &amp; -1\\\\6 &amp; 1 &amp; -8 &amp; -4\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; -4/3 &amp; -1\\\\0 &amp; 1 &amp; 0 &amp; 2\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\tag{2}$$the general solution has the form:$$\\vec{x} = \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}-1+\\frac{4}{3}x_3\\\\2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}-1\\\\2\\\\0\\end{bmatrix} + x_3\\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix}=\\vec{p} + x_3\\vec{v},where\\space \\vec{v} = \\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix},\\vec{p} = \\begin{bmatrix}-1\\\\2\\\\0\\end{bmatrix}$$ Summary： $$Homogeneous: \\vec{x} = t\\vec{v}\\\\Nonhomogeneous: \\vec{x} = \\vec{p} + t\\vec{v}\\\\From \\space Geometry：translation$$ the solution set of $A\\vec{x} = \\vec{b}$ is a line through $\\vec{p}$ parallel to the solution set of $A\\vec{x} = \\vec{0}$. Figure 6 illustrates the case in which there are two free variables, the solution set is a plane, not a line. Application of linear systemsNetwork FlowFor Example: The network in Figure 2 shows the traffic flow (in vehicles per hour) over several one-way streets in downtown Baltimore during a typical early afternoon.Determine the general flow pattern for the network. Solution: Intersection Flow in Flow out A 300+500 = $x_1 + x2$ B $x_2 + x4$ = 300 + $x_3$ C 100 + 400 = $x_4 + x5$ D $x_1 + x5$ = 600 $$列方程，解方程如下：\\begin{matrix}x_1 + x_2 = 800\\\\x_2 - x_3 + x4 = 300\\\\x_4 + x_5 = 500\\\\x1 + x5 = 600\\\\x_3 = 400\\end{matrix}\\sim\\begin{cases}x_1 = 600 - x_5 \\\\x_2 = 200 + x_5 \\\\x_3 = 400 \\\\x_4 = 500 - x_5\\\\x_5 is free\\end{cases}$$ Linear independence线性相关（有非零解）和线性无关（只有零解） An indexed set of vectors {${\\vec{v_1}, \\cdots, \\vec{v_p}}$} in $R^n$ is said to be linearly independent if the vector equation$$x_1\\vec{v_1} + x_2\\vec{v_2} + \\cdots + x_p\\vec{v_p} = \\vec{0}$$has only the trivial solution. The set {${\\vec{v_1}, \\cdots, \\vec{v_p}}$} is said to be linealy dependent if there exist weights $c_1, \\cdots, c_p$, not all zeros, such that$$c_1\\vec{v_1} + c_2\\vec{v_2} + \\cdots + c_p\\vec{v_p} = \\vec{0}$$ The columns of a matrix $A$ are linearly independent if and only if the equation $A\\vec{x} = \\vec{0}$has only the trivial solution, 当然可以通过观察发现两组向量是否线性相关,比如:$$\\vec{v_1} = \\begin{bmatrix}3\\\\1\\end{bmatrix},\\space\\vec{v_2} = \\begin{bmatrix}6\\\\2\\end{bmatrix}\\rightarrow\\vec{v_2} = 2\\vec{v_1},linear\\space dependent$$ A set of two vectors {$\\vec{v_1},\\vec{v_2}$} is linearly dependent if at least one of the vectors is a multiple of the other. The set is linearly independent if and only if neither of the vectors is a multiple of the other. Sets of Two or More Vectors 如果至少能找到一个向量是其他向量的线性组合，那么这个向量集就是线性相关的 If a set contains more vectors than there are entries in each vector, then the set is linearly dependent. That is, any set {$\\vec{v_1}, \\cdots, \\vec{v_p}$} in $R^n$ is linearly dependent if p &gt; n $$n= 3, p = 5,\\overbrace{\\begin{bmatrix}* &amp; * &amp; * &amp; * &amp; * \\\\* &amp; * &amp; * &amp; * &amp; * \\\\* &amp; * &amp; * &amp; * &amp; * \\\\\\end{bmatrix}}^{p}$$ $$\\begin{bmatrix}2\\\\1\\end{bmatrix},\\begin{bmatrix}4\\\\-1\\end{bmatrix},\\begin{bmatrix}-2\\\\2\\end{bmatrix}linearly-dependent, cause\\space three\\space vectors,but\\space two\\space entries\\space in \\space each \\space vector$$ Introduction to Linear TransformationIn Computer Graphics, $A\\vec{x}$ is not related to linear combination. We think the matrix $A$ as an object that “acts” on a vector $\\vec{v}$ by multiplication to produce a new vector called $A\\vec{x}$. Matrix Transformation$$\\vec{x}\\mapsto A\\vec{x}$$ A projection transformation if $A = \\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0\\end{bmatrix}$, then the transformation $\\vec{x}\\mapsto A\\vec{x}$ projects points in $R^3$ onto the $x_1x_2-plane$, because$$\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}\\mapsto\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}x_1\\\\x_2\\\\0\\end{bmatrix}$$ A shear transformation if $A = \\begin{bmatrix}1 &amp; 3 \\\\0 &amp; 1 \\\\\\end{bmatrix}$, the transformation $T:R^2\\rightarrow R^2$ defined by $T(\\vec{x}) = A\\vec{x}$ is called shear transformation, because $$\\begin{bmatrix}0\\\\2\\\\\\end{bmatrix}\\mapsto\\begin{bmatrix}1 &amp; 3 \\\\0 &amp; 1 \\\\\\end{bmatrix}\\begin{bmatrix}0\\\\2\\\\\\end{bmatrix}=\\begin{bmatrix}6\\\\2\\end{bmatrix}，\\begin{bmatrix}2\\\\2\\end{bmatrix}\\mapsto\\begin{bmatrix}1 &amp; 3 \\\\0 &amp; 1\\end{bmatrix}\\begin{bmatrix}2\\\\2\\end{bmatrix}=\\begin{bmatrix}8\\\\2\\end{bmatrix}$$ A transformation (or mapping) $T$ is linear if:(i) $T(\\vec{u} + \\vec{v}) = T(\\vec{u}) + T(\\vec{v})$ for all $\\vec{u},\\vec{v}$ in the domain of $T$.(ii) $T(c\\vec{u}) = cT(\\vec{u})$ for all scalars $c$ and all $\\vec{u}$ in the domain of $T$. Linear transformations preserve the operations of vector addition and scalar multiplication. The Matrix of Linear TransformationLet $T: R^n \\rightarrow R^m$ be a linear transformation. Then there exists a unique matrix $A$ such that $$T(\\vec{x}) = A\\vec{x},for\\space all\\space x \\space in R^n$$ In fact, $A$ is the $m \\times n$ matrix whose $j-th$ column is the vector $T(\\vec{e_j})$, where $\\vec{e_j}$ is the $j-th$ column of the identity matrix in $R^n$: $$A = \\begin{bmatrix} T(\\vec{e_1}) \\cdots T(\\vec{e_n})\\end{bmatrix}\\tag{3}$$ the matrxi in (3) is called the standard matrix for the linear transformation T. For example: The columns of $I_2 = \\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix}$ are $\\vec{e_1} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ and $\\vec{e_2} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$. Suppose T is a linear transformation from $R^2$ into $R^3$ such that $$T(\\vec{e_1}) = \\begin{bmatrix}5 \\\\ -7 \\\\ 2\\end{bmatrix}and \\spaceT(\\vec{e_2}) = \\begin{bmatrix}-3 \\\\ 8 \\\\ 0\\end{bmatrix}$$ with no additional information, find a formula for the image of an arbitrary $\\vec{x}$ in $R^2$ Solution：$$\\vec{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}= x_1\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + x_2\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}= x_1\\vec{e_1} + x_2\\vec{e_2}\\\\T(\\vec{x}) = x_1T(\\vec{e_1}) + x_2T(\\vec{e_2})= \\begin{bmatrix}5x_1 - 3x_2 \\\\ -7x_1+8x_2 \\\\ 2x_1+0\\end{bmatrix}\\\\T(\\vec{x}) = \\begin{bmatrix}T(\\vec{e_1}) &amp; T(\\vec{e_2})\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}= A\\vec{x}$$ A Rotation transformation 根据上文公式可以快速推导出2D旋转矩阵 $\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ rotates into $\\begin{bmatrix}\\cos\\psi \\\\ \\sin\\psi\\end{bmatrix}$, and $\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ rotates into $\\begin{bmatrix}-\\sin\\psi \\ \\cos\\psi\\end{bmatrix}$ so the rotation matrix is $\\begin{bmatrix}\\cos\\psi &amp; -\\sin\\psi\\\\ \\sin\\psi &amp; \\cos\\psi\\end{bmatrix}$ 同理，其他如Reflection， expansion…变换都可以根据基向量进行推导 Let $T: R^n \\rightarrow R^m$ be a linear transformation. Then $T$ is one-to-one if and only if the equation $T(\\vec{x}) = \\vec{0}$ has only the trivial solution. Linear models in business,science, and engineeringDifference Equations(差分方程)Several features of the system are each measured at discrete time intervals, producing a sequence of vectors $\\vec{x_0},\\vec{x_1},\\vec{x_2},\\cdots,\\vec{x_k}$ The entries in $\\vec{x_k}$ provide information about the state of the system at the time of the $k-th$ measurement. If there is a matrix A such that $\\vec{x_1} = A\\vec{x_0}, \\vec{x_2} = A\\vec{x_1}$ and, in general, $$\\vec{x_{k+1}} = A\\vec{x_k}, k = 0,1,2,…\\tag{5}$$ then (5) is called a linear difference equation (or recurrence relation). 比如, 一个地区的人口增长可以用这种模型简单的表示 思考：矩阵快速幂怎么求？首先考虑普通的快速幂: $$\\begin{aligned}4 ^ {11} &amp;= 4^{(1011)_2}\\\\&amp;= 4^{2^0}\\cdot4^{2^1}\\cdot4^{2^3}\\\\4^{2^1} &amp;= 4^{2^0} \\cdot 4^{2^0}\\\\4^{2^2} &amp;= 4^{2^1} \\cdot 4^{2^1}\\\\4^{2^3} &amp;= 4^{2^2} \\cdot 4^{2^2}\\end{aligned}$$ the c++ code is: 12345678910111213//get a^k mod pint qmi(int a, int k, int p){ int res = 1 % p; int t = a; //此时相当于a^(2^0) while(k) { if(k &amp; 1) res = (LL)res * t % p; t = (LL)t * t % p; k &gt;&gt;= 1; } return res;} 接下来考虑矩阵快速幂： for example: 如何快速计算Fibonacci的f(1000)？ $$Fibonacci\\\\f(0) = 0, f(1) = 1\\\\f(n) = f(n-1) + f(n-2), n &gt; 1\\\\建立矩阵方程如下：let \\space F(n) = \\begin{bmatrix}f(n)\\\\f(n+1)\\end{bmatrix}\\\\AF(n) = F(n+1)\\rightarrowA\\begin{bmatrix}f(n)\\\\ f(n+1)\\end{bmatrix} =\\begin{bmatrix}f(n+1)\\\\ f(n+2)\\end{bmatrix}\\rightarrowA = \\begin{bmatrix}0 &amp; 1\\\\1 &amp; 1\\end{bmatrix}\\RightarrowF(n) = A^nF(0), F(0) = \\begin{bmatrix}0\\\\1\\end{bmatrix}$$ 1234567891011121314//get A^k * F_0 % p//示例使用左乘int qmi(Matrix A, int k, int p, Vector F_0){ Vector res = F_0 % p; Matrix t = A; //A^(2^0) while(k) { if(k &amp; 1) res = t * res % p; t = t * t % p; k &gt;&gt;=1; } return res;}","link":"/Math/Linear-Algebra/Algebra-C1-Linear-Equation-in-Linear-Algebra/"},{"title":"Algebra-C8-The-Geometry-of-Vector-Spaces","text":"Keywords: Affine Combinations, Barycentric Coordinates,Bézier Curves This is the Chapter8 ReadingNotes from book Linear Algebra and its Application. Affine CombinationsAn affine combination of vectors is a special kind of linear combination.Given vectors (or “points”) $v_1, v_2, \\cdots, v_p$ in $R^n$ and scalars $c_1, \\cdots, c_p$, an affine combination of $v_1, v_2, \\cdots, v_p$ is a linear combination $$c_1v_1 + \\cdots + c_pv_p,\\\\ c_1 + \\cdots + c_p = 1$$ A point $y$ in $R^n$ is an affine combination of $v_1, \\cdots v_p$ in $R^n$ if and only if $(y - v_1)$ is a linear combination of the translated points $v_2 - v_1, \\cdots, v_p - v_1$. Affine Independence An indexed set of points $v_1, … v_p$ in $R^n$ is affinely dependent if there exist real numbers $c_1, .. c_p$, not all zero, such that$$c_1 + \\cdots + c_p = 0 \\space and \\space c_1v_1 + \\cdots + c_pv_p = 0$$Otherwise, the set is affinely independent. Given an indexed set $S = \\lbrace v_1, \\cdots, v_p \\rbrace$ in $R^n$, with $p \\geq 2$,the following statements are logically equivalent. That is, either they are all true statements or they are all false.a. $S$ is affinely dependent.b. One of the points in $S$ is an affine combination of the other points in $S$.c. The set $\\lbrace v_2 - v_1, \\cdots v_p - v_1\\rbrace$ in $R^n$ is linearly dependent.d. The set $\\widetilde{v_1}, \\cdots, \\widetilde{v_p}$ of homogeneous forms in $R^{n+1}$ is linearly dependent. For example: let $\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 7\\end{bmatrix}, \\vec{v_2} = \\begin{bmatrix} 2 \\\\ 7 \\\\ 6.5\\end{bmatrix}, \\vec{v_3} = \\begin{bmatrix} 0 \\\\ 4 \\\\ 7\\end{bmatrix}, \\vec{v_4} = \\begin{bmatrix} 0 \\\\ 14 \\\\ 6\\end{bmatrix}$, and $S = \\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_3}, \\vec{v_4} \\rbrace$. Determine whether $S$ is affinely independent. Solution: $$\\vec{v_2} -\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 4 \\\\ -0.5\\end{bmatrix},\\vec{v_3} -\\vec{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0\\end{bmatrix},\\vec{v_4} -\\vec{v_1} = \\begin{bmatrix} -1 \\\\ 11 \\\\ -1\\end{bmatrix}\\Rightarrow\\begin{bmatrix}1 &amp; -1 &amp; -1\\\\4 &amp; 1 &amp; 11\\\\-0.5 &amp; 0 &amp; -1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -1 &amp; -1\\\\0 &amp; 5 &amp; 15\\\\0 &amp; 0 &amp; 0\\end{bmatrix}$$the columns are linearly dependent because not every column is a pivot column, so $v_2 - v_1, v_3 - v_1, v_4 - v_1$ are linearly dependent. Thus, $v_1, v_2, v_3, v_4$ is affinely dependent. $$\\vec{v_4} - \\vec{v_1} = 2(\\vec{v_2} - \\vec{v_1}) + 3(\\vec{v_3} - \\vec{v_1})\\\\\\vec{v_4} = -4\\vec{v_1} + 2\\vec{v_2} + 3\\vec{v_3}$$ Barycentric Coordinates let $S = \\lbrace \\vec{v_1},\\cdots, \\vec{v_k}\\rbrace$ be an affinely independent set in $R^n$. Then each $\\vec{p}$ in aff S has a unique representation as an affine combination of $\\vec{v_1},\\cdots,\\vec{v_k}$. That is, for each $\\vec{p}$ there exists a unique set of scalars $c_1, \\cdots, c_k$ such that$$\\vec{p} = c_1\\vec{v_1} + \\cdots + c_k\\vec{v_k}, and \\space c_1 + \\cdots + c_k = 1 \\tag{7}$$the coefficients $c_1 \\cdots c_k$ in the unique representation (7) of $\\vec{p}$ are called the barycentric (or, sometimes, affine) coordinates of $\\vec{p}$ Observe that (7) is equivalent to the single equation $$\\begin{bmatrix}\\vec{p}\\\\1\\end{bmatrix} = c_1\\begin{bmatrix} \\vec{v_1}\\\\1 \\end{bmatrix} + \\cdots + c_k\\begin{bmatrix} \\vec{v_k }\\\\1\\end{bmatrix}\\tag{8}$$ involving the homogeneous forms of the points. Row reduction of the augmented matrix $\\begin{bmatrix} \\widetilde{\\vec{v_1}} &amp; \\cdots \\widetilde{\\vec{v_k}} &amp; \\widetilde{\\vec{p}}\\end{bmatrix}$ for (8) produces the barycentric coordinates of $\\vec{p}$. For Example: Let $\\vec{a} = \\begin{bmatrix} 1 \\\\ 7\\end{bmatrix}, \\vec{b} = \\begin{bmatrix} 3 \\\\ 0\\end{bmatrix}, \\vec{c} = \\begin{bmatrix} 9 \\\\ 3\\end{bmatrix}, and \\space \\vec{p} = \\begin{bmatrix} 5 \\\\ 3\\end{bmatrix}$ Find the barycentric coordinates of $\\vec{p}$ determined by the affinely independent set $\\lbrace \\vec{a}, \\vec{b}, \\vec{c}\\rbrace$. Solution: $$\\begin{bmatrix}\\widetilde{\\vec{a}} &amp; \\widetilde{\\vec{b}} &amp; \\widetilde{\\vec{c}} &amp; \\widetilde{\\vec{p}}\\end{bmatrix}=\\begin{bmatrix}1 &amp; 3 &amp; 9 &amp; 5\\\\7 &amp; 0 &amp; 3 &amp; 3\\\\1 &amp; 1 &amp; 1 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; \\frac{1}{4}\\\\7 &amp; 0 &amp; 3 &amp; \\frac{1}{3}\\\\1 &amp; 1 &amp; 1 &amp; \\frac{5}{12}\\end{bmatrix}$$so, $\\vec{p} = \\frac{1}{4}\\vec{a} + \\frac{1}{3}\\vec{b} + \\frac{5}{12}\\vec{c}$ Barycentric Coordinates in Computer GraphicsBarycentric coordinates provide the tool for smoothly interpolating the vertex information over the interior of a triangle. The color and other information displayed in the pixel on the screen should come from the object that the ray first intersects. See Figure 7. When the objects in the graphics scene are approximated by wire frames with triangular patches, the hidden surface problem can be solved using barycentric coordinates. For example: Let$$\\vec{v_1} =\\begin{bmatrix}1\\\\1\\\\-6\\end{bmatrix},\\vec{v_2} =\\begin{bmatrix}8\\\\1\\\\-4\\end{bmatrix},\\vec{v_3} =\\begin{bmatrix}5\\\\11\\\\-2\\end{bmatrix},\\vec{a} =\\begin{bmatrix}0\\\\0\\\\10\\end{bmatrix},\\vec{b} =\\begin{bmatrix}0.7\\\\0.4\\\\-3\\end{bmatrix}$$and $x(t) = a + tb$ for $t \\geq 0$. Find the point where the ray $x(t)$ intersects the plane that contains the triangle with vertices $v_1, v_2, and v_3$. Is this point inside the triangle? Solution: The plane is $aff \\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_3} \\rbrace$. The ray $x(t)$ intersects the plane when $c_2, c_3$, and $t$ satisfy $$(1-c_2-c_3)\\vec{v_1} + c_2\\vec{v_2} + c_3\\vec{v_3} = \\vec{a} + t\\vec{b} \\mapsto\\\\\\begin{bmatrix}\\vec{v_2} - \\vec{v_1} &amp; \\vec{v_3} - \\vec{v_1} &amp; -\\vec{b}\\end{bmatrix}\\begin{bmatrix}c_2 \\\\ c_3 \\\\ t\\end{bmatrix}=\\vec{a} - \\vec{v_1}$$ thus, get the solution and the intersection point is $$\\vec{x(5)} = \\vec{a} + 5\\vec{b} = \\begin{bmatrix} 3.5 \\\\ 2.0 \\\\ -5.0\\end{bmatrix}$$ The intersection point is inside the triangle because the barycentric weights for $\\vec{x(5)}$ are all positive. Convex Combinations（凸包） A convex combination of points $\\vec{v_1}, \\vec{v_2},\\cdots, \\vec{v_k}$ in $R^n$ is a linear combination of the form$$c_1\\vec{v_1} + c_2\\vec{v_2} + \\cdots + c_k\\vec{v_k}$$such that $c_1 + c_2 + \\cdots + c_k = 1$ and $c_i \\geq 0$ for all $i$. The set of all convex combinations of points in a set $S$ is called the convex hull of $S$, denoted by $conv S$. the point in $cov \\lbrace \\vec{v_1}, \\vec{v_2} \\rbrace$ may be written as $$\\vec{y} = (1-t)\\vec{v_1} + t\\vec{v_2} , 0 \\leq t \\leq 1$$which is a line segment between $\\vec{v_1}$ and $\\vec{v_2}$, hereafter denoted by $\\overline{\\vec{v_1}\\vec{v_2}}$ A set $S$ is convex if for each $\\vec{p}, \\vec{q} \\in S$, the line segment $\\overline{\\vec{p}\\vec{q}}$ is contained in $S$. Intuitively, a set $S$ is convex if every two points in the set can “see” each other without the line of sight leaving the set. Figure 1 illustrates this idea. consider a set $S$ that lies inside some large rectangle in $R^2$, and imagine stretching a rubber band around the outside of $S$. As the rubber band contracts around $S$, it outlines the boundary of the convex hull of $S$. Or to use another analogy, the convex hull of $S$ fills in all the holes in the inside of $S$ and fills out all the dents in the boundary of $S$. A set $S$ is convex if and only if every convex combination of points of $S$ lies in $S$. That is, $S$ is convex if and only if $S$ = $conv S$. More about ConvexHull Algorithms&gt;&gt; Hyperplanes（超平面）The key to working with hyperplanes is to use simple implicit descriptions, rather than the explicit or parametric representations of lines and planes used in the earlier work with affine sets. An implicit equation of a line in $R^2$ has the form $ax + by = d$. An implicit equation of a plane in $R^3$ has the form $ax + by + cz = d$. Both equations describe the line or plane as the set of all points at which a linear expression (also called a linear functional（线性泛函）) has a fixed value, d. A linear functional on $R^n$ is a linear transformation $f$ from $R^n$ into $R$. For eachscalar $d$ in $R$, the symbol $[f:d]$ denotes the set of all $\\vec{x}$ in $R^n$ at which the valueof $f$ is $d$. That is,$$[f:d] \\space is \\space the \\space set \\lbrace \\vec{x} \\in R^n : f(\\vec{x}) = d\\rbrace$$line: $x - 4y = 13$, is a hyperplane in $R^2$, it is the set of points at which the linear functional $f(x,y) = x - 4y$ has the value 13, the line is the set $[f:13]$ If $f$ is a linear functional on $R^n$, then the standard matrix of this linear transformation $f$ is a $1 \\times n$ matrix $A$, say $A = \\begin{bmatrix}a_1 &amp; a_2 &amp; \\cdots &amp; a_n \\end{bmatrix}$. So$$[f:0] \\Leftrightarrow \\lbrace \\vec{x} \\in R^n: A\\vec{x} = \\vec{0}\\rbrace \\Leftrightarrow NullA$$We know the relation between $A\\vec{x} = \\vec{0}$ and $A\\vec{x} = \\vec{b}$, thus $$[f:d] = [f:0] + \\vec{p}, \\vec{p} \\in [f:d]$$ $$[f:0] = \\lbrace \\vec{x} \\in R^n: \\vec{n} \\cdot \\vec{x} = 0\\rbrace$$ $\\vec{n}$ is called a normal vector（法向量） to $[f:0]$, sometimes it’s also called gradient（梯度）. For example: In $R^2$, give an explicit description of the line $x-4y = 13$ in parametric vector form. Solution: $$\\begin{aligned}\\vec{x} &amp;= \\begin{bmatrix}x \\\\ y\\end{bmatrix}= \\begin{bmatrix}13 + 4y \\\\ y\\end{bmatrix}\\\\&amp;= \\begin{bmatrix} 13 \\\\ 0\\end{bmatrix} + y \\begin{bmatrix} 4 \\\\ 1\\end{bmatrix} = \\vec{p} + y\\vec{q}\\end{aligned}$$ For example: Let $\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\vec{v_2} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 4\\end{bmatrix}, \\vec{v_3} = \\begin{bmatrix} 3 \\\\ 1 \\\\ 2\\end{bmatrix}$. Find an implicit description $[f : d]$ of the plane $H_1$ that passes through $\\vec{v_1}, \\vec{v_2}, \\vec{v_3}$. Solution: $H_1$ is parallel to a plane $H_0$ through the origin that contains the translated points $$\\vec{v_2} - \\vec{v_1} =\\begin{bmatrix}1 \\\\ -2 \\\\ 3\\end{bmatrix},\\vec{v_3} - \\vec{v_1} =\\begin{bmatrix}2 \\\\ 0 \\\\ 1\\end{bmatrix}$$ Since these two points are linearly independent, $H_0 = Span \\lbrace \\vec{v_2} - \\vec{v_1}, \\vec{v_3} - \\vec{v_1} \\rbrace$. Let $\\vec{n} = \\begin{bmatrix} a \\\\ b \\\\ c\\end{bmatrix}$ be the normal to $H_0$. $$\\begin{bmatrix} 1 &amp; -2 &amp; 3\\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c\\end{bmatrix} = 0,\\begin{bmatrix} 2 &amp; 0 &amp; 1\\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c\\end{bmatrix} = 0\\Longrightarrow \\\\\\begin{bmatrix} 1 &amp; -2 &amp; 3 &amp; 0\\\\ 2 &amp; 0 &amp; 1 &amp; 0\\end{bmatrix}\\Longrightarrow \\\\\\vec{n} = \\begin{bmatrix} -2 \\\\ 5 \\\\ 4\\end{bmatrix}$$or$$\\begin{aligned}\\vec{n} &amp;= (\\vec{v_2} - \\vec{v_1}) \\times (\\vec{v_3} - \\vec{v_1})\\\\&amp;= \\left|\\begin{array}{cccc} 1 &amp; 2 &amp; i \\\\ -2 &amp; 0 &amp; j \\\\ 3 &amp; 1 &amp; k\\end{array}\\right|\\\\&amp;= -2i + 5j + 4k\\end{aligned}$$Thus,$$f(x) = -2x_1 + 5x_2 + 4x_3 \\\\d = f(\\vec{v_1}) = 7\\LongrightarrowH_1 : [f: d]$$ A subset $H$ of $R^n$ is a hyperplane if and only if $H = [f:d]$ for some nonzero linear functional $f$ and some scalar $d$ in $R$. Thus, if $H$ is a hyperplane, there exist a nonzero vector $\\vec{n}$ and a real number $d$ such that $H = \\lbrace \\vec{x} : \\vec{n} \\cdot \\vec{x} = d\\rbrace$. Topology in $R^n$For any point $p$ in $R^n$ and any real $\\delta &gt; 0$, the open ball $B(p,\\delta)$ with center $p$ and radius $\\delta$ is given by$$B(p,\\delta) = \\lbrace x : ||x - p|| &lt; \\delta\\rbrace$$Given a set in $R^n$,A set is open if it contains none of its boundary points.A set is closed if it contains all of its boundary points.A set $S$ is bounded if there exists a $\\delta &gt; 0$ such that $S \\subset B(0, \\delta)$.A set in $R^n$ is compact if it is closed and bounded. Polytopes（多面体） Let $S$ be a compact convex subset of $R^n$. A nonempty subset $F$ of $S$ is called a (proper) face of $S$ if $F \\neq S$ and there exists a hyperplane $H = [f:d]$ such that $F = S \\cap H$ and either $f(S) \\leq d$ or $f(S) &gt; \\geq d$. The hyperplane $H$ is called a supporting hyperplane to $S$. If the dimension of $F$ is $k$, then $F$ is called a $k-face$ of $S$.If $P$ is a polytope of dimension $k$, then $P$ is called a $k-polytope$. A $0-face$ of $P$ is called a vertex (plural: vertices), a $1-face$ is an edge, and a $(k-1)-dimensional$ face is a facet of $S$. SimplexHyperCube（超立方体）Curves and SurfacesBézier Curves quadratic and cubic Bézier curves:$$\\begin{aligned}&amp;\\vec{w(t)} = (1-t^2)\\vec{p_0} + 2t(1-t)\\vec{p_1} + t^2\\vec{p_2}\\\\&amp;\\vec{x(t)} = (1-t^3)\\vec{p_0} + 3t(1-t)^2\\vec{p_1} + 3t^2(1-t)\\vec{p_2} + t^3\\vec{p_3}\\\\\\end{aligned}$$Bézier curves are useful in computer graphics because their essential properties are preserved under the action of linear transformations and translations. $$\\begin{aligned}A\\vec{x(t)} &amp;= A[(1-t^3)\\vec{p_0} + 3t(1-t)^2\\vec{p_1} + 3t^2(1-t)\\vec{p_2} + t^3\\vec{p_3}]\\\\&amp;= (1-t^3)A\\vec{p_0} + 3t(1-t)^2A\\vec{p_1} + 3t^2(1-t)A\\vec{p_2} + t^3A\\vec{p_3}\\end{aligned}$$the new control points are $A\\vec{p_0}, \\cdots, A\\vec{p_3}$. Connecting Two Bézier CurvesThe combined curve is said to have $G^0$ geometric continuity (at $p_2$) because the two segments join at $p_2$. To avoid a sharp bend, it usually suffices to adjust the curves to have what is called $G^1$ geometric continuity, where both tangent vectors at $p_2$ point in the same direction. When the tangent vectors are actually equal at $p_2$, the tangent vector is continuous at $p_2$, and the combined curve is said to have $C^1$ continuity, or $C^1$ parametric continuity. Figure 4 shows $C^1$ continuity for two cubic Bézier curves. Notice how the point joining the two segments lies in the middle of the line segment between the adjacent control points. Two curves have $C^2$ (parametric) continuity when they have $C^1$ continuity and the second derivatives are equal. Another class of cubic curves, called B-splines, always have $C^2$ continuity because each pair of curves share three control points rather than one. Matrix Equations for Bézier CurvesSince a Bézier curve is a linear combination of control points using polynomials as weights, the formula for $\\vec{x(t)}$ may be written as $$\\begin{aligned}\\vec{x(t)} &amp;= \\begin{bmatrix}\\vec{p_0} &amp; \\vec{p_1} &amp; \\vec{p_2} &amp; \\vec{p_3}\\end{bmatrix}\\begin{bmatrix}(1-t)^3 \\\\ 3t(1-t)^2 \\\\ 3t^2(1-t) \\\\ t^3\\end{bmatrix}\\\\&amp;=\\underbrace{\\begin{bmatrix}\\vec{p_0} &amp; \\vec{p_1} &amp; \\vec{p_2} &amp; \\vec{p_3}\\end{bmatrix}}_{Geometry-matrix: G}\\begin{bmatrix}1 &amp; -3 &amp; 3 &amp; -1\\\\0 &amp; 3 &amp; -6 &amp; 3\\\\0 &amp; 0 &amp; 3 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\\\end{aligned}$$ $$\\Longrightarrow\\vec{x(t)} = GM_B\\vec{u(t)},M_B : Bezier-basis-matrix\\tag{4}$$A Hermite cubic curve arises when the matrix $M_B$ is replaced by a Hermite basis matrix. Bézier SurfacesThe Bézier curve in equation (4) can also be “factored” in another way, to be used in the discussion of Bézier surfaces.$$\\begin{aligned}\\vec{x(s)} &amp;= \\vec{u(s)}^TM_B^T\\begin{bmatrix}\\vec{p_0} \\\\ \\vec{p_1} \\\\ \\vec{p_2} \\\\ \\vec{p_3}\\end{bmatrix}\\\\&amp;=\\begin{bmatrix}1 &amp; s &amp; s^2 &amp; s^3\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\-3 &amp; 3 &amp; 0 &amp; 0\\\\3 &amp; -6 &amp; 3 &amp; 0\\\\-1 &amp; 3 &amp; -3 &amp; 1\\end{bmatrix}\\begin{bmatrix}\\vec{p_0} \\\\ \\vec{p_1} \\\\ \\vec{p_2} \\\\ \\vec{p_3}\\end{bmatrix}\\\\&amp;=\\begin{bmatrix} (1-s)^3 &amp; 3s(1-s)^2 &amp; 3s^2(1-s) &amp; s^3\\end{bmatrix}\\underbrace{\\begin{bmatrix}\\vec{p_0} \\\\ \\vec{p_1} \\\\ \\vec{p_2} \\\\ \\vec{p_3}\\end{bmatrix}}_{Geometry-vector}\\end{aligned}$$ Approximations to Curves and SurfacesThe basic idea for approximating a Bézier curve or surface is to divide the curve or surface into smaller pieces, with more and more control points. Recursive Subdivision of Bézier Curves and Surfaces $$x(t) = (1-3t+3t^2-t^3)p_0 + (3t-6t^2+3t^3)p_1 + (3t^2-3t^3)p_2 + t^3p_3, 0 \\leq t \\leq 1$$Thus, the new control points $q_3$ and $r_0$ are given by$$q_3 = r_0 = x(0.5) = \\frac{1}{8}(p_0 + 3p_1 + 3p_2 + p_3)$$the tangent vector to a parameterized curve $x(t)$ is the derivative $x(t)’$.$$x(t)’ = (-3 + 6t -3t^2)p_0 + (3 - 12t + 9t^2)p_1 + (6t-9t^2)p_2 + 3t^2p_3, 0 \\leq t \\leq 1$$In particular,$$x’(0) = 3(p_1 - p_0)\\\\x’(1) = 3(p_3 - p_2)\\\\x’(0.5) = \\frac{3}{4}(-p_0 - p_1 + p_2 + p_3)$$ Let $y(t)$ be the Bézier curve determined by $q_0, \\cdots, q_3$, and let $z(t)$ be the Bézier curve determined by $r_0, \\cdots, r_3$. Since $y(t)$ traverses the same path as $x(t)$ but only gets to $x(0.5)$ as $t$ goes from $0$ to $1$,$$y(t) = x(0.5t), 0 \\leq t \\leq 1$$ Similarly$$z(t) = x(0.5 + 0.5t), 0 \\leq t \\leq 1$$ By chain rule: $$y’(t) = 0.5x’(0.5t),\\\\z’(t) = 0.5x’(0.5 + 0.5t)$$ the control points for $y(t)$ satisfy $$3(q_1-q_0) = y’(0) = 0.5x’(0) = \\frac{3}{2}(p_1-p_0)\\\\3(q_3-q_2) = y’(1) = 0.5x’(0.5) = \\frac{3}{8}(-p_0-p_1+p_2+p_3)$$ Once the subdivision completely stops, the endpoints of each curve are joined by line segments, and the scene is ready for the next step in the final image preparation. More about Curves in Graphics&gt;&gt;","link":"/Math/Linear-Algebra/Algebra-C8-The-Geometry-of-Vector-Spaces/"},{"title":"Algebra-C6-Orthogonality-And-Least-Squares","text":"Keywords: Orthogonal Projections, Gram-Schmidt Process, Least-Squares Problems, Linear Model This is the Chapter6 ReadingNotes from book Linear Algebra and its Application. The Inner Productinner product is dot product. $$\\vec{u}^T\\vec{v} = \\vec{u} \\cdot \\vec{v}$$ The Length of a VectorThe length (or norm) of $\\vec{v}$ is the nonnegative scalar $||\\vec{v}||$ defined by$$||\\vec{v}|| = \\sqrt{\\vec{v} \\cdot \\vec{v}} = \\sqrt{v_1^2 + \\cdots + v_n^2}$$ Distance in $R^n$$$dis(\\vec{u}, \\vec{v}) = ||\\vec{u} - \\vec{v}||$$ Orthogonal Vectors$$\\vec{u} \\cdot \\vec{v} = 0\\Leftrightarrow||\\vec{u}+\\vec{v}||^2 = ||\\vec{u}||^2 + ||\\vec{v}||^2$$ Orthogonal ComplementsLet $A$ be an $m \\times n$ matrix. The orthogonal complement of the row space of $A$ is the null space of $A$, and the orthogonal complement of the column space of $A$ is the null space of $A^T$ : $$(Row A)^\\bot = NullA\\\\(Col A)^\\bot = NulA^T$$ Angles in $R^2$ and $R^3$$$\\vec{u} \\cdot \\vec{v} = ||\\vec{u}|| ||\\vec{v}|| \\cos \\theta$$ Orthogonal SetsLet $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ be an orthogonal basis(orthogonal $\\neq$ orthonormal) for a subspace $W$ of $R^n$. For each $\\vec{y}$ in $W$, the weights in the linear combination$$\\vec{y} = c_1\\vec{u_1} + \\cdots + c_p\\vec{u_p}$$are give by$$c_j = \\frac{\\vec{y} \\cdot \\vec{u_j}}{\\vec{u_j} \\cdot \\vec{u_j}}$$ An Orthogonal ProjectionGiven a nonzero vector $\\vec{u}$ in $R^n$, consider the problem of decomposing a vector $\\vec{y}$ in $R^n$ into the sum of two vectors, one a multiple of $u$ and the other orthogonal to $u$. We wish to write $$\\vec{y} = \\hat{\\vec{y}} + \\vec{z}$$where $\\hat{\\vec{y}} = \\alpha \\vec{u}$ for some scalar $\\alpha$˛ and $\\vec{z}$ is some vector orthogonal to $\\vec{u}$. $$\\vec{z} \\cdot \\vec{u} = 0\\Rightarrow(\\vec{y} - \\alpha \\vec{u}) \\cdot \\vec{u} = 0\\\\\\Rightarrow\\alpha = \\frac{\\vec{y}\\cdot \\vec{u}}{\\vec{u}\\cdot\\vec{u}},\\hat{\\vec{y}} = \\frac{\\vec{y}\\cdot \\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\vec{u}$$that means:subspace $L$ spanned by $\\vec{u}$ (the line through $\\vec{u}$ and $\\vec{0}$)$$\\hat{\\vec{y}} = proj_L\\vec{y} = \\frac{\\vec{y}\\cdot \\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\vec{u}$$ Orthonormal SetsA set $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ is an orthonormal set if it is an orthogonal set of unit vectors. If $W$ is the subspace spanned by such a set, then $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ is an orthonormal basis for $W$ , since the set is automatically linearly independent. （正交矩阵）An $m \\times n$ matrix $U$ has orthonormal columns if and only if $U^TU = I$. Let $U$ be an $m \\times n$ matrix with orthonormal columns, and let $\\vec{x}$ and $\\vec{y}$ be in $R^n$. Thena.$||U\\vec{x}|| = ||\\vec{x}||$转换保留长度和正交性b.$(U\\vec{x})\\cdot (U\\vec{y}) = \\vec{x}\\cdot \\vec{y}$c.$(U\\vec{x})\\cdot (U\\vec{y}) = 0 ,if \\space and \\space only \\space if,\\vec{x}\\cdot \\vec{y} = 0$ Orthogonal ProjectionsLet $W$ be a subspace of $R^n$, let $\\vec{y}$ be any vector in $R^n$, and let $\\hat{\\vec{y}}$ be the orthogonal projection of $\\vec{y}$ onto $W$ . Then $\\hat{\\vec{y}}$ is the closest point in $W$ to $\\vec{y}$, in the sense that $$||\\vec{y}-\\hat{\\vec{y}}|| &lt; ||\\vec{y}-\\vec{v}||$$for all $\\vec{v}$ in $W$ distinct from $\\hat{\\vec{y}}$. If $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ is an orthonormal basis for a subspace $W$ of $R^n$, then $$proj_W\\vec{y} = (\\vec{y} \\cdot \\vec{u_1})\\vec{u_1} + \\cdots + (\\vec{y} \\cdot \\vec{u_p})\\vec{u_p}$$if $U = \\begin{bmatrix} \\vec{u_1} &amp; \\vec{u_2} &amp; \\vec{u_p} \\end{bmatrix}$, then $$proj_W\\vec{y} = UU^T\\vec{y}$$ The Gram–Schmidt Process（构建标准正交基）For Example: Let $\\vec{x_1} = \\begin{bmatrix}1 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{x_2} = \\begin{bmatrix}0 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{x_3} = \\begin{bmatrix}0 \\\\ 0 \\\\1 \\\\1\\end{bmatrix}$. Then $\\lbrace \\vec{x_1}, \\vec{x_2}, \\vec{x_3}\\rbrace$ is clearly linearly independent and thus is a basis for a subspace $W$ of $R^4$. Construct an orthogonal basis for $W$ . Solution: Step 1. Let $\\vec{v_1} = \\vec{x_1}$ and $W_1 = Span \\lbrace \\vec{x_1} \\rbrace =Span \\lbrace \\vec{v_1} \\rbrace$. Step 2. Let $\\vec{v_2}$ be the vector produced by subtracting from $\\vec{x_2}$ its projection onto the subspace $W_1$. That is, let $$\\vec{v_2} = \\vec{x_2} - proj_{W_1}\\vec{x_2}\\\\= \\vec{x_2} - \\frac{\\vec{x_2} \\cdot \\vec{v_1}}{\\vec{v_1} \\cdot \\vec{v_1}} \\vec{v_1}\\\\=\\begin{bmatrix}-\\frac{3}{4}\\\\\\frac{1}{4}\\\\\\frac{1}{4}\\\\\\frac{1}{4}\\end{bmatrix}$$$\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ is an orthogonal basis for the subspace $W_2$ spanned by $\\vec{x_1}$ and $\\vec{x_2}$ . Step 2 (optional). If appropriate, scale $\\vec{v_2}$ to simplify later computations. Since $\\vec{v_2}$ has fractional entries, it is convenient to scale it by a factor of $4 $and replace $\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ by the orthogonal basis: $$\\vec{v_1} = \\begin{bmatrix}1 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-3 \\\\ 1 \\\\1 \\\\1\\end{bmatrix}$$ Step 3. Let $\\vec{v_3}$ be the vector produced by subtracting from $\\vec{v_3}$ its projection onto the subspace $W_2$. Use the orthogonal basis $\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ to compute this projection onto $W_2$: $$proj_{W_2}\\vec{x_3} =\\frac{\\vec{x_3} \\cdot \\vec{v_1}}{\\vec{v_1} \\cdot \\vec{v_1}} \\vec{v_1} +\\frac{\\vec{x_3} \\cdot \\vec{v_2}}{\\vec{v_2} \\cdot \\vec{v_2}} \\vec{v_2},\\vec{v_3} = \\vec{x_3} - proj_{W_2}\\vec{x_3}=\\begin{bmatrix}0\\\\\\frac{-2}{3}\\\\\\frac{1}{3}\\\\\\frac{1}{3}\\end{bmatrix}$$$\\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_3}\\rbrace$ is an orthogonal basis for $W$ . QR Factorization of Matrices If $A$ is an $m \\times n$ matrix with linearly independent columns, then $A$ can be factored as $A = QR$, where $Q$ is an $m \\times n$ matrix whose columns form an orthonormal basis for $Col A$ and $R$ is an $n \\times n$ upper triangular invertible matrix with positive entries on its diagonal. For Example: Find a $QR$ factorization of $A = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\1 &amp; 1 &amp; 0\\\\1 &amp; 1 &amp; 1\\\\1 &amp; 1 &amp; 1 \\end{bmatrix}$. Solution: An orthogonal basis for $ColA = Span\\lbrace \\vec{x_1}, \\vec{x_2}, \\vec{x_3}\\rbrace$ was found by Gram–Schmidt Process:$$\\vec{v_1} = \\begin{bmatrix}1 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-3 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{v_3} =\\begin{bmatrix}0\\\\\\frac{-2}{3}\\\\\\frac{1}{3}\\\\\\frac{1}{3}\\end{bmatrix}$$Then normalize the three vectors to obtain $\\vec{u_1}, \\vec{u_2}, \\vec{u_3}$, and use these vectors as the columns of $Q$: $$Q =\\begin{bmatrix}\\frac{1}{2} &amp; \\frac{-3}{\\sqrt{12}} &amp; 0 \\\\\\frac{1}{2} &amp; \\frac{1}{\\sqrt{12}} &amp; \\frac{-2}{\\sqrt{6}}\\\\\\frac{1}{2} &amp; \\frac{1}{\\sqrt{12}} &amp; \\frac{1}{\\sqrt{6}}\\\\\\frac{1}{2} &amp; \\frac{1}{\\sqrt{12}} &amp; \\frac{1}{\\sqrt{6}}\\end{bmatrix}$$because: $$Q^TA = Q^T(QR) = R$$thus: $$R = Q^TA =\\begin{bmatrix}2 &amp; \\frac{3}{2} &amp; 1 \\\\0 &amp; \\frac{3}{\\sqrt{12}} &amp; \\frac{2}{\\sqrt{12}} \\\\0 &amp; 0 &amp; \\frac{2}{\\sqrt{6}} \\\\\\end{bmatrix}$$ More About LU Factorization &gt;&gt; Least-Squares Problems（最小二乘问题）For $A\\vec{x} = \\vec{b}$, when a solution is demanded and none exists, the best one can do is to find an $\\vec{x}$ that makes $A\\vec{x}$ as close as possible to $\\vec{b}$. The general least-squares problem is to find an $\\vec{x}$ that makes $||\\vec{b} - A\\vec{x}||$ as small as possible. The adjective “least-squares” arises from the fact that $||\\vec{b} - A\\vec{x}||$ is the square root of a sum of squares（平方和的平方根）. If A is $m \\times n$ and $\\vec{b}$ is in $R^m$, a least-squares solution of $A\\vec{x} = \\vec{b}$ is an $\\hat{x}$ in $R^n$such that$$||\\vec{b} - A\\hat{\\vec{x}}|| \\leq ||\\vec{b} - A\\vec{x}||$$for all $\\vec{x}$ in $R^n$. The most important aspect of the least-squares problem is that no matter what $\\vec{x}$ we select, the vector $A\\vec{x}$ will necessarily be in the column space $ColA$. So we seek an $\\vec{x}$ that makes $A\\vec{x}$ the closest point in $Col A$ to $\\vec{b}$. See Figure 1. (Of course, if $\\vec{b}$ happens to be in $Col A$, then $\\vec{b}$ is $A\\vec{x}$ for some $\\vec{x}$, and such an $\\vec{x}$ is a “least-squares solution.”) Solution of the General Least-Squares Problem $$\\hat{\\vec{b}} = proj_{ColA}\\vec{b}$$Because $\\hat{\\vec{b}}$ is in the column space of $A$, the equation $A\\vec{x} = \\hat{\\vec{b}}$ is consistent, and there is an $\\hat{\\vec{x}}$ in $R^n$ such that $$A\\hat{\\vec{x}} = \\hat{\\vec{b}}$$Since $\\hat{\\vec{b}}$ is the closest point in $Col A$ to $\\vec{b}$, a vector $\\hat{\\vec{x}}$ is a least-squares solution of $A\\vec{x} = \\vec{b}$. Suppose $\\hat{\\vec{x}}$ satisfies$A\\hat{\\vec{x}} = \\hat{\\vec{b}}$. The projection $\\hat{\\vec{b}}$ has the property that $\\vec{b} - \\hat{\\vec{b}}$ is orthogonal to $Col A$. so $\\vec{b} - \\hat{\\vec{b}}$ is orthogonal to each column of $A$. If $\\vec{a_j}$ is any column of $A$, then $\\vec{a_j}\\cdot (\\vec{b} - A\\hat{\\vec{x}}) = 0$, and $\\vec{a_j}^T(\\vec{b} - A\\hat{\\vec{x}}) = 0$（one is vector dot, one is matrix multiplication）. Since each $\\vec{a_j}^T$ is a row of $A^T$, $$A^T(\\vec{b} - A\\hat{\\vec{x}}) = \\vec{0}\\tag{2}\\RightarrowA^T\\vec{b} - A^TA\\hat{\\vec{x}} = \\vec{0}\\RightarrowA^T\\vec{b} = A^TA\\hat{\\vec{x}}$$These calculations show that each least-squares solution of $A\\vec{x} = \\vec{b}$ satisfies the equation $$A^TA\\vec{x} = A^T\\vec{b}\\tag{3}$$The matrix equation (3) represents a system of equations called the normal equations（正规方程） for $A\\vec{x} = \\vec{b}$. A solution of (3) is often denoted by $\\hat{\\vec{x}}$. For Example: Find a least-squares solution of $A\\vec{x} = \\vec{b}$ for$$A =\\begin{bmatrix}1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 0 &amp; 1\\\\1 &amp; 0 &amp; 0 &amp; 1\\\\\\end{bmatrix},\\vec{b} =\\begin{bmatrix}-3\\\\-1\\\\0\\\\2\\\\5\\\\1\\end{bmatrix}$$Solution:$$A^TA =\\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2\\\\2 &amp; 2 &amp; 0 &amp; 0\\\\2 &amp; 0 &amp; 2 &amp; 0\\\\2 &amp; 0 &amp; 0 &amp; 2\\\\\\end{bmatrix},A^T\\vec{b} =\\begin{bmatrix}4\\\\-4\\\\2\\\\6\\end{bmatrix}$$The augmented matrix for $A^TA\\vec{x} = A^T\\vec{b}$ is$$\\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2 &amp; 4\\\\2 &amp; 2 &amp; 0 &amp; 0 &amp; -4\\\\2 &amp; 0 &amp; 2 &amp; 0 &amp; 2\\\\2 &amp; 0 &amp; 0 &amp; 2 &amp; 6\\\\\\end{bmatrix}\\sim\\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2 &amp; 4\\\\2 &amp; 2 &amp; 0 &amp; 0 &amp; -4\\\\2 &amp; 0 &amp; 2 &amp; 0 &amp; 2\\\\2 &amp; 0 &amp; 0 &amp; 2 &amp; 6\\\\\\end{bmatrix}\\Rightarrow\\hat{\\vec{x}} =\\begin{bmatrix}3\\\\-5\\\\-2\\\\0\\end{bmatrix} + x_4\\begin{bmatrix}-1\\\\1\\\\1\\\\1\\end{bmatrix}$$ Let A be an $m \\times n$ matrix. The following statements are logically equivalent:a. The equation $A\\vec{x} = \\vec{b}$ has a unique least-squares solution for each $\\vec{b}$ in $R^m$.b. The columns of $A$ are linearly independent.c. The matrix $A^TA$ is invertible.When these statements are true, the least-squares solution $\\hat{\\vec{x}}$ is given by$$\\hat{\\vec{x}} = (A^TA)^{-1}A^T\\vec{b}$$ Alternative Calculations of Least-Squares SolutionsGiven an $m \\times n$ matrix $A$ with linearly independent columns, let $A = QR$ be a $QR$ factorization of $A$. Then, for each $\\vec{b}$ in $R^m$, the equation $A\\vec{x} = \\vec{b}$ has a unique least-squares solution, given by $$\\hat{\\vec{x}} = R^{-1}Q^T\\vec{b}$$ Applications to Linear ModelsInstead of $A\\vec{x} = \\vec{b}$, we write $X\\vec{\\beta} = \\vec{y}$ and refer to $X$ as the design matrix（模型的输入）, $\\vec{\\beta}$ as the parameter vector（模型的参数）, and $\\vec{y}$ as the observation vector（模型的输出，俗称观测值）. Least-Squares LinesThe simplest relation between two variables $x$ and $y$ is the linear equation $y = \\beta_0 + \\beta_1x$. Experimental data often produce points $(x_1, y_1), \\cdots, (x_n, y_n)$ that, when graphed, seem to lie close to a line. We want to determine the parameters $\\beta_0$ and $\\beta_1$ that make the line as “close” to the points as possible. This line is also called a line of regression of $\\vec{y}$ on $\\vec{x}$, because any errors in the data are assumed to be only in the $y-coordinates$. Computing the least-squares solution of $X\\vec{\\beta} = \\vec{y}$ is equivalent to finding the $\\vec{\\beta}$ that determines the least-squares line in Figure 1. The General Linear ModelStatisticians usually introduce a residual vector $\\vec{\\epsilon}$ defined by $\\vec{\\epsilon} = \\vec{y} - X\\vec{\\beta}$, and write $$\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}$$Any equation of this form is referred to as a linear model. Once $X$ and $\\vec{y}$ are determined, the goal is to minimize the length of $\\vec{\\epsilon}$, which amounts to finding a least-squares solution of $X\\vec{\\beta} = \\vec{y}$. In each case, the least-squares solution $\\widehat{\\vec{\\beta}}$ is a solution of the normal equations: $$X^TX\\vec{\\beta} = X^T\\vec{y}$$ Least-Squares Fitting of Other CurvesThe most common example is curve fitting. like $$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\tag{3}$$ The general form is $$y = \\beta_0 f_0(x) + \\beta_1f_1(x) + \\cdots + \\beta_kf_k(x)\\tag{2}$$where $f_0, \\cdots, f_k$ are known functions and $\\beta_0, \\cdots, \\beta_k$ are parameters that must be determined. As we will see, equation (2) describes a linear model because it is linear in the unknown parameters. See Figure5, it shows fitting over a cubic curve.$$\\begin{bmatrix}y_1\\\\y_2\\\\ \\cdots \\\\y_n\\end{bmatrix}=\\begin{bmatrix}1 &amp; x_1 &amp; x_1^2\\\\1 &amp; x_2 &amp; x_2^2\\\\&amp; \\cdots \\\\1 &amp; x_n &amp; x_n^2\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{bmatrix}+\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\cdots\\\\\\epsilon_n\\\\\\end{bmatrix}\\\\\\Longleftrightarrow\\\\\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}$$ Multiple RegressionLeast-squares plane. $$y = \\beta_0 f_0(u,v) + \\beta_1f_1(u,v) + \\cdots + \\beta_kf_k(u,v)$$ where $f_0, \\cdots, f_k$ are known functions and $\\beta_0, \\cdots, \\beta_k$ are parameters that must be determined. More about Linear Models and Least-Squares Problems in Machine Learning&gt;&gt; (求最大似然函数 = 最小二乘 = 最小误差值 = 问题的最优解) Inner Product Spaces An inner product on a vector space $V$ is a function that, to each pair of vectors $\\vec{u}$ and $\\vec{v}$ in $V$ , associates a real number $&lt;\\vec{u}, \\vec{v}&gt;$ and satisfies the following axioms, for all $\\vec{u}, \\vec{v}, \\vec{w}$ in $V$ and all scalars $c$: $&lt;\\vec{u}, \\vec{v}&gt; = &lt;\\vec{v}, \\vec{u}&gt;$ $&lt;\\vec{u + v}, \\vec{w}&gt; = &lt;\\vec{u}, \\vec{w}&gt; + &lt;\\vec{v}, \\vec{w}&gt;$ $&lt;c\\vec{u}, \\vec{v}&gt; = c&lt;\\vec{u}, \\vec{v}&gt;$ $&lt;\\vec{u}, \\vec{u}&gt; \\geq 0$ and $&lt;\\vec{u}, \\vec{u}&gt; = 0$ if and only if $\\vec{u} = \\vec{0}$A vector space with an inner product is called an inner product space. For example:Let $t_0, \\cdots, t_n$ be distinct real numbers. For $p$ and $q$ in $P_n$, define $$&lt;p,q&gt; = p(t_0)q(t_0) + \\cdots + p(t_n)q(t_n)\\\\&lt;p,p&gt; = [p(t_0)]^2 + \\cdots + [p(t_n)]^2$$ Lengths, Distances, and Orthogonality$$||\\vec{v}|| = \\sqrt{&lt;\\vec{v}, \\vec{v}&gt;}$$ The Gram–Schmidt Processcannot understand, to be added…$$ $$ Best Approximation in Inner Product SpacesTwo InequalitiesAn Inner Product for $C[a,b]$ (Calculus required)Applications of Inner Product SpacesWeighted Least-SquaresTrend Analysis of DataFourier Series (Calculus required)","link":"/Math/Linear-Algebra/Algebra-C6-Orthogonality-And-Least-Squares/"},{"title":"Algebra-C4-Vector-Spaces","text":"Keywords: SubSpaces, NullSpace and ColSpace, Coordinates Mapping, Dimension, Rank, Difference Equation, Markov Chains This is the Chapter4 ReadingNotes from book Linear Algebra and its Application. Vector Spaces and SubSpaces A subspace of a vector space $V$ is a subset $H$ of $V$ that has three properties:a. The zero vector of $V$ is in $H$.b. $H$ is closed under vector addition. That is, for each $\\vec{u}$ and $\\vec{v}$ in $H$, the sum $\\vec{u} + \\vec{v}$ is in $H$.c. $H$ is closed under multiplication by scalars. That is, for each $\\vec{u}$ in $H$ and each scalar $c$, the vector $c\\vec{u}$ is in $H$. if $\\vec{v_1} \\cdots \\vec{v_p}$ are in a vector space $V$, then Span{$\\vec{v_1} \\cdots \\vec{v_p}$} is a subspace of $V$. For Example: Let $H$ be the set of all vectors of the form $(a - 3b, b - a, a, b)$, where $a$ and $b$ are arbitrary scalars. That is, let $H = \\lbrace a - 3b, b - a, a, b \\rbrace$, $a$ and $b$ in $R$. Show that $H$ is a subspace of $R^4$. Proof： Write the vectors in $H$ as column vectors. Then the arbitrary vector in $H$ has the form $$\\begin{bmatrix}a-3b \\\\ b-a \\\\ a \\\\ b\\end{bmatrix}=a\\begin{bmatrix}1 \\\\ -1 \\\\ 1 \\\\ 0\\end{bmatrix} +b\\begin{bmatrix}-3 \\\\ 1 \\\\ 0 \\\\ 1\\end{bmatrix}$$ This calculation shows that H = Span{$\\vec{v_1}, \\vec{v_2}$}, where $\\vec{v_1}$ and $\\vec{v_2}$ are the vectors indicated above. Thus $H$ is a subspace of $R^4$. Null Spaces, Column Spaces, And Linear TransformationsThe Null Space of a MatrixThe null space of an $m \\times n$ matrix $A$, written as $Nul A$, is the set of all solutions of the homogeneous equation $A\\vec{x} = \\vec{0}$. In set notation,$$Nul A = \\lbrace\\vec{x} : \\vec{x} \\space in\\space R^n and A\\vec{x} = 0\\rbrace$$ The null space of an $m \\times n$ matrix $A$ is a subspace of $R^n$. An Explicit Description of Nul AFor Example: Find a spanning set for the null space of the matrix$$A =\\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4\\end{bmatrix}$$ Solution: $$\\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7 &amp; 0\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1 &amp; 0\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -2 &amp; 0 &amp; -1 &amp; 3 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\x_4 \\\\x_5\\end{bmatrix} =\\begin{bmatrix}2x_2+x_4-3x_5 \\\\ x_2 \\\\ -2x_4+2x_5 \\\\x_4 \\\\x_5\\end{bmatrix} =x_2\\begin{bmatrix}2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} +x_4\\begin{bmatrix}1 \\\\ 0 \\\\ -2 \\\\ 1 \\\\ 0\\end{bmatrix} +x_5\\begin{bmatrix}-3 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 1\\end{bmatrix}= x_2\\vec{u} + x_4\\vec{v} + x_5\\vec{w}$$ Every linear combination of $\\vec{u},\\vec{v},\\vec{w}$ is an element of $Nul A$ and vice versa. Thus $\\lbrace\\vec{u},\\vec{v},\\vec{w}\\rbrace$ is a spanning set for $Nul A$. The Column Space of a MatrixThe column space of an $m \\times n$ matrix A, written as $Col A$, is the set of all linear combinations of the columns of A. If $A = \\left[\\vec{a_1}, \\cdots, \\vec{a_n}\\right]$, then$$ColA = Span\\lbrace\\vec{a_1}, \\cdots, \\vec{a_n}\\rbrace$$ The column space of an $m \\times n$ matrix A is a subspace of $R^m$. Note that a typical vector in $Col A$ can be written as $A\\vec{x}$ for some $\\vec{x}$ because the notation $A\\vec{x}$ stands for a linear combination of the columns of $A$. That is, $$Col A = \\lbrace \\vec{b} : \\vec{b} = A \\vec{x} for \\space some\\space x \\space in \\space R^n\\rbrace$$ The Contrast Between Nul A and Col AContrast Between $NulA$ and $ColA$ for an $m \\times n$ Matrix $A$ $NulA$ $ColA$ 1. $NulA$ is a subspace of $R^n$. 1.$ColA$ is a subspace of $R^m$. 2.$NulA$ is implicitly defined; that is, you are given only a condition $A\\vec{x}=\\vec{0}$that vectors in $NulA$ must satisfy. 2.$ColA$ is explicitly defined; that is, you are told how to build vectors in $ColA$. 3. it takes time to find vectors in $NulA$. Row operations on $\\begin{bmatrix}A &amp; \\vec{0} \\end{bmatrix}$ are required. 3. It is easy to find vectors in $ColA$. The columns of $A$ are displayed; others are formed from them 4. There is no obvious relation between $NulA$ and the entries in $A$. 4. There is an obvious relation between $ColA$ and the entries in $A$, since each column of $A$ is in $ColA$. 5. A typical vector $\\vec{v}$ in $NulA$ has the property that $A\\vec{v} = \\vec{0}$. 5. A typical vector $\\vec{v}$ in$ColA$has the property that the equation $A\\vec{x} = \\vec{v}$ is consistent. 6. Given a specific vector $\\vec{v}$, it is easy to tell if $\\vec{v}$ is in $NulA$. Just compute $A\\vec{v}$. 6. Given a specific vector $\\vec{v}$, it may take time to tell if $\\vec{v}$ is in $ColA$. Row operations on are required. 7. $NulA = \\lbrace\\vec{0}\\rbrace$ if and only if the equation $A\\vec{x} = \\vec{0}$ has only the trivial solution. 7. $ColA = R^m$ if and only if the equation $A\\vec{x} = \\vec{b}$ has a solution for every $\\vec{b}$ in $R^m$. 8. $NulA = \\lbrace\\vec{0}\\rbrace$ if and only if the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ is one-to-one. 8.$ColA = R^m$ if and only if the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ maps $R^n$ onto $R^m$. Linearly Independent Sets; Bases An indexed set $\\lbrace \\vec{v_1}, \\cdots, \\vec{v_p} \\rbrace$ of two or more vectors, with $\\vec{v_1} \\neq \\vec{0}$, is linearly depentdent if and only if some $\\vec{v_j}$(with $j &gt; 1$) is a linear combination if the preceding vectors $\\vec{v_1}, \\cdots, \\vec{v_{j-1}}$. Let $H$ be a subspace of a vector space $V$. An indexed set of vectors $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_p}\\rbrace$ in $V$ is a basis for $H$ if(i) $\\beta$ is a linearly independent set, and(ii) the subspace spanned by $\\beta$ coincides with $H$; that is $$H = Span \\lbrace \\vec{b_1}, \\cdots, \\vec{b_p} \\rbrace$$ For Example: Let $S = \\lbrace 1, t, t^2, \\cdots, t^n \\rbrace$. Verify that $S$ is a basis for $P_n$. This basis is called the standard basis for $P_n$. Solution: Certainly $S$ spans $P_n$. To show that S is linearly independent, suppose that $c_0, \\cdots, c_n$ satisfy $$c_0 \\cdot 1 + c_1 \\cdot t + c_2 \\cdot t^2 + \\cdots + c_n \\cdot t^n = \\vec{0_t}\\tag{2}$$ This equality means that the polynomial on the left has the same values as the zero polynomial on the right. A fundamental theorem in algebra says that the only polynomial in $P_n$ with more than $n$ zeros is the zero polynomial. That is, equation (2) holds for all $t$ only if $c_0 = \\cdots = c_n = 0$. This proves that $S$ is linearly independent and hence is a basis for $P_n$. The Spanning Set Theorem Let $S = \\lbrace \\vec{v_1}, \\cdots, \\vec{v_p} \\rbrace$ be a set in $V$, and let $H = \\lbrace \\vec{v_1}, \\cdots, \\vec{v_p} \\rbrace$.a. If one of the vectors in $S$-say, $\\vec{v_k}$- is a linear combination of the remaining vectors in $S$, then the set formed from $S$ by removing $\\vec{v_k}$ still spans $H$.b. If $H \\neq \\lbrace \\vec{0}\\rbrace$, some subset of $S$ is a basis for $H$. Bases for Nul A and Col A The pivot columns of a matrix A form a basis for Col A. Two Views of a BasisFor Example: The following three sets in $R^3$ show how a linearly independent set can be enlarged to a basis and how further enlargement destroys the linear independence of the set. Also, a spanning set can be shrunk to a basis, but further shrinking destroys the spanning property. Linearly independent,but does not span $R^3$:$$\\lbrace\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}2\\\\3\\\\0\\end{bmatrix}\\rbrace$$A basis for $R^3$:$$\\lbrace\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}2\\\\3\\\\0\\end{bmatrix},\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}\\rbrace$$Spans $R^3$ but is linearly dependent:$$\\lbrace\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}2\\\\3\\\\0\\end{bmatrix},\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix},\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix}\\rbrace$$ Coordinate SystemsLet $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$ be a basis for a vector space $V$. Then for each $\\vec{x}$ in $V$, there exists a unique set of scalars $c_1, \\cdots, c_n$ such that$$\\vec{x} = c_1\\vec{b_1} + \\cdots + c_n \\vec{b_n}$$ Suppose $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$ is a basis for $V$ and $\\vec{x}$ is in $V$. The coordinates of x relative to the basis $\\beta$ (or the $\\beta $-coordinates of x) are the weights $c_1, \\cdots, c_n$ such that$$\\vec{x} = c_1\\vec{b_1} + \\cdots + c_n\\vec{b_n}$$If $c_1, \\cdots, c_n$ are the $\\beta$-coordinates of $\\vec{x}$, then the vector in $R^n$$$[\\vec{x}]_\\beta =\\begin{bmatrix}c_1 \\\\ \\cdots \\\\c_n\\end{bmatrix}$$is the coordinate vector of $\\vec{x}$ (relative to $\\beta$), the mapping $\\vec{x} \\mapsto [\\vec{x}]_\\beta$ is the coordinate mapping (determined by $\\beta$). A Graphical Interpretation of Coordinates1 unit in the $\\vec{e_1}$ direction, 6 units in the $\\vec{e_2}$ direction:$$\\vec{x} =\\begin{bmatrix}1 \\\\ 6\\end{bmatrix}$$2 units in the $\\vec{b_1}$ direction, 3 units in the $\\vec{b_2}$ direction:$$[\\vec{x}]_\\beta =\\begin{bmatrix}-2 \\\\ 3\\end{bmatrix}$$ Coordinates in $R^n$For Example: Let $\\vec{b_1} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$,$\\vec{b_2} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}$,$\\vec{x} = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}$, and $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$. Find the coordinate vector $[\\vec{x}]_\\beta$ of $\\vec{x}$ relative to $\\beta$. Solution: The $\\beta$-coordinates $c_1, c_2$ of $\\vec{x}$ satisfy $$c_1\\begin{bmatrix}2 \\\\ 1\\end{bmatrix} + c_2\\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}= \\begin{bmatrix}4 \\\\ 5\\end{bmatrix},\\begin{bmatrix}2 &amp; -1 \\\\ 1 &amp; 1\\end{bmatrix} \\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix}= \\begin{bmatrix}4 \\\\ 5\\end{bmatrix}\\tag{3}$$ This equation can be solved by row operations on an augmented matrix or by using the inverse of the matrix on the left. In any case, the solution is$$c_1 = 3, c_2 = 2 \\\\\\vec{x} = 3\\vec{b_1} + 2\\vec{b_2}, and \\space[\\vec{x}]_\\beta = \\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix}=\\begin{bmatrix}3 \\\\ 2\\end{bmatrix}$$ The matrix in (3) changes the $\\beta$ -coordinates of a vector $\\vec{x}$ into the standard coordinates for $\\vec{x}$. An analogous change of coordinates can be carried out in $R^n$ for a basis $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$. Let $$P_\\beta =\\begin{bmatrix}\\vec{b_1} &amp; \\vec{b_2} &amp; \\cdots &amp; \\vec{b_n}\\end{bmatrix}$$Then the vector equation $$\\vec{x} = c_1\\vec{b_1} + c_2\\vec{b_2} + \\cdots + c_n\\vec{b_n}$$is equivalent to$$\\vec{x} = P_\\beta[\\vec{x}]_\\beta$$ We call $P_\\beta$ the change-of-coordinates matrix from $\\beta$ to the standard basis in $R^n$. Left-multiplication by $P_\\beta$ transforms the coordinate vector $[\\vec{x}]_\\beta$ into $\\vec{x}$. Since the columns of $P_\\beta$ form a basis for $R^n$, $P_\\beta$ is invertible. Left-multiplication by $P_\\beta^{-1}$converts $\\vec{x}$ into its $\\beta$-coordinate vector: $$P_\\beta^{-1}\\vec{x} = [\\vec{x}]_\\beta$$ The Coordinate MappingChoosing a basis $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$ for a vector space $V$ introduces a coordinate system in $V$. The coordinate mapping $\\vec{x} \\mapsto [\\vec{x}]_\\beta$ connects the possibly unfamiliar space $V$ to the familiar space $R^n$: Let $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$ be a basis for a vector space $V$. Then the coordinate mapping $\\vec{x} \\mapsto [\\vec{x}]_\\beta$ is a one-to-one linear transformation from $V$ onto $R^n$.这里一定要注意，beta是向量空间V的基向量，x也是属于V空间的向量 Proof: two typical vectors in $V$:$$\\vec{u} = c_1\\vec{b_1} + \\cdots + c_n\\vec{b_n},\\vec{w} = d_1\\vec{b_1} + \\cdots + d_n\\vec{b_n}\\stackrel{addition-operation}\\longrightarrow\\vec{u} + \\vec{w} = (c_1 + d_1)\\vec{b_1} + \\cdots + (c_n + d_n)\\vec{b_n}\\longrightarrow\\\\[\\vec{u} + \\vec{w}]_\\beta =\\begin{bmatrix}c_1 + d_1 \\\\\\cdots \\\\c_n + d_n\\end{bmatrix} =\\begin{bmatrix}c_1 \\\\\\cdots \\\\c_n\\end{bmatrix} +\\begin{bmatrix}d_1 \\\\\\cdots \\\\d_n\\end{bmatrix} =[\\vec{u}]_\\beta + [\\vec{w}]_\\beta\\\\常量乘法也一样性质，略过$$ Thus the coordinate mapping also preserves scalar multiplication and hence is a linear transformation. In general, a one-to-one linear transformation from a vector space $V$ onto a vector space$W$ is called an isomorphism from $V$ onto $W$. For Example: Let $\\beta$ be the standard basis of the space $P_3$ of polynomials; that is, let $\\beta = \\lbrace \\vec{1}, \\vec{t}, \\vec{t^2}, \\vec{t^3}\\rbrace$. A typical element $p$ of $P_3$ has the form: $$\\vec{p(t)} = a_0 + a_1\\vec{t} + a_2\\vec{t^2} + a_3\\vec{t^3}$$Since $\\vec{p}$is already displayed as a linear combination of the standard basis vectors, weconclude that:$$[\\vec{p}]_\\beta =\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\\\a_3\\end{bmatrix}$$ Thus the coordinate mapping $\\vec{p} \\mapsto [\\vec{p}]_\\beta$is an isomorphism from$P_3$ onto $R_4$. The Dimension of A Vector Space If a vector space $V$ has a basis $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$, then any set in $V$ containing more than $n$ vectors must be linearly dependent. If a vector space $V$ has a basis of $n$ vectors, then every basis of $V$ must consist of exactly $n$ vectors. If $V$ is spanned by a finite set, then $V$ is said to be finite-dimensional, and the dimension of $V$ , written as $dim V$ , is the number of vectors in a basis for $V$. The dimension of the zero vector space ${\\vec{0}}$ is defined to be zero. If $V$ is not spanned by a finite set, then V is said to be infinite-dimensional. For Example: Find the dimension of the subspace:$H = \\lbrace \\begin{bmatrix} a - 3b + 6c\\\\5a + 4d\\\\ b - 2c - d\\\\5d\\end{bmatrix} a,b,c,d \\space in R \\rbrace$ Solution: $H$ is the set of all linear combinations of the vectors:$$\\vec{v_1} =\\begin{bmatrix}1\\\\5\\\\0\\\\0\\end{bmatrix},\\vec{v_2} =\\begin{bmatrix}-3\\\\0\\\\1\\\\0\\end{bmatrix},\\vec{v_3} =\\begin{bmatrix}6\\\\0\\\\-2\\\\0\\end{bmatrix},\\vec{v_4} =\\begin{bmatrix}0\\\\4\\\\-1\\\\5\\end{bmatrix}$$ $\\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_4}\\rbrace$ is linearly independent and hence is a basis for $H$. Thus $dim H = 3$. Subspaces of a Finite-Dimensional SpaceLet $H$ be a subspace of a finite-dimensional vector space $V$ . Any linearly independent set in $H$ can be expanded, if necessary, to a basis for $H$ . Also, $H$ is finite-dimensional and $$dim H \\leqslant dim V$$ The Dimensions of $Nul A$ and $Col A$ The dimension of $Nul A$ is the number of free variables in the equation $A\\vec{x} = \\vec{0}$, and the dimension of $Col A$ is the number of pivot columns in $A$. For Example: Find the dimensions of the null space and the column space of $A = \\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4\\end{bmatrix}$ $$\\begin{bmatrix}1 &amp; -2 &amp; 2 &amp; 3 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\x_2, x_4, x_5: free\\space variables\\\\\\longrightarrow dimNulA = 3, dimColA = 2.$$ RankThe Row Space If two matrices $A$ and $B$ are row equivalent, then their row spaces are the same. If $B$ is in echelon form, the nonzero rows of $B$ form a basis for the row space of $A$ as well as for that of $B$. For Example: Find bases for the row space, the column space, and the null space of the matrix $\\begin{bmatrix}-2 &amp; -5 &amp; 8 &amp; 0 &amp; -17\\\\1 &amp; 3 &amp; -5 &amp; 1 &amp; 5\\\\3 &amp; 11 &amp; -19 &amp; 7 &amp; 1\\\\1 &amp; 7 &amp; -13 &amp; 5 &amp; -3\\end{bmatrix}$. $$A \\sim B =\\begin{bmatrix}1 &amp; 3 &amp; -5 &amp; 1 &amp; 5\\\\0 &amp; 1 &amp; -2 &amp; 2 &amp; -7\\\\0 &amp; 0 &amp; 0 &amp; -4 &amp; 20\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\sim C =\\begin{bmatrix}1 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\\\0 &amp; 1 &amp; -2 &amp; 0 &amp; 3\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; -5\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\\\（只需看前几行非零）Basis-for-RowA:\\lbrace(1, 3, 5, 1, 5),(0, 1, 2, 2, 7),(0, 0, 0, 4, 20)\\rbrace\\\\（要看pivot \\space columns）Basis-for-ColA:\\lbrace\\begin{bmatrix}-2\\\\ 1\\\\ 3\\\\ 1\\end{bmatrix},\\begin{bmatrix}-5\\\\ 3\\\\ 11\\\\ 7\\end{bmatrix},\\begin{bmatrix}0\\\\ 1\\\\ 7\\\\ 5\\end{bmatrix}\\rbrace\\\\（求解，有两个自由变量）Basis-for-NullA:\\lbrace\\begin{bmatrix}-1\\\\ 2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix},\\begin{bmatrix}-1\\\\ -3\\\\ 0\\\\ 5\\\\ 1\\end{bmatrix}\\rbrace$$ The Rank Theorem The rank of $A$ is the dimension of the column space of $A$. The dimensions of the column space and the row space of an $m \\times n$ matrix $A$ are equal. This common dimension, the rank of $A$, also equals the number of pivot positions in $A$ and satisfies the equation $$rank A + dim NullA = n\\\\其实就是：\\\\\\lbrace number-of-pivot-columns\\rbrace +\\lbrace number-of-nonpivot-columns\\rbrace =\\lbrace number-of-columns\\rbrace$$ Applications to Systems of EquationsFor Example: A scientist has found two solutions to a homogeneous system of 40 equations in 42 variables. The two solutions are not multiples, and all other solutions can be constructed by adding together appropriate multiples of these two solutions. Can the scientist be certain that an associated nonhomogeneous system (with the same coefficients) has a solution? Solution: Yes. Let $A$ be the $40 \\times 42$ coefficient matrix of the system. The given information implies that the two solutions are linearly independent and span $Nul A$.So $dim Nul A = 2$. By the Rank Theorem, $dim Col A = 42 - 2 = 40$. Since $R^{40}$ is the only subspace of $R^{40}$ whose dimension is $40$, $Col A$ must be all of $R^{40}$. This means that every nonhomogeneous equation $Ax = b$ has a solution. Rank and the Invertible Matrix Theorem The Invertible Matrix TheoremLet $A$ be an $n \\times n$ matrix. Then the following statements are each equivalent to the statement that $A$ is an invertible matrix.m.The columns of $A$ form a basis of $R^n$.n. $Col A = R^n$o. $dim Col A = n$p. $rank A = n$q. $Nul A = \\lbrace 0 \\rbrace $r. $dim Nul A = 0$ Change of BasisFor Example: Consider two bases $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$ and $\\gamma = \\lbrace \\vec{c_1}, \\vec{c_2}\\rbrace$ for a vector space $V$, such that $$\\vec{b_1} = 4 \\vec{c_1} + \\vec{c_2} \\space and \\space \\vec{b_2} = -6 \\vec{c_1} + \\vec{c_2}\\tag{1}$$ Suppose $$\\vec{x} = 3\\vec{b_1} + \\vec{b_2}\\tag{2}$$ That is, suppose $[\\vec{x}]_\\beta = \\begin{bmatrix}3\\\\1\\end{bmatrix}$. Find $[\\vec{x}]_\\gamma$. Solution: Apply the coordinate mapping determined by $\\gamma$ to $\\vec{x}$ in (2). Since the coordinate mapping is a linear transformation. $$[\\vec{x}]_\\gamma = [3\\vec{b_1}+\\vec{b_2}]_\\gamma= 3[\\vec{b_1}]_\\gamma+[\\vec{b_2}]_\\gamma$$ We can write this vector equation as a matrix equation, using the vectors in the linear combination as the columns of a matrix: $$[\\vec{x}]_\\gamma =\\begin{bmatrix}[\\vec{b_1}]_\\gamma &amp; [\\vec{b_2}]_\\gamma\\end{bmatrix}\\begin{bmatrix}3\\\\ 1\\end{bmatrix}$$ From (1), $$[\\vec{b_1}]_\\gamma =\\begin{bmatrix}4\\\\ 1\\end{bmatrix},[\\vec{b_2}]_\\gamma =\\begin{bmatrix}-6\\\\ 1\\end{bmatrix}$$ Thus, $$[\\vec{x}]_\\gamma =\\begin{bmatrix}4 &amp; -6\\\\1 &amp; 1\\end{bmatrix}\\begin{bmatrix}3\\\\ 1\\end{bmatrix}=\\begin{bmatrix}6\\\\ 4\\end{bmatrix}$$ Let $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$ and $\\gamma = \\lbrace \\vec{c_1}, \\cdots, \\vec{c_n}\\rbrace$ be bases of a vector space $V$. Then there is a unique $n \\times n$ matrix $\\gamma \\stackrel{P}\\leftarrow \\beta$ such that $$[\\vec{x}]_\\gamma =\\gamma \\stackrel{P}\\leftarrow \\beta [\\vec{x}]_\\beta$$ The columns of $\\gamma \\stackrel{P}\\leftarrow \\beta$ are the $\\gamma$-coordinate vectors of the vectors in the basis $\\beta$. That is, $$\\gamma \\stackrel{P}\\leftarrow \\beta =\\begin{bmatrix}[\\vec{b_1}]\\gamma &amp; [\\vec{b_2}]\\gamma \\cdots [\\vec{b_n}]\\gamma\\end{bmatrix}$$ $\\gamma \\stackrel{P}\\leftarrow \\beta$ is called the change-of-coordinates matrix from $\\beta$ to $\\gamma$.Multiplication by $\\gamma \\stackrel{P}\\leftarrow \\beta$ converts $\\beta$-coordinates into $\\gamma$-coordinates. Change of Basis in $R^n$For Example: Let $\\vec{b_1} = \\begin{bmatrix}-9\\\\1\\end{bmatrix}$,$\\vec{b_2} = \\begin{bmatrix}-5\\\\-1\\end{bmatrix}$,$\\vec{c_1} = \\begin{bmatrix}1\\\\-4\\end{bmatrix}$,$\\vec{c_2} = \\begin{bmatrix}3\\\\-5\\end{bmatrix}$, and consider the bases for $R^2$ given by $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$ and $\\gamma = \\lbrace \\vec{c_1}, \\vec{c_2}\\rbrace$. Find the change-of-coordinates matrix from $\\beta$ to $\\gamma$. Solution: The matrix $\\gamma \\stackrel{P}\\leftarrow \\beta$ involves the $\\gamma$-coordinate vectors of $\\vec{b_1}$ and $\\vec{b_2}$. Let $[\\vec{b_1}_\\gamma] = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ and $[\\vec{b_2}_\\gamma] = \\begin{bmatrix}y_1\\\\y_2\\end{bmatrix}$. Then, by definition, $$\\begin{bmatrix}\\vec{c_1} &amp; \\vec{c_2}\\end{bmatrix}\\begin{bmatrix}\\vec{x_1} \\\\ \\vec{x_2}\\end{bmatrix}=\\vec{b_1} \\space and \\space\\begin{bmatrix}\\vec{c_1} &amp; \\vec{c_2}\\end{bmatrix}\\begin{bmatrix}\\vec{y_1} \\\\ \\vec{y_2}\\end{bmatrix}=\\vec{b_2}$$ To solve both systems simultaneously, augment the coefficient matrix with $\\vec{b1}$ and $\\vec{b2}$, and row reduce: $$\\left[ \\begin{array}{cc|c} \\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{b_1} &amp; \\vec{b_2}\\end{array}\\right] =\\left[ \\begin{array}{cc|cc} -1 &amp; 3 &amp; -9 &amp; -5 \\\\ -4 &amp; -5 &amp; 1 &amp; -1\\end{array}\\right]\\sim\\left[ \\begin{array}{cc|cc} 1 &amp; 0 &amp; 6 &amp; 4 \\\\ 0 &amp; 1 &amp; -5 &amp; -3\\end{array}\\right]$$ Thus $$[\\vec{b_1}]_\\gamma =\\begin{bmatrix}6 \\\\ -5\\end{bmatrix}\\space and \\space[\\vec{b_2}]_\\gamma =\\begin{bmatrix}4 \\\\ -3\\end{bmatrix}$$ The desired change-of-coordinates matrix is therefore $$\\gamma \\stackrel{P}\\leftarrow \\beta=\\begin{bmatrix}[\\vec{b_1}]_\\gamma &amp; [\\vec{b_2}]_\\gamma\\end{bmatrix}=\\begin{bmatrix}6 &amp; 4 \\\\ -5 &amp; -3\\end{bmatrix}$$ An analogous procedure works for finding the change-of-coordinates matrix between any two bases in $R^n$: $$\\left[ \\begin{array}{cc|c} \\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{b_1} &amp; \\vec{b_2}\\end{array}\\right]\\sim\\left[ \\begin{array}{c|c} I &amp; \\gamma \\stackrel{P}\\leftarrow \\beta \\end{array}\\right]$$ the change-ofcoordinate matrices $P_\\beta$ and $P_\\gamma$ that convert $\\beta$-coordinates and $\\gamma$-coordinates, respectively, into standard coordinates. $$P_\\beta[\\vec{x}]_\\beta = \\vec{x}, \\spaceP_\\gamma[\\vec{x}]_\\gamma = \\vec{x}, \\space and \\space[\\vec{x}]_\\gamma = P_\\gamma^{-1}\\vec{x}$$ Thus, $$[\\vec{x}]_\\gamma = P_\\gamma^{-1}\\vec{x} = P_\\gamma^{-1}P_\\beta[\\vec{x}]_\\beta\\\\\\Rightarrow\\\\\\gamma \\stackrel{P}\\leftarrow \\beta = P_\\gamma^{-1}P_\\beta$$ More about Coordinates Space and Transformations in Graphics &gt;&gt; Applications to Difference Equation差分方程Linear Independence in the Space $S$ of Signalswe consider a set of only three signals in $S$, say, $\\lbrace u_k \\rbrace,\\lbrace v_k \\rbrace,\\lbrace w_k \\rbrace$.They are linearly independent precisely when the equation $$c_1 u_k + c_2 v_k + c_3 w_k = 0 for\\space all \\space k\\tag{1}$$ implies that $c1 = 0, c2 = 0, c3 = 0$.Then equation (1) holds for any three consecutive values of $k$, say, $k, k + 1, \\space and \\space k + 2$.Hence $c1, c2, c3$ satisfy $$\\begin{bmatrix}u_k &amp; v_k &amp; w_k\\\\u_{k+1} &amp; v_{k+1} &amp; w_{k+1}\\\\u_{k+2} &amp; v_{k+2} &amp; w_{k+2}\\end{bmatrix}\\begin{bmatrix}c_1\\\\c_2\\\\c_3\\\\\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}\\tag{2}$$ The coefficient matrix in this system is called the Casorati matrix of the signals, and the determinant of the matrix is called the Casoratian of $u_k, v_k, w_k$. If the Casorati matrix is invertible for at least one value of k, then (2) will imply that $c1 = c2 = c3 = 0$, which will prove that the three signals are linearly independent. For Example: Verify that $1^k, (-2)^k, and \\space 3^k$ are linearly independent signals. Solution: The Casorati matrix is: $$\\begin{bmatrix}1^k &amp; (-2)^k &amp; 3^k\\\\1^{k+1} &amp; (-2){k+1} &amp; 3^{k+1}\\\\1^{k+2} &amp; (-2)^{k+2} &amp; 3^{k+2}\\end{bmatrix}$$ The Casorati matrix is invertible for $k = 0$. So $1^k, (-2)^k, 3^k$ are linearly independent. Linear Difference Equations（线性差分方程）Given scalars $a_0,\\cdots, a_n$, with $a_0$ and $a_n$ nonzero, and given a signal $\\lbrace z_k \\rbrace$, the equation $$a_0y_{k+n} + a_1y_{k+n-1} + \\cdots + a_ny_k = z_k for\\space all\\space k \\tag{3}$$ is called a linear difference equation (or linear recurrence relation) of order $n$. For simplicity, $a_0$ is often taken equal to 1. If $\\lbrace z_k \\rbrace$ is the zero sequence, the equation is homogeneous; otherwise, the equation is nonhomogeneous. 在第一章节的笔记中，有提一嘴Fibonacci数列是差分方程，具体形式如下,为什么看起来是差分方程的定义不太一样？（接着看下面的内容） $$\\vec{x_{k+1}} = A\\vec{x_k}, k = 0,1,2,…\\tag{5}$$ $$Fibonacci\\\\f(0) = 0, f(1) = 1\\\\f(n) = f(n-1) + f(n-2), n &gt; 1$$建立矩阵方程如下：$$let \\space F(n) = \\begin{bmatrix}f(n) \\\\ f(n+1)\\end{bmatrix}\\\\AF(n) = F(n+1)\\rightarrowA\\begin{bmatrix}f(n) \\\\ f(n+1)\\end{bmatrix} =\\begin{bmatrix}f(n+1) \\\\ f(n+2)\\end{bmatrix}\\\\\\rightarrowA = \\begin{bmatrix}0 &amp; 1 \\\\ 1 &amp; 1\\end{bmatrix}\\\\\\RightarrowF(n) = A^nF(0), F(0) = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$$ For Example: In digital signal processing, a difference equation such as (3) describes a linear filter, and $a_0,\\cdots, a_n$ are called the filter coefficients. If $\\lbrace y_k \\rbrace$ is treated as the input and $\\lbrace z_k \\rbrace$ as the output, then the solutions of the associated homogeneous equation are the signals that are filtered out and transformed into the zero signal. Let us feed two different signals into the filter $$0.35y_{k+2} + 0.5y_{k+1} + 0.35y_k = z_k$$ Here $0.35$ is an abbreviation for $\\sqrt2/4$. The first signal is created by sampling the continuous signal $y = \\cos(\\pi t / 4)$at integer values of $t$, as in Figure3(a). The discrete signal is $$\\lbrace y_k\\rbrace = \\lbrace \\cdots, \\cos(0), \\cos(\\pi/4), cos(2\\pi/4), cos(3\\pi/4), \\cdots\\rbrace$$ For simplicity, write $\\pm0.7$ in place of $\\pm\\sqrt2/2$, so that $$\\lbrace y_k\\rbrace = \\lbrace \\cdots, 1, 0.7, 0, -0.7,-1,-0.7,0,0.7,1,0.7,0 \\cdots\\rbrace$$ Table 1 shows a calculation of the output sequence $\\lbrace z_k \\rbrace$, where $0.35\\cdot0.7$ is an abbreviation for $(\\sqrt2/4)(\\sqrt2/2)=0.25$. The output is $\\lbrace y_k\\rbrace$, shifted by one term. A different input signal is produced from the higher frequency signal $y = \\cos(3\\pi/4)$, shown in Figure 3(b). Sampling at the same rate as before produces a new input sequence: $$\\lbrace w_k\\rbrace = \\lbrace \\cdots, 1, -0.7, 0, 0.7, -1, 0.7, 0 \\cdots\\rbrace$$ When $\\lbrace w_k\\rbrace$ is fed into the filter, the output is the zero sequence. The filter, called a low-pass filter（低通滤波器）, lets $\\lbrace y_k \\rbrace$ pass through, but stops the higher frequency $\\lbrace w_k \\rbrace$. In many applications, a sequence $\\lbrace z_k \\rbrace$ is specified for the right side of a difference equation (3), and a $\\lbrace y_k \\rbrace$ that satisfies (3) is called a solution of the equation. The next example shows how to find solutions for a homogeneous equation. （比如机器学习就类似于如此，给定输入和输出，学习最优权重） For Example: Solutions of a homogeneous difference equation often have the form $y_k = r^k$ for some $r$. Find some solutions of the equation $$y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0 , for\\space all\\space k$$ Solution: Substitute $r^k$ for $y_k$ in the equation and factor the left side: $$r^{k+3} - 2r^{k+2} - 5r^{k+1} + 6r^{k} = 0\\rightarrowr^k(r-1)(r+2)(r-3) = 0$$ Thus, $(1)^k, (-2)^k, (3)^k$ are all solutions. In general, a nonzero signal $r^k$ satisfies the homogeneous difference equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_ny_k = 0 for\\space all\\space k $$ if and only if $r$ is a root of the auxiliary equation（辅助方程） $$r^n + a_1r^{n-1} + \\cdots + a_{n-1}r + a_n = 0$$ Solution Sets of Linear Difference Equationsif $a_n \\neq 0$ and if $\\lbrace z_k \\rbrace$ is given, the equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = z_k, for\\space all\\space k$$ has a unique solution whenever $y_0, \\cdots, y_{n-1}$ are specified. The set $H$ of all solutions of the nth-order homogeneous linear difference equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = 0, for\\space all\\space k$$ is an $n$-dimensional vector space. Nonhomogeneous Equations（非齐次差分方程）The general solution of the nonhomogeneous difference equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = z_k, for\\space all\\space k$$ For Example: Verify that the signal $y_k = k^2$ satisfies the difference equation $$y_{k+2} - 4y_{k+1} + 3y_k = -4k, for\\space all\\space k\\tag{12}$$ Then find a description of all solutions of this equation. Solution: Substitute $k^2$ for $y_k$ on the left side of (12): $$(k+2)^2-4(k+1)^2+3k^2 = -4k$$ so $k^2$is indeed a solution of (12). The next step is to solve the homogeneous equation $$y_{k+2} - 4y_{k+1} + 3y_k = 0, for\\space all\\space k\\tag{13}$$ The auxiliary equation is $$r^2 -4r + 3 = (r-1)(r+3) = 0$$ The roots are $r = 1,3$.So two solutions of the homogeneous difference equation are $1^k$ and $3^k$. They are obviously not multiples of each other, so they are linearly independent signals. the solution space is two-dimensional, so 3k and 1k form a basis for the set of solutions of equation (13).Translating that set by a particular solution of the nonhomogeneous equation (12), we obtain the general solution of (12): $$k^2 + c_1 1^k + c_2 3^k, or \\space k^2 + c_1 + c_23^k$$ Figure 4 gives a geometric visualization of the two solution sets. Each point in the figure corresponds to one signal in S. Reduction to Systems of First-Order Equations（约简到一阶方程组） 接下来可以解释Fibonacci数列是差分方程 $$AF(n) = F(n+1)\\rightarrowA\\begin{bmatrix}f(n)\\\\f(n+1)\\end{bmatrix} =\\begin{bmatrix}f(n+1)\\\\f(n+2)\\end{bmatrix}\\$$ A modern way to study a homogeneous $nth$-order linear difference equation is to replace it by an equivalent system of first-order difference equations, written in the form $$\\vec{x_{k+1}} = A\\vec{x_{k}}, for \\space all\\space k$$ where the vectors $\\vec{x_{k}}$ are in $R^n$ and $A$ is an $n \\times n$ matrix. For Example: Write the following difference equation as a first-order system: $$y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0, for \\space all\\space k$$ Solution: for each k, set $$\\vec{x_k} =\\begin{bmatrix}y_k\\\\y_{k+1}\\\\y_{k+2}\\end{bmatrix}$$ The difference equation says that $y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0$, so $$\\vec{x_{k+1}} =\\begin{bmatrix}y_{k+1} \\\\ y_{k+2} \\\\ y_{k+3}\\end{bmatrix} =\\begin{bmatrix}0 + y_{k+1} + 0\\\\0 + 0 + y_{k+2}\\\\-6y_k + 5y_{k+1} + 2y_{k+2}\\end{bmatrix} =\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\\\-6 &amp; 5 &amp; 2\\end{bmatrix}\\begin{bmatrix}y_{k}\\\\y_{k+1}\\\\y_{k+2}\\end{bmatrix}$$ That is, $$\\vec{x_{k+1}} = A\\vec{x_{k}}, for \\space all\\space k , whereA =\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\\\-6 &amp; 5 &amp; 2\\end{bmatrix}$$ In general, the equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = 0, for\\space all\\space k$$ can be written as $\\vec{x_{k+1}} = A\\vec{x_{k}}, for \\space all\\space k$, where $$\\vec{x_{k}} = \\begin{bmatrix}y_{k}\\\\ y_{k+1} \\\\ \\cdots \\\\ y_{k+n-1} \\end{bmatrix},A = \\begin{bmatrix}0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0\\\\\\cdots\\\\0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1\\\\-a_n &amp; -a_{n-1} &amp; -a_{n-2} &amp; \\cdots &amp; -a_1\\end{bmatrix}$$ Applications to Markov ChainsFor example, if the population of a city and its suburbs were measured each year, then a vector such as $$\\vec{x_0} =\\begin{bmatrix}0.60\\\\0.40\\end{bmatrix}\\tag{1}$$ could indicate that 60% of the population lives in the city and 40% in the suburbs. The decimals in $\\vec{x_0}$ add up to 1 because they account for the entire population of the region.Percentages are more convenient for our purposes here than population totals. A vector with nonnegative entries that add up to 1 is called a probability vector（概率向量）. A stochastic matrix（随机矩阵） is a square matrix whose columns are probability vectors. A Markov chain（马尔科夫链） is a sequence of probability vectors $\\vec{x_0}, \\vec{x_1}, \\vec{x_2}$ together with a stochastic matrix $P$ , such that $$\\vec{x_1} = P\\vec{x_0}, \\vec{x_2} = P\\vec{x_1}, \\vec{x_3} = P\\vec{x_2}, \\cdots\\\\\\longrightarrow\\vec{x_{k+1}} = P\\vec{x_k}$$ $\\vec{x_k}$ is often called a state vector（状态向量). Predicting the Distant FutureFor Example: Let $P = \\begin{bmatrix}0.5 &amp; 0.2 &amp; 0.3\\\\0.3 &amp; 0.8 &amp; 0.3\\\\0.2 &amp; 0 &amp; 0.4\\end{bmatrix}$ and $\\vec{x_0} = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}$. Consider a system whose state is described by the Markov chain $\\vec{x_{k+1}} = P\\vec{x_{k}}, for k = 0,1,\\cdots$ What happens to the system as time passes? Compute the state vectors $\\vec{x_1},\\cdots, \\vec{x_{15}}$to find out. Solution:$$\\vec{x_1} = P\\vec{x_0} = \\begin{bmatrix}0.5\\\\0.3\\\\0.2\\end{bmatrix},\\\\\\\\\\cdots\\\\\\vec{x_{14}} = P\\vec{x_{13}} = \\begin{bmatrix}0.3001\\\\0.59996\\\\0.10002\\end{bmatrix},\\\\\\vec{x_{15}} = P\\vec{x_{14}} = \\begin{bmatrix}0.3001\\\\0.59998\\\\0.10001\\end{bmatrix}$$These vectors seem to be approaching $\\vec{q} = \\begin{bmatrix}0.3\\\\0.6\\\\0.1\\end{bmatrix}$The probabilities are hardly changing from one value of $k$ to the next. $$P\\vec{q} = \\begin{bmatrix}0.5 &amp; 0.2 &amp; 0.3\\\\0.3 &amp; 0.8 &amp; 0.3\\\\0.2 &amp; 0 &amp; 0.4\\end{bmatrix}\\begin{bmatrix}0.3\\\\0.6\\\\0.1\\end{bmatrix}=\\begin{bmatrix}0.3\\\\0.6\\\\0.1\\end{bmatrix}=\\vec{q}$$When the system is in state $q$, there is no change in the system from one measurement to the next. Steady-State VectorsIf $P$ is a stochastic matrix, then a steady-state vector (or equilibrium vector（平衡向量）) for $P$ is a probability vector $\\vec{q}$ such that $$P\\vec{q} = \\vec{q}$$ For Example: Let $P = \\begin{bmatrix} 0.6 &amp; 0.3 \\\\ 0.4 &amp; 0.7\\end{bmatrix}$. Find a steady-state vector for $P$. Solution:$$P\\vec{x} = \\vec{x}\\rightarrow (P-I)\\vec{x} = \\vec{0}\\\\longrightarrow\\begin{bmatrix}-0.4 &amp; 0.3 &amp; 0 \\\\0.4 &amp; -0.3 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -3/4 &amp; 0 \\\\0 &amp; 0 &amp; 0\\end{bmatrix}$$$$\\vec{x} = x_2\\begin{bmatrix}3/4 \\\\ 1\\end{bmatrix}$$ Obviously, One obvious choice is $\\begin{bmatrix}3/4 \\\\ 1\\end{bmatrix}$,but a better choice with no fractions is $\\vec{w} = \\begin{bmatrix}3 \\\\ 4\\end{bmatrix}, (x_2 = 4)$, since every solution is a multiple of the solution$\\vec{w}$ above. Divide $\\vec{w}$ by the sum of its entries and obtain $$\\vec{q} = \\begin{bmatrix}3/7 \\\\ 4/7\\end{bmatrix}$$ if $P$ is an $n\\times n$ regular stochastic matrix, then $P$ has a unique steady-state vector $\\vec{q}$. Further, if $\\vec{x_0}$ is any initial state and $\\vec{x_{k+1}} = P \\vec{x_k}$, then the Markov chain $\\vec{x_k}$ converges to $\\vec{q}$ as $k \\rightarrow \\infty$. Also, $\\vec{q}$ is the eigenvector of $P$. See C5 ReadingNotes.","link":"/Math/Linear-Algebra/Algebra-C4-Vector-Spaces/"},{"title":"Calculus-C12-Vectors-and-the-Geometry-of-Space","text":"Keywords: Vector, Dot Product, Lines and Planes in Space This is the Chapter12 ReadingNotes from book Thomas Calculus 14th.","link":"/Math/Calculus/Calculus-C12-Vectors-and-the-Geometry-of-Space/"},{"title":"Calculus-C15-Multiple-Integrals","text":"Keywords: Double Integration This is the Chapter15 ReadingNotes from book Thomas Calculus 14th. Double and Iterated Integrals over RectanglesDouble Integrals over General RegionsArea by Double IntegrationDouble Integrals in Polar FormTriple Integrals in Rectangular CoordinatesApplicationsTriple Integrals in Cylindrical and Spherical CoordinatesSubstitutions in Multiple Integrals","link":"/Math/Calculus/Calculus-C15-Multiple-Integrals/"},{"title":"Calculus-C10-Infinite-Sequences-and-Series","text":"Keywords: Taylor’s Theorem, Absolute Convergence This is the Chapter10 ReadingNotes from book Thomas Calculus 14th. Taylor and Maclaurin Series（泰勒和麦克劳林级数）Series Representations(级数表示)Taylor and Maclaurin Series DEFINITIONSLet $ƒ$ be a function with derivatives of all orders throughout some interval containing a as an interior point. Then the Taylor series generated by $ƒ$ at $x = a$ is$$\\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!}(x-a)^k = f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n + \\cdots$$The Maclaurin series of $ƒ$ is the Taylor series generated by $ƒ$ at $x = 0$, or$$\\sum_{k=0}^{\\infty} \\frac{f^{(k)}(0)}{k!}(x-0)^k = f(0) + f’(0)(x-0) + \\frac{f’’(0)}{2!}(x-0)^2 + \\cdots + \\frac{f^{(n)}(0)}{n!}(x-0)^n + \\cdots$$ Taylor Polynomials DEFINITIONLet $ƒ$ be a function with derivatives of order $k$ for $k = 1, 2, . . . , N$ in some interval containing $a$ as an interior point. Then for any integer $n$ from $0$ through $N$, the Taylor polynomial of order $n$ generated by $ƒ$ at $x = a$ is the polynomial$$P_n(x) = f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(k)}(a)}{k!}(x-a)^k + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n$$ Just as the linearization of $ƒ$ at $x = a$ provides the best linear approximation of $ƒ$ in the neighborhood of $a$, the higher-order Taylor polynomials provide the “best” polynomial approximations of their respective degrees. Convergence of Taylor Series THEOREM 23—Taylor’s TheoremIf $ƒ$ and its first $n$ derivatives $ƒ’, ƒ’’, \\cdots, ƒ^{(n)}$ are continuous on the closed interval between $a$ and $b$, and $ƒ^{(n)}$ is differentiable on the open interval between $a$ and $b$, then there exists a number $c$ between $a$ and $b$ such that$$f(b) = f(a) + f’(a)(b-a) + \\frac{f’’(a)}{2!}(b-a)^2 + \\cdots + \\frac{f^{n}(a)}{n!}(b-a)^n + \\frac{f^{n+1}(c)}{(n+1)!}(b-a)^{n+1}$$ Taylor’s Theorem is a generalization of the Mean Value Theorem. Taylor’s FormulaIf $ƒ$ has derivatives of all orders in an open interval $I$ containing $a$, then for each positive integer $n$ and for each $x$ in $I$,$$f(x) = f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^n(a)}{n!}(x-a)^n + R_n(x)\\tag{1}$$where$$R_n(x) = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}\\tag{2}$$for some $c$ between $a$ and $x$ Equation (1) is called Taylor’s formula. The function $R_n(x)$ is called the remainder of order $n$ or the error term for the approximation of $ƒ$ by $P_n(x)$ over $I$. If $R_n(x) \\rightarrow 0$ as $n \\rightarrow \\infty$ for all $x \\in I$, we say that Taylor series generated by $f$ at $x = a$ converges to $f$ on $I$, and we write,$$f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!}(x-a)^k$$ Estimating the Remainder THEOREM 24—The Remainder Estimation TheoremIf there is a positive constant $M$ such that $|ƒ^{(n+1)}(t)| \\leq M$ for all $t$ between $x$ and $a$, inclusive, then the remainder term $R_n(x)$ in Taylor’s Theorem satisfies the inequality$$|R_n(x)| \\leq M\\frac{|x-a|^{n+1}}{(n+1)!}$$If this inequality holds for every $n$ and the other conditions of Taylor’s Theorem are satisfied by $ƒ$, then the series converges to $ƒ(x)$.","link":"/Math/Calculus/Calculus-C10-Infinite-Sequences-and-Series/"},{"title":"Calculus-C17-Second-Order-Differential-Equations","text":"Keywords: Second-Order Linear Equations, Euler Equations, Nonhomogeneous Linear Equations This is the Chapter17 ReadingNotes from book Thomas Calculus 14th. Second-Order Linear EquationsNonhomogeneous Linear EquationsApplicationsEuler EquationsPower-Series Solutions","link":"/Math/Calculus/Calculus-C17-Second-Order-Differential-Equations/"},{"title":"Calculus-C11-Parametric-Equations-and-Polar-Coordinates","text":"Keywords: Cycloids, Brachistochrones（最速降线）, Parametric Curves This is the Chapter11 ReadingNotes from book Thomas Calculus 14th. Parametrizations of Plane CurvesParametric Equations DefinitionIf $x$ and $y$ are given as functions$$x = f(t), y = g(t)$$over an interval $I$ of $t$-values, then the set of points $(x, y) = (ƒ(t), g(t))$ defined by these equations is a parametric curve. The equations are parametric equations for the curve. Cycloids（摆线）For example: A wheel of radius a rolls along a horizontal straight line. Find parametric equations for the path traced by a point $P$ on the wheel’s circumference. The path is called a cycloid. Solution: We take the line to be the $x$-axis, mark a point $P$ on the wheel, start the wheel with $P$ at the origin, and roll the wheel to the right. As parameter, we use the angle $t$ through which the wheel turns, measured in radians. Figure 11.9 shows the wheel a short while later when its base lies at units from the origin. The wheel’s center $C$ lies at $(at, a)$ and the coordinates of $P$ are $$x = at + a \\cos \\theta, y = a + a \\sin \\theta$$To express $\\theta$ in terms of $t$, we observe that $t + \\theta = \\frac{3\\pi}{2}$ in the figure, so that$$\\theta = \\frac{3\\pi}{2} - t$$This makes$$\\cos\\theta = -\\sin t, \\sin \\theta = -\\cos t$$The equations we seek are$$x = at - a \\sin t, y = a(1-\\cos t)$$ Brachistochrones（最速降线） and Tautochrones（等时曲线）two properties: Among all smooth curves joining these points, the cycloid is the curve along which a frictionless bead, subject only to the force of gravity, will slide from $O$ to $B$ the fastest. This makes the cycloid a brachistochrone (“brah-kiss-toe-krone”), or shortest-time curve for these points. even if you start the bead partway down the curve toward $B$, it will still take the bead the same amount of time to reach $B$. This makes the cycloid a tautochrone (“taw-toe-krone”),or same-time curve for $O$ and $B$. Are there any other brachistochrones joining $O$ and $B$, or is the cycloid the only one?We can formulate this as a mathematical question in the following way. At the start, the kinetic energy of the bead is zero, since its velocity (speed) is zero. The work done by gravity in moving the bead from $(0, 0)$ to any other point $(x, y)$ in the plane is $mgy$, and this must equal the change in kinetic energy. That is, $$mgy = \\frac{1}{2}mv^2 - \\frac{1}{2}m(0)^2$$Thus, the speed of the bead when it reaches $(x, y)$ has to be $v = \\sqrt{2gy}$. That is,$$\\frac{ds}{dT} = \\sqrt{2gy}, s: arc-length, T: time$$or$$dT = \\frac{ds}{\\sqrt{2gy}} = \\frac{\\sqrt{1+(dy/dx)^2}dx}{\\sqrt{2gy}}$$The time $T_ƒ$ it takes the bead to slide along a particular path $y = ƒ(x)$ from $O$ to $B(a\\pi, 2a)$ is$$T_f = \\int_{x=0}^{x=a\\pi} \\sqrt{\\frac{1+(dy/dx)^2}{2gy}}dx$$What curves $y = ƒ(x)$, if any, minimize the value of this integral? More about Calculus of Variations &gt;&gt; Calculus with Parametric CurvesTangents and AreasA parametrized curve $x = ƒ(t)$ and $y = g(t)$ is differentiable at $t$ if $ƒ$ and $g$ are differentiable at $t$.At a point on a differentiable parametrized curve where $y$ is also a differentiable function of $x$ follows the Chain Rule:$$\\frac{dy}{dt} = \\frac{dy}{dx} \\cdot \\frac{dx}{dt}$$ Parametric Formula for $dy/dx$If all three derivatives exist and $dx/dt \\neq 0$, then$$\\frac{dy}{dt} = \\frac{dy/dt}{dx/dt}\\tag{1}$$If parametric equations define $y$ as a twice-differentiable function of $x$, then$$\\frac{d^2y}{d^2x} = \\frac{d}{dx}y’ = \\frac{d’y/dt}{dx/dt}$$ Length of a Parametrically Defined CurveWe assume the functions $ƒ$ and $g$ are continuously differentiable, the derivatives $ƒ’(t)$ and $g’(t)$ are not simultaneously zero, which prevents the curve $C$ from having any corners or cusps. Such a curve is called a smooth curve. $$\\begin{aligned}L_k &amp;= \\sqrt{(\\Delta x_k)^2 + (\\Delta y_k)^2}\\\\&amp;= \\sqrt{[f(t_k) - f(t_{k-1})]^2 + [g(t_k) - g(t_{k-1})]^2}\\end{aligned}$$By the Mean Value Theorem there are numbers $t_k^\\ast$ and $t_k^{\\ast\\ast}$ in $[t_{k-1}, t_{k}]$ such that $$\\Delta x_k = f(t_k) - f(t_{k-1}) = f’(t_k^\\ast)\\Delta t_k$$$$\\Delta y_k = g(t_k) - g(t_{k-1}) = g’(t_k^{\\ast\\ast})\\Delta t_k$$ thus, $$\\begin{aligned}\\sum_{k=1}^{n} L_k &amp;= \\sum_{k=1}^{n}\\sqrt{[f’(t_k^\\ast)\\Delta t_k]^2 + [g’(t_k^{\\ast\\ast})\\Delta t_k]^2}\\\\&amp;= \\sum_{k=1}^{n}\\sqrt{[f’(t_k^\\ast)]^2 + [g’(t_k^{\\ast\\ast})]^2} \\Delta t_k\\end{aligned}$$ Although this last sum on the right is not exactly a Riemann sum (because $ƒ’$ and $g’$ are evaluated at different points), it can be shown that its limit, as the norm of the partition tends to zero and the number of segments $n \\rightarrow \\infty$, is the definite integral $$\\lim_{||P|| \\rightarrow 0} \\sum_{k=1}^{n}\\sqrt{[f’(t_k^\\ast)]^2 + [g’(t_k^{\\ast\\ast})]^2} \\Delta t_k =\\int_a^b \\sqrt{[f’(t)]^2 + [g’(t)]^2}dt$$ DefinitionIf a curve $C$ is defined parametrically by $x = ƒ(t)$ and $y = g(t)$, $a \\leq t \\leq b$, where $ƒ’$ and $g’$ are continuous and not simultaneously zero on $[a,b]$, and $C$ is traversed exactly once as $t$ increases from $t = a$ to $t = b$, then the length of $C$ is the definite integral$$L = \\int_a^b \\sqrt{[f’(t)]^2 + [g’(t)]^2} dt\\tag{3}$$ Length of a Curve $y = ƒ(x)$$$x = t, y = f(t), a \\leq t \\leq b$$thus,$$\\frac{dx}{dt} = 1, \\frac{dy}{dt} = f’(t)$$thus,$$L = \\int_a^b \\sqrt{1 + [f’(x)]^2} dx$$ The Arc Length Differential$$ds = \\sqrt{dx^2 + dy^2}$$ To be added…","link":"/Math/Calculus/Calculus-C11-Parametric-Equations-and-Polar-Coordinates/"},{"title":"Calculus-C16-Integrals-and-Vector-Fields","text":"Keywords: Vector Fields and Line Integrals, Green’s Theorem, Stokes’ Theorem, Divergence This is the Chapter16 ReadingNotes from book Thomas Calculus 14th. Line Integrals of Scalar Functions Suppose that $ƒ(x, y, z)$ is a real-valued function we wish to integrate over the curve $C$ lying within the domain of $ƒ$ and parametrized by $r(t) = g(t)i + h(t)j + k(t)k$, $a \\leq t \\leq b$. The values of $ƒ$ along the curve are given by the composite function $ƒ(g(t), h(t), k(t))$. We are going to integrate this composition with respect to arc length from $t = a$ to $t = b$. To begin, we first partition the curve $C$ into a finite number $n$ of subarcs (Figure 16.1). The typical subarc has length $\\Delta s_k$. In each subarc we choose a point $(x_k, y_k, z_k)$ and form the sum $$S_n = \\sum_{k=1}^n f(x_k, y_k, z_k) \\Delta s_k$$ Vector Fields and Line Integrals: Work, Circulation, and FluxPath Independence, Conservative Fields, and Potential FunctionsGreen’s Theorem in the PlaneSurfaces and AreaSurface IntegralsStokes’ TheoremThe Divergence Theorem and a Unified Theory","link":"/Math/Calculus/Calculus-C16-Integrals-and-Vector-Fields/"},{"title":"Calculus-C14-Partial-Derivatives","text":"Keywords: Partial Derivatives, Directional Derivatives and Gradient Vectors, Lagrange Multipliers This is the Chapter14 ReadingNotes from book Thomas Calculus 14th. Functions of Several Variables DEFINITIONSSuppose $D$ is a set of $n$-tuples of real numbers $(x_1, x_2, \\cdots , x_n)$. A real-valued function $ƒ$ on $D$ is a rule that assigns a unique (single) real number$$w = f(x_1, x_2, \\cdots , x_n)$$to each element in $D$. The set $D$ is the function’s domain. The set of $w$-values taken on by $ƒ$ is the function’s range. The symbol $w$ is the dependent variable of $ƒ$, and $ƒ$ is said to be a function of the $n$ independent variables $x_1$ to $x_n$. We also call the $x_j$’s the function’s input variables and call $w$ the function’s output variable. Domains and RangesThe domain of a function is assumed to be the largest set for which the defining rule generates real numbers, unless the domain is otherwise specified explicitly. The range consists of the set of output values for the dependent variable. Functions of Two Variables DEFINITIONSA point $(x_0, y_0)$ in a region (set) $R$ in the $xy$-plane is an interior point of $R$ if it is the center of a disk of positive radius that lies entirely in R (Figure 14.2). A point $(x_0, y_0)$ is a boundary point of $R$ if every disk centered at $(x_0, y_0)$ contains points that lie outside of $R$ as well as points that lie in $R$. (The boundary point itself need not belong to $R$.)The interior points of a region, as a set, make up the interior of the region. The region’s boundary points make up its boundary. A region is open if it consists entirely of interior points. A region is closed if it contains all its boundary points (Figure 14.3). DEFINITIONSA region in the plane is bounded if it lies inside a disk of finite radius. A region is unbounded if it is not bounded. Graphs, Level Curves, and Contours of Functions of Two Variables DEFINITIONSThe set of points in the plane where a function $ƒ(x, y)$ has a constant value $ƒ(x, y) = c$ is called a level curve（等高线） of $ƒ$. The set of all points $(x, y, ƒ(x, y))$ in space, for $(x, y)$ in the domain of $ƒ$, is called the graph of $ƒ$. The graph of $ƒ$ is also called the surface $z = ƒ(x, y)$. Functions of Three Variables DEFINITIONThe set of points $(x, y, z)$ in space where a function of three independent variables has a constant value $ƒ(x, y, z) = c$ is called a level surface of $ƒ$. Since the graphs of functions of three variables consist of points $(x, y, z, ƒ(x, y, z))$ lying in a four-dimensional space, we cannot sketch them effectively in our three-dimensional frame of reference. We can see how the function behaves, however, by looking at its three-dimensional level surfaces. DEFINITIONSA point $(x_0, y_0, z_0)$ in a region $R$ in space is an interior point of $R$ if it is the center of a solid ball that lies entirely in $R$ (Figure 14.9a). A point $(x_0, y_0, z_0)$ is a boundary point of $R$ if every solid ball centered at $(x_0, y_0, z_0)$ contains points that lie outside of $R$ as well as points that lie inside R (Figure 14.9b). The interior of $R$ is the set of interior points of $R$. The boundary of $R$ is the set of boundary points of $R$.A region is open if it consists entirely of interior points. A region is closed if it contains its entire boundary. Computer GraphingFigure 14.11 shows computer-generated graphs of a number of functions of two variables together with their level curves. Limits and Continuity in Higher DimensionsLimits for Functions of Two Variables DefinitionWe say that a function ƒ(x, y) approaches the limit $L$ as $(x, y)$ approaches $(x_0 , y_0)$, and write$$\\lim_{(x,y)\\rightarrow (x_0,y_0)} f(x,y) = L$$if, for every number $\\epsilon &gt; 0$, there exists a corresponding number $\\delta &gt; 0$ such that for all $(x, y)$ in the domain of $ƒ$,$$|f(x,y) - L| &lt; \\epsilon, whenever, 0 &lt; \\sqrt{(x-x_0)^2 + (y-y_0)^2} &lt; \\delta$$ Continuity DefinitionA function ƒ(x, y) is continuous at the point $(x_0 , y_0)$ if ƒ is defined at $(x_0 , y_0)$, $\\lim_{(x, y)\\rightarrow (x_0, y_0)} ƒ(x, y)$ exists, $\\lim_{(x, y)\\rightarrow (x_0, y_0)} ƒ(x, y) = f(x_0,y_0)$.A function is continuous if it is continuous at every point of its domain. Continuity of CompositionsIf $ƒ$ is continuous at $(x_0 , y_0)$ and $g$ is a single-variable function continuous at $ƒ(x0 , y0)$, then the composition $h = g \\circ f$ defined by $h(x, y) = g(ƒ(x, y))$ is continuous at $(x_0, y_0)$. Partial DerivativesPartial Derivatives of a Function of Two Variables DEFINITIONThe partial derivative of $ƒ(x, y)$ with respect to $x$ at the point $(x_0 , y_0)$ is$$\\left. \\frac{\\partial f}{\\partial x} \\right|_{x_0, y_0} = \\lim_{h\\rightarrow 0} \\frac{f(x_0+h,y_0) - f(x_0,y_0)}{h}$$The partial derivative of $ƒ(x, y)$ with respect to $y$ at the point $(x_0 , y_0)$ is$$\\left. \\frac{\\partial f}{\\partial y} \\right|_{x_0, y_0} = \\lim_{h\\rightarrow 0} \\frac{f(x_0,y_0 + h) - f(x_0,y_0)}{h}$$ CalculationsFor example: The plane $x = 1$ intersects the paraboloid（抛物面） $z = x_2 + y_2$ in a parabola（抛物线）. Find the slope of the tangent to the parabola at $(1, 2, 5)$ (Figure 14.19). Solution: The parabola lies in a plane parallel to the $yz$-plane, and the slope is the value of the partial derivative $\\frac{\\partial z}{\\partial y}$ at $(1, 2)$: $$\\left. \\frac{\\partial z}{\\partial y} \\right|_{(1,2)} = \\left. 2y \\right|_{(1,2)} = 4$$ Functions of More Than Two VariablesPartial Derivatives and ContinuityA function $ƒ(x, y)$ can have partial derivatives with respect to both $x$ and $y$ at a point without the function being continuous there.（对于多变量（元）函数，偏导存在(可导)不一定连续） For example: Let$$f(x,y) =\\begin{cases}0, xy \\neq 0\\\\1, xy = 0\\end{cases}$$(Figure 14.21).(a) Find the limit of $ƒ$ as $(x, y)$ approaches $(0, 0)$ along the line $y = x$.(b) Prove that $ƒ$ is not continuous at the origin.(c) Show that both partial derivatives $\\partial ƒ /\\partial x$ and $\\partial ƒ /\\partial y$ exist at the origin. Solution: (a). Since $ƒ(x, y)$ is constantly zero along the line $y = x$ (except at the origin), we have $$\\left. \\lim_{(x,y) \\rightarrow (0,0)} f(x,y) \\right|_{y=x} = \\lim_{(x,y) \\rightarrow (0,0)} 0 = 0$$ (b). Since $ƒ(0, 0) = 1$, the limit in part (a) is not equal to $ƒ(0, 0)$, which proves that $ƒ$ is not continuous at $(0, 0)$. (c). To find $\\partial ƒ /\\partial x$ at $(0, 0)$, we hold $y$ fixed at $y = 0$. Then $ƒ(x, y) = 1$ for all $x$, and the graph of $ƒ$ is the line $L1$ in Figure 14.21. The slope of this line at any $x$ is $\\partial ƒ /\\partial x$. In particular, $\\partial ƒ /\\partial x = 0$ at $(0, 0)$. Similarly, $\\partial ƒ /\\partial y$ is the slope of line $L2$ at any $y$, so $\\partial ƒ /\\partial y = 0$ at $(0, 0)$. Second-Order Partial Derivatives$$\\frac{\\partial f}{\\partial x \\partial y}\\Longleftrightarrowf_{yx}$$Differentiate first with respect to $y$, then with respect to $x$. The Mixed Derivative Theorem THEOREM 2—The Mixed Derivative TheoremIf $ƒ(x, y)$ and its partial derivatives $f_x, f_y, f_{xy}, f_{yx}$ are defined throughout an open region containing a point $(a, b)$ and are all continuous at $(a, b)$, then$$f_{xy}(a,b) = f_{yx}(a,b)$$ Partial Derivatives of Still Higher OrderDifferentiability DEFINITIONA function $z = ƒ(x, y)$ is differentiable at $(x_0, y_0)$ if $ƒ_x(x_0 , y_0)$ and $ƒ_y(x_0, y_0)$ exist and $\\Delta z = ƒ(x_0 + \\Delta x, y_0 + \\Delta y) - ƒ(x_0, y_0)$ satisfies an equation of the form$$\\Delta z = f_x(x_0, y_0) \\Delta x + f_y(x_0,y_0)\\Delta y + \\epsilon_1 \\Delta x + \\epsilon_2 \\Delta y$$in which each of $\\epsilon_1, \\epsilon_2 \\rightarrow 0$ as both $\\Delta x, \\Delta y \\rightarrow 0$. We call $ƒ$ differentiable if it is differentiable at every point in its domain, and say that its graph is a smooth surface. THEOREM 4—Differentiability Implies Continuity（对于多变量（元）函数，可微一定连续）If a function $ƒ(x, y)$ is differentiable at $(x_0 , y_0)$, then $ƒ$ is continuous at $(x_0 , y_0)$. More about Derivative and Differentiation in Single Variable Function The Chain RuleFunctions of Two Variables Theorem 5—Chain Rule For Functions of One Independent Variable and Two Intermediate VariablesIf $w = ƒ(x, y)$ is differentiable and if $x = x(t), y = y(t)$ are differentiable functions of $t$, then the composition $w = ƒ(x(t), y(t))$ is a differentiable function of $t$ and$$\\frac{dw}{dt} = f_x(x(t), y(t)) x’(t) + f_y(x(t), y(t)) y’(t)$$or$$\\frac{dw}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt}$$ Functions of Three Variables Theorem 6—Chain Rule for Functions of One Independent Variable and Three Intermediate VariablesIf $w = ƒ(x, y, z)$ is differentiable and $x, y$, and $z$ are differentiable functions of $t$, then $w$ is a differentiable function of $t$ and$$\\frac{dw}{dt} = \\frac{\\partial w}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial w}{\\partial y} \\frac{dy}{dt} + + \\frac{\\partial w}{\\partial z} \\frac{dz}{dt}$$ Functions Defined on Surfaces THEOREM 7—Chain Rule for Two Independent Variables and Three Intermediate VariablesSuppose that $w = ƒ(x, y, z)$, $x = g(r, s), y = h(r, s)$, and $z = k(r, s)$. If all four functions are differentiable, then $w$ has partial derivatives with respect to $r$ and $s$, given by the formulas$$\\frac{\\partial w}{\\partial r} = \\frac{\\partial w}{\\partial x}\\frac{\\partial x}{\\partial r} + \\frac{\\partial w}{\\partial y}\\frac{\\partial y}{\\partial r} + \\frac{\\partial w}{\\partial z}\\frac{\\partial z}{\\partial r}$$$$\\frac{\\partial w}{\\partial s} = \\frac{\\partial w}{\\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial w}{\\partial y}\\frac{\\partial y}{\\partial s} + \\frac{\\partial w}{\\partial z}\\frac{\\partial z}{\\partial s}$$ Implicit Differentiation RevisitedSuppose that The function $F(x, y)$ is differentiable and The equation $F(x, y) = 0$ defines $y$ implicitly as a differentiable function of $x$, say$y = h(x)$. Since $w = F(x, y) = 0$, the derivative $dw/ dx$ must be zero. $$0 = \\frac{dw}{dx} = F_x\\frac{dx}{dx} + F_y\\frac{dy}{dx}= F_x \\cdot 1 + F_y \\cdot \\frac{dy}{dx}$$ If $F_y = \\frac{\\partial w}{\\partial y} \\neq 0$, then $$\\frac{dy}{dx} = -\\frac{F_x}{F_y}$$ Functions of Many VariablesDirectional Derivatives and Gradient VectorsDirectional Derivatives in the PlaneSuppose that the function $ƒ(x, y)$ is defined throughout a region $R$ in the $xy$-plane, that $P_0(x_0, y_0)$ is a point in $R$, and that $\\vec{u} = u_1\\vec{i} + u_2\\vec{j}$ is a unit vector. Then the equations$$x = x_0 + s\\vec{u_1}, y = y_0 + s\\vec{u_2}$$parametrize the line through $P_0$ parallel to $\\vec{u}$.If the parameter $s$ measures arc length from $P_0$ in the direction of $\\vec{u}$, we find the rate of change of $ƒ$ at $P_0$ in the direction of $\\vec{u}$ by calculating $dƒ / ds$ at $P_0$. DEFINITIONThe derivative of $f$ at $P_0(x_0, y_0)$ in the direction of the unit vector $\\vec{u} = u_1\\vec{i} + u_2\\vec{j}$ is the number$$\\left( \\frac{df}{ds} \\right)_{\\vec{u}, P_0} = \\lim_{s \\rightarrow 0} \\frac{f(x_0 + su_1, y_0 + su_2) - f(x_0,y_0)}{s}\\tag{1}$$The directional derivative defined by Equation (1) is also denoted by$$D_{\\vec{u}} f(P_0)\\\\or\\\\\\left. D_{\\vec{u}} f \\right|_{P_0}$$ Interpretation of the Directional Derivative The equation $z = ƒ(x, y)$ represents a surface $S$ in space. If $z_0 = ƒ(x_0 , y_0)$, then the point $P(x_0 , y_0 , z_0)$ lies on $S$. The vertical plane that passes through $P$ and $P_0(x_0 , y_0)$ parallel to $\\vec{u}$ intersects $S$ in a curve $C$ (Figure 14.28). The rate of change of $ƒ$ in the direction of $\\vec{u}$ is the slope of the tangent to $C$ at $P$ in the right-handed system formed by the vectors $\\vec{u}$ and $\\vec{k}$. Calculation and GradientsBy Chain Rule: $$\\begin{aligned}\\left( \\frac{df}{ds}\\right)_{\\vec{u},P_0} &amp;= \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} \\frac{dx}{ds} + \\left. \\frac{\\partial f}{\\partial y} \\right|_{P_0} \\frac{dy}{ds}\\\\&amp;= \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} u_1 + \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} u_2 \\\\&amp;= \\underbrace{\\left[ \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} \\vec{i} + \\left. \\frac{\\partial f}{\\partial y} \\right|_{P_0} \\vec{j} \\right]}_{Gradient-of-f-at-P_0} \\cdot \\underbrace{\\left[ u_1\\vec{i} + u_2\\vec{j} \\right]}_{Direction-of-\\vec{u}}\\end{aligned}\\tag{3}$$ DefinitionThe gradient vector (or gradient) of $ƒ(x, y)$ is the vector$$\\nabla f = \\frac{\\partial f}{\\partial x} \\vec{i} + \\frac{\\partial f}{\\partial y} \\vec{j}$$The value of the gradient vector obtained by evaluating the partial derivatives at a point $P_0(x_0, y_0)$ is written$$\\left. \\nabla f \\right|_{P_0}\\\\or\\\\\\nabla f(x_0,y_0)$$ theorem 9—The Directional Derivative Is a Dot ProductIf $ƒ(x, y)$ is differentiable in an open region containing $P_0(x_0, y_0)$, then$$\\left( \\frac{df}{ds} \\right)_{\\vec{u}, P_0} = \\left. \\nabla f \\right|_{P_0} \\cdot \\vec{u}$$ For example: Find the derivative of $ƒ(x, y) = xe^y + cos (xy)$ at the point $(2, 0)$ in the direction of $\\vec{v} = 3\\vec{i} - 4\\vec{j}$.（directional derivative） Solution: Recall that the direction of a vector $\\vec{v}$ is the unit vector obtained by dividing $\\vec{v}$ by its length: $$\\vec{u} = \\frac{\\vec{v}}{|\\vec{v}|} = \\frac{3}{5}\\vec{i} - \\frac{4}{5}\\vec{j}$$ The partial derivatives of $f$ are everywhere continuous and at $(2, 0)$ are given by $$f_x(2,0) = \\left. (e^y - y\\sin(xy)) \\right|_{(2,0)} = 1\\\\f_y(2,0) = 2$$ The gradient of $ƒ$ at $(2, 0)$ is $$\\left. \\nabla f \\right|_{2,0} = f_x(2,0) \\vec{i} + f_y(2,0)\\vec{j} = \\vec{i} + 2\\vec{j}$$ The derivative of $f$ at $(2, 0)$ in the direction of $\\vec{v}$ is therefore $$D_{\\vec{u}} f |_{(2,0)} = \\nabla f_{(2,0)} \\cdot \\vec{u}= -1$$ Evaluating the dot product in the brief version of Equation (4) gives$$D_{\\vec{u}} f = \\nabla f \\cdot \\vec{u} = |\\nabla f| |\\vec{u}| \\cos \\theta = |\\nabla f| \\cos \\theta$$ Properties of the Directional Derivative $D_{\\vec{u}} f = \\nabla f \\cdot \\vec{u} = |\\nabla f| \\cos \\theta$ The function $ƒ$ increases most rapidly when $\\cos\\theta = 1$, which means that $\\theta = 0$ and $\\vec{u}$ is the direction of $\\nabla f$. That is, at each point $P$ in its domain, $ƒ$ increases most rapidly in the direction of the gradient vector $\\nabla f$ at $P$. The derivative in this direction is$$D_{\\vec{u}} f = |\\nabla f| \\cos(0) = |\\nabla f|$$ Similarly, $ƒ$ decreases most rapidly in the direction of $-\\nabla f$. The derivative in this direction is $D_{\\vec{u}} f = |\\nabla f| \\cos(\\pi) = -|\\nabla f|$. Any direction $\\vec{u}$ orthogonal to a gradient $\\nabla f \\neq 0$ is a direction of zero change in $ƒ$ because $\\theta$ then equals $\\pi / 2$ and$$D_{\\vec{u}} f = |\\nabla f| \\cos(\\pi/2) = 0$$ Gradients and Tangents to Level CurvesIf a differentiable function $ƒ(x, y)$ has a constant value $c$ along a smooth curve $r = g(t)i + h(t)j$, then$$\\frac{d}{dt}f(g(t), h(t)) = \\frac{d}{dt} c$$$$\\frac{\\partial f}{\\partial x}\\frac{dg}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dh}{dt} = 0$$ $$\\underbrace{\\left( \\frac{\\partial f}{\\partial x} \\vec{i} + \\frac{\\partial f}{\\partial y} \\vec{j}\\right)}_{\\nabla f} \\cdot\\underbrace{\\left( \\frac{dg}{dt} \\vec{i} + \\frac{dh}{dt} \\vec{j}\\right)}_{\\frac{dr}{dt}} = 0\\tag{5}$$Equation (5) says that $\\nabla ƒ$ is normal to the tangent vector $dr / dt$, so it is normal to the curve. Tangent Line to a Level Curve$$f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0) = 0$$ Functions of Three VariablesThe Chain Rule for PathsIf $r(t) = x(t) i + y(t) j + z(t)k$ is a smooth path $C$, and $w = ƒ(r(t))$ is a scalar functionevaluated along $C$, then according to the Chain Rule,$$\\frac{dw}{dt} = \\frac{\\partial w}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial w}{\\partial y} \\frac{dy}{dt} + \\frac{\\partial w}{\\partial z} \\frac{dz}{dt}$$ The Derivative Along a Path$$\\frac{d}{dt}f(r(t)) = \\nabla f(r(t)) \\cdot r’(t)\\tag{7}$$What Equation (7) says is that the derivative of the composite function $ƒ(r(t))$ is the “derivative” (gradient) of the outside function $ƒ$ “times” (dot product) the derivative of the inside function $r$. Tangent Planes and DifferentialsRecall how the derivative defined the tangent line to the graph of a differentiable function at a point on the graph. The tangent line then provided for a linearization of the function at the point. More about Tangent Line in Singule Variable Function &gt;&gt; how the gradient defines the tangent plane to the level surface of a function $w = ƒ(x, y, z)$ at a point on the surface. The tangent plane then provides for a linearization of $ƒ$ at the point and defines the total differential of the function. Tangent Planes and Normal Lines DEFINITIONSThe tangent plane to the level surface $ƒ(x, y, z) = c$ of a differentiable function $ƒ$ at a point $P_0$ where the gradient is not zero is the plane through $P_0$ normal to $\\nabla f|_{P_0}$. For example: Find the tangent plane and normal line of the level surface$$f(x,y,z) = x^2 + y^2 + z - 9 = 0$$at the point $P_0 (1, 2, 4)$. Solution: The tangent plane is the plane through $P_0$ perpendicular to the gradient of $ƒ$ at $P_0$. The gradient is$$\\nabla f |_{P_0} = (2xi + 2yj + k)|_{(1,2,4)} = 2i + j + k$$The tangent plane is therefore the plane$$2(x-1) + 4(y-2) + (z-4) = 0$$The line normal to the surface at $P_0$ is$$x = 1 + 2t, y = 2 + 4t, z = 4 + t$$ Plane Tangent to a Surface $z = ƒ(x, y)$ at $(x_0 , y_0 , ƒ(x_0 , y_0))$The plane tangent to the surface $z = ƒ(x, y)$ of a differentiable function $ƒ$ at the point $P_0(x_0 , y_0 , z_0) = (x_0 , y_0 , ƒ(x_0 , y_0))$ is$$f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0)-(z-z_0) = 0$$ Estimating Change in a Specific DirectionHow to Linearize a Function of Two VariablesDifferentialsFunctions of More Than Two VariablesExtreme Values and Saddle PointsDerivative Tests for Local Extreme ValuesAbsolute Maxima and Minima on Closed Bounded RegionsLagrange MultipliersConstrained Maxima and MinimaThe Method of Lagrange MultipliersLagrange Multipliers with Two ConstraintsTaylor’s Formula for Two VariablesDerivation of the Second Derivative TestThe Error Formula for Linear ApproximationsTaylor’s Formula for Functions of Two VariablesPartial Derivatives with Constrained VariablesDecide Which Variables Are Dependent and Which Are IndependentHow to Find ew,ex When the Variables in w \u001d ƒ(x, y, z) Are Constrained by Another EquationNotationArrow Diagrams","link":"/Math/Calculus/Calculus-C14-Partial-Derivatives/"},{"title":"Calculus-C2-Limits-and-Continuty","text":"Keywords: Continuity, The Intermediate Value Theorem This is the Chapter2 ReadingNotes from book Thomas Calculus 14th. ContinuityContinuity at a Point DEFINITIONSLet $c$ be a real number that is either an interior point or an endpoint of an interval in the domain of $ƒ$.The function $ƒ$ is continuous at c $if$$$\\lim_{x\\rightarrow c} f(x) = f(c)$$The function $ƒ$ is right-continuous at $c$ (or continuous from the right) if$$\\lim_{x\\rightarrow c^+} f(x) = f(c)$$The function $ƒ$ is** left-continuous** at c (or continuous from the left) if$$\\lim_{x\\rightarrow c^-} f(x) = f(c)$$ Continuous FunctionsInverse Functions and ContinuityContinuity of Compositions of FunctionsIntermediate Value Theorem for Continuous Functions THEOREM 11—The Intermediate Value Theorem for Continuous FunctionsIf $ƒ$ is a continuous function on a closed interval $[a,b]$, and if $y_0$ is any value between $ƒ(a)$ and $ƒ(b)$, then $y_0 = ƒ(c)$ for some $c$ in $[a,b]$.","link":"/Math/Calculus/Calculus-C2-Limits-and-Continuty/"},{"title":"Calculus-C3-Derivatives","text":"Keywords: Derivative, Differentiation, ChainRule, Linearization and Differentials This is the Chapter3 ReadingNotes from book Thomas Calculus 14th. The Derivative as a Function DefinitionThe derivative of the function $f(x)$ with respect to the variable $x$ is the function $f’$ whose value at $x$ is$$\\begin{aligned}f’(x) &amp;= \\lim_{h\\rightarrow0}\\frac{f(x+h)-f(x)}{h}\\\\&amp;= \\lim_{z\\rightarrow x}\\frac{f(z)-f(x)}{z-x}\\end{aligned}$$ If $f’$ exists at a particular $x$, we say that $ƒ$ is differentiable （可微）(has a derivative（有导数）) at $x$. If $f’$ exists at every point in the domain of $f$, we call $f$ differentiable. The process of calculating a derivative is called differentiation.（求导的过程叫做微分） $$f’(x) = \\frac{d}{dx}f(x)$$ Differentiable Functions Are Continuous If $f$ has a derivative at $x = c$, then $ƒ$ is continuous at $x = c$. Differentiation RulesProducts and Quotients Derivative Product RuleIf $u$ and $v$ are differentiable at $x$, then so is their product $uv$, and$$\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}$$ The Chaine Rule If $ƒ(u)$ is differentiable at the point $u = g(x)$ and $g(x)$ is differentiable at $x$, then the composite function $(ƒ \\circ g) (x) = ƒ(g(x))$ is differentiable at x, and$$(f\\circ g)’(x) = f’(g(x)) \\cdot g’(x)$$In Leibniz’s notation, if $y = ƒ(u)$ and $u = g(x)$, then$$\\frac{dy}{dx} = \\frac{dy}{dy}\\frac{du}{dx}$$ Implicit Differentiation$$\\begin{aligned}&amp;x^3 + y^3 -9xy = 0\\\\&amp;y^2-x=0\\\\&amp;x^2 + y^2 - 25 = 0\\end{aligned}$$These equations define an implicit relation between the variables $x$ and $y$, meaning that a value of $x$ determines one or more values of $y$, even though we do not have a simple formula for the $y$-values. Implicitly Defined Functions Differentiate both sides of the equation with respect to $x$, treating $y$ as a differentiable function of $x$. Collect the terms with $\\frac{dy}{dx}$ on one side of the equation and solve for $\\frac{dy}{dx}$. Derivatives of Higher OrderLenses, Tangent Lines, and Normal Lines For example: Show that the point $(2, 4)$ lies on the curve $x^3 + y^3 - 9xy = 0$. Then find the tangent and normal to the curve there (Figure 3.34). Solution: To find the slope of the curve at $(2, 4)$, we first use implicit differentiation to find a formula for $\\frac{dy}{dx}$: $$\\begin{aligned}&amp;x^3 + y^3 -9xy = 0\\\\&amp;\\frac{d}{dx}(x^3) - \\frac{d}{dx}(y^3) - \\frac{d}{dx}(9xy) = \\frac{d}{dx}(0)\\\\&amp;\\frac{dy}{dx} = \\frac{3y-x^2}{y^2-3x}\\end{aligned}$$We then evaluate the derivative at $(x, y) = (2, 4)$:$$\\left. \\frac{dy}{dx}\\right|_{(2,4)} = \\frac{4}{5}$$The tangent at $(2, 4)$ is the line through $(2, 4)$ with slope $\\frac{4}{5}$:$$y = 4 + \\frac{4}{5}(x-2)$$The normal line:$$y = 4 - \\frac{5}{4}(x-2)$$ Derivatives of Inverse Functions and LogarithmsDerivatives of Inverses of Differentiable Functions If $ƒ$ has an interval $I$ as domain and $ƒ’(x)$ exists and is never zero on $I$, then $ƒ^{-1}$ is differentiable at every point in its domain (the range of $ƒ$). The value of $(ƒ^{-1})’$ at a point $b$ in the domain of $ƒ^{-1}$ is the reciprocal of the value of $f’$ at the point $a = ƒ^{-1}(b)$: $$(f^{-1})’(b) = \\frac{1}{f’(f^{-1}(b))}$$or$$\\left. \\frac{df^{-1}}{dx}\\right|_{x=b} =$$ $$\\frac{1}{\\left. \\frac{df}{dx}\\right|_{x=f^{-1}(b)}}$$ Linearization and Differentials（线性化和微分)It is often useful to approximate complicated functions with simpler ones that give the accuracy we want for specific applications and at the same time are easier to work with than the original functions. The approximating functions discussed in this section are called linearizations, and they are based on tangent lines. Other approximating functions, such as polynomials, are discussed in Chapter 10. We introduce new variables $dx$ and $dy$, called differentials, and define them in a way that makes Leibniz’s notation for the derivative $\\frac{dy}{dx}$ a true ratio. Linearization DefinitionIf $ƒ$ is differentiable at $x = a$, then the approximating function$$L(x) = ƒ(a) + ƒ’(a)(x - a)$$is the linearization of $ƒ$ at $a$. The approximation$$f(x) \\approx L(x)$$of $ƒ$ by $L$ is the standard linear approximation of $ƒ$ at $a$. The point $x = a$ is the center of the approximation. Differentials DefinitionLet $y = ƒ(x)$ be a differentiable function. The differential $dx$ is an independent variable. The differential $dy$ is$$dy = f’(x)dx$$ Estimating with DifferentialsFor example: Use differentials to estimate $\\sin(\\pi/6+0.01)$. Solution: The dierential associated with $y = \\sin x$ is $$dy = \\cos x dx$$To estimate $\\sin(\\pi/6+0.01)$, we set $a = \\pi/6$ and $dx = 0.01$. Then$$\\begin{aligned}f(\\pi/6+0.01) &amp;= f(a + dx) \\\\&amp;\\approx f(a) + dy\\\\&amp;= \\sin\\frac{\\pi}{6} + (\\cos\\frac{\\pi}{6})(0.01)\\\\&amp;\\approx 0.5087\\end{aligned}$$For comparison, the true value of $\\sin(\\pi/6+0.01)$ to $6$ decimals is $0.508635$. The method in this example can be used in computer algorithms to give values of trigonometric functions. The algorithms store a large table of sine and cosine values between $0$ and $\\pi/4$. Values between these stored values are computed using differentials as in Example. Values outside of $[0,\\pi/4]$ are computed from values in this interval using trigonometric identities. Error in Differential ApproximationLet $ƒ(x)$ be differentiable at $x = a$ and suppose that $dx = \\Delta x$ is an increment of $x$. We have two ways to describe the change in $ƒ$ as $x$ changes from $a$ to $a + \\Delta x$: the true change:$$\\Delta f = f(a + \\Delta x) - f(a)$$the differential estimate:$$df = f’(a)\\Delta x$$ Change in $y = ƒ(x)$ near $x = a$If $y = ƒ(x)$ is differentiable at $x = a$ and $x$ changes from $a$ to $a + \\Delta x$, the change $\\Delta y$ in $ƒ$ is given by$$\\Delta y = f’(a)\\Delta x + \\epsilon \\Delta x$$in which $\\epsilon \\rightarrow 0$ as $\\Delta x \\rightarrow 0$. To be added…","link":"/Math/Calculus/Calculus-C3-Derivatives/"},{"title":"Calculus-C4-Applications-of-Derivatives","text":"Keywords: Applied Optimization, Concavity and Curve Sketching, Newton’s Method, The Mean Value Theorem This is the Chapter4 ReadingNotes from book Thomas Calculus 14th. Extreme Values of Functions on Closed Intervals DEFINITIONSLet $ƒ$ be a function with domain $D$. Then $ƒ$ has an absolute maximum value on $D$ at a point $c$ if$$f(x) \\leq f(c), for \\space x \\space in \\space D$$and an absolute minimum value on $D$ at $c$ if$$f(x) \\geq f(c), for \\space x \\space in \\space D$$Maximum and minimum values are called extreme values of the function $ƒ$. Absolute maxima or minima are also referred to as global maxima or minima. THEOREM 1—The Extreme Value TheoremIf $ƒ$ is continuous on a closed interval $[a,b]$, then $ƒ$ attains both an absolute maximum value $M$ and an absolute minimum value $m$ in $[a,b]$. That is, there are numbers $x_1$ and $x_2$ in $[a,b]$ with $ƒ(x_1) = m, ƒ(x_2) = M$, and $m \\leq ƒ(x) \\leq M$ for every other $x$ in $[a,b]$. Local (Relative) Extreme Values DEFINITIONSA function $ƒ$ has a local maximum value at a point $c$ within its domain $D$ if $ƒ(x) \\leq ƒ(c)$ for all $x \\in D$ lying in some open interval containing $c$.A function $ƒ$ has a local minimum value at a point $c$ within its domain $D$ if $ƒ(x) \\geq ƒ(c)$ for all $x \\in D$ lying in some open interval containing $c$. Finding Extrema THEOREM 2—The First Derivative Theorem for Local Extreme ValuesIf $ƒ$ has a local maximum or minimum value at an interior point $c$ of its domain, and if $ƒ’$ is defined at $c$, then$$f’(c) = 0$$ DEFINITIONAn interior point of the domain of a function $ƒ$ where $ƒ’$ is zero or undefined is a critical point（临界点） of $ƒ$. The Mean Value TheoremRolle’s Theorem（罗尔定理） THEOREM 3—Rolle’s TheoremSuppose that $y = ƒ(x)$ is continuous over the closed interval $[a,b]$ and di£erentiable at every point of its interior $(a, b)$. If $ƒ(a) = ƒ(b)$, then there is at least one number $c$ in $(a, b)$ at which $ƒ’(c) = 0$. The Mean Value Theorem（中值定理） THEOREM 4—The Mean Value TheoremSuppose $y = ƒ(x)$ is continuous over a closed interval $[a,b]$ and differentiable on the interval’s interior $(a, b)$. Then there is at least one point $c$ in $(a, b)$ at which$$\\frac{f(b) - f(a)}{b-a} = f’(c)$$ A Physical InterpretationWe can think of the number $\\frac{f(b) - f(a)}{b-a}$ as the average change in $ƒ$ over $[a,b]$ and $ƒ’(c)$ as an instantaneous change. Then the Mean Value Theorem says that the instantaneous change at some interior point is equal to the average change over the entire interval. Finding Velocity and Position from AccelerationMonotonic Functions and the First Derivative TestIncreasing Functions and Decreasing Functions COROLLARY 3Suppose that $ƒ$ is continuous on $[a,b]$ and di£erentiable on $(a, b)$.If $ƒ’(x) &gt; 0$ at each point x \\in (a, b), then $ƒ$ is increasing on $[a,b]$.If $ƒ’(x) &lt; 0$ at each point $x \\in (a, b)$, then $ƒ$ is decreasing on $[a,b]$. First Derivative Test for Local Extrema First Derivative Test for Local ExtremaSuppose that $c$ is a critical point of a continuous function $ƒ$, and that $ƒ$ is differentiable at every point in some interval containing $c$ except possibly at $c$ itself. Moving across this interval from left to right, if ƒ’ changes from negative to positive at $c$, then $ƒ$ has a local minimum at $c$; if $ƒ’$ changes from positive to negative at $c$, then $ƒ$ has a local maximum at $c$; if $ƒ’$ does not change sign at $c$ (that is, ƒ’ is positive on both sides of $c$ or negative on both sides), then $ƒ$ has no local extremum at $c$. Concavity and Curve SketchingConcavity DEFINITIONThe graph of a differentiable function $y = ƒ(x)$ is(a) concave up on an open interval $I$ if $ƒ’$ is increasing on $I$;(b) concave down on an open interval $I$ if $ƒ’$ is decreasing on $I$. A function whose graph is concave up is also often called convex. The Second Derivative Test for ConcavityLet $y = ƒ(x)$ be twice-di£erentiable on an interval $I$. If $ƒ’’&gt; 0$ on $I$, the graph of $ƒ$ over $I$ is concave up. If $ƒ’’ &lt; 0$ on $I$, the graph of $ƒ$ over $I$ is concave down. Points of Inflection DEFINITIONA point $(c, ƒ(c))$ where the graph of a function has a tangent line and where the concavity changes is a point of inflection（拐点）.At a point of inflection $(c, ƒ(c))$, either $ƒ’’(c) = 0$ or $ƒ’’(c)$ fails to exist. Second Derivative Test for Local Extrema THEOREM 5—Second Derivative Test for Local Extrema If $ƒ’(c) = 0$ and $ƒ’’(c) &lt; 0$, then $ƒ$ has a local maximum at $x = c$. If $ƒ’(c) = 0$ and ƒ’’(c) &gt; 0, then $ƒ$ has a local minimum at $x = c$. If $ƒ’(c) = 0$ and $ƒ’’(c) = 0$, then the test fails. The function $ƒ$ may have a local maximum, a local minimum, or neither. The following figure summarizes how the first derivative and second derivative affect the shape of a graph. Indeterminate Forms and L’Hôpital’s Rule（洛必达法则）Indeterminate Form 0/0 THEOREM 6—L’Hôpital’s RuleSuppose that $ƒ(a) = g(a) = 0$, that $ƒ$ and $g$ are differentiable on an open interval $I$ containing $a$, and that $g’(x) \\neq 0$ on $I$ if $x \\neq a$. Then$$\\lim_{x\\rightarrow a}\\frac{f(x)}{g(x)} = \\lim_{x\\rightarrow a}\\frac{f’(x)}{g’(x)}$$ Applied Optimization Solving Applied Optimization Problems Read the problem. Read the problem until you understand it. What is given? What is the unknown quantity to be optimized? Draw a picture. Label any part that may be important to the problem. Introduce variables. List every relation in the picture and in the problem as an equation or algebraic expression, and identify the unknown variable. Write an equation for the unknown quantity. If you can, express the unknown as a function of a single variable or in two equations in two unknowns. This may require considerable manipulation. Test the critical points and endpoints in the domain of the unknown. Use what you know about the shape of the function’s graph. Use the first and second derivatives to identify and classify the function’s critical points. Examples from Mathematics and PhysicsFor example: The speed of light depends on the medium through which it travels, and is generally slower in denser media. Fermat’s principle in optics states that light travels from one point to another along a path for which the time of travel is a minimum. Describe the path that a ray of light will follow in going from a point $A$ in a medium where the speed of light is $c_1$ to a point $B$ in a second medium where its speed is $c_2$. Solution: $$t_1 = \\frac{AP}{c_1} = \\frac{\\sqrt{a^2+x^2}}{c_1}\\\\t_2 = \\frac{PB}{c_2} = \\frac{\\sqrt{v^2 + (d-x)^2}}{c_2}\\\\t = t_1 + t_2$$ This equation expresses $t$ as a differentiable function of $x$ whose domain is $[0,d]$. We want to find the absolute minimum value of $t$ on this closed interval. We find the derivative $$\\frac{dt}{dx} = \\frac{x}{c_1\\sqrt{a^2+x^2}} - \\frac{d-x}{c_2\\sqrt{b^2+(d-x)^2}}$$ and observe that it is continuous. In terms of the angles $\\theta_1$ and $\\theta_2$ in Figure 4.43,$$\\frac{dt}{dx} = \\frac{\\sin\\theta_1}{c_1} - \\frac{\\sin\\theta_2}{c_2}$$ The function $t$ has a negative derivative at $x = 0$ and a positive derivative at $x = d$. Since $dt/dx$ is continuous over the interval [0,d], by the Intermediate Value Theorem for continuous functions, there is a point $x \\in [0,d]$, where $dt/dx = 0$ $$\\frac{\\sin\\theta_1}{c_1} = \\frac{\\sin\\theta_2}{c_2}$$ This equation is Snell’s Law or the Law of Refraction, Newton’s MethodProcedure for Newton’s MethodApplying Newton’s MethodThe goal of Newton’s method for estimating a solution of an equation $ƒ(x) = 0$ is to produce a sequence of approximations that approach the solution. Newton’s Method Guess a first approximation to a solution of the equation $ƒ(x) = 0$. A graph of $y = ƒ(x)$ may help. Use the first approximation to get a second, the second to get a third, and so on, using the formula$$x_{n+1} = x_n - \\frac{f(x_n)}{f’(x_n)}, if f’(x_n) \\neq 0.$$ Convergence of the ApproximationsIn practice, Newton’s method usually gives convergence with impressive speed, but this is not guaranteed. One way to test convergence is to begin by graphing the function to estimate a good starting value for $x_0$. You can test that you are getting closer to a zero of the function by checking that $|ƒ(x_n)|$ is approaching zero, and you can check that the approximations are converging by evaluating $|x_n - x_{n+1}|$. Newton’s method does not always converge. For instance, if$$f(x) =\\begin{cases}-\\sqrt{r-x}, x &lt; r\\\\\\sqrt{x-r}, x \\geq r\\end{cases}$$ there are other fails: To be added…","link":"/Math/Calculus/Calculus-C4-Applications-of-Derivatives/"},{"title":"Calculus-C7-Integrals-and-Transcendental-Functions","text":"Keywords: Separable Differential Equations This is the Chapter7 ReadingNotes from book Thomas Calculus 14th. Exponential Change and Separable Differential EquationsExponential ChangeSeparable Differential Equations","link":"/Math/Calculus/Calculus-C7-Integrals-and-Transcendental-Functions/"},{"title":"Calculus-C5-Integrals","text":"Keywords: Riemann Sums, Mean Value Theorem This is the Chapter5 ReadingNotes from book Thomas Calculus 14th. Sigma Notation and Limits of Finite SumsRiemann Sums The set of all of these points,$$P = \\lbrace x_0, x_1, \\cdots, x_n \\rbrace$$is called a partition $P$ of $[a,b]$.$$S_p = \\sum_{k=1}^n f(c_k)\\Delta x_k$$The sum $S_P$ is called a Riemann sum for $ƒ$ on the interval $[a,b]$. we could choose $n$ subintervals all having equal width $\\Delta x = (b-a) / n$ to partition $[a,b]$, This choice leads to the Riemann sum formula$$S_n = \\sum_{k=1}^n f(a + k\\frac{(b-a)}{n}) \\cdot (\\frac{b-a}{n})$$ We define the norm（范数） of a partition $P$, written $||P||$, to be the largest of all the subinterval widths. If $||P||$ is a small number, then all of the subintervals in the partition $P$ have a small width. 范数(norm)是数学中的一种基本概念。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。 The Fundamental Theorem of CalculusFundamental Theorem of Calculus, which is the central theorem of integral calculus. It connects integration and differentiation, enabling us to compute integrals by using an antiderivative of the integrand function rather than by taking limits of Riemann sums. Mean Value Theorem for Definite Integrals(定积分的中值定理) THEOREM 3—The Mean Value Theorem for Definite IntegralsIf $ƒ$ is continuous on $[a,b]$, then at some point $c$ in [a,b],$$f(c) = \\frac{1}{b-a}\\int_a^b f(x) dx$$ Fundamental Theorem, Part 1If $ƒ(t)$ is an integrable function over a finite interval $I$, then the integral from any fixed number $a \\in I$ to another number $x \\in I$ defines a new function $F$ whose value at $x$ is$$F(x) = \\int_a^xf(t)dt\\tag{1}$$ It is reasonable to expect that $F’(x)$, which is the limit of this difference quotient as $h\\rightarrow 0$, equals ƒ(x), so that $$F’(x) = \\lim_{h\\rightarrow 0} \\frac{F(x+h) - F(x)}{h} = f(x)$$ THEOREM 4—The Fundamental Theorem of Calculus, Part 1If $ƒ$ is continuous on [a,b], then $F(x) = \\int_a^x ƒ(t) dt$ is continuous on $[a,b]$ and differentiable on $(a,b)$ and its derivative is $ƒ(x)$:$$F’(x) = \\frac{d}{dx}\\int_a^xf(t)dt = f(x)\\tag{2}$$ Fundamental Theorem, Part 2 (The Evaluation Theorem) THEOREM 4 (Continued)—The Fundamental Theorem of Calculus, Part 2If $ƒ$ is continuous over [a,b], and $F$ is any antiderivative of $ƒ$ on [a,b], then$$\\int_a^b f(x)dx = F(b) - F(a)$$ The Integral of a Rate THEOREM 5—The Net Change TheoremThe net change in a differentiable function $F(x)$ over an interval $a \\leq x \\leq b$ is the integral of its rate of change:$$F(b) - F(a) = \\int_a^bF’(x)dx\\tag{6}$$ The Relationship Between Integration and DifferentiationThe conclusions of the Fundamental Theorem tell us several things. Equation (2) can be rewritten as $$\\frac{d}{dx}\\int_a^xf(t)dt = f(x)$$ which says that if you first integrate the function $ƒ$ and then differentiate the result, you get the function $ƒ$ back again. Likewise, replacing $b$ by $x$ and $x$ by $t$ in Equation (6) gives$$\\int_a^x F’(t)dt = F(x)-F(a)$$so that if you first differentiate the function $F$ and then integrate the result, you get the function $F$ back (adjusted by an integration constant). In a sense, the processes of integration and differentiation are “inverses” of each other. Indefinite Integrals and the Substitution MethodThe connection between antiderivatives and the definite integral stated in the Fundamental Theorem now explains this notation: $$\\begin{aligned}\\int_a^b f(x)dx &amp;= F(b) - F(a) \\\\&amp;= \\left[F(b) + C\\right] - \\left[F(a) + C\\right] \\\\&amp;= \\left[F(x) + C\\right]_a^b \\\\&amp;= \\left[\\int f(x) dx\\right]_a^b\\end{aligned}$$ Substitution: Running the Chain Rule BackwardsIf $u$ is a differentiable function of $x$, then$$du = \\frac{du}{dx} dx$$ THEOREM 6—The Substitution RuleIf $u = g(x)$ is a differentiable function whose range is an interval $I$, and $ƒ$ is continuous on $I$, then$$\\int f(g(x)) \\cdot g’(x)dx = \\int f(u) du$$ Definite Integral Substitutions and the Area Between CurvesThe Substitution Formula THEOREM 7—Substitution in Definite IntegralsIf $g’$ is continuous on the interval $[a,b]$ and $ƒ$ is continuous on the range of $g(x) = u$, then$$\\int_a^b f(g(x))g’(x)dx = \\int_{g(a)}^{g(b)} f(u) du$$ Definite Integrals of Symmetric Functions THEOREM 8 Let $ƒ$ be continuous on the symmetric interval $[-a,a]$.a. If $f$ is even, then $\\int_{-a}^a f(x) dx = 2\\int_0^a f(x)dx$b. If $f$ is odd, then, $\\int_{-a}^a f(x) dx = 0$ Areas Between Curves DEFINITIONIf $ƒ$ and $g$ are continuous with $ƒ(x) \\geq g(x)$ throughout $[a,b]$, then the area of the region between the curves $y = f (x)$ and $y = g(x)$ from $a$ to $b$ is the integral of $(ƒ - g)$ from $a$ to $b$:$$A = \\int_a^b \\left[ f(x) - g(x) \\right] dx$$ To be added…","link":"/Math/Calculus/Calculus-C5-Integrals/"},{"title":"GameProgramming-CSVFile","text":"Keywords: Singleton, Resource Management","link":"/GameProgramming/System/GameProgramming-System-CSV/"},{"title":"Calculus-C13-Vector-Valued-Fuctions-and-Motion-in-Space","text":"Keywords: Vector Functions, Curves, Tangents, Normal Vectors This is the Chapter13 ReadingNotes from book Thomas Calculus 14th. Curves in Space and Their TangentsWe use the calculus of these functions to describe the paths and motions of objects moving in a plane or in space, so their velocities and accelerations are given by vectors. A curve in space can also be represented in vector form. The vector$$\\vec{r(t)} = \\vec{OP} = f(t)\\vec{i} + g(t)\\vec{j} + h(t)\\vec{k}\\tag{2}$$ Limits and Continuity DEFINITIONLet $\\vec{r(t)} = f(t)\\vec{i} + g(t)\\vec{j} + h(t)\\vec{k}$ be a vector function with domain $D$, and let $\\vec{L}$ be a vector. We say that $\\vec{r}$ has limit $\\vec{L}$ as $t$ approaches $t_0$ and write$$\\lim_{t\\rightarrow t_0} \\vec{r(t)} = \\vec{L}$$if, for every number $\\epsilon &gt; 0$, there exists a corresponding number $\\delta &gt; 0$ such that for all $t \\in D$$$|\\vec{r(t)} - \\vec{L}| &lt; \\epsilon, whenever, 0 &lt; |t - t_0| &lt; \\delta$$ DEFINITIONA vector function $\\vec{r(t)}$ is continuous at a point $t = t_0$ in its domain if $\\lim_{t \\rightarrow t_0} \\vec{r(t)} = \\vec{r(t_0)}$. The function is continuous if it is continuous at every point in its domain. Derivatives and Motion DEFINITIONThe vector function $\\vec{r(t)} = f(t)\\vec{i} + g(t)\\vec{j} + h(t)\\vec{k}$ has a derivative (is differentiable) at $t$ if $ƒ, g, h$ have derivatives at $t$. The derivative is the vector function$$\\begin{aligned}\\vec{r’(t)} &amp;= \\frac{d\\vec{r}}{dt} \\\\&amp;= \\lim_{\\Delta t \\rightarrow 0} \\frac{\\vec{r(t + \\Delta t)} - \\vec{r(t)}}{\\Delta t} \\\\&amp;= \\frac{df}{dt}\\vec{i} + \\frac{dg}{dt}\\vec{j} + \\frac{dh}{dt}\\vec{k}\\end{aligned}$$ DEFINITIONIf $\\vec{r}$ is the position vector of a particle moving along a smooth curve in space, then$$\\vec{v(t)} = \\frac{d\\vec{r}}{dt}$$is the particle’s velocity vector, tangent to the curve. At any time $t$, the direction of $\\vec{v}$ is the direction of motion, the magnitude of $\\vec{v}$ is the particle’s speed, and the derivative $\\vec{a} = d\\vec{v}/dt$, when it exists, is the particle’s acceleration vector. In summary, Velocity is the derivative of position: $\\vec{v} = \\frac{d\\vec{r}}{dt}$ Speed is the magnitude of velocity: $Speed = |\\vec{v}|$ Acceleration is the derivative of velocity: $\\vec{a} = \\frac{d\\vec{v}}{dt} = \\frac{d^2\\vec{r}}{dt^2}$ The unit vector $\\vec{v} / |\\vec{v}|$ is the direction of motion at time $t$. Differentiation Rules Vector Functions of Constant Length the position vector has a constant length equal to the radius of the sphere. The velocity vector $dr / dt$, tangent to the path of motion, is tangent to the sphere and hence perpendicular to $r$. If $r$ is a differentiable vector function of $t$ and the length of $r(t)$ is constant, then$$r \\cdot \\frac{dr}{dt} = 0\\tag{4}$$ Integrals of Vector Functions; Projectile MotionIntegrals of Vector Functions DEFINITIONThe indefinite integral of $\\vec{r}$ with respect to $t$ is the set of all antiderivatives of $\\vec{r}$, denoted by $\\int \\vec{r(t)} dt$. If $\\vec{R}$ is any antiderivative of $\\vec{r}$, then$$\\int \\vec{r(t)} dt = \\vec{R(t)} + C$$ DEFINITIONIf the components of $r(t) = ƒ(t)i + g(t)j + h(t)k$ are integrable over $[a,b]$, then so is $r$, and the definite integral of $r$ from $a$ to $b$ is$$\\int_a^b r(t) dt = \\left( \\int_a^b f(t) dt \\right) i + \\left( \\int_a^b g(t) dt \\right) j + \\left( \\int_a^b h(t) dt \\right) k$$ The Vector and Parametric Equations for Ideal Projectile Motion（抛射运动） $$\\vec{v_0} = (|\\vec{v_0}| \\cos \\alpha) \\vec{i} + (|\\vec{v_0}| \\sin \\alpha) \\vec{j}$$ use the simpler notation $v_0$ for the initial speed $\\vec{v_0}$,$$\\vec{v_0} = (v_0 \\cos \\alpha) \\vec{i} + (v_0 \\sin \\alpha) \\vec{j}\\tag{3}$$The projectile’s initial position is$$\\vec{r_0} = 0\\vec{i} + 0\\vec{j} + 0\\vec{k}\\tag{4}$$ Newton’s second law of motion says that the force acting on the projectile is equal to the projectile’s mass $m$ times its acceleration, or $m(d^2\\vec{r} / dt^2)$ if $\\vec{r}$ is the projectile’s position vector and $t$ is time. $$m\\frac{d^2\\vec{r}}{dt^2} = -mg\\vec{j}$$ Thus, Differential equation:$$\\frac{d^2\\vec{r}}{dt^2} = -g\\vec{j}$$Initial conditions:$$\\vec{r} = \\vec{r_0}, \\frac{d\\vec{r}}{dt} = \\vec{v_0}, when \\space t = 0$$The first integration gives:$$\\frac{d\\vec{r}}{dt} = -(gt)\\vec{j} + \\vec{v_0}$$A second integration gives:$$\\vec{r} = -\\frac{1}{2}gt^2\\vec{j} + \\vec{v_0}t + \\vec{r_0}$$Substituting the values of $\\vec{v_0}$ and $\\vec{r_0}$ from Equations (3) and (4) gives$$\\vec{r} = (v_0\\cos \\alpha) t \\vec{i} + \\left( (v_0\\sin \\alpha)t - \\frac{1}{2}gt^2\\right)\\vec{j}$$ Projectile Motion with Wind GustsArc Length in SpaceArc Length Along a Space Curve DEFINITIONThe length of a smooth curve $r(t) = x(t)i + y(t)j + z(t)k$, $a \\leq t \\leq b$, that is traced exactly once as $t$ increases from $t = a$ to $t = b$, is$$\\begin{aligned}L &amp;= \\int_a^b \\sqrt{(\\frac{dx}{dt})^2 + (\\frac{dy}{dt})^2 + (\\frac{dz}{dt})^2} dt \\\\&amp;= \\int_a^b |\\vec{v}| dt\\end{aligned}\\tag{1}$$ If we choose a base point $P(t_0)$ on a smooth curve $C$ parametrized by $t$, each value of $t$ determines a point $P(t) = (x(t), y(t), z(t))$ on $C$ and a “directed distance”$$\\begin{aligned}s(t) &amp;= \\int_{t_0}^t \\sqrt{[x’(\\tau)]^2 + [y’(\\tau)]^2 + [z’(\\tau)]^2} d\\tau\\\\&amp;= \\int_{t_0}^{t} |\\vec{v(\\tau)}| d\\tau\\end{aligned}\\tag{3}$$ Recall the similar definition form in Fundamental Theorem, Part 1 &gt;&gt;? We call $s$ an arc length parameter for the curve.We use the Greek letter $\\tau$ (“tau”) as the variable of integration in Equation (3) because the letter $t$ is already in use as the upper limit. Speed on a Smooth CurveSince the derivatives beneath the radical in Equation (3) are continuous (the curve is smooth), the Fundamental Theorem of Calculus tells us that $s$ is a differentiable function of $t$ with derivative$$\\frac{ds}{dt} = |\\vec{v(t)}|\\tag{4}$$ Notice that $ds/dt &gt; 0$ since, by definition, $|\\vec{v}|$ is never zero for a smooth curve. We see once again that $s$ is an increasing function of $t$. Unit Tangent Vector $$\\vec{T} = \\frac{\\vec{v}}{|\\vec{v}|}$$ The velocity vector is the change in the position vector $\\vec{r}$ with respect to time $t$, but how does the position vector change with respect to arc length? More precisely, what is the derivative $d\\vec{r}/ds$? Since $d\\vec{s}/dt &gt; 0$ for the curves we are considering, $s$ is one-to-one and has an inverse that gives $t$ as a differentiable function of $s$. The derivative of the inverse is$$\\frac{dt}{ds} = \\frac{1}{ds/dt} = \\frac{1}{|\\vec{v}|}$$This makes $\\vec{r}$ a differentiable function of $s$ whose derivative can be calculated with the Chain Rule to be$$\\frac{d\\vec{r}}{ds} = \\frac{d\\vec{r}}{dt} \\frac{dt}{ds} = \\vec{v}\\frac{1}{|\\vec{v}|} = \\frac{\\vec{v}}{|\\vec{v}|} = T$$ Curvature and Normal Vectors of a CurveCurvature of a Plane Curve The rate at which $T$ turns per unit of length along the curve is called the curvature (Figure 13.17). DEFINITIONIf $T$ is the unit vector of a smooth curve, the curvature function of the curve is$$\\kappa = |\\frac{dT}{ds}|$$ If $|dT/ds|$ is large, $T$ turns sharply as the particle passes through $P$, and the curvatureat $P$ is large. According to Chain Rule: $$\\kappa = |\\frac{dT}{ds}| = |\\frac{dT}{ds}\\frac{dt}{ds}| = \\frac{1}{|ds/dt|}|\\frac{dT}{dt}| = \\frac{1}{|\\vec{v}|}|\\frac{dT}{dt}|$$ Formula for Calculating CurvatureIf $\\vec{r(t)}$ is a smooth curve, then the curvature is the scalar function$$\\kappa = \\frac{1}{|\\vec{v}|}|\\frac{dT}{dt}|\\tag{}$$where T = v&gt; 0 v 0 is the unit tangent vector. Circle of Curvature for Plane Curves Curvature and Normal Vectors for Space Curves Tangential and Normal Components of AccelerationThe TNB FrameTangential and Normal Components of AccelerationTorsionFormulas for Computing Curvature and TorsionVelocity and Acceleration in Polar CoordinatesMotion in Polar and Cylindrical CoordinatesKepler’s First Law (Ellipse Law)Kepler’s Second Law (Equal Area Law)Kepler’s Third Law (Time–Distance Law)","link":"/Math/Calculus/Calculus-C13-Vector-Valued-Fuctions-and-Motion-in-Space/"},{"title":"Calculus-C8-Techniques-of-Integration","text":"Keywords: Numerical Integration, Simpson’s Rule This is the Chapter8 ReadingNotes from book Thomas Calculus 14th. Integration by PartsProduct Rule in Integral FormIf $u$ and $v$ are differentiable functions of $x$, the Product Rule says that $$\\frac{d}{dx}\\left[ u(x)v(x) \\right] = u’(x)v(x) + u(x)’v(x)$$In terms of indefinite integrals, this equation becomes$$\\begin{aligned}\\int \\frac{d}{dx}\\left[ u(x)v(x)\\right]dx &amp;= \\int \\left[u’(x)v(x) + u(x)’v(x)\\right]dx \\\\&amp;= \\int u’(x)v(x) dx + \\int u(x)’v(x) dx \\\\\\Longrightarrow\\int u(x)’v(x) dx &amp;= \\int \\frac{d}{dx}\\left[ u(x)v(x)\\right]dx - \\int u’(x)v(x) dx\\end{aligned}$$ Integration by Parts Formula$$\\int u(x) v’(x) dx = u(x)v(x) - \\int v(x)u’(x)dx\\tag{1}$$ With $v’(x) dx = dv$ and $u’(x) dx = du$, the integration by parts formula becomes $$\\int udv = uv - \\int vdu$$ Evaluating Definite Integrals by PartsThe integration by parts formula in Equation (1) can be combined with Part 2 of the FundamentalTheorem in order to evaluate definite integrals by parts. Assuming that both $u’$ and $y’$ are continuous over the interval $[a,b]$, Part 2 of the Fundamental Theorem gives Integration by Parts Formula for Definite Integrals$$\\int_a^b u(x)v’(x) dx = \\left. u(x)v(x) \\right|_z^b - \\int_a^b v(x)u’(x)dx$$ Numerical IntegrationWhen we cannot find a workable antiderivative（不定积分）for a function $ƒ$ that we have to integrate, we can partition the interval of integration, replace $ƒ$ by a closely fitting polynomial on each subinterval, integrate the polynomials, and add the results to approximate the definite integral of $ƒ$. This procedure is an example of numerical integration. Trapezoidal Approximations（梯形近似） The Trapezoidal RuleTo approximate $\\int_a^b f(x) dx$, use$$T = \\frac{\\Delta x}{2}(y_0 + 2y_1 + 2y_2 + \\cdots + y_n)$$The y’s are the values of $ƒ$ at the partition points$$x_0 = a, x_1 = a + \\Delta x, x_2 = a + 2 \\Delta x, \\cdots, x_{n-1} = a + (n-1)\\Delta x, x_n = b,$$where, $\\Delta x = (b-a)/n$ Simpson’s Rule: Approximations Using Parabolas(抛物线近似)Another rule for approximating the definite integral of a continuous function results from using parabolas instead of the straight-line segments that produced trapezoids. Simpson’s RuleTo approximate $\\int_a^b f(x) dx$, use$$S = \\frac{\\Delta x}{3}(y_0 + 4y_1 + 4y_2 + \\cdots + 2y_{n-2} + 4y_{n-1} + y_n)$$The y’s are the values of $ƒ$ at the partition points$$x_0 = a, x_1 = a + \\Delta x, x_2 = a + 2 \\Delta x, \\cdots, x_{n-1} = a + (n-1)\\Delta x, x_n = b,$$where, $\\Delta x = (b-a)/n$ Error AnalysisTo be added…","link":"/Math/Calculus/Calculus-C8-Techniques-of-Integration/"},{"title":"Calculus-C6-Applications-of-Definite-Integrals","text":"Keywords: Arc Length, Work and Fluid Forces, Moments and Centers of Mass This is the Chapter6 ReadingNotes from book Thomas Calculus 14th. Arc LengthWe divide the curve into many pieces, and we approximate each piece by a straight-line segment.The total length of the curve is the limiting value of these approximations as the number of segments goes to infinity. Length of a Curve $y = ƒ(x)$ If we set $\\Delta x_k = x_k - x_{k-1}$ and $\\Delta y_k = y_k - y_{k-1}$, then a representative line segment in the path has length (see Figure 6.23) $$L_k = \\sqrt{(\\Delta x_k)^2 + (\\Delta y_k)^2}$$ so the length of the curve is approximated by the sum $$\\sum_{k=1}^n L_k = \\sum_{k=1}^n \\sqrt{(\\Delta x_k)^2 + (\\Delta y_k)^2}\\tag{1}$$ In order to evaluate this limit, we use the Mean Value Theorem, which tells us that there is a point $c_k$ , with $x_{k-1} &lt; c_k &lt; x_k$, such that$$\\Delta y_k = f’(c_k)\\Delta x_k$$Substituting this for $\\Delta y_k$, the sums in Equation (1) take the form$$\\sum_{k=1}^n L_k = \\sum_{k=1}^n \\sqrt{(\\Delta x_k)^2 + (f’(c_k)\\Delta x_k)^2}=\\Delta x_k \\sum_{k=1}^n \\sqrt{1 + [f’(c_k)]^2}\\tag{2}$$ This is a Riemann sum whose limit we can evaluate. Because $\\sqrt{1 + [f’(c_k)]^2}$ is continuous on $[a,b]$, the limit of the Riemann sum on the right-hand side of Equation (2) exists and has the value$$\\lim_{n\\rightarrow \\infty} \\sum_{k=1}^n L_k = \\lim_{n\\rightarrow \\infty} \\Delta x_k \\sum_{k=1}^n \\sqrt{1 + [f’(c_k)]^2} =\\int_a^b \\sqrt{1 + [f’(x)]^2} dx$$ DEFINITIONIf $ƒ’$ is continuous on $[a,b]$, then the length (arc length) of the curve $y = ƒ(x)$ from the point $A = (a, ƒ(a))$ to the point $B = (b, ƒ(b))$ is the value of the integral$$L = \\int_a^b \\sqrt{1 + [f’(x)]^2} dx = \\int_a^b \\sqrt{1 + [\\frac{dy}{dx}]^2} dx$$ Work and Fluid ForcesMoments and Centers of MassTo be added…","link":"/Math/Calculus/Calculus-C6-Applications-of-Definite-Integrals/"},{"title":"GameProgramming-MVCFramework","text":"Keywords: MVC Framework, TCP Communication","link":"/GameProgramming/System/GameProgramming-System-MVC/"},{"title":"Calculus-C9-First-Order-Differential-Equations","text":"Keywords: Euler’s Method, Slope Fields, Autonomous Equations This is the Chapter9 ReadingNotes from book Thomas Calculus 14th. Solutions, Slope Fields, and Euler’s MethodMany differential equations cannot be solved by obtaining an explicit formula for the solution. However, we can often find numerical approximations to solutions. General First-Order Differential Equations and SolutionsA first-order differential equation is an equation$$\\frac{dy}{dx} = f(x,y)\\tag{1}$$in which $ƒ(x, y)$ is a function of two variables defined on a region in the $xy$-plane. In a typical situation $y$ represents an unknown function of $x$, and $ƒ(x, y)$ is a known function. Such as:$$y’ = x + y, y’ = y / x, y’ = 3xy$$ A solution of Equation (1) is a differentiable function y = y(x) defined on an interval $I$ of $x$-values (perhaps infinite) such that$$\\frac{d}{dx}y(x) = f(x,y(x))$$ For example: Show that the function $y = (x+1)-\\frac{1}{3}e^x$ is a solution to the first-order initial value problem $\\frac{dy}{dx} = y-x, y(0) = \\frac{2}{3}$ Solution: The equation$$\\frac{dy}{dx} = y-x$$is a first-order differential equation with $ƒ(x, y) = y - x$. On the left side of the equation:$$\\frac{dy}{dx} = \\frac{d}{dx} (x+1-\\frac{1}{3}e^x) = 1 - \\frac{1}{3}e^x$$ On the left side of the equation:$$y-x = 1 - \\frac{1}{3}e^x$$ The function satisfies the initial condition because$$y(0) = [(x+1)-\\frac{1}{3}e^x]_{x=0} = \\frac{2}{3}$$The graph of the function is shown in Figure 9.1. Slope Fields: Viewing Solution CurvesEach time we specify an initial condition $y(x_0) = y_0$ for the solution of a differential equation $y’ = ƒ(x, y)$, the solution curve (graph of the solution) is required to pass through the point $(x_0, y_0)$ and to have slope $ƒ(x_0, y_0)$ there. We can picture these slopes graphically by drawing short line segments of slope $ƒ(x, y)$ at selected points $(x, y)$ in the region of the $xy$plane that constitutes the domain of $ƒ$. Each segment has the same slope as the solution curve through $(x, y)$ and so is tangent to the curve there. The resulting picture is called a slope field (or direction field) and gives a visualization of the general shape of the solution curves. Figure 9.2a shows a slope field, with a particular solution sketched into it in Figure 9.2b. We see how these line segments indicate the direction the solution curve takes at each point it passes through. Slope fields are useful because they display the overall behavior of the family of solution curves for a given differential equation. Euler’s MethodThe basis of Euler’s method is to patch together a string of linearizations to approximate the curve over a longer stretch.（本质上还是数值分析, 迭代逼近） Given a differential equation $dy/dx = ƒ(x, y)$ and an initial condition $y(x_0) = y_0$, we can approximate the solution $y = y(x)$ by its linearization $$L(x) = y(x_0) + y’(x_0)(x-x_0)$$ or $$L(x) = y_0 + f(x_0, y_0)(x-x_0)$$ For example: Use Euler’s method to solve $y’ = 1 + y , y(0) = 1$, on the interval $0 \\leq x \\leq 1$, starting at $x_0 = 0$ and taking $dx = 0.1$, Compare the approximations with the values of the exact solution $y = 2e^x - 1$. Solution: First-Order Linear EquationsA first-order linear differential equation is one that can be written in the form$$\\frac{dy}{dx} + P(x)y = Q(x)$$where $P$ and $Q$ are continuous functions of $x$. Equation (1) is the linear equation’s standard form Solving Linear Equations$v(x)$ is choosen to make $v\\frac{dy}{dx} + Pvy = \\frac{d}{dx}(v \\cdot y)$: $$\\begin{aligned}&amp;\\frac{dy}{dx} + P(x)y = Q(x)\\\\&amp;v(x)\\frac{dy}{dx} + P(x)v(x)y = v(x)Q(x)\\\\&amp;\\frac{d}{dx}(v(x) \\cdot y) = v(x)Q(x)\\\\&amp;v(x) \\cdot y = \\int v(x)Q(x)dx\\\\&amp;y = \\frac{1}{v(x)} \\int v(x)Q(x)dx\\end{aligned}$$ about $v(x)$: $$\\begin{aligned}&amp;\\frac{d}{dx}(vy) = v\\frac{dy}{dx} + yPv\\\\&amp;v\\frac{dy}{dx} + y\\frac{dv}{dx} = v\\frac{dy}{dx} + yPv\\\\&amp;y\\frac{dv}{dx} = yPv\\\\&amp;\\frac{dv}{dx} = Pv\\\\&amp;\\frac{dv}{v} = Pdx\\\\&amp;\\int\\frac{dv}{v} = \\int Pdx\\\\&amp;\\ln v = \\int Pdx\\\\&amp;e^{\\ln v} = e^{\\int Pdx}\\\\&amp;v = e^{\\int Pdx}\\end{aligned}$$ ApplicationsMotion with Resistance Proportional to VelocityOrthogonal TrajectoriesGraphical Solutions of Autonomous EquationsSystems of Equations and Phase PlanesTo be added…","link":"/Math/Calculus/Calculus-C9-First-Order-Differential-Equations/"},{"title":"GameProgramming-NGUI","text":"Keywords: NGUI Render NGUI ComponentsUIPanelUIWidgetsUISpriteUIScrollviewUITableUIAnchor","link":"/GameProgramming/System/GameProgramming-System-NGUI/"},{"title":"Geometry-ConvexHull","text":"Keywords: 2D ConvexHull, 3D ConvexHull, Algorithms, Half Plane Intersection, C++ More about Convex Concept&gt;&gt; Andrew AlgorithmHalf Plane IntersectionIncremental Method ConvexHull","link":"/Math/Geometry/Geometry-ConvexHull/"},{"title":"GameProgramming-LadderSeason-Interface","text":"Keywords: Auto Scrolling, Parametric Animation, Sort by Filter The new ladder season is coming and sometimes the ladder system interface should be updated. Mission Auto ScrollingReward PopWindowLadder Settlement AnimationLarge Rank Change AnimationNormal Rank Change AnimationLadderboard SortConceptsBattle Royal Rank: A video game genre that blends elements of survival games with last-man-standing gameplay. Players search for equipment while eliminating competitors in a shrinking safe zone. Usually there are many more players involved than in other kinds of multi-player games.(from Wiki) Clash Squad Rank: always a 4v4 game mode. In this game mode, the goal is to win four out of seven rounds by eliminating other players. At the start of each round, players can stay only within their starting area and purchase weapons and items in the shop. The Balance is given out each round based on what players did in the previous round (with the exception of the first round, where everyone gets the same amount).(from esports)","link":"/GameProgramming/System/GameProgramming-System-LadderSeason/"},{"title":"NumericalAnalysis-C1-Solving-Equations","text":"Keywords: Bisection Method, Fixed-point Method, Newton’s Method, Brent’s Method, Kinematics, Matlab This is the Chapter1 ReadingNotes from book Numerical Analysis by Timothy. THE BISECTION METHODBracketing a root（二分法） DEFINITION 1.1The function $f(x)$ has a root at $x = r$ if $f(r) = 0$. This fact is summarized in the following corollary of the Intermediate Value Theorem: THEOREM 1.2Let $f$ be a continuous function on $[a,b]$, satisfying $f(a)f(b) &lt; 0$. Then $f$ has a root between $a$ and $b$, that is, there exists a number $r$ satisfying $a &lt; r &lt; b$ and $f (r) = 0$. The algorithm can be written in the following Matlab code: 1234567891011121314151617181920212223242526272829303132%bisect.m%Program 1.1 Bisection Method%Computes approximate solution of f(x)=0%Input: function handle f; a,b such that f(a)*f(b)&lt;0,% and tolerance tol%Output: Approximate solution xcfunction xc=bisect(f,a,b,tol)if sign(f(a))*sign(f(b)) &gt;= 0error(\"f(a)f(b)&lt;0 not satisfied!\") %ceases executionendfa=f(a);fb=f(b);while (b-a)/2&gt;tolc=(a+b)/2;fc=f(c);if fc == 0 %c is a solution, done breakendif sign(fc)*sign(fa)&lt;0 %a and c make the new intervalb=c;fb=fc;else %c and b make the new intervala=c;fa=fc;endendxc=(a+b)/2; %new midpoint is best estimate&gt;&gt; f=@(x) x^3+x-1;&gt;&gt; xc=bisect (f,0,1,0.00005)xc = 0.682342529296875 How accurate and how fast?If $[a,b]$ is the starting interval, then after $n$ bisection steps, the interval $[a_n,b_n]$ has length $(b − a)/2^n$. Choosing the midpoint $x_c = (a_n + b_n)/2$ gives a best estimate of the solution $r$, which is within half the interval length of the true solution.$$\\begin{aligned}Solution-error &amp;= |x_c - r| \\\\&amp;= |\\frac{a_n + b_n}{2} - r| \\\\&amp;&lt; \\frac{b_n - a_n}{2} \\\\&amp;= \\frac{b-a}{2^{n+1}}\\end{aligned}\\tag{1.1}$$and$$Function-evaluations = n + 2\\tag{1.2}$$ DEFINITION 1.3A solution is correct within $p$ decimal places if the error is less than $0.5 × 10^{−p}$ (在小数点后p位内正确). For example: Use the Bisection Method to find a root of $f(x) = cosx − x$ in the interval $[0,1]$ to within six correct places. Solution: $$\\frac{b-a}{2^{n+1}} &lt; 0.5 \\times 10^{-6}$$$$n &gt; 19.9$$Therefore, $n = 20$ steps will be needed. FIXED-POINT ITERATION (FPI)Fixed points of a function DEFINITION 1.4The real number $r$ is a fixed point of the function $g$ if $g(r) = r$. The number $r = 0.7390851332$ is an approximate fixed point for the function $g(x) = cosx$. The function $g(x) = x^3$ has three fixed points, $r =−1, 0$, and $1$. According to (Continuous Limits) Let $f$ be a continuous function in a neighborhood of $x_0$, and assume $\\lim_{n\\rightarrow \\infty}x_n = x_0$. Then$$\\lim_{n \\rightarrow \\infty} f(x_n) = f(\\lim_{n\\rightarrow \\infty} x_n) = f(x_0)$$ we can get$$g(r) = g(\\lim_{i\\rightarrow \\infty}x_i) = \\lim_{i\\rightarrow \\infty} g(x_i) = r$$ The Fixed-Point Iteration algorithm applied to a function g is easily written in Matlab code: 12345678910111213141516171819%fpi.m%Program 1.2 Fixed-Point Iteration%Computes approximate solution of g(x)=x%Input: function handle g, starting guess x0,% number of iteration steps k%Output: Approximate solution xcfunction xc=fpi(g, x0, k)x(1)=x0;for i=1:kx(i+1)=g(x(i));endxc=x(k+1);&gt;&gt; g=@(x) cos(x)&gt;&gt; xc=fpi(g,0,10)xc = 0.731404042422510 Can every equation $f(x) = 0$ be turned into a fixed-point problem $g(x) = x$? Yes. For example: $$x^3 + x - 1 = 0\\tag{1.4}$$ Three Solutions： Initial value $x_0 = 0.5$ 1.$$x = 1-x^3\\longrightarrow g(x) = 1 - x^3\\tag{1.5}$$Instead of converging, the iteration tends to alternate between the numbers $0$ and $1$. 2. $$x = \\sqrt[3]{1-x}\\longrightarrow g(x) = \\sqrt[3]{1-x}\\tag{1.6}$$ This time FPI is successful. 3. $$3x^3 + x = 1 + 2x^3\\\\x = \\frac{1+2x^3}{1+3x^2}\\longrightarrow g(x) = \\frac{1+2x^3}{1+3x^2}\\tag{1.7}$$This time FPI is successful, but in a much more striking way. Geometry of Fixed-Point IterationThis geometric illustration of a Fixed-Point Iteration is called a cobweb diagram（蛛网图）. Linear convergence of Fixed-Point Iteration Figure1.4 shows Fixed-Point Iteration for two linear functions$$g_1(x) = -\\frac{3}{2}x + \\frac{5}{2}\\\\g_2(x) = -\\frac{1}{2}x + \\frac{3}{2}$$From geometric view, For $g_1(x)$, Because the slope of $g_1(x)$ at the fixed point is greater than one, the vertical segments（看箭头段）, the ones that represent the change from $x_n$ to $x_{n+1}$, are increasing in length as FPI proceeds. As a result, the iteration “spirals out’’ from the fixed point $x = 1$, even if the initial guess $x_0$ was quite near. For $g_2(x)$, it “spirals in”. From equation view, $$g_1(x) = -\\frac{3}{2}(x-1) + 1$$$$g_1(x) - 1 = -\\frac{3}{2}(x-1)$$$$x_{i+1} - 1 = -\\frac{3}{2}(x_i-1)\\tag{1.8}$$ If we view $e_i = |r − x_i|$ as the error at step $i$ (meaning the distance from the best guess at step $n$ to the fixed point), we see from (1.8) that $e_{i+1} = \\frac{3e_i}{2}$, implying that errors increase at each step by a factor of approximately $3/2$. This is divergence. $g_2(x)$ is convergence. DEFINITION 1.5Let $e_i$ denote the error at step $i$ of an iterative method. If$$\\lim_{i\\rightarrow \\infty} \\frac{e_{i + 1}}{e_i} = S &lt; 1$$the method is said to obey linear convergence with rate $S$. THEOREM 1.6Assume that $g$ is continuously differentiable, that $g(r) = r$, and that $S = |g’(r)| &lt; 1$. Then Fixed-Point Iteration converges linearly with rate $S$ to the fixed point $r$ for initial guesses sufficiently close to $r$. DEFINITION 1.7An iterative method is called locally convergent to $r$ if the method converges to $r$ for initial guesses sufficiently close to $r$. For example: Calculate $\\sqrt{2}$ by using FPI. Solution: Suppose we want to find the first $10$ digits of $\\sqrt 2$. Start with the initial guess $x_0 = 1$. Obviously this guess is too low, we want to find a fomula to change 1 to higher number to approximate $\\sqrt{2}$. Obviously $\\frac{2}{1}$ seems too high. In fact, any initial guess $0 &lt; x_0 &lt; 2$, together with $\\frac{2}{x_0}$, form a bracket for $\\sqrt{2}$. (像一对括号一样，不断逼近$\\sqrt{2}$) We guess $$x_1 = \\frac{1 + \\frac{2}{1}}{2} = \\frac{3}{2}$$$$x_2 = \\frac{\\frac{3}{2} + \\frac{4}{3}}{2} = 1.4\\overline{16}$$$$x_3 = \\frac{\\frac{17}{12} + \\frac{24}{17}}{2} \\approx 1.414215686$$ The FPI we are executing is$$x_{i+1} = \\frac{x_i + \\frac{2}{x_i}}{2}$$Note that $\\sqrt{2}$ is a fixed point of the iteration.$$g’(\\sqrt{2}) = 0$$ Stopping criteriaan absolute error stopping criterion$$|x_{i+1} - x_{i}| &lt; TOL\\tag{1.16}$$the relative error stopping criterion$$\\frac{|x_{i+1} - x_{i}|}{|x_{i+1}|} &lt; TOL\\tag{1.17}$$A hybrid absolute/relative stopping criterion such as$$\\frac{|x_{i+1} - x_{i}|}{max(|x_{i+1}|, \\theta)} &lt; TOL\\tag{1.18}$$ LIMITS OF ACCURACYWorking in double precision means that we store and operate on numbers that are kept to 52-bit accuracy, about 16 decimal digits. Forward and backward errorFor example: Use the Bisection Method to find the root of $f (x) = x^3 − 2x^2 + \\frac{4}{3}x − \\frac{8}{27}$ to within six correct significant digits（6个正确的有效位）. Solution: $20$ bisection steps should be sufficient for six correct places. In fact, it is easy to check without a computer that $r = 2/3 = 0.666666666. . .$ is a root: How many of these digits can the Bisection Method obtain? From the figure above, we can see that Bisection Method stops after 16 steps, because the computer get $f(0.6666641) = 0$, satisfying the stop condition. From Figure1.7, we can see that the computer thinks there are many floating point numbers within $10^{−5}$ of the correct root $r = 2/3$ that are evaluated to machine zero, and therefore have an equal right to be called the root! This is not the method fault, but the computer! (Computer is not precise enough!) If the computer arithmetic is showing the function to be zero at a nonroot, there is no way the method can recover. DEFINITION 1.8Assume that $f$ is a function and that $r$ is a root, meaning that it satisfies $f(r) = 0$. Assume that $x_a$ is an approximation to $r$. For the root-finding problem, the backward error of the approximation $x_a$ is $|0 - f(x_a)|$ and the forward error is $|r − x_a|$. Backward error is on the left or input (problem data) side. It is the amount we would need to change the problem (the function $f$ ) to make the equation balance with the output approximation $x_a$, which is $|0-f(x_a)|$.Forward error is the error on the right or output (problem solution) side. It is the amount we would need to change the approximate solution to make it correct, which is $|r - x_a|$. The difficulty with Example 1.7 is that, according to Figure 1.7, the backward error is near $\\epsilon_{mach} \\approx 2.2 \\times 10^{−16}$, while forward error is approximately $10^{−5}$. Double precision numbers cannot be computed reliably below a relative error of machine epsilon($2^{-52}$ for double precision). Since the backward error cannot be decreased further with reliability, neither can the forward error. （总结来讲，到了第16步，计算的数值已经超出double的精度了，所以不会再迭代精确了…） DEFINITION 1.9Assume that $r$ is a root of the differentiable function $f$; that is, assume that $f(r) = 0$. Then if $0 = f(r) = f’(r) = f’’(r) = \\cdots = f^(m−1)(r)$, but $f^{(m)}(r) \\neq 0$, we say that $f$ has a root of multiplicity $m$ at $r$. We say that $f$ has a multiple root at $r$ if the multiplicity is greater than one. The root is called simple if the multiplicity is one. For example, $f(x) = x^2$ has a multiplicity two, or double, root at $r = 0$, because $f(0) = 0,f’(0) = 2(0) = 0$, but $f’’(0) = 2 \\neq 0$. Back to Figure1.7, Because the graph of the function is relatively flat near a multiple root, a great disparity exists between backward and forward errors for nearby approximate solutions. The backward error, measured in the vertical direction, is often much smaller than the forward error, measured in the horizontal direction. TheWilkinson polynomialThe Wilkinson polynomial is$$W(x) = (x − 1)(x − 2) · · · (x − 20)\\tag{1.19}$$which, when multiplied out, is$$\\begin{aligned}W(x) = &amp;x^{20} − 210x^{19} + 20615x^{18} − 1256850x^{17} + 53327946x^{16} − 1672280820x^{15}\\\\&amp;+ 40171771630x^{14} − 756111184500x^{13} + 11310276995381x^{12}\\\\&amp;− 135585182899530x^{11} + 1307535010540395x^{10} − 10142299865511450x^{9}\\\\&amp;+ 63030812099294896x^{8} − 311333643161390640x^{7}\\\\&amp;+ 1206647803780373360x^{6} − 3599979517947607200x^{5}\\\\&amp;+ 8037811822645051776x^{4}− 12870931245150988800x^{3}\\\\&amp;+ 13803759753640704000x^{2} − 8752948036761600000x\\\\&amp;+ 2432902008176640000\\end{aligned}\\tag{1.20}$$The roots are the integers from $1$ to $20$. However, when $W(x)$ is defined according to its unfactored（没有因式分解的形式） form (1.20), its evaluation suffers from cancellation of nearly equal, large numbers. Sensitivity of root-findingSmall floating point errors in the equation can translate into large errors in the root. To understand what causes this magnification of error, we will establish a formula predicting how far a root moves when the equation is changed. Assume that the problem is to find a root $r$ of $f(x) = 0$, but that a small change $\\epsilon g(x)$ is made to the input, where $\\epsilon$ is small. Let $\\Delta r$ be the corresponding change in the root, so that$$f(r+\\Delta r) + \\epsilon g(r + \\Delta r) = 0$$ Expanding $f$ and $g$ in degree-one Taylor polynomials &gt;&gt; implies that $$f(r) + (\\Delta r) f’(r) + \\epsilon g(r) + \\epsilon(\\Delta r)g’(r) + O((\\Delta r)^2) = 0$$For small $\\Delta r$, the $O((\\Delta r)^2)$ terms can be neglected to get$$(\\Delta r) (f’(r) + \\epsilon g’(r)) \\approx -f(r)-\\epsilon g(r) = -\\epsilon g(r)$$or$$\\Delta r \\approx \\frac{-\\epsilon g(r)}{f’(r)+\\epsilon g’(r)} \\approx -\\epsilon \\frac{g(r)}{f’(r)}$$assuming that $\\epsilon$ is small compared with $f’(r)$, and in particular, $f’(r) \\neq 0$. Sensitivity Formula for RootsAssume that $r$ is a root of f(x) and $r + \\Delta r$ is a root of $f(x) + \\epsilon g(x)$. Then,$$\\Delta r \\approx -\\epsilon \\frac{g(r)}{f’(r)}$$if $\\epsilon \\ll f’(r)$ A problem with high condition number is called ill-conditioned, and a problem with a condition number near $1$ is called well-conditioned. NEWTON’S METHOD Quadratic convergence of Newton’s Method DEFINITION 1.10Let $e_i$ denote the error after step $i$ of an iterative method. The iteration is quadratically convergent（成平方收敛） if$$M = \\lim_{i\\rightarrow \\infty} \\frac{e_{i+1}}{e_i^2} &lt; \\infty$$ THEOREM 1.11Let $f$ be twice continuously differentiable and $f(r) = 0$. If $f’(r) \\neq 0$, then Newton’s Method is locally and quadratically convergent to $r$. The error $e_i$ at step $i$ satisfies$$\\lim_{i \\rightarrow \\infty} \\frac{e_{i+1}}{e_i^2} = M$$where$$M = \\frac{f’’(r)}{2f’(r)}$$ Linear convergence of Newton’s Method THEOREM 1.12Assume that the $(m + 1)$-times continuously differentiable function $f$ on $[a,b]$ has a multiplicity $m$ root at $r$. Then Newton’s Method is locally convergent to $r$, and the error $e_i$ at step $i$ satisfies$$\\lim_{i \\rightarrow \\infty} \\frac{e_{i+1}}{e_i} = S\\tag{1.29}$$where,$$S = (m-1)/m,$$ If the multiplicity of a root is known in advance, convergence of Newton’s Method can be improved with a small modification. THEOREM 1.13If f is $(m + 1)$-times continuously differentiable on $[a,b]$, which contains a root $r$ of multiplicity $m&gt;1$, then Modified Newton’s Method$$x_{i+1} = x_i - \\frac{mf(x_i)}{f’(x_i)}\\tag{1.32}$$converges locally and quadratically to $r$. For example: Find the multiplicity of the root $r = 0$ of $f(x) = \\sin x + x^2 \\cos x − x^2 − x$, and estimate the number of steps of Newton’s Method required to converge within six correct places (use $x_0 = 1$). Solution: $$f(x) = \\sin x + x^2 \\cos x - x^2 - x\\\\f’(x) = \\cos x + 2x \\cos x - x^2 \\sin x - 2x - 1\\\\f’’(x) = -\\sin x + 2 \\cos x - 4x\\sin x - x^2 \\cos x - 2\\\\$$and that each evaluates to $0$ at $r = 0$. The third derivative,$$f’’’(x) = -\\cos x - 6\\sin x - 6x \\cos x + x^2 \\sin x\\tag{1.30}$$satisfies $f(0)=−1$, so the root $r = 0$ is a triple root, meaning that the multiplicity is $m = 3$. By Theorem 1.12, Newton should converge linearly with $e_{i+1} \\approx \\frac{2e_i}{3}$. So that $$(\\frac{2}{3})^n &lt; 0.5 \\times 10^{-6}\\\\n &gt; 35.78$$ Approximately 36 steps will be needed. The first 20 steps are shown in the table. But if we apply Modified Newton’s Method to achieve quadratic convergence. After five steps, convergence to the root $r = 0$ has taken place to about eight digits of accuracy: ROOT-FINDING WITHOUT DERIVATIVESBrent’s Method, a hybrid method which combines the best features of iterative and bracketing methods. Secant Method and variantsThe Secant Method is similar to the Newton’s Method, but replaces the derivative by a difference quotient. An approximation for the derivative at the current guess $x_i$ is the difference quotient$$\\frac{f(x_i) - f(x_{i-1})}{x_i - x_{i-1}}$$ Unlike Fixed-Point Iteration and Newton’s Method, two starting guesses are needed to begin the Secant Method. the approximate error relationship of Secant Method is $$e_{i+1} \\approx |\\frac{f’’(r)}{2f’(r)}| e_ie_{i-1}$$ and it implies that $$e_{i+1} \\approx |\\frac{f’’(r)}{2f’(r)}|^{\\alpha - 1}e_i^\\alpha$$where $\\alpha = (1 + \\sqrt{5}) / 2 \\approx 1.62$ The convergence of the Secant Method to simple roots is called superlinear, meaning that it lies between linearly and quadratically convergent methods. Brent’s MethodReality Check 1: Kinematics of the Stewart platform","link":"/Math/Numerical-Analysis/NumericalAnalysis-C1-Solving-Equations/"},{"title":"NumericalAnalysis-C12-Eigenvalues-and-Singular-Values","text":"Keywords: Power Iteration Method, QR Algorithm, SVD(Singular Value Decomposition), Mathlab This is the Chapter12 ReadingNotes from book Numercial Analysis(Timothy Sauer). Power Iteration MethodsPower IterationConvergence of Power IterationInverse Power IterationRayleigh Quotient IterationQR AlgorithmsSimultaneous iterationReal Schur form and the QR algorithmUpper Hessenberg formSingular Value DecompositionFinding the SVD in generalSpecial case: symmetric matricesApplication of SVDProperties of the SVDDimension reductionCompressionCalculating the SVD","link":"/Math/Numerical-Analysis/NumericalAnalysis-C12-Eigenvalues-and-Singular-Values/"},{"title":"NumericalAnalysis-C10-Trigonometric-Interpolation-and-the-FFT","text":"Keywords: Fourier Transform, Trigonometric Interpolation, FFT, Mathlab The Fourier TransformComplex arithmeticDiscrete Fourier TransformThe Fast Fourier TransformTrigonometric InterpolationThe DFT Interpolation TheoremEfficient evaluation of trigonometric functionsThe FFT and Signal ProcessingOrthogonality and interpolationLeast squares fitting with trigonometric functionsSound, noise, and filteringReality Check 10: The Wiener Filter","link":"/Math/Numerical-Analysis/NumericalAnalysis-C10-Trigonometric-Interpolation-and-the-FFT/"},{"title":"NumericalAnalysis-C0-Fundamentals","text":"Keywords: Polynomial, Binary, Floating, Matlab This is the Chapter0 ReadingNotes from book Numerical Analysis by Timothy. EVALUATING A POLYNOMIALWhat is the best way to evaluate $$P(x) = 2x^4 + 3x^3 -3x^2 + 5x - 1$$ nested multiplication or Horner’s method:$$P(x) =−1 + x ∗ (5 + x ∗ (−3 + x ∗ (3 + x ∗ 2)))\\tag{0.2}$$evaluates the polynomial in $4$ multiplications and $4$ additions. A general degree $d$ polynomial can be evaluated in $d$ multiplications and $d$ additions. While the standard form for a polynomial $c_1 + c_2x + c_3x^2 + c_4x^3 + c_5x^4$ can be written in nested form as$$c_1 + x(c_2 + x(c_3 + x(c_4 + x(c_5))))\\tag{0.4}$$ In particular, interpolation calculations in Chapter 3 will require the form$$c_1 + (x − r_1)(c_2 + (x − r_2)(c_3 + (x − r_3)(c_4 + (x − r_4)(c_5))))\\tag{0.5}$$where we call $r_1,r_2,r_3$, and $r_4$ the base points. 123456789101112131415161718192021%nest.m%Program 0.1 Nested multiplication%Evaluates polynomial from nested form using Horner’s Method%Input: degree d of polynomial,% array of d+1 coefficients c (constant term first),% x-coordinate x at which to evaluate, and% array of d base points b, if needed%Output: value y of polynomial at xfunction y = nest(d,c,x,b)if nargin&lt;4, b=zeros(d,1); endy=c(d+1);for i=d:-1:1y = y.*(x-b(i))+c(i);end&gt;&gt; nest(4,[-1 5 -3 3 2],1/2)ans = 1.2500 BINARY NUMBERS$$4 = (100.)_2, \\frac{3}{4} = (0.11)_2$$ Decimal to binary$$(53.7)_{10} = (53)_2 + (0.7)_2 = (?)_2$$ Integer partConvert decimal integers to binary by dividing by $2$ successively and recording the remainders.$$53 \\div 2 = 26 … 1\\\\26 \\div 2 = 13 … 0\\\\13 \\div 2 = 6 … 1\\\\6 \\div 2 = 3 … 0\\\\3 \\div 2 = 1 … 1\\\\1 \\div 2 = 0 … 1\\\\$$$$(53)_10 = (110101.)_2.$$ Fractional partConvert $(0.7)_10$ to binary by reversing the preceding steps. Multiply by $2$ successively and record the integer parts, moving away from the decimal point to theright. $$.7 × 2 = .4 + 1\\\\.4 × 2 = .8 + 0\\\\.8 × 2 = .6 + 1\\\\.6 × 2 = .2 + 1\\\\.2 × 2 = .4 + 0\\\\.4 × 2 = .8 + 0\\\\\\cdots$$$$(0.7)_10 = (.1011001100110. . .)_2 = (.1\\overline{0110})_2,$$ Thus,$$(53.7)_{10} = (53)_2 + (0.7)_2 = (110101.1\\overline{0110})2.$$ Binary to decimalInteger part$$(10101)_2 = 1 \\cdot 2^4 + 0 \\cdot 2^3 + 1 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0= (21)_{10}$$ Fractional part$$(.1011)_2 = \\frac{1}{2} + \\frac{1}{8} + \\frac{1}{16}= (\\frac{11}{16})_{10}$$ What if the fractional part is not a finite base 2 expansion? For example: $$x = (0.\\overline{1011})_2$$ Solution: Multiply $x$ by $2^4$, which shifts $4$ places to the left in binary.$$2^4x = (1011.\\overline{1011})_2$$Thus,$$(2^4-1)x = (1011)_2 = (11)_{10}$$So that$$x = \\frac{11}{15}$$ What if the fractional part does not immediately repeat? For example: $$x = (0.10\\overline{101})_2$$ Solution: Multiplying by $2^2$ shifts to $y = 2^2x = 10.\\overline{101}$. The fractional part of $y$, call it $z = .\\overline{101}$, is calculated as before: $$2^3z = 101.\\overline{101}\\\\z = 000.\\overline{101}$$Therefore,$$7z = 5$$$$y = 2 + \\frac{5}{7}$$$$x = 2^{-2}y = \\frac{19}{28}$$ FLOATING POINT REPRESENTATION OF REAL NUMBERSSimple algorithms, such as Gaussian elimination or methods for solving differential equations, can magnify microscopic errors to macroscopic size. Floating point formatsThe IEEE standard consists of a set of binary representations of real numbers. A floating point number consists of three parts: the sign (+ or −)（符号位）, a mantissa（尾数）, which contains the string of significant bits, and an exponent（指数）. The three parts are stored together in a single computer word(字). word, Byte, bit1Byte = 8bitcomputer word: 16,32,64 bits The form of a normalized IEEE floating point number is$$\\pm 1.bbb \\cdots b \\times 2^p\\tag{0.6}$$where each of theN $b$’s is $0$ or $1$, and $p$ is an $M$-bit binary number representing the exponent. The double precision number $1$ is$$+1. \\underbrace{0000000000000000000000000000000000000000000000000000}_{52} × 2^0$$where we have 52 bits of the mantissa. The next floating point number greater than $1$ is$$+1. \\underbrace{0000000000000000000000000000000000000000000000000001}_{52} × 2^0$$which is $1 + 2^{−52}$. DEFINITION 0.1The number machine epsilon, denoted $\\epsilon_{match}$, is the distance between $1$ and the smallest floating point number greater than $1$.For the IEEE double precision floating point standard,$$\\epsilon_{match} = 2^{-52}$$ How do we fit the infinite binary number representing 9.4 in a finite number of bits? IEEE Rounding to Nearest RuleFor double precision, if the 53rd bit to the right of the binary point is 0, then round down (truncate after the 52nd bit). If the 53rd bit is 1, then round up (add 1 to the 52 bit), unless all known bits to the right of the 1 are 0’s, in which case 1 is added to bit 52 if and only if bit 52 is 1. $$9.4 = +1. \\underbrace{0010110011001100110011001100110011001100110011001100}_{52} 110 \\cdots × 2^3$$ $$fl(9.4) = +1. \\underbrace{0010110011001100110011001100110011001100110011001101}_{52} × 2^3$$ DEFINITION 0.2Denote the IEEE double precision floating point number associated to $x$, using the Rounding to Nearest Rule, by $fl(x)$. Obviously, for 9.4, we discarding the infinite tail$$\\begin{aligned}.\\overline{1100} \\times 2^{-52} \\times 2^3&amp;=.\\overline{0110} \\times 2^{-51} \\times 2^3 \\\\&amp;= .4 \\times 2^{-48}\\end{aligned}$$from the right end of the number. Then, we adding $2^{-52} \\times 2^3 = 2^{-49}$ in the rounding step. Therefore,$$\\begin{aligned}fl(9.4) &amp;= 9.4 + 2^{-49} - .4 \\times 2^{-48} \\\\&amp;= 9.4 + 0.2 \\times 2^{-49}\\end{aligned}\\tag{0.8}$$ We call $0.2 \\times 2^{-49}$ the rounding error. DEFINITION 0.3Let $x_c$ be a computed version of the exact quantity $x$. Then$$absolute-error = |x_c - x|$$and$$relative-error = \\frac{|x_c - x|}{x}$$ Relative rounding errorIn the IEEE machine arithmetic model, the relative rounding error of $fl(x)$ is no more than one-half machine epsilon:$$\\frac{|fl(x) - x|}{x} \\leq \\frac{1}{2} \\epsilon_{match}$$ Machine representationEach double precision floating point number is assigned an $8$-byte word, or $64$ bits, to store its three parts. Each such word has the form$$se_1e_2 . . . e_{11}b_1b_2 . . . b_{52}\\tag{0.10}$$11 bits representing the exponent and 52 bits following the decimal point, representing the mantissa. $11$ bits can represent $2^{11} = 2048$ states $\\longrightarrow [0,2047]$. In left-justifying, we find that the exponent can be negative, like $0.4 = 1.10\\overline{0110} \\times 2^{-2}$. For brevity, we don’t want to consider negative in computer exponents. So we introduce a exponent bias. In double precision, the exponent bias is $2^{10}-1 = 1023$. So that, for left-justifying double exponent $[-1022, 1023]$, the computer exponent covers $[1, 2046]$. For example: $$1 = +1. \\underbrace{0000000000000000000000000000000000000000000000000000}_{52} × 2^0,$$ Adding the exponent bias to $0$, we get $1023$, so the double precision machine number form is$$\\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000}$$the hex format is$$3FF0000000000000$$ What about 0 and 2017 ? &gt;&gt;&gt;&gt; $0$ and $2047$ for special purposes. $2047$, is used to represent $\\infty$ if the mantissa bit string is all zeros and NaN, which stands for Not a Number,otherwise. the first twelve bits of Inf and -Inf are $\\boxed{0111} \\boxed{1111} \\boxed{1111}$ and $\\boxed{1111} \\boxed{1111} \\boxed{1111}$ , respectively, and the remaining 52 bits (the mantissa) are zero. The machine number NaN also begins$\\boxed{1111} \\boxed{1111} \\boxed{1111}$ but has a nonzero mantissa. The special exponent 0, also denotes a departure from the standard floating point form. In this case the machine number is interpreted as the non-normalized floating point number $$\\pm 0.\\boxed{b_1b_2\\cdots b_{52}} \\times 2^{-1022}\\tag{0.11}$$ These non-normalized numbers are called subnormal floating point numbers. They extend the range of very small numbers by a few more orders of magnitude. Therefore, $2^{−52} × 2^{−1022} = 2^{−1074}$ is the smallest nonzero representable number in double precision. Its machine word is$$\\boxed{0} \\boxed{00000000000} \\boxed{0000000000000000000000000000000000000000000000000001}$$The subnormal numbers include the most important number $0$. Addition of floating point numbersFor example: Calculate $9.4-9-0.4$ in Computer. Solution: 9.4 is stored as$$9.4 + 0.2 \\times 2^{-49}$$thus,$$9.4 - 9 = 0.4 + 0.2 \\times 2^{-49}$$Since, 0.4 is stored as$$fl(0.4) = 0.4 + 0.1 \\times 2^{-52}$$so, the result is $$0.2 \\times 2^{-49} - 0.1 \\times 2^{-52} = 3 \\times 2^{-53}$$ 123456789101112131415161718&gt;&gt; format long&gt;&gt; x = 9.4x = 9.400000000000000&gt;&gt; y = x-9y = 0.400000000000000&gt;&gt; z = y-0.4z = 3.330669073875470e-16 LOSS OF SIGNIFICANCE（丢失有效数字）For example: We assume that we are using a three-decimal-digit computer. Now Calculate $\\sqrt{9.01} - 3$. Solution: Checking on a hand calculator, we see that the correct answer is approximately $0.0016662 = 1.6662 × 10^{−3}$. But in Computer, $\\sqrt{9.01} \\approx 3.0016662$, we store it as $3.00$. Subtracting $3.00$, we get a final answer of $0.00$. No significant digits in our answer are correct. What is causing the loss of significance is the fact that we are explicitly subtracting nearly equal numbers, We can avoid this problem by using algebra to rewrite the expression:$$\\sqrt{9.01} - 3 = \\frac{(\\sqrt{9.01} - 3) (\\sqrt{9.01} + 3)}{\\sqrt{9.01} + 3}\\approx 1.67 \\times 10^{-3}$$ The lesson is that it is important to find ways to avoid subtracting nearly equal numbers in calculations, if possible. Often, specific identities can be used, as with trigonometric expressions. For example: Compare the two expressions:$$E_1 = \\frac{1-\\cos x}{\\sin ^2 x}, E_2 = \\frac{1}{1+\\cos x}$$ $E_2$ is better than $E_1$.The quadratic formula is often subject to loss of significanc REVIEWOF CALCULUSIntermediate Value Theorem &gt;&gt; Mean Value Theorem &gt;&gt; Taylor’s Theorem &gt;&gt;","link":"/Math/Numerical-Analysis/NumericalAnalysis-C0-Fundamentals/"},{"title":"NumericalAnalysis-C11-Compression","text":"Keywords: DCT, Image compression, Quantization, Mathlab The Discrete Cosine TransformOne-dimensional DCTThe DCT and least squares approximationTwo-Dimensional DCT and Image CompressionTwo-dimensional DCTImage compressionQuantization（量化）Huffman CodingInformation theory and codingHuffman coding for the JPEG formatModified DCT and Audio CompressionModified Discrete Cosine TransformBit quantizationReality Check 11: A Simple Audio Codec","link":"/Math/Numerical-Analysis/NumericalAnalysis-C11-Compression/"},{"title":"NumericalAnalysis-C13-Optimization","text":"Keywords: Unconstrained Optimization, Mathlab Unconstrained Optimization without DerivativesGolden Section SearchSuccessive parabolic interpolationNelder–Mead searchUnconstrained Optimization with DerivativesNewton’s MethodSteepest DescentConjugate Gradient SearchReality Check 13: Molecular Conformation and Numerical Optimization","link":"/Math/Numerical-Analysis/NumericalAnalysis-C13-Optimization/"},{"title":"NumericalAnalysis-C3-Interpolation","text":"Keywords: Lagrange interpolation, Chebyshev Interpolation, Cubic Splines, Bézier Curves, Mathlab This is the Chapter3 ReadingNotes from book Numerical Analysis by Timothy. First Understanding about Interpolation and Curves&gt;&gt; Data and Interpolating Functions DEFINITION 3.1The function $y = P(x)$ interpolates the data points $(x_1,y_1), \\cdots , (x_n,y_n)$ if $P(x_i) = y_i$ for each $1 \\leq i \\leq n$. No matter how many points are given, there is some polynomial $y = P(x)$ that runs through all the points. Lagrange interpolation THEOREM 3.2Main Theorem of Polynomial Interpolation. Let $(x_1,y_1), . . . , (x_n,y_n)$ be $n$ points in the plane with distinct $x_i$. Then there exists one and only one polynomial $P$ of degree $n − 1$ or less that satisfies $P(x_i) = y_i$ for $i = 1,…,n$. Newton’s divided differences（牛顿分差法） DEFINITION 3.3Denote by $f[x_1 . . . x_n]$ the coefficient of the $x^{n−1}$ term in the (unique) polynomial that interpolates $(x_1,f(x_1)), . . . , (x_n,f(x_n))$. For example: Find an interpolating polynomial for the data points $(0,1), (2,2)$, and $(3,4)$ in Figure 3.1. Solution: By Lagrange’s formula: $$P_2(x) = \\frac{1}{2}x^2 - \\frac{1}{2}x + 1$$ Check that $P_2(0) = 1,P_2(2) = 2$, and $P_2(3) = 4$. Thus, by definition 3.3$$f[0 \\space 2 \\space3] = \\frac{1}{2}$$ Using this definition, the following somewhat remarkable alternative formula for the interpolating polynomial holds, called the Newton’s divided difference formula $$\\begin{aligned}P(x) = f[x_1] &amp;+ f[x_1 x_2](x - x_1) \\\\&amp;+ f[x_1 x_2 x_3](x - x_1)(x - x_2)\\\\&amp;+ \\cdots \\\\&amp;+ f[x_1 x_2 \\cdots x_n](x - x_1)(x-x_2)\\cdots(x - x_{n-1})\\end{aligned}\\tag{3.2}$$and it’s easy to calculate$$\\begin{aligned}&amp;f[x_k] = f(x_k)\\\\&amp;f[x_k x_{k+1}] = \\frac{f[x_{k+1}]-f[x_k]}{x_{k+1} - x_{k}}\\\\&amp;f[x_k x_{k+1} x_{k+2}] = \\frac{f[x_{k+1} x_{k+2}] - f[x_k x_{k+1}]}{x_{k+2} - x_{k}}\\\\&amp;f[x_k x_{k+1} x_{k+2} x_{k+3}] = \\frac{f[x_{k+1} x_{k+2} x_{k+3}] - f[x_k x_{k+1} x_{k+2}]}{x_{k+3} - x_{k}}\\\\\\end{aligned}\\tag{3.3}$$ For three points the table has the form(convenient to understand) The divided difference approach has a “real-time updating’’ property that the Lagrange form lacks: The Lagrange polynomial must be restarted from the beginning when a new point is added; none of the previous calculation can be used; in divided difference form, we keep the earlier work and add one new term to the polynomial. How many degree d polynomials pass through n points?Code for interpolation12345678910111213141516171819%newtdd.m%Program 3.1 Newton Divided Difference Interpolation Method%Computes coefficients of interpolating polynomial%Input: x and y are vectors containing the x and y coordinates% of the n data points%Output: coefficients c of interpolating polynomial in nested form%Use with nest.m to evaluate interpolating polynomialfunction c=newtdd(x,y,n)for j=1:n v(j,1)=y(j); % Fill in y column of Newton triangleendfor i=2:n % For column i, for j=1:n+1-i % fill in column from top to bottom v(j,i)=(v(j+1,i-1)-v(j,i-1))/(x(j+i-1)-x(j)); endendfor i=1:n c(i)=v(1,i); % Read along top of triangleend % for output coefficients Note that nest.m is in Chapter0, so you can add this file to search path, the program will run successfully. 1234567891011121314151617181920212223%clickinterp.m%Program 3.2. Polynomial Interpolation Program%Click in MATLAB figure window to locate data point.% Continue, to add more points.% Press return to terminate program.function clickinterpxl=-3;xr=3;yb=-3;yt=3;plot([xl xr],[0 0],'k',[0 0],[yb yt],'k');grid on;xlist=[];ylist=[];k=0; % initialize counter kwhile(0==0) [xnew,ynew] = ginput(1); % get mouse click if length (xnew) &lt;1 break % if return pressed, terminate end k=k+1; % k counts clicks xlist(k)=xnew; ylist(k)=ynew; % add new point to the list c=newtdd(xlist,ylist,k); % get interpolation coeffs x=xl:.01:xr; % define x coordinates of curve y=nest(k-1,c,x,xlist); % get y coordinates of curve plot(xlist,ylist,'o',x,y,[xl xr],[0,0],'k',[0 0],[yb yt],'k'); axis([xl xr yb yt]);grid on;end Representing functions by approximating polynomialsFor example: Interpolate the function $f(x) = \\sin x$ at $4$ equally spaced points on $[0,\\pi/2]$. Solution: The interval $[0,\\pi/2]$ is a so-called fundamental domain for sine, meaning that an input from any other interval can be referred back to it. The degree 3 interpolating polynomial is therefore $$P_3(x) = 0 + x(0.9549 + (x − \\pi/6)(−0.2443 + (x − \\pi/3)(−0.1139)))$$ 123456789101112131415161718192021222324%sin1.m%Program 3.3 Building a sin calculator key, attempt #1%Approximates sin curve with degree 3 polynomial% (Caution: do not use to build bridges,% at least until we have discussed accuracy.)%Input: x%Output: approximation for sin(x)function y=sin1(x)%First calculate the interpolating polynomial and% store coefficientsb=pi*(0:3)/6;yb=sin(b); % b holds base pointsc=newtdd(b,yb,4);%For each input x, move x to the fundamental domain and evaluate% the interpolating polynomials=1; % Correct the sign of sinx1=mod(x,2*pi);if x1&gt;pi x1 = 2*pi-x1; s = -1;endif x1 &gt; pi/2 x1 = pi-x1;endy = s*nest(3,c,x1,b); Interpolation ErrorInterpolation error formula THEOREM 3.4Assume that $P(x)$ is the (degree $n − 1$ or less) interpolating polynomial fitting the $n$ points $(x_1,y_1), . . . , (x_n,y_n)$. The interpolation error is$$f(x) - P(x) = \\frac{(x-x_1)(x-x_2)\\cdots(x-x_n)}{n!}f^{(n)}(c)\\tag{3.6}$$where $c$ lies between the smallest and largest of the numbers $x,x_1, . . . , x_n$. For example:Still $f(x) = \\sin x, x \\in [0, \\pi/2]$. Cal the 4 points interpolation error. By (3.6), we get,$$\\sin x - P(x) = \\frac{(x-0)(x-\\frac{\\pi}{6})(x-\\frac{\\pi}{3})(x-\\frac{\\pi}{2})}{4!} f’’’’(c),0 &lt; c &lt; \\pi/2$$ The fourth derivative $f’’’’(c) = \\sin c$. At worst, $|\\sin c|$ is no more than 1, so we can be assured of an upper bound on interpolation error:$$|\\sin x - P(x)| \\leq \\frac{(x-0)(x-\\frac{\\pi}{6})(x-\\frac{\\pi}{3})(x-\\frac{\\pi}{2})}{24} |1|$$ Note that the interpolation error will tend to be smaller close to the center of the interpolation interval. Proof of Newton form and error formulato be added… Runge phenomenon（龙格现象）For example: Interpolate $f(x) = 1/(1 + 12x^2)$ at evenly spaced points in $[−1,1]$. Runge phenomenon: polynomial wiggle near the ends of the interpolation interval. The cure for this problem is intuitive: Move some of the interpolation points toward the outside of the interval, where the function producing the data can be better fit. Chebyshev InterpolationIt turns out that the choice of base point spacing can have a significant effect on the interpolation error. Chebyshev interpolation refers to a particular optimal way of spacing the points. Chebyshev’s theoremThe interpolation error is$$\\frac{(x-x_1)(x-x_2)\\cdots(x-x_n)}{n!}f^{(n)}(c)$$Let’s fix the interval to be $[−1,1]$ for now. The numerator of the interpolation error formula is itself a degree $n$ polynomial in $x$ and has some maximum value on $[−1,1]$. $$(x-x_1)(x-x_2)\\cdots(x-x_n)\\tag{3.9}$$ We need to minimum it. So this is called the minimax problem of interpolation. THEOREM 3.6The choice of real numbers $−1 \\leq x_1, . . . , x_n \\leq 1$ that makes the value of$$\\underbrace{max}_{-1 \\leq x \\leq 1} |(x-x_1)(x-x_2)\\cdots(x-x_n)|$$as small as possible is$$x_i = \\cos\\frac{(2i-1)\\pi}{2n}, for, i = 1, \\cdots, n$$and the minimum value is $\\frac{1}{2^{n-1}}$. In fact, the minimum is achieved by$$(x-x_1)(x-x_2)\\cdots(x-x_n) = \\frac{1}{2^{n-1}}T_n(x)$$where $T_n(x)$ denotes the degree $n$ Chebyshev polynomial（$n$阶切比雪夫多项式）. We conclude from the theorem that interpolation error can be minimized if the $n$ interpolation base points in $[−1,1]$ are chosen to be the roots of the degree $n$ Chebyshev polynomial $T_n(x)$. We will call the interpolating polynomial that uses the Chebyshev roots as base points the Chebyshev interpolating polynomial（$n-1$阶切比雪夫插值多项式）. Chebyshev polynomialsThe $n$th Chebyshev polynomial is$$T_n(x) = \\cos(n \\arccos x)$$Set $y = \\arccos x$, so that $\\cos y = x$. $$T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)\\tag{3.13}$$is called the recursion relation for the Chebyshev polynomials. Several facts followfrom (3.13): FACT 1The $T_n$’s are polynomials. We showed this explicitly for $T_0,T_1, and T_2$. Since $T_3$ is a polynomial combination of $T_1$ and $T_2$, $T_3$ is also a polynomial. The same argument goes for all $T_n$. The first few Chebyshev polynomials (see Figure 3.9) are$$T_0(x) = 1\\\\T_1(x) = x\\\\T_2(x) = 2x^2-1\\\\T_3(x) = 4x^3 - 3x\\\\$$ FACT 2$deg(T_n) = n$, and the leading coefficient is $2n−1$. This is clear for $n = 1$ and $2$, and the recursion relation extends the fact to all $n$. FACT 3$T_n(1) = 1$ and $T_n(−1) = (−1)^n$. FACT 4The maximum absolute value of $T_n(x)$ for $−1 \\leq x \\leq 1$ is $1$. This follows immediately from the fact that $T_n(x) = \\cos y$ for some $y$. FACT 5All zeros of $T_n(x)$ are located between $−1$ and $1$. See Figure 3.10. In fact, the zeros are the solution of $0 = cos(n \\arccos x)$. Since $cosy = 0$ if and only if $y = odd-integer \\cdot (\\pi/2)$, we find that$$n\\arccos x = odd \\cdot \\pi/2\\\\x = \\cos \\frac{odd \\cdot \\pi}{2n}$$ Change of interval Cubic SplinesProperties of splinesA cubic spline $S(x)$ through the data points $(x_1,y_1), . . . , (x_n,y_n)$ is a set of cubic polynomials $$S_1(x) = y_1 + b_1(x-x_1) + c_1(x-x_1)^2 + d_1(x - x_1)^3, on [x_1,x_2]\\\\S_2(x) = y_2 + b_2(x-x_2) + c_2(x-x_2)^2 + d_2(x - x_2)^3, on [x_2,x_3]\\\\\\cdots\\\\S_{n-1}(x) = y_{n-1} + b_{n-1}(x-x_{n-1}) + c_{n-1}(x-x_{n-1})^2 + d_{n-1}(x - x_{n-1})^3, on [x_{n-1},x_n]\\tag{3.17}$$ with the following properties: Property 1$$S_i(x_i) = y_i\\\\S_i(x_{i+1}) = y_{i+1}, i = 1, \\cdots, n-1$$ Property 2$$S’_{i-1}(x_i) = S’_i(x_i), i = 2, \\cdots, n-1, slope$$ Property 3$$S’’_{i-1}(x_i) = S’’_i(x_i), i = 2, \\cdots, n-1, curvature$$ Property 4a Natural spline.$$S_1’’(x_1) = 0, S’’_{n-1}(x_n) = 0$$ Constructing a spline from a set of data points means finding the coefficients $b_i,c_i,d_i$ that make Properties 1–3 hold. THEOREM 3.7Let $n \\neq 2$. For a set of data points $(x_1,y_1), . . . , (x_n,y_n)$ with distinct $x_i$, there is a unique natural cubic spline fitting the points. Endpoint conditionsBézier Curves Reality Check 3: Fonts from Bézier curves","link":"/Math/Numerical-Analysis/NumericalAnalysis-C3-Interpolation/"},{"title":"NumericalAnalysis-C4-Least-Squares","text":"Keywords: Least Squares, QR Factorization, Levenberg–Marquardt Method, Gauss–Newton Method, Mathlab Least Squares and the Normal EquationsWhy Least Squares? For Chapter2, get the solution of $Ax = b$, what if there’s no Solution? When the equations are inconsistent, which is likely if the number of equations exceeds the number of unknowns, the answer is to find the next best thing: the least squares approximation. For Chapter3, find the polynomials to fit data points. However, if the data points are numerous, or the data points are collected only within some margin of error, fitting a high-degree polynomial exactly is rarely the best approach. In such cases, it is more reasonable to fit a simpler model that may only approximate the data points. More about Least Squares in Algebra &gt;&gt; Inconsistent systems of equationsA system of equations with no solution is called inconsistent. There are at least three ways to express the size of the residual. The Euclidean length of a vector, $$||r||_2 = \\sqrt{r_1^2 + r_2^2 + \\cdots + r_m^2}\\tag{4.7}$$is a norm, called the 2-norm. The squared error,$$SE = r_1^2 + r_2^2 + \\cdots + r_m^2$$and the root mean squared error (the root of the mean of the squared error)$$RMSE = \\sqrt{SE/m} = \\frac{||r||_2}{\\sqrt m}\\tag{4.8}$$ Fitting models to dataFitting data by least squares STEP 1. Choose a model. STEP 2. Force the model to fit the data. STEP 3. Solve the normal equations. Usually, the reason for using least squares is to replace noisy data with a plausible underlying model.The model is then often used for signal prediction or classification purposes. We have presented the normal equations as the most straightforward approach to solving the least squares problem, and it is fine for small problems.However, the condition number $cond(A^TA)$ is approximately the square of the original $cond(A)$, which will greatly increase the possibility that the problem is ill-conditioned. More sophisticated methods allow computing the least squares solution directly from $A$ without forming $A^TA$. These methods are based on the QR-factorization, introduced in Section 4.3, and the singular value decomposition of Chapter 12. Conditioning of least squaresA Survey of ModelsPeriodic dataData linearizationQR FactorizationGram–Schmidt orthogonalization and least squaresModified Gram–Schmidt orthogonalizationHouseholder reflectors4.4 Generalized Minimum Residual (GMRES) MethodKrylov methodsPreconditioned GMRESNonlinear Least SquaresGauss–Newton MethodModels with nonlinear parametersThe Levenberg–Marquardt MethodReality Check 4: GPS,Conditioning, and Nonlinear Least Squares","link":"/Math/Numerical-Analysis/NumericalAnalysis-C4-Least-Squares/"},{"title":"Geometry-Curves","text":"Keywords: Parametic Polynomial Curves, Bezier Curves, Spline Curves, Subdivision, C++ This is the Chapter13 ReadingNotes from book 3D Math Primer for Graphics and Game Development 2nd Edition. Parametic Polynomial CurvesParametric CurvesThe word parametric in the phrase “parametric polynomial curve” means that the curve can be described by a function of an independent parameter, which is often assigned the symbol $t$. This curve function is of the form $p(t)$, taking a scalar input (the parameter $t$) and returning the point on the curve corresponding to that parameter value as a vector output. The function $p(t)$ traces out the shape of the curve as $t$ varies. $$x^2 + y^2 = 1 \\tag{implicit}$$$$y = \\sqrt{1-x^2}\\tag{explicit}$$$$\\vec{p(t)} = (x(t), y(t))\\\\\\begin{cases}x(t) = \\cos(2\\pi t)\\\\y(t) = sin(2\\pi t)\\end{cases}\\tag{parametric}$$ Polynomial Curves（多项式曲线）$$\\vec{p(t)} = \\vec{c_0} + \\vec{c_1}t + \\vec{c_2}t^2 + \\cdots + \\vec{c_{n-1}}t^{n-1} + \\vec{c_n}t^n$$ The number $n$ is called the degree of the polynomial. Cubic Curve in Monomial Form$$\\vec{p(t)} = \\vec{c_0} + \\vec{c_1}t + \\vec{c_2}t^2 + \\vec{c_3}t^3\\tag{13.2}$$ Matrix Notation2D cubic curve in expanded monomial form:$$x(t) = c_{1,0} + c_{1,1}t + c_{1,2}t^2 + c_{1,3}t^3\\\\y(t) = c_{2,0} + c_{2,1}t + c_{2,2}t^2 + c_{2,3}t^3$$thus,$$\\begin{aligned}\\vec{p(t)} = C\\vec{t} &amp;=\\begin{bmatrix}c_{1,0} &amp; c_{1,1} &amp; c_{1,2}t^2 &amp; c_{1,3}t^3\\\\ c_{2,0} &amp; c_{2,1} &amp; c_{2,2} &amp; c_{2,3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\&amp;=\\begin{bmatrix}\\vec{c_0} &amp;\\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{c_3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\end{aligned}$$ Two Trivial Types of CurvesConsider a ray from the point $p_0$ to the point $p_1$. If we let $\\vec{d}$ be the delta vector $\\vec{p_1} − \\vec{p_0}$, then the ray is expressed parametrically as $$\\vec{p(t)} = \\vec{p_0} + \\vec{d}t\\tag{13.3}$$ let $\\vec{c_0} = \\vec{p_0}$, $\\vec{c1} = \\vec{d}$,this linear curve is a polynomial curve of degree 1. Endpoints in Monomial FormSuppose, $0 \\leq t \\leq 1$, start and end points are $\\vec{p(0)}$ and $\\vec{p(1)}$. $c_0$ specifies the start point; The endpoint is the sum of the coefficients.$$\\vec{p_0} = \\vec{c_0}\\\\\\vec{p_1} = \\vec{c_0} + \\vec{c_1} + \\vec{c_2} + \\vec{c_3}$$ Velocities and Tangentsthe velocity function $v(t)$ is the first derivative of the position function $p(t)$ because velocity measures the rate of change in position over time. Likewise, the acceleration function $a(t)$ is the derivative of the velocity function $v(t)$ because acceleration measures the rate of change of velocity over time. Velocity and Acceleration of Cubic Monomial Curve$$\\vec{p(t)} = \\vec{c_0} + \\vec{c_1}t + \\vec{c_2}t^2 + \\vec{c_3}t^3\\tag{13.4}$$$$\\vec{v(t)} = \\dot{\\vec{p(t)}} = \\vec{c_1} + 2\\vec{c_2}t + 3\\vec{c_3}t^2\\tag{13.5}$$$$\\vec{a(t)} = \\ddot{\\vec{p(t)}} = 2\\vec{c_2} + 6\\vec{c_3}t\\tag{13.6}$$ Two curves that define the same “shape,” but not the same “path”: $$p(t) = p_0 + dt$$$$Let \\space s(t) = t^2 \\mapsto p(s(t)) = p_0 + dt^2$$ The tangent at a point is the direction the curve is moving at that point, the line that just touches the curve. The tangent is basically the normalized velocity of the curve. $$\\vec{t(t)} = \\widehat{\\vec{v(t)}} = \\frac{\\vec{v(t)}}{||\\vec{v(t)}||}$$ The second derivative is related to curvature, which is sometimes denoted $\\kappa$, $$\\vec{\\kappa(t)} = \\frac{||\\vec{v(t)} \\times \\vec{a(t)}||}{||\\vec{v(t)}||^3}$$ Polynomial Interpolationlinear interpolation: Given two “endpoint” values, create a function that transitions at a constant rate (spatially, in a straight line) from one to the other (is simply first-degree polynomial interpolation). Polynomial interpolation: Given a series of control points, our goal is to construct a polynomial that interpolates them. A polynomial of degree $n$ can be made to interpolate $n + 1$ control points. We use the word “knot” to refer to control points that are interpolated, invoking the metaphor of a rope with knots in it. Aitken’s Algorithm(Geometry View)recursive algorithms, divide and conquer. To solve a difficult problem, we first divide it into two (or more) easier problems, solve the easier problems independently, and then combine the results to get the solution to the harder problem. We split this curve into two “easier” curves: (1) one that interpolates only the first $n − 1$ points, disregarding the last point; and (2) another that interpolates the last $n − 1$ points without worrying about the first point. Then, we blend these two curves together. We let $y_i^1(t)$ denote the linear curve between $y_i$ and $y_{i+1}$, the notation $y_i^2(t)$ denote the quadratic curve between $y_i$ and $y_{i+2}$, and so on Linear interpolation between two control points:$$y_1^1(t) = \\frac{(t_2 - t)y_1 + (t - t_1)y_2}{t_2 - t_1}$$$$y_2^1(t) = \\frac{(t_3 - t)y_2 + (t - t_2)y_3}{t_3 - t_2}$$Linear interpolation of lines yields a quadratic curve $$y_1^2(t) = \\frac{(t_3 - t)[y_1^1(t)] + (t - t_1)[y_2^1(t)]}{t_3 - t_1}$$ Lagrange Basis Polynomials(Math View)Each control point gives us one equation, and each coefficient gives us one unknown. This system of equations can be put into an $n \\times n$ matrix, which can be solved by standard techniques such as Gaussian elimination or $LU$ decomposition. we could create a polynomial for each knot $t_i$ such that the polynomial evaluates to unity at that knot, but for all the other knots it evaluates to zero. $$\\zeta_1(t_1) = 1, \\zeta_2(t_1) = 1, \\zeta_3(t_1) = 1, \\zeta_4(t_1) = 1\\\\\\cdots \\\\\\zeta_1(t_4) = 0, \\zeta_2(t_4) = 0, \\zeta_3(t_4) = 0, \\zeta_4(t_4) = 1\\\\$$ Interpolating polynomial in Lagrange basis form$$p(t) = \\sum_{i=1}^n y_i\\zeta_i(t) =y_1\\zeta_1(t) + y_2\\zeta_2(t) + \\cdots + y_3\\zeta_3(t)\\tag{13.7}$$ Lagrange Basis Polynomial$$\\zeta_i(t) = \\prod_{1 \\leq j \\leq n, j \\neq i} \\frac{t - t_i}{t_i - t_j}$$ Thus, we could calculate: $$\\zeta_1(t) = -(\\frac{9}{2})t^3 + 9t^2 - (\\frac{11}{2})t + 1$$ we scale each basis polynomial by the corresponding coordinate value, adding the scaled basis vectors together yields the interpolating polynomial $P$. $$\\begin{aligned}P(t) &amp;= y_1\\zeta_1(t) + y_2\\zeta_2(t) + \\cdots + y_3\\zeta_3(t)\\\\&amp;= 18t^3 − 27t^2 + 10^t + 2.\\end{aligned}$$ We can think of basis polynomials as functions yielding barycentric coordinates (blending weights). view1: the control points are the building blocks and the basis polynomials provide the scale factors, although we prefer to be more specific and call these scale factors barycentric coordinates. view2: any arbitrary vector can be described as a linear combination of the basis vectors. In our case, the space being spanned by the basis is not a geometric 3D space, but the vector space of all possible polynomials of a certain degree, and the scale values for each curve are the known values of the polynomial at the knots. More about Curve Fitting in PatternRecognition&gt;&gt; Hermite CurvesPolynomial interpolation doesn’t work as well as we would like, because of the tendency to oscillate and overshoot, so let’s try a different approach. Instead of specifying the interior positions to interpolate, let’s control the shape of the curve through the tangents at the endpoints. System of equations for Hermite conditions (cubic curve) $$p(0) = p_0 \\Longrightarrow c_0 = p_0\\tag{13.9}$$$$v(0) = v_0 \\Longrightarrow c_1 = v_0\\tag{13.10}$$$$v(1) = v_1 \\Longrightarrow c_1 + 2c_2 + 3c_3 = v_1\\tag{13.11}$$$$p(1) = p_1 \\Longrightarrow c_0 + c_1 + c_2 + c_3 = p_1\\tag{13.12}$$Converting Hermite form to monomial form:$$c_0 = p_0\\tag{13.13}$$$$c_1 = v_0\\tag{13.14}$$$$c_2 = -3p_0 - 2v_0 - v_1 +3p_1\\tag{13.15}$$$$c_3 = 2p_0 +v_0 +v_1 -2p_1\\tag{13.16}$$Thus,$$\\begin{aligned}\\vec{p(t)} &amp;= C\\vec{t}\\\\&amp;= \\begin{bmatrix}\\vec{c_0} &amp;\\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{c_3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\&amp;= PH\\vec{t}\\\\&amp;=\\begin{bmatrix}\\vec{p_0} &amp;\\vec{v_0} &amp; \\vec{v_1} &amp; \\vec{p_1}\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; -3 &amp; 2\\\\0 &amp; 1 &amp; -2 &amp; 1\\\\0 &amp; 0 &amp; -1 &amp; 1\\\\0 &amp; 0 &amp; 3 &amp; -2\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\end{aligned}$$$C = PH$, in which case, multiplication by $H$ can be considered a conversion from the Hermite basis to the monomial basis,（this book right multiply） We convert a curve from Hermite form to monomial form by using Equations (13.13)–(13.16), and from monomial form to Hermite form with Equations (13.9)–(13.12). $$\\vec{p(t)} = \\begin{bmatrix}\\vec{p_0} &amp;\\vec{v_0} &amp; \\vec{v_1} &amp; \\vec{p_1}\\end{bmatrix}\\begin{bmatrix}1-3t^2+2t^3 \\\\ t-2t^2+t^3 \\\\ -t^2+t^3 \\\\ 3t^3-2t^3\\end{bmatrix}$$ The curve $H_3(t)$ deserves special attention. It is also is known as the smoothstep function and is truly a gem that every game programmer should know. To remove the rigid, robotic feeling from any linear interpolation (especially camera transitions), simply compute the normalized interpolation fraction $t$ as usual (in the range $0 ≤ t ≤ 1$), and then replace $t with $3t^2 − 2t^3$. The reason for this is that the smoothstep function eliminates the sudden jump in velocity at the endpoints: $H_3’(0) = H_3’(1) = 0$. Bezier CurvesImportantly, B´ezier curves approximate rather than interpolate. The de Casteljau AlgorithmThe de Casteljau algorithm defines a method for constructing B´ezier curves through repeated linear interpolation. De Casteljau Recurrence Relation$$b_i^0(t) = b_i\\\\b_i^n(t) = (1-t)[b_i^{n-1}(t)] + t[b_{i+1}^{n-1}(t)]$$ The cubic equation for a specific point on the curve $p(t)$ is written in matrix notation as $$\\begin{aligned}\\vec{p(t)} &amp;= C\\vec{t}\\\\&amp;= \\begin{bmatrix}\\vec{c_0} &amp;\\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{c_3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\&amp;= BM\\vec{t}\\\\&amp;=\\begin{bmatrix}\\vec{b_0} &amp;\\vec{b_1} &amp; \\vec{b_2} &amp; \\vec{b_3}\\end{bmatrix}\\begin{bmatrix}1 &amp; -3 &amp; 3 &amp; 1\\\\0 &amp; 3 &amp; -6 &amp; 3\\\\0 &amp; 0 &amp; -3 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\end{aligned}$$ The Bernstein BasisWe can also write the polynomial in B´ezier form by collecting the terms on the control points rather than the powers of $t$. When written this way, each control point has a coefficient that represents the barycentric weight as a function of $t$ that the control point contributes to the curve. Now that we get: $$\\begin{aligned}b_0^1(t) &amp;= (1-t)b_0 + tb_1\\\\b_0^2(t) &amp;= (1-t)^2b_0 + 2t(1-t)b_1 + t^2b_2\\\\b_0^3(t) &amp;= (1-t)^3b_0 + 3t(1-t)^2b_1 + 3t^2(1-t)b_2 + t^3b_3\\\\\\cdots\\end{aligned}$$ Thus, we can see the pattern: $$\\begin{aligned}b_0^n(t) &amp;= \\sum_{i=0}^n[B_i^n(t)]b_i\\\\B_i^n(t) &amp;= \\begin{pmatrix}n \\\\ i\\end{pmatrix}t^i(1-t)^{n-i}, 0 \\leq i \\leq n\\\\\\begin{pmatrix}n \\\\ i\\end{pmatrix} &amp;= \\frac{n!}{k!(n-k)!}\\end{aligned}$$ The properties of the Bernstein polynomials tell us a lot about how B´ezier curves behave. Sum to one. The Bernstein polynomials sum to unity for all values of $t$, which is nice because if they didn’t, then they wouldn’t define proper barycentric coordinates. Convex hull property.The range of the Bernstein polynomials is $0 \\cdots 1$ for the entire length of the curve, $0 ≤ t ≤ 1$. The curve is bounded to stay within the convex hull of the control points. Endpoints interpolated. Global support.The practical result is that when any one control point is moved, the entire curve is affected. This is not a desirable property for curve design. One local maximum.Each Bernstein polynomial $B_i^n(t)$, which serves as the blend weight for the control point $b_i$, has one maximum at the auspicious time $t = i/n$. B´ezier Derivatives and Their Relationship to the Hermite Form The $nth$ derivative at either endpoint is completely determined by the nearest $n + 1$ control points. The second derivative (acceleration) at the end of the curve is determined by the closest three control points. SubdivisionWhy need subdivision? Curve refinement. Approximation techniques. Subdividing Curves in Monomial Formthe problem of subdivision can easily be viewed as a simple problem of reparameterization. $$t = F(s)\\\\q(s) = p(t) = p(F(s))$$$$\\begin{cases}F(0) = a\\\\F(1) = b\\end{cases}\\longmapstot = F(s) = a + s(b-a), s \\in [0,1]$$ Subdividing Curves in B´ezier FormAs it turns out, each round of de Casteljau interpolation produces one of our B´ezier control points. Thus， To extract the left half of a curve, $0 ≤ t ≤ b$, we perform de Casteljau subdivision as if we were trying to locate the endpoint at $t = b$. The first control point from each round of interpolation gives us another control point for our subdivided curve. SplineRules of the Gamethe function $q(s)$ refers to the entire spline, and the parameter $s$ (without subscript) is a global parameter. As $s$ varies from $0$ to $n$, the function $q(s)$ traces out the entire spline. In general, we can define $p(t)$ in terms of $q(s)$ by creating a function that maps a time value $t$ to a parameter value $s$. If you’re a computer programmer, you can think of $p(t)$ as the public interface, and $q(s)$ as an internal implementation detail. Map the time value $t$ into a value of $s$ by evaluating the time-toparameter function $s(t)$. Extract the integer portion of $s$ as $i$, and the fractional portion as $s_i$. Evaluate the curve segment $q_i(s_i)$. KnotsFor the curve to be continuous, clearly the ending point of one segment must be coincident with the starting point of the next segment. These shared control points that are interpolated by the spline are called the knots of the spline. In animation contexts, the knots are sometimes called keys. In computer animation, a key can be any position, orientation, or other piece of data whose value at a particular time is specified by a human animator (or any other source). The role of the apprentice to “fill in the missing frames” is played by the animation program, using interpolation methods such as the ones being discussed in this chapter. Hermite and B´ezier SplinesWhen we were focused on a single segment, we denoted the positions by $p_0$ and $p_1$, and the velocities(tangents) by $v_0$ and $v_1$. the knot $k_i$, which is the starting position of the segment $q_i(0)$, also serves as the ending position of the previous segment at $q_{i−1}(1)$. For velocities, the notation $v^{out}_i$ refers to the outgoing velocity at knot $i$ and defines the starting velocity for the segment $q_i$. (Photoshop calls the knots the “anchor points” and refers to the interior B´ezier control points that are not interpolated as “control points.”) When we were dealing with only a single B´ezier segment, we referred to the $ith$ control point on that segment as $b_i$. Here we use the notation $f_i$ to refer to the control point “in front” of the ith knot, and $a_i$ for the control point “after” it. Converting between B´ezier and Hermite forms:$$v_i^{in} = 3(k_i - f_i), f_i = k_i - v_i^{in}/3$$$$v_i^{out} = 3(a_i - k_i), a_i = k_i + v_i^{out}/3$$ ContinuityParametric ContinuityA curve is said to have $C^n$ continuity if its first $n$ derivatives are continuous. $C^1$ continuity condition for Hermite splines:$$v_i^{in} = v_i^{out}$$ $C^1$ continuity condition for cubic B´ezier splines $$k_i - f_i = a_i - k_i$$ $C^2$ continuity condition for cubic B´ezier splines$$f_i + (f_i - a_{i-1}) = a_i + (a_i - f_{i+1})$$ Geometric Continuity$G^1$: the tangents are parallel at the knot. if the tangents are parallel, then the discontinuity is purely a change in speed, not a change in direction. We say that a curve is $G^2$ continuous if its curvature changes continuously. Automatic Tagent ControlWe want to get the smooth curve only by several knots, without the need for the user to specify any additional criteria(like the $C^1$ continuity). CatmullRom Splinesthe formular of Catmull-Rom Spline$$\\begin{aligned}v_i^{in} &amp;= v_i^{out} = \\frac{k_{i+1} - k_{i-1}}{2}\\\\&amp;=\\frac{(k_{i+1} - k_i) + (k_i - k_{i-1})}{2}\\end{aligned}$$ TCB SplinesEndpoint Conditions","link":"/Math/Geometry/Geometry-Curves/"},{"title":"GameProgramming-Relationship-Graph","text":"Keywords: Relationship graph, Random distribution, LineRenderer Knuth-Durstenfeld ShuffleGiven an integer array nums, design an algorithm to randomly shuffle the array. All permutations of the array should be equally likely as a result of the shuffling. 123456789vector&lt;int&gt; shuffle() { int len = nums.size(); for(int i = len - 1; i &gt;= 0; i--) { int j = rand() % (i + 1); //rand() % M : [0,M) swap(nums[i],nums[j]); } return nums;} If we need to distribute the players at random positions, we can shuffle the position array and assign them to the players in sequence. EllipseThough we know the formula of ellipse $\\frac{x^2}{a^2} + \\frac{y^2}{b^2} = 1$, it’s not efficient to compute the data strictly by it. Sometimes we view an ellipse as a squashed circle, which means $x$ stay the same, but $y$ compress by a factor. For example, there are several elliptical concentric tracks, we want to sample $N$ points randomly along each orbitals (for $N$ players). Method: Set a maxValue $M$, denotes the maximum number of points can be sampled from one orbital, $N \\leq M$. 123456789101112131415161718192021222324for(int i = 1; i &lt;= orbitalNums; i++){ int maxPointNum = M; //M can be set by designer //uniformly sample M points in the orbital for(int j = 0; j &lt; maxPointNum; j++) { float x = Math.Cos(2pi / M * j); float y = Math.Sin(2pi / M * j) * compressCoefficient; //compress y value //the radius of the concentric circle is gradually increased Vector2 point = Vector2(x,y) * circleRadius * i; //add a perturbation(disturbance) avoiding rigid distribution float disturbance = circleRadius * UnityEngine.Random.Range(-offset, offset); orbit.Add(point.x + disturbance, point.y + disturbance); } //shuffle orbit array shuffle(orbit); //n players' positions &lt;-- first n of the m points assignPos();} Connect Nodes ClockwiselySometimes we want to connect several nodes by specific rule. For example, we want to connect those players on the first three orbits clockwisely, connect those players on the second four orbits clockwisely…In general, a closer orbit indicates a closer relationship to me. Method: sort the players according to the degree of intimacy, decide which orbit and which position they belong to. group those players by groupId, hash: &lt;groupId, List&lt;accountId&gt;()&gt; create a LinkedList clockwisely, linklist records the previous and next neighbor accountId for this node(player) Render the line while iterate linklist, thus we can see the palyers distribute on the orbits and connected clockwisely by group. 12345678//Vector B should be in the clickwies direction of Afloat a = Vector2.Angle(Vector2.up, A);float b = Vector2.Angle(Vector2.up, B);a = A.x &gt;= 0 ? a : 360 - a;b = B.x &gt;= 0 ? b : 360 - b;//B is in the clockwise direction of Aif(b &gt; a){A-&gt;next = B}else{B-&gt;next = A} Also, the line effect is realized by LineRenderer, inner class in Unity. Play Animations In OrderSometimes animation can be broken down into successive steps, for example, 1. the two lines disappear 2. two nodes move toward each other 3. new lines generate. Step1,2,3 must be done in sequence, but in animation1, the two lines disappear animation can run at the same time. The normal code can write like this: 1234567891011121314151617181920212223242526272829303132333435363738394041424344void updatedata(){ //data change causes the animation to occur StartCoroutine(PlayXAnimation);}IEnumerator PlayXAnimation(){ //Step1. yield return PlayStep1Animation(); //Step2. yield return PlayStep2Animation(); //Step3. yield return PlayStep3Animation();}//Step1.IEnumerator PlayStep1Animation(){ StartCoroutine(Play1.1); StartCoroutine(Play1.2); //suppose 3f is the animation length yield return new WaitForSeconds(3f);}//Step2.IEnumerator PlayStep2Animation(){ ... yield return new WaitForSeconds(3f);}//Step3.IEnumerator PlayStep3Animation(){ ... yield return new WaitForSeconds(3f);}//Step1.1IEnumerator Play1.1(){ AnimationClip clip = getclip(\"clipname\"); if(clip) { //Animation ani; ani.play(\"clipname\"); }} The essence of Coroutine is an Iterator. Click and LongPressSometimes we need to distinguish the click action and longpress action on one object. We can think of it in terms of state machines. For example, once we press our fingure on this object, $t \\leq 0.2s$, it’s click action, $0.2s &lt; t \\leq 0.7s$, it’s longpress action. once the press time $t &gt; 0.7s$, we think the action is invalid, it’s reset to idle state. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879enum State{ Idle = 0, Click = 1, Holding = 2, LongHolding = 3}State st = State.Idle; //no click and pressfloat t = 0f;bool pressed = false;//UIEventTriggervoid init(){ EventDelegate onPress = new EventDelegate(onPressobBtn); EventDelegate onRelease = new EventDelegate(onReleaseobBtn); UIEventTrigger trigger = ob.gameObject.GetComponent&lt;UIEventTrigger&gt;(); if(trigger != null) { trigger.onPress.Add(onPress); trigger.onRelease.Add(onRelease); }}void onPressobBtn(){ pressed = true;}void onReleaseobBtn(){ pressed = false;}//update functionvoid Update(){ switch (st) { case State.Idle: if(pressed) { st = State.Click; t = 0; } break; case State.Click: if(!pressed) { st = State.Idle; //click effect: show click UI, cause click effects are often displayed at the moment of release return; } t += Time.deltaTime; if(t &gt; 0.2) { st = State.Holding; //holding effect: show holding UI } break; case State.Holding: if(!pressed) { st = State.Idle; } //holding effect: cause holding effects are often displayed with holding time. like fillAmount if(t &gt; 0.7) { st = State.LongHolding; //hide holding UI } break; case State.LongHolding: if(!pressed) { st = State.Idle; } break; }}","link":"/GameProgramming/System/GameProgramming-System-RelationshipGraph/"},{"title":"NumericalAnalysis-C2-Systems-of-Equations","text":"Keywords: Gaussian Elimination, LU Factorization, Jacobi Matrix, Jacobi Iterative Method, Symmetric positve-definite matrice, Nonlinear Systems, Mathlab This is the Chapter2 ReadingNotes from book Numerical Analysis by Timothy. Gaussian EliminationConsider the system$$\\begin{cases}x + y = 3\\\\3x - 4y = 2\\end{cases}\\tag{2.1}$$ Naive Gaussian EliminationThe advantage of the tableau form is that the variables are hidden during elimination.（其实就是增广矩阵, 矩阵行变换，最简阶梯式求解的过程就叫消元）$$\\begin{bmatrix}1 &amp; 1 &amp; 3\\\\3 &amp; -4 &amp; 1\\end{bmatrix}\\longrightarrow\\begin{bmatrix}1 &amp; 1 &amp; 3\\\\0 &amp; -7 &amp; -7\\end{bmatrix}$$the corresponding equation is:$$\\begin{cases}x + y = 3\\\\-7y = -7\\end{cases}$$we solve y, x in order, this part is called back substitution, or backsolving. Operation countsThe general form of the tableau for $n$ equations in $n$ unknowns is$$\\begin{bmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2\\\\\\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; | &amp; \\cdots\\\\a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} &amp; | &amp; b_n\\\\\\end{bmatrix}$$ The elimination process can be written as: 12345for j = 1 : n-1 for i = j+1 : n eliminate entry a(i,j) endend This number and the other numbers $a_{ii}$ that are eventually divisors（除数） in Gaussian elimination are called pivots. More specific of elimination process: 1234567891011121314//columnfor j = 1 : n-1 if abs(a(j,j))&lt;eps; error(’zero pivot encountered’); end //row for i = j+1 : n mult = a(i,j)/a(j,j); //elements in this row for k = j+1:n a(i,k) = a(i,k) - mult * a(j,k); end //diagnal element b(i) = b(i) - mult*b(j); endend For the operation, we can see that: $$\\begin{bmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2\\\\\\end{bmatrix}\\longrightarrow\\begin{bmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\0 &amp; a_{22} - \\frac{a_{21}}{a_{11}}a_{12} &amp; \\cdots &amp; a_{2n} - \\frac{a_{21}}{a_{11}}a_{1n} &amp; | &amp; b_2 - \\frac{a_{21}}{a_{11}}b_1\\\\\\end{bmatrix}$$Accounting for the operations, this requires one division (to find the multiplier $\\frac{a_{21}}{a_{11}}$), plus $n$ multiplications and $n$ additions, which is $2n+1$. The total operations in the matrix is$$\\begin{bmatrix}0\\\\2n+1 &amp; 0\\\\2n+1 &amp; 2(n-1)+1 &amp; 0\\\\2n+1 &amp; 2(n-1)+1 &amp; 2(n-2)+1\\\\\\cdots &amp; \\cdots &amp; \\cdots \\\\2n+1 &amp; 2(n-1)+1 &amp; 2(n-2)+1 &amp; \\cdots &amp; 2(3)+1 &amp; 0\\\\2n+1 &amp; 2(n-1)+1 &amp; 2(n-2)+1 &amp; \\cdots &amp; 2(3)+1 &amp; 2(2) + 1 &amp; 0\\\\\\end{bmatrix}$$We total up the operation as$$\\begin{aligned}\\sum_{j=1}^{n-1}\\sum_{i=1}^{j} 2(j+1) + 1 &amp;= \\frac{2}{3}n^3 + \\frac{1}{2}n^2 -\\frac{7}{6}n\\end{aligned}$$The operation count shows that direct solution of $n$ equations in $n$ unknowns by Gaussian elimination is an $O(n^3)$ process. The back-substitution step is: 123456for i = n : -1 : 1 for j = i+1 : n b(i) = b(i) - a(i,j)*x(j); end x(i) = b(i)/a(i,i);end Counting operations yield$$1 + 3 + 5 + \\cdots + (2n-1) = n^2$$ The computer can carry out $(5000)^2$ operations in $0.1$ seconds, or $(5000)^2(10) =2.5 × 10^8$ operations/second. The $LU$ FactorizationMatrix form of Gaussian elimination DEFINITION 2.2An $m \\times n$ matrix $L$ is lower triangular if its entries satisfy $l_{ij} = 0$ for $i &lt; j$. An $m \\times n$ matrix $U$ is upper triangular if its entries satisfy $u_{ij} = 0$ for $i &gt; j$. For example: Fine the $LU$ factorization of$$A = \\begin{bmatrix}1 &amp; 2 &amp; -1\\\\2 &amp; 1 &amp; -2\\\\-3 &amp; 1 &amp; 1\\end{bmatrix}\\tag{2.13}$$ Solution: $$A = \\begin{bmatrix}1 &amp; 2 &amp; -1\\\\2 &amp; 1 &amp; -2\\\\-3 &amp; 1 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space 2 \\times row \\space 1 }\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -3 &amp; 0\\\\-3 &amp; 1 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space 3 \\times row \\space 1 }\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -3 &amp; 0\\\\0 &amp; 7 &amp; -2\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space -\\frac{7}{3} \\times row \\space 2 }\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -3 &amp; 0\\\\0 &amp; 0 &amp; -2\\end{bmatrix} = U$$ The lower triangular $L$ matrix is formed, as in the previous example, by putting $1$’s on the main diagonal and the multipliers in the lower triangle—in the specific places they were used for elimination. That is,$$L =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\2 &amp; 1 &amp; 0\\\\-3 &amp; -\\frac{7}{3} &amp; 1\\end{bmatrix}\\tag{2.14}$$ Back substitution with the LU factorization $Ax = b$(a) Solve $Lc = b$ for $c$.(b) Solve $Ux = c$ for $x$. Complexity of the LU factorizationNow, suppose that we need to solve a number of different problems with the same $A$ and different $b$. That is, we are presented with the set of problems$$Ax = b_1, \\\\Ax = b_2, \\\\…\\\\Ax = b_k\\\\$$with naive Gaussian Elimination, the complexity is$$\\frac{2kn^3}{2} + kn^2$$ but with $LU$, the complesity is $$\\underbrace{\\frac{2n^3}{3}}_{LU \\space Factorization} + \\underbrace{2kn^2}_{back \\space substitutions}$$ The $LU$ approach allows efficient handling of all present and future problems that involve the same coefficient matrix $A$. Source of ErrorError magnification and condition number DEFINITION 2.3The infinity norm, or maximum norm, of the vector $x = (x_1, \\cdots , x_n)$ is $||x||_\\infty = max|x_i|$, $i = 1,\\cdots,n$, that is, the maximum of the absolute values of the components of $x$. DEFINITION 2.4Let $x_a$ be an approximate solution of the linear system $Ax = b$. The residual is the vector $r = b − Ax_a$. The backward error is the norm of the residual $||b − Ax_a||_\\infty$, and the forward error is $||x − x_a||_\\infty$. For example: Find the forward and backward errors for the approximate solution $[−1,3.0001]$ of the system$$\\begin{cases}x_1 + x_2 = 2\\\\1.0001x_1 + x_2 = 2.0001.\\end{cases}\\tag{2.17}$$ Solution: The correct solution is$$[x_1, x_2] = [1,1]$$ The backward error is the infinity norm of the vector$$b - Ax_a =\\begin{bmatrix}-0.0001\\\\0.0001\\end{bmatrix}$$which is $0.0001$. The forward error is the infinity norm of the difference $$x - x_a =\\begin{bmatrix}2\\\\−2.0001\\end{bmatrix}$$which is $2.0001$. Figure 2.2 helps to clarify how there can be a small backward error and large forward error at the same time. Even though the “approximate root’’ $(−1,3.0001)$ is relatively far from the exact root $(1,1)$, it nearly lies on both lines. This is possible because the two lines are almost parallel. If the lines are far from parallel, the forward and backward errors will be closer in magnitude. Denote the residual by $r = b - Ax_a$. The relative backward error of system $Ax = b$is defined to be$$\\frac{||r||_\\infty}{||b||_\\infty}$$ and the relative forward error is $$\\frac{||x - x_a||_\\infty}{||x||_\\infty}$$ The error magnification factor for $Ax = b$ is the ratio of the two, or$$error-magnification-factor = \\frac{relative-forward-error}{relative-backward-error} = \\frac{\\frac{||x - x_a||_\\infty}{||x||_\\infty}}{\\frac{||r||_\\infty}{||b||_\\infty}}\\tag{2.18}$$ For system(2.17), the error magnification factor is$$\\frac{\\frac{2.0001}{1}}{\\frac{0.0001}{2.0001}} \\approx 40004.0001$$ DEFINITION 2.5The condition number of a square matrix $A$, $cond(A)$, is the maximum possible error magnification factor for solving $Ax = b$, over all right-hand sides $b$.Surprisingly, there is a compact formula for the condition number of a square matrix. Analogous to the norm of a vector, define the matrix norm of an $n \\times n$ matrix $A$ as$$||A||_\\infty = maximum-absolute-row-sum\\tag{2.19}$$ THEOREM 2.6The condition number of the $n \\times n$ matrix $A$ is$$cond(A) = ||A|| \\cdot ||A^{-1}||$$ Thus, The norm of$$A =\\begin{bmatrix}1 &amp; 1\\\\1.0001 &amp; 1\\end{bmatrix}$$is $||A|| = 2.0001$ The condition number of $A$ is $$cond(A) = 40004.0001.$$ This is exactly the error magnification we found in Example above, which evidently achieves the worst case, defining the condition number. The error magnification factor for any other $b$ in this system will be less than or equal to $40004.0001$. If $cond(A) ≈ 10^k$, we should prepare to lose $k$ digits of accuracy in computing $x$. In the example above, since $cond(A) \\approx 40004.0001$. so in double precision we should expect about $16 − 4 = 12$ correct digits in the solution $x$. We use Matlab for a computation: 12345&gt;&gt; A = [1 1;1.0001 1]; b=[2;2.0001];&gt;&gt; xa = A\\bxa =1.000000000002220.99999999999778 Compared with the correct solution $x = [1,1]$, the computed solution has about $11$ correct digits, close to the prediction from the condition number. vector norm $||x||$, which satisfies three properties:(i) $||x|| \\neq 0$ with equality if and only if $x = [0, . . . ,0]$(ii) for each scalar $\\alpha$ and vector $x$, $||\\alpha x|| = |\\alpha| \\cdot ||x||$(iii) for vectors $x,y$, $||x + y|| \\leq ||x|| + ||y||$. matrix norm $||A||\\infty$ which satisfies three properties:(i) $||A|| \\geq 0$ with equality if and only if $A = 0$(ii) for each scalar $\\alpha$ and matrix $A$, $||\\alpha A|| = |\\alpha| \\cdot ||A||$(iii) for matrices $A,B$, $||A + B|| \\leq ||A|| + ||B||$. the vector 1-norm of the vector $x$ is $||x||_1 = |x_1| + |x_2| + \\cdots + |x_n|$. The matrix 1-norm of the $n \\times n$ matrix $A$ is $||A||_1 = maximum-absolute-column-sum$, which is the maximum of the 1-norms of the column vectors. SwampingFor example: $$\\begin{cases}10^{-20}x_1 + x_2 = 1\\\\x_1 + 2x_2 = 4\\end{cases}$$We will solve the system three times: once with complete accuracy, second where we mimic a computer following IEEE double precision arithmetic, and once more where we exchange the order of the equations first. Solution: Exact solution. $$\\begin{bmatrix}10^{-20} &amp; 1 &amp; | &amp; 1\\\\1 &amp; 2 &amp; | &amp; 4\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space 10^{20} \\times row \\space 1 }\\begin{bmatrix}10^{-20} &amp; 1 &amp; | &amp; 1\\\\0 &amp; 2 - 10^{20} &amp; | &amp; 4 - 10^{20}\\end{bmatrix}$$The bottom equation is$$(2 - 10^{20}) x_2 = 4 - 10^{20}\\longrightarrowx_2 = \\frac{4 - 10^{20}}{2 - 10^{20}}$$and the top equation yields$$x_1 = \\frac{-2 \\times 10^{20}}{2 - 10^{20}}$$The exact solution is$$[x_1, x_2] \\approx [2,1]$$ IEEE double precision. $2 - 10^{20}$ is the same as $-10^{20}$, due to rounding. $4 - 10^{20}$ is stored as $-10^{20}$ Thus,$$[x_1, x_2] = [0,1]$$ IEEE double precision, after row exchange.$$\\begin{bmatrix}1 &amp; 2 &amp; | &amp; 4 \\\\10^{-20} &amp; 1 &amp; | &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space 10^{-20} \\times row \\space 1 }\\begin{bmatrix}1 &amp; 2 &amp; | &amp; 4\\\\0 &amp; 1 - 2 \\times 10^{-20} &amp; | &amp; 1 - 4 \\times 10^{-20}\\\\\\end{bmatrix}$$ $1 - 2 \\times 10^{-20}$ is stored as $1$, $1 - 4 \\times 10^{-20}$ is stored as $1$. $$[x_1, x_2] = [2,1]$$ The effect of subtracting $10^20$ times the top equation from the bottom equation was to overpower, or “swamp”, the bottom equation. The $PA = LU$ FactorizationPartial pivotingThe partial pivoting protocol consists of comparing numbers before carrying out each elimination step. (一种交换矩阵行的方法，用于避免因主对角线元素过小而导致的数值不稳定问题) For example: Apply Gaussian elimination with partial pivoting to solve the system$$\\begin{cases}x_1 - x_2 + 3x_3 = -3\\\\-x_1 - 2x_3 = 1\\\\2x_1 + 2x_2 + 4x_3 = 0\\end{cases}$$ Solution: This example is written in tableau form as$$\\begin{bmatrix}1 &amp; -1 &amp; 3 &amp; | &amp; -3\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\\\2 &amp; 2 &amp; 4 &amp; | &amp; 0\\end{bmatrix}$$Under partial pivoting we compare $|a_{11}| = 1$ with $|a_{21}| = 1$ and $|a_{31}| = 2$, choose $a_{31}$ for the new pivot. $$\\begin{bmatrix}1 &amp; -1 &amp; 3 &amp; | &amp; -3\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\\\2 &amp; 2 &amp; 4 &amp; | &amp; 0\\end{bmatrix}\\xrightarrow[exchange]{row \\space 1 &lt;-&gt;row \\space 3}\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\1 &amp; -1 &amp; 3 &amp; | &amp; -3\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space -\\frac{1}{2} \\times row \\space 1}\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; | &amp; 1\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space \\frac{1}{2} \\times row \\space 1}\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; | &amp; 1\\\\0 &amp; -2 &amp; 1 &amp; | &amp; -3\\end{bmatrix}$$ Before eliminating column $2$ we must compare the current $|a_{22}|$ with the current $|a_{32}|$.Because the latter is larger, we again switch rows: $$\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\0 &amp; -2 &amp; 1 &amp; | &amp; -3\\\\0 &amp; 1 &amp; 0 &amp; | &amp; 1\\end{bmatrix}\\longrightarrow x = [1,1,-1]$$ Permutation matrices（置换矩阵） DEFINITION 2.7A permutation matrix is an $n \\times n$ matrix consisting of all zeros, except for a single $1$ in every row and column.（主元法） THEOREM 2.8Fundamental Theorem of Permutation Matrices. Let $P$ be the $n \\times n$ permutation matrix formed by a particular set of row exchanges applied to the identity matrix. Then, for any $n \\times n$ matrix $A$, $PA$ is the matrix obtained by applying exactly the same set of row exchanges to $A$. $PA = LU$ factorizationAs its name implies, the $PA=LU$ factorization is simply the $LU$ factorization of a row-exchanged version of $A$. For example: Find the $PA=LU$ factorization of the matrix, and solve $Ax = b$, where$$A =\\begin{bmatrix}2 &amp; 1 &amp; -5\\\\4 &amp; 4 &amp; -4\\\\1 &amp; 3 &amp; 1\\end{bmatrix},b =\\begin{bmatrix}5\\\\0\\\\6\\end{bmatrix}$$ Solution: $$\\begin{bmatrix}2 &amp; 1 &amp; -5\\\\4 &amp; 4 &amp; -4\\\\1 &amp; 3 &amp; 1\\end{bmatrix}\\xrightarrow[P = \\begin{bmatrix} 0 &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}]{exchange \\space row \\space 1&lt;-&gt;row \\space 2}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\2 &amp; 1 &amp; -5\\\\1 &amp; 3 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space \\frac{1}{2} \\times row \\space 1}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{2}} &amp; -1 &amp; 7\\\\1 &amp; 3 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space \\frac{1}{4} \\times row \\space 1}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{2}} &amp; -1 &amp; 7\\\\\\boxed{\\frac{1}{4}} &amp; 2 &amp; 2\\end{bmatrix}$$ $$\\xrightarrow[P = \\begin{bmatrix} 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 0\\end{bmatrix}]{exchange \\space row \\space 2&lt;-&gt;row \\space 3}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{4}} &amp; 2 &amp; 2\\\\\\boxed{\\frac{1}{2}} &amp; -1 &amp; 7\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space -\\frac{1}{2} \\times row \\space 2}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{4}} &amp; 2 &amp; 2\\\\\\boxed{\\frac{1}{2}} &amp; \\boxed{-\\frac{1}{2}} &amp; 8\\end{bmatrix}$$ So the $PA = LU$ factorization is :$$\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\\\1 &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix}2 &amp; 1 &amp; -5\\\\4 &amp; 4 &amp; -4\\\\1 &amp; 3 &amp; 1\\end{bmatrix} =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\\\boxed{\\frac{1}{4}} &amp; 1 &amp; 0\\\\\\boxed{\\frac{1}{2}} &amp; \\boxed{-\\frac{1}{2}} &amp; 1\\end{bmatrix}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\0 &amp; 2 &amp; 2\\\\0 &amp; 0 &amp; 8\\end{bmatrix}$$ This solving solution proess is:$$PAx = Pb \\Leftrightarrow LUx = Pb.$$ $Lc = Pb$$$c = \\begin{bmatrix} 0\\\\ 6 \\\\ 8\\end{bmatrix}$$ $Ux = c$$$x = [-1,2,1]$$（其实$x$是个列向量…） 12345678910111213141516171819202122&gt;&gt; A=[2 1 5; 4 4 -4; 1 3 1];&gt;&gt; [L,U,P]=lu(A)L = 1.0000 0 0 0.2500 1.0000 0 0.5000 -0.5000 1.0000U = 4 4 -4 0 2 2 0 0 8P = 0 1 0 0 0 1 1 0 0 Reality Check 2: The Euler–Bernoulli Beamto be added… Iterative MethodsGaussian elimination is called a direct method for solving systems of linear equations.So-called iterative methods also can be applied to solving systems of linear equations. Similar to Fixed-Point Iteration, the methods begin with an initial guess and refine the guess at each step, converging to the solution vector. Jacobi MethodThe Jacobi Method is a form of Fixed-point Iteration &gt;&gt; for a system of equations. DEFINITION 2.9The $n \\times n$ matrix $A = (a_{ij})$ is strictly diagonally dominant（严格对角占优） if, for each $1 \\leq i \\leq n$, $|a_{ii}| &gt; \\sum_{j \\neq i}|a_{ij} |$. In other words, each main diagonal entry dominates its row in the sense that it is greater in magnitude than the sum of magnitudes of the remainder of the entries in its row. THEOREM 2.10If the $n \\times n$ matrix $A$ is strictly diagonally dominant, then (1) $A$ is a nonsingular matrix（invertible）, and (2) for every vector $b$ and every starting guess, the Jacobi Method applied to $Ax = b $ converges to the (unique) solution. Note that strict diagonal dominance is only a sufficient condition. The Jacobi Method may still converge in its absence. (严格的对角占优只是充分条件，没有这个条件Jacobi Method 也可能收敛) Let $D$ denote the main diagonal of $A$, $L$ denote the lower triangle of $A$ (entries below the main diagonal), and $U$ denote the upper triangle (entries above the main diagonal). Then $A = L + D + U$, and the equation to be solved is $Lx + Dx + Ux = b$. $$Ax = b\\\\(D + L + U)x = b\\\\Dx = b - Lx - Ux\\\\x = D^{-1}(b - (L+u)x)\\tag{2.39}$$ Gauss–Seidel Method and SORGauss–Seidel often converges faster than Jacobi if the method is convergent. The method called Successive Over-Relaxation (SOR) takes the Gauss–Seidel direction toward the solution and “overshoots” to try to speed convergence. Let $\\omega$ be a real number, and define each component of the new guess $x_{k+1}$ as a weighted average of $\\omega$ times the Gauss–Seidel formula and $1 - \\omega$ times the current guess $x_k$. The number $\\omega$ is called the relaxation parameter, and $\\omega &gt; 1 $ is referred to as over-relaxation. For example:Apply the Jacobi Method, Gauss-Seidel Method and SOR Method to the system$$\\begin{cases}3u + v = 5\\\\u + 2v = 5\\end{cases}$$ Solution: We use the initial guess $(u_0, v_0) = (0,0)$.$$\\begin{bmatrix}3 &amp; 1\\\\1 &amp; 2\\end{bmatrix}\\begin{bmatrix}u \\\\ v\\end{bmatrix}=\\begin{bmatrix}5\\\\ 5\\end{bmatrix}$$ Jacobi Method$$\\begin{aligned}\\begin{bmatrix}u_{k+1} \\\\ v_{k+1}\\end{bmatrix}&amp;= D^{-1}(b - (L+U)x_k)\\\\&amp;=\\begin{bmatrix}\\frac{1}{3} &amp; 0\\\\0 &amp; \\frac{1}{2}\\end{bmatrix}(\\begin{bmatrix} 5 \\\\ 5\\end{bmatrix} - \\begin{bmatrix} 0 &amp; 1\\\\ 1 &amp; 0\\end{bmatrix}\\begin{bmatrix}u_{k} \\\\ v_{k}\\end{bmatrix})\\\\&amp;= \\begin{bmatrix}(5-v_k)/3\\\\(5-u_k)/2\\end{bmatrix}\\end{aligned}$$ Gauss-Seidel Method $$\\begin{bmatrix}u_{k+1} \\\\ v_{k+1}\\end{bmatrix}=\\begin{bmatrix}(5-v_k)/3\\\\(5-u_{k+1})/2\\end{bmatrix}$$ SOR Method $$u_{k+1} = (1-\\omega)u_k + \\omega \\frac{5-v_k}{3}\\\\v_{k+1} = (1-\\omega)v_k + \\omega \\frac{5-u_{k+1}}{2}$$ For example: Compare Jacobi, Gauss–Seidel, and SOR on the system of six equations in six unknowns:$$\\begin{bmatrix}3 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{2}\\\\−1 &amp; 3 &amp; −1 &amp; 0 &amp; \\frac{1}{2} &amp; 0\\\\0 &amp; −1 &amp; 3 &amp; −1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; -1 &amp; 3 &amp; -1 &amp; 0\\\\0 &amp; \\frac{1}{2} &amp; 0 &amp; −1 &amp; 3 &amp; −1\\\\\\frac{1}{2} &amp; 0 &amp; 0 &amp; 0 &amp; −1 &amp; 3\\\\\\end{bmatrix}\\begin{bmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\\\ u_5 \\\\ u_6\\end{bmatrix}=\\begin{bmatrix}\\frac{5}{2} \\\\ \\frac{3}{2} \\\\ 1 \\\\ 1 \\\\ \\frac{3}{2} \\\\ \\frac{5}{2}\\end{bmatrix}$$ Solution: The solution is $x = [1,1,1,1,1,1]$. The approximate solution vectors $x_6$, after running six steps of each of the three methods, are shown in the following table: SOR appears to be superior for this problem. Convergence of iterative methodsto be added… Sparse matrix computationsA coefficient matrix is called sparse if many of the matrix entries are known to be zero Often, of the $n^2$ eligible entries in a sparse matrix, only $O(n)$ of them are nonzero. There are two reasons to use iterative methods while Gaussian elimination provide the user a finite number of steps that terminate the solution. For real-time applications, suppose that a solution to $Ax = b$ is known, after which $A$ and/or $b$ change by a small amount. In this case, we can iterative from the origin solution, which brings fast convergence. It’s called polish technique. Gaussian elimination for an $n \\times n$ matrix costs on the order of $n^3$ operations.A single step of Jacobi’s Method, for example, requires about $n^2$ multiplications (one for each matrix entry) and about the same number of additions. For sparse matrix, Gaussian elimination often brings fill-in, where the coefficient matrix changes from sparse to full due to the necessary row operations. Iterative method can avoid this problem. For example: Use the Jacobi Method to solve the 100,000-equation version of the Example above. Let $n$ be an even integer, and consider the $n \\times n$ matrix $A$ with $3$ on the main diagonal, $−1$ on the super-and subdiagonal, and $1/2$ in the $(i,n + 1 − i)$ position for all $i = 1,…,n$, except for $i = n/2$ and $n/2 + 1$. For $n = 12$, Define the vector $b = (2.5,1.5, . . . ,1.5,1.0,1.0,1.5, . . . ,1.5,2.5)$, where there are $n − 4$repetitions of $1.5$ and $2$ repetitions of $1.0$. Solution: Since fewer than $4n$ of the potential entries are nonzero, we may call the matrix sparse. In size, treating the coefficient matrix $A$ as a full matrix means storing $n^2 = 10^{10}$ entries, we need $10^{10} * 8Byte \\approx 80GB$ to store this matrix in computer. In time, $n^3 \\approx 10^{15}$, if the machine runs $10^9$ cycles per second, and upper bound on the number of floating point operations per second is around $10^{8}$. Then you need $\\frac{10^{15}}{10^{8}} = 10^7$ seconds, and there are $3 \\times 10^{7}$ seconds in a year, which means you need $\\frac{1}{3}$ year to solve this problem… On the other hand, one step of an iterative method will require approximately $2 \\times 4n = 800,000$ operations. We could do $100$ steps of Jacobi iteration and still finish with fewer than $10^8$ operations, which should take roughly a second or less on a modern PC. 1234567891011%sparsesetup.m% Program 2.1 Sparse matrix setup% Input: n = size of system% Outputs: sparse matrix a, r.h.s. bfunction [a,b] = sparsesetup(n)e = ones(n,1); n2=n/2;a = spdiags([-e 3*e -e],-1:1,n,n); % Entries of ac=spdiags([e/2],0,n,n);c=fliplr(c);a=a+c;a(n2+1,n2) = -1; a(n2,n2+1) = -1; % Fix up 2 entriesb=zeros(n,1); % Entries of r.h.s. bb(1)=2.5;b(n)=2.5;b(2:n-1)=1.5;b(n2:n2+1)=1; 12345678910111213%jacobi.m% Program 2.2 Jacobi Method% Inputs: full or sparse matrix a, r.h.s. b,% number of Jacobi iterations, k% Output: solution xfunction x = jacobi(a,b,k)n=length(b); % find nd=diag(a); % extract diagonal of ar=a-diag(d); % r is the remainderx=zeros(n,1); % initialize vector xfor j=1:k % loop for Jacobi iterationx = (b-r*x)./d;end % End of Jacobi iteration loop 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt;&gt; [a,b] = sparsesetup(6)a = (1,1) 3.0000 (2,1) -1.0000 (6,1) 0.5000 (1,2) -1.0000 (2,2) 3.0000 (3,2) -1.0000 (5,2) 0.5000 (2,3) -1.0000 (3,3) 3.0000 (4,3) -1.0000 (3,4) -1.0000 (4,4) 3.0000 (5,4) -1.0000 (2,5) 0.5000 (4,5) -1.0000 (5,5) 3.0000 (6,5) -1.0000 (1,6) 0.5000 (5,6) -1.0000 (6,6) 3.0000b = 2.5000 1.5000 1.0000 1.0000 1.5000 2.5000&gt;&gt; jacobi(a,b,40)ans = 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 Methods for symmetric positive-definite matricesSymmetric positive-definite matricesMore about Symmetric Matrix &gt;&gt; DEFINITION 2.12The $n \\times n$ matrixAis symmetric if $A^T = A$. The matrixAis positive-definite if $x^TAx &gt; 0$ for all vectors $x \\neq 0$. Note that a symmetric positive-definite matrix must be nonsingular（invertible）, since it is impossible for a nonzero vector $x$ to satisfy $Ax = 0$. Property 1If the $n \\times n$ matrix $A$ is symmetric, then $A$ is positive-definite if and only if all of its eigenvalues are positive. Property 2If A is $n \\times n$ symmetric positive-definite and $X$ is an $n \\times m$ matrix of full rank &gt;&gt; with $n \\geq m$, then $X^TAX$ is $m × m$ symmetric positive-definite. DEFINITION 2.13A principal submatrix of a square matrix $A$ is a square submatrix whose diagonal entries are diagonal entries of $A$. Property 3Any principal submatrix of a symmetric positive-definite matrix is symmetric positive definite.(对称正定矩阵的任何主子矩阵都是对称正定矩阵) Extra DefinitionA principal submatrix of a square matrix $A$ is the matrix obtained by deleting any $k$ rows and the corresponding $k$ columns.The determinant of a principal submatrix is called the principal minor of $A$. Extra DefinitionThe leading principal submatrix of order $k$ of an $n \\times n$ matrix is obtained by deleting the last $n - k$ rows and column of the matrix.The determinant of a leading principal submatrix is called the leading principal minor of $A$. Principal minors can be used in definitess tests Cholesky factorization THEOREM 2.14(Cholesky Factorization Theorem) If $A$ is a symmetric positive-definite $n \\times n$ matrix, then there exists an upper triangular $n \\times n$ matrix $R$ such that $A = R^TR$. For example: Find the Cholesky factorization of$$\\begin{bmatrix}4 &amp; -2 &amp; 2\\\\-2 &amp; 2 &amp; -4\\\\2 &amp; -4 &amp; 11\\end{bmatrix}$$ Solution: The top row of $R$ is $R_{11} = \\sqrt{a_{11}} = 2$, followed by $u^T = \\frac{1}{R_{11}}A_{1,2:3} = [-2,2]/R_{11} = [-1,1]$:$$R =\\left[\\begin{array}{c:cc}2 &amp; -1 &amp; 1\\\\\\hdashline\\\\\\end{array}\\right]$$So, $uu^T = \\begin{bmatrix}-1 &amp; 1\\end{bmatrix} \\begin{bmatrix}-1\\\\1\\end{bmatrix}$. The lower principle $2 \\times 2$ submatrix $A_{2:3,2:3}$ of $A$ is$$\\left[\\begin{array}{c:cc}&amp; &amp; &amp;\\\\\\hdashline&amp; 2 &amp; -4 \\\\&amp; -4 &amp; 11\\end{array}\\right]-\\left[\\begin{array}{c:cc}&amp; &amp; &amp;\\\\\\hdashline&amp; 1 &amp; -1 \\\\&amp; -1 &amp; 1\\end{array}\\right]=\\left[\\begin{array}{c:cc}&amp; &amp; &amp;\\\\\\hdashline&amp; 1 &amp; -3 \\\\&amp; -3 &amp; 10\\end{array}\\right]$$Now we repeat the same steps on the $2 \\times 2$ submatrix to find $R_{22} = 1$ and $R_{23} = −3/1=−3$:$$R = \\left[\\begin{array}{c:cc}2 &amp; -1 &amp; 1\\\\\\hdashline&amp; 1 &amp; -3\\\\\\end{array}\\right]$$The lower $1 \\times 1$ principal submatrix of $A$ is $10 − (−3)(−3) = 1$, so $R_{33} = \\sqrt 1$. The Cholesky factor of $A$ is$$R = \\left[\\begin{array}{ccc}2 &amp; -1 &amp; 1\\\\0 &amp; 1 &amp; -3\\\\0 &amp; 0 &amp; 1\\end{array}\\right]$$ Conjugate Gradient Method（共轭梯度法） DEFINITION 2.15Let $A$ be a symmetric positive-definite $n \\times n$ matrix. For two $n$-vectors $v$ and $w$, define the $A$-inner product$$(v,w)_A = v^TAw$$The vectors $v$ and $w$ are $A$-conjugate if $(v,w)_A = 0$. The vector $x_k$ is the approximate solution at step $k$. The vector $r_k$ represents the residual of the approximate solution $x_k$. The vector $d_k$ represents the new search direction used to update the approximation $x_k$ to the improved version $x_{k+1}$. THEOREM 2.16Let $A$ be a symmetric positive-definite $n \\times n$ matrix and let $b \\neq 0$ be a vector. In the Conjugate Gradient Method, assume that $r_k \\neq 0$ for $k &lt; n$ (if $r_k = 0$ the equation is solved). Then for each $1 \\leq k \\leq n$,(a) The following three subspaces of $R^n$ are equal: $&lt;x_1, . . . , x_k&gt; = &lt;r_0, . . . , r_{k−1}&gt; = &lt;d_0, . . . , d_{k−1}&gt;$(b) the residuals $r_k$ are pairwise orthogonal: $r^T_k r_j = 0$ for $j &lt; k$,(c) the directions $d_k$ are pairwise $A$-conjugate: $d^T_k A d_j = 0$ for $j &lt; k$. Preconditioning Nonlinear Systems of EquationsThis section describes Newton’s Method and variants for the solution of systems of nonlinear equations. Multivariate Newton’s MethodOne-variable Newton’s Method is$$f(x) = 0\\\\x_{k+1} = x_k - \\frac{f(x)}{f’(x_k)}$$ Let $$f_1(u,v,w) = 0\\\\f_2(u,v,w) = 0\\\\f_3(u,v,w) = 0\\\\\\tag{2.49}$$be three nonlinear equations in three unknowns $u,v,w$. Define the vector-valued function $F(u,v,w) = (f_1, f_2,f_3)$. The analogue of the derivative $f’$ in the one-variable case is the Jacobian matrix defined by $$DF(x) =\\begin{bmatrix} \\frac{\\partial f_1}{\\partial u} &amp; \\frac{\\partial f_1}{\\partial v} &amp; \\frac{\\partial f_1}{\\partial w}\\\\ \\frac{\\partial f_2}{\\partial u} &amp; \\frac{\\partial f_2}{\\partial v} &amp; \\frac{\\partial f_2}{\\partial w}\\\\ \\frac{\\partial f_3}{\\partial u} &amp; \\frac{\\partial f_3}{\\partial v} &amp; \\frac{\\partial f_3}{\\partial w}\\end{bmatrix}$$ The Taylor expansion for vector-valued functions around $x_0$ is $$F(x) = F(x_0) + DF(x_0) \\cdot (x - x_0) + O(x - x_0)^2$$ Newton’s Method is based on a linear approximation, ignoring the $O(x^2)$ terms. Since computing inverses is computationally burdensome, we use a trick to avoid it. Set $x_{k+1} = x_k − s$, where s is the solution of $DF(x_k)s = F(x_k)$. Now, only Gaussian elimination ($n^3/3$ multiplications) is needed to carry out a step, instead of computing an inverse (about three times as many). $$\\begin{cases}DF(x_k)s = -F(x_k)\\\\x_{k+1} = x_k + s\\end{cases}\\tag{2.51}$$ For example: Use Newton’s Method with starting guess $(1,2)$ to find a solution of the system$$\\begin{cases}v - u^3 = 0\\\\u^2 + v^2 - 1= 0\\end{cases}$$ Solution: The Jacobian matrix is$$DF(u,v) =\\begin{bmatrix}-3u^2 &amp; 1\\\\2u &amp; 2v\\end{bmatrix}$$Using starting point $x_0 = (1,2)$, on the first step we must solve the matrix equation (2.51): $$\\begin{bmatrix} -3 &amp; 1\\\\ 2 &amp; 4\\end{bmatrix}\\begin{bmatrix} s_1\\\\s_2\\end{bmatrix}=-\\begin{bmatrix} 1\\\\4\\end{bmatrix}$$The solution is s = $(0,−1)$, so the first iteration produces $x_1 = x_0 + s = (1,1)$. The secondstep requires solving $$\\begin{bmatrix} -3 &amp; 1\\\\ 2 &amp; 2\\end{bmatrix}\\begin{bmatrix} s_1\\\\s_2\\end{bmatrix}=-\\begin{bmatrix} 0\\\\1\\end{bmatrix}$$ … Broyden’s MethodNewton’s Method is a good choice if the Jacobian can be calculated. If not, the best alternative is Broyden’s Method. Suppose $A_i$ is the best approximation available at step $i$ to the Jacobian matrix, and that it has been used to create$$x_{i+1} = x_i - A_i^{-1}F(x_i)\\tag{2.52}$$To update $A_i$ to $A_{i+1}$ for the next step, we would like to respect the derivative aspect of the Jacobian $DF$（从导数的定义出发）, and satisfy$$A_{i+1}\\delta_{i+1} = \\Delta_{i+1}\\tag{2.53}$$where, $\\delta_{i+1} = x_{i+1} - x_i$ and $\\Delta_{i+1} = F(x_{i+1}) - F(x_{i})$. = = Don’t understand.. = = On the other hand, for the orthogonal complement(正交补) &gt;&gt; of $\\delta_{i+1}$, we have no new information. Therefore, we ask that$$A_{i+1}w = A_i w\\tag{2.54}$$for every $w$ satisfying (why?)$$\\delta_{i+1}^T w = 0$$One checks that a matrix that satisfies both (2.53) and (2.54) is$$A_{i+1} = A_i + \\frac{(\\Delta_{i+1} - A_i \\delta_i)\\delta_{i+1}^T}{\\delta_{i+1}^T\\delta_{i+1}}\\tag{2.55}$$Broyden’s Method uses the Newton’s Method step (2.52) to advance the current guess, while updating the approximate Jacobian by (2.55).","link":"/Math/Numerical-Analysis/NumericalAnalysis-C2-Systems-of-Equations/"},{"title":"NumericalAnalysis-C7-Boundary-Value-Problems","text":"Keywords: Finite elements and the Galerkin Method, Finite Difference Methods, Mathlab Shooting MethodSolutions of boundary value problemsShooting Method implementationReality Check 7: Buckling of a Circular RingFinite Difference MethodsLinear boundary value problemsNonlinear boundary value problemsCollocation and the Finite Element MethodCollocationFinite elements and the Galerkin Method","link":"/Math/Numerical-Analysis/NumericalAnalysis-C7-Boundary-Value-Problems/"},{"title":"NumericalAnalysis-C5-Numerical-Differentiation-and-Integration","text":"Keywords: Numerical Differentiation, Simpson’s Rule, Gaussian Quadrature, Mathlab Numerical DifferentiationFinite difference formulasRounding errorExtrapolationSymbolic differentiation and integrationNewton–Cotes Formulas for Numerical IntegrationTrapezoid RuleSimpson’s RuleComposite Newton–Cotes formulasOpen Newton–Cotes MethodsRomberg IntegrationAdaptive QuadratureGaussian QuadratureReality Check 5: Motion Control in Computer-Aided Modeling","link":"/Math/Numerical-Analysis/NumericalAnalysis-C5-Numerical-Differentiation-and-Integration/"},{"title":"PatterRecognition-C3-LinearModels-for-Regression","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. Linear Basis Function ModelsThe Bias-Variance DecompositionBayesian Linear RegressionBayesian Model ComparisonThe Evidence Approximation","link":"/MachineLearning/PatternRecognition/PatterRecognition-C3-LinearModels-for-Regression/"},{"title":"NumericalAnalysis-C6-Ordinary-Differential-Equations","text":"Keywords: Euler’s Method, First-order linear equations, Systems of Ordinary Differential Equations, Implicit Methods and Stiff Equations, Mathlab Initial Value ProblemsEuler’s MethodExistence, uniqueness, and continuity for solutionsFirst-order linear equationsAnalysis of IVP SolversLocal and global truncation errorThe explicit Trapezoid MethodTaylor MethodsSystems of Ordinary Differential EquationsHigher order equationsComputer simulation: the pendulumComputer simulation: orbital mechanicsRunge–Kutta Methods and ApplicationsThe Runge–Kutta familyComputer simulation: the Hodgkin–Huxley neuronComputer simulation: the Lorenz equationsReality Check 6: The Tacoma Narrows BridgeVariable Step-Size MethodsEmbedded Runge–Kutta pairsOrder 4/5 methodsImplicit Methods and Stiff EquationsMultistep MethodsGenerating multistep methodsExplicit multistep methodsImplicit multistep methods","link":"/Math/Numerical-Analysis/NumericalAnalysis-C6-Ordinary-Differential-Equations/"},{"title":"NumericalAnalysis-C8-Partial-Differential-Equations","text":"Keywords: Newton solver, Backward Difference Method, Finite Element Method, Nonlinear partial differential equations, Mathlab Parabolic EquationsForward Difference MethodStability analysis of Forward Difference MethodBackward Difference MethodCrank–Nicolson MethodHyperbolic EquationsThe wave equationThe CFL conditionElliptic EquationsFinite Difference Method for elliptic equationsReality Check 8: Heat distribution on a cooling finFinite Element Method for elliptic equationsNonlinear partial differential equationsImplicit Newton solverNonlinear equations in two space dimensions","link":"/Math/Numerical-Analysis/NumericalAnalysis-C8-Partial-Differential-Equations/"},{"title":"NumericalAnalysis-C9-Random-Numbers-and-Applications","text":"Keywords: Monte Carlo Simulation, Stochastic Differential Equations, Numerical methods for SDEs, Mathlab Random NumbersPseudo-random numbersExponential and normal random numbersMonte Carlo SimulationPower laws for Monte Carlo estimationQuasi-random numbersDiscrete and Continuous Brownian MotionRandom walksContinuous Brownian motionStochastic Differential EquationsAdding noise to differential equationsNumerical methods for SDEsReality Check 9: The Black–Scholes Formula","link":"/Math/Numerical-Analysis/NumericalAnalysis-C9-Random-Numbers-and-Applications/"},{"title":"Paper-Sketch-based-Facial-Generation","text":"Keywords: 3D Facial Generation, Sketch Editing, Neural Network SketchFaceNeRF: Sketch-based Facial Generation and Editing in Neural Radiance Fields LIN GAO∗, Institute of Computing Technology, CAS and University of Chinese Academy of Sciences , ChinaFENG-LIN LIU, Institute of Computing Technology, CAS and University of Chinese Academy of Sciences, ChinaSHU-YU CHEN, Institute of Computing Technology, Chinese Academy of Sciences, ChinaKAIWEN JIANG, Institute of Computing Technology, CAS and Beijing Jiaotong University, ChinaCHUNPENG LI, Institute of Computing Technology, Chinese Academy of Sciences, ChinaYU-KUN LAI, School of Computer Science and Informatics, Cardiff University, UKHONGBO FU, School of Creative Media, City University of Hong Kong, China http://www.geometrylearning.com/SketchFaceNeRF/http://mlops.ccloud.conestore.cn:30010/sketchface/ WorkFlowInspirationChallengesThe Main ProcessInnovationLimitationCode Analysishttps://github.com/IGLICT/SketchFaceNeRF/","link":"/Paper/Sketch-Generation/Paper-Sketch-based-Facial-Generation/"},{"title":"Paper-StripMaker-Sketch-Consolidation","text":"Keywords: ClusterStrokes, Random Forest, Vector Sketch Consolidation StripMaker: Perception-driven Learned Vector Sketch Consolidation CHENXI LIU, University of British Columbia, CanadaTOSHIKI AOKI, University of Tokyo, JapanMIKHAIL BESSMELTSEV, Université de Montréal, CanadaALLA SHEFFER, University of British Columbia, Canada https://www.cs.ubc.ca/labs/imager/tr/2023/stripmaker/ WorkFlowInspirationThe Main Process Innovation avoid the need for an unsustainably large manually annotated learning corpus by leveraging observations about artist workflow and perceptual cues viewers employ when mentally consolidating sketches. Our method is the first to use a principled classification-based approach to vector sketch consolidation. LimitationOur performance is constrained by data scarcity which prevents greater reliance on context, due to overfitting concerns. MoreRandom ForestMore about Random Forest in PatternRecognition&gt;&gt; StrongRelatedPaperDave Pagurek van Mossel, Chenxi Liu, Nicholas Vining, Mikhail Bessmeltsev, and Alla Sheffer. 2021. StrokeStrip: Joint Parameterization and Fitting of Stroke Clusters. ACM Trans. Graph. 40, 4 (July 2021), 50:1–50:18 Peng Xu, Timothy M Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang, and Liang Wang. 2022. Deep learning for free-hand sketch: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) Jerry Yin, Chenxi Liu, Rebecca Lin, Nicholas Vining, Helge Rhodin, and Alla Sheffer. 2022. Detecting Viewer-Perceived Intended Vector Sketch Connectivity. ACM Transactions on Graphics 41 (2022). Issue 4. CodeSince the author doesn’t upload codes, so I just guess the rough code framwork as follows: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485//.h//used to get the likelyhood between different strips, so as to merge or splitclass RandomForestClassfier{//StrokeStrip parameterization[van Mossel et al. 2021].public: RandomForestClassfier(); virtual ~RandomForestClassfier(); //stroke and stroke virtual float getLikelihoodSS(Stroke&amp; a, Stroke&amp; b); //stroke and strip virtual float getLikelihoodSP(Stroke&amp; a, vector&lt;Stroke&gt;&amp; b); //strip and strip virtual float getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b);};class LocalClassifier : public RandomForestClassfier{public: LocalClassifier(); ~LocalClassifier(); //stroke and stroke virtual float getLikelihoodSS(Stroke&amp; a, Stroke&amp; b); //stroke and strip virtual float getLikelihoodSP(Stroke&amp; a, vector&lt;Stroke&gt;&amp; b); //strip and strip virtual float getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b);};class GlobalClassifier : public RandomForestClassfier{public: //stroke and stroke virtual float getLikelihoodSS(Stroke&amp; a, Stroke&amp; b); //stroke and strip virtual float getLikelihoodSP(Stroke&amp; a, vector&lt;Stroke&gt;&amp; b); //strip and strip virtual float getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b);};class Stroke{private: vector&lt;Point&gt; controlPoints; //each stroke is denoted by controlpoints, sorted by time //Time t; //the time this stroke is drawed by user vector&lt;Point&gt; samplePoints; int stripIndx; //which strip this stroke belongs topublic: bool getSamplePoints(float sampleRate); bool setStripIndx(int idx); int getStripIndx();};class VectorSketch{private: vector&lt;Stroke&gt; Strokes; //the vector Sketch consists of raw strokes vector&lt;vector&lt;Stroke&gt;&gt; Strips; //group the strokes to different strips int strokeWidth;public: bool Sample(); bool LocalConsolidation(); bool GlobalConsolidation(); bool IsCompatible(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b); bool Merge(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b); bool IsFormjectionByYin2022(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b); void FittingCurvesByVanMossel2021();};//main.cppint main(){ //Input, get the information of strokes VectorSketch vectorSketch; //Preprocess vectorSketch.Sample(); //Step1. temporal consolidation vectorSketch.LocalConsolidation(); //Step2. refinement vectorSketch.GlobalConsolidation(); //Compare Results Experiments..} Sample, LocalConsolidation and GlobalConsolidation funtions can be implemented as follows: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200//Stroke.cppbool Stroke::getSamplePoints(float sampleRate){ for (float t = 0; t &lt; 155; t += sampleRate) { Point p = Point(3 * t * t + 4 * (1 - t), 4 * t); //suppose the stroke function is like this samplePoints.emplace_back(p); }}bool Stroke::setStripIndx(int idx){ stripIndx = idx;}int Stroke::getStripIndx(){ int stripIndx;}//VectorSketch.cppbool VectorSketch::Sample(){ float sampleRate = 1.2 * strokeWidth; for (auto stroke : Strokes) { stroke.getSamplePoints(sampleRate); } //remove hook artifacts //....}bool IsCompatible(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ return false;}//merge strip b to strip abool Merge(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ int stripIdxOfa = a[0].getStripIndx(); for (auto bb : b) { //update the information of strokes of strip b, and update the elements of strip a bb.setStripIndx(stripIdxOfa); a.emplace_back(bb); } return true;}bool IsFormjectionByYin2022(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ //2022 Yin Algorithms //..}void FittingCurvesByVanMossel2021(){ //2021 Van Mossel Algorithms //...}bool VectorSketch::LocalConsolidation(){ auto Classfier = std::make_unique&lt;LocalClassifier&gt;(); int stripIdx = -1; for (int i = 0; i &lt; Strokes.size(); i++) { Stroke curStroke = Strokes[i]; Stroke previousStroke = Strokes[i - 1]; vector&lt;Stroke&gt; previousStrip = Strips[previousStroke.getStripIndx()]; float probability = Classfier-&gt;getLikelihoodSP(curStroke, previousStrip); //curStroke belongs to the sub-strip the previousStroke belongs to if (probability &gt; 0.5) { //update curstroke information Strokes[i].setStripIndx(stripIdx); //update the previous strip previousStrip.emplace_back(curStroke); //proceed to the next stroke continue; } else { //save curStroke as a new sub-strip Strokes[i].setStripIndx(++stripIdx); Strips[stripIdx].emplace_back(curStroke); //previousStrip has more than one stroke, decide if it can be merged with other strips. if (previousStrip.size() &gt; 1) { int p_max = 0; int idx_max = -1; for (int j = 0; j &lt; Strips.size(); j++) { //only call the classifier if two sub-strips are deemed compatible if (IsCompatible(Strips[j], previousStrip)) { float p = Classfier-&gt;getLikelihoodPP(Strips[j], previousStrip); if (p &gt; p_max) { p_max = p; idx_max = j; } } } //merge the most similar two strips to one strip Merge(Strips[idx_max], previousStrip); Strips[stripIdx].clear(); --stripIdx; previousStrip.clear(); } } } //after the iteration above, the whole strokes are grouped into different strips, as the differnt-color curves in the figure. //fitting curves to these strips FittingCurvesByVanMossel2021();}bool VectorSketch::GlobalConsolidation(){ auto Classfier = std::make_unique&lt;GlobalClassifier&gt;(); //strip reevaluation int n = Strips.size(); int StripIdx = n; for (int i = 0; i &lt; n; i++) { vector&lt;Stroke&gt; strip = Strips[i]; //find the least likelihood two seed strokes in this strip int min_likelihood = INT_MAX; vector&lt;Stroke&gt; seedStrokes(5); //Stroke seed_s1, seed_s2; for (int k = 0; k &lt; strip.size(); k++) { for (int l = k + 1; l &lt; strip.size(); l++) { Stroke s1 = strip[k]; Stroke s2 = strip[l]; float p = Classfier-&gt;getLikelihoodSS(s1, s2); if (p &lt; min_likelihood) { min_likelihood = p; seedStrokes[0] = s1; seedStrokes[1] = s2; } } } //thie likelihood is high enougth, don't change this strip if (min_likelihood &gt; 0.6) { continue; } else { //Reassign other strokes to sub-strip formed by seed_s1 or sub-strip formed by seed_s2 int newStripIdx = StripIdx; //seedStips.. vector&lt;vector&lt;Stroke&gt;&gt; seedStrips(5); seedStrokes[1].setStripIndx(newStripIdx); seedStrips[0].emplace_back(seedStrokes[0]); //seed_s1_strip seedStrips[1].emplace_back(seedStrokes[1]); //seed_s2_strip for (auto stroke : strip) { float p_max = 0; for (auto sub_strip : seedStrips) { float pi = Classfier-&gt;getLikelihoodSP(stroke, sub_strip); p_max = max(p_max, pi); } //this stroke doesn't belong to any seedStrip, so make this stroke as a new seed stroke if (p_max &lt; 0.5) { seedStrokes.emplace_back(stroke); stroke.setStripIndx(++newStripIdx); } else { //assign this stroke to seed_s1_strip or seed_s2_strip or ..., according to the likelihood //... } } //merge this seedStrips according the likelihood //update Strips, cause may produce new strips in step above } } //Connectivity Preservation classifier in Yin et al.[2022] for(auto strip : Strips) for (auto stripb : Strips) { if (IsFormjectionByYin2022(strip, strip)) { Merge(strip, strip); } } //fitting curves to these strips FittingCurvesByVanMossel2021();} The classifier can be implemented as follow: 12345678910111213141516//RandomForestClassfier.cppfloat RandomForestClassfier::getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ //call the python module, the trained model.. /* the classifier extract the features as follows: //Angles and Distance //Density //Narrows and Side-by-Side Extent //Evenness //1d Parameterization Distortion //Relative Precision */ return -1;}","link":"/Paper/Sketch-Consolidation/Paper-StripMaker-Sketch-Consolidation/"},{"title":"Paper-StrokeStrip-Fitting-Stroke-Clusters","text":"Keywords: Curve Fitting to Strokes, Variational Optimization, Cluster Strokes, C++ https://www.cs.ubc.ca/labs/imager/tr/2021/StrokeStrip/ WorkFlowInspirationWe achieve this goal by leveraging insights about human perception of stroke clusters. Challengeswe do not know which points on different strokes that are adjacent in Euclidean space are perceived as WTS adjacent. our algorithm needs to compute these crosssections given only the raw strokes as input. We consequently face two interconnected challenges: we must compute optimal isovalues along each cluster cross-section while simultaneously computing the cross-sections themselves. Address the ChallengesWe robustly address both challenges at once by casting the computation of the parameterization and the underlying isoline crosssections as a constrained variational problem. We then solve it using a combined discrete-continuous optimization framework. Step1. determine the optimal orientations of parameterization gradients along each stroke（决定沿着每个笔画的最优梯度(参数化)朝向）, converting monotonicity constraints into more tractable inequality ones（把单调约束转化为更容易解决的不等式问题） Step2. jointly solve for cluster cross-sections and parameter values along them using a variational optimization framework（使用变分优化框架联合解决等值线和沿着等值线的参数值） The Main Process Some DefinitionsA strip is well described by its centerline $\\gamma(t) \\in R^2$, parameterized by its arclength $t \\in [0, L]$, and width $W(t)$ The two sides of a strip are then the two curves at the distance of $W(t)/2$ from the centerline in the normal direction, or more formally $\\gamma(t) \\pm \\frac{W(t)}{2}n(t)$. Simply put, it is a function $u(x)$ defined for all points in the strip, such that straight line segments $C(t) = \\lbrace \\gamma(t) + an(t)| a \\in [-\\frac{W(t)}{2}, \\frac{W(t)}{2}]\\rbrace$ are the parameterization’s isolines（等值线，和中心线正交的线）. $t = u(x), x ∈ C(t)$. centerline tangent $\\tau(t)$, about the centerline, we prioritize tangent alignment over centrality when computing the strip parameterizations and paths. thus, given the oriendted strokes $S = \\lbrace s_i, i = 1, \\cdots, N\\rbrace$, we are looking for a map(映射)$$u(x): U_iS_i \\mapsto [0;L]$$ For a non self-intersecting strip $S \\subset R^2$, its arc length parameterization $u: S \\rightarrow R$ is the minimizer of the following variational problem:$$\\underset{u}{min} \\int_0^L ||\\frac{1}{W(t)} \\int_{C(t)} \\nabla u(x)dx - \\tau(t)||^2dt$$ GRADIENT ORIENTATION InnovationLimitationMorearc-length parameterizationVariational PrincipleCode Analysishttps://github.com/davepagurek/StrokeStrip","link":"/Paper/Sketch-Consolidation/Paper-StrokeStrip-Fitting-Stroke-Clusters/"},{"title":"Probability-Bayes-Formula","text":"","link":"/Math/Probability/Probability-Bayes-Formula/"},{"title":"PatterRecognition-RandomForest","text":"Keywords: Random Forest, PatternRecognition For Beginnershttps://www.section.io/engineering-education/introduction-to-random-forest-in-machine-learning/https://www.datacamp.com/tutorial/random-forests-classifier-python An Exampleto be added… DetailsTypically, for a classification problem with $p$ features, $\\sqrt{p}$ (rounded down) features are used in each split. For regression problems the inventors recommend $p/3$ (rounded down) with a minimum node size of $5$ as the default.","link":"/MachineLearning/PatternRecognition/PatterRecognition-RandomForest/"},{"title":"Rendering-First-Met-With-RayTracing","text":"Keywords: RayTracing, Radiometry, Monte Carlo Methods, BRDF Hey guys,speaking of RayTracing,I have to tell u that I’m really confused at the first time–What the hell is RayTracing?! Well u know I’m not the kind of person who is addicted to games, all I know about Graphics begins with the course UnityGame in my first year of Graduate,before that I am the one who doesn’t even know what pixel is! So not to mention those much more advanced technologies such as RayTracing! I assume readers are the same with me! You just learned the really really basic graphics—！ coordinates transformation,！ uv mapping,！ something about graphicspipeline,！ unity games,(ok I admit that I’m a little irritable at present, cuz a bug cannot be fixed for a few days)suddenly a voice told you time to learn RayTracing, and it seems a mess with the existing knowledges in your head. Don’t worry, learn with me. This article is going to be very very long, it’s about RayTracing,Acceleration,BRDF,PathTracing,Calculus,Global illumination,Unity,VScode etc, there will be lots pics and formulas. If you are patient enough to read through it, I promise you can learn something, but before this article, please scan the two articles:Lighting-Shading-Texture,Graphicspipeline,cuz we have lots things to link up. RAY-TRACINGWhy RayTracing?AccelerationRADIOMETRYRadiometry is closely related to calculus. If you know nothing about Calculus, please go to learn it and later come back, at least have the conception in your head. Though I’ll begin with mathematics first. Spherical CoordinatesWe all know Cartesian coordinates, cuz we define the points and vectors in Cartesian coordinate system since high school. Actually,we can also use Spherical coordinates to define them. In spherical coordinate system,the position of a point or direction and length of a vector are defined by two angles(denoted $\\theta$ and $\\phi$) and a radial distance($r$). The angle $\\theta$ is called polar angle and is measured from the fixed zenith direction.The zenith direction in relation to which this polar angle will be measured is the y-axis. To go from spherical coordinates to cartesian coordinates,we can use the following equation: $$x = r sin(\\theta)cos(\\phi)$$$$y = r cos(\\theta)$$$$z = r sin(\\theta)sin(\\phi)\\tag{spherical2cartesian}$$ Also to go from cartesian coordinates to spherical coordinates,we can use the following equation: $$r = \\sqrt{x^2+y^2+z^2}$$$$\\theta = cos^{-1}(\\frac{y}{r})$$$$\\phi = tan^{-1}(\\frac{z}{x})\\tag{catesian2spherical}$$ Differential and Integral CalculusCalculus includes differential and integral. Since this is really basic math,I’ll just make a rough review. The first fundamental theorem of calculus: $$F = \\int f(x)dx\\tag{1}$$ The second fundamental theorem of calculus: $$\\int_{a}^{b}f(x)dx = F(b)-F(a)\\tag{2}$$ The above is about functions defined in one dimension($f(x)$ takes one variable only, $x$). How about the functions in two dimension,even three dimension? So the integral of the function in the above pic is :$$\\int_{ax}^{bx}\\int_{ay}^{by}f(x,y)dxdy\\tag{2d integral}$$ So the integral of the function in the above pic is :$$\\rho = \\int_{ax}^{bx}\\int_{ay}^{by}\\int_{az}^{bz}f(x,y,z)dxdydz\\tag{3d integral}$$$\\rho$ means the density of the volume. RadiometryPreviously on the RayTracing part, the Whitted style ray tracing cannot give us correct results. That’s because the shading part only used empirical model. Blinn-phong Model cannot give us correct resutls, it’s only an empirical model, not based on physics, there’s lots materials blinn-phong cannot present. We want to simulate much more materials in our real life on computer, which means we need the PBS(physically based shading),and Radiometry is the basic concept of PBS. To begin with radiometry, we have new terms to remember: Radiant flux intensity irradiance radiance Radiant Energy and Flux(Power)We all know what Energy and Power mean in physics. They are almost the same in CG. Radiant energy is the energy of electromagnetic radiation. It is measured in units of joules, and denoted by the symbol: $$Q[J = Joule]$$ Radiant flux(power) is the energy emitted,reflected,transmitted or received, per unit time. $$\\Phi = \\frac{dQ}{dt} [W = Watt][lm = lumen]\\tag{flux}$$ Before go to intensity,irradiance,radiance,here’s a pic. Radiant IntensityRadiant intensity is the power per unit solid angle emitted by a point light source. $$I(\\omega) = \\frac{d\\Phi}{d\\omega} [\\frac{W}{sr}][\\frac{lm}{sr}=cd=candela]\\tag{intensity}$$ Angle:ration of subtended arc length on circle to radius $\\theta = \\frac{l}{r}$ Circle has $2\\pi$ radians Solid angle:ration of subtended area on sphere to radius squared $\\Omega = \\frac{A}{r^2}$ Sphere has $4\\pi$ steradians $$dA = (rd\\theta)(rsin\\theta d\\phi) = r^2 sin\\theta d\\theta d\\phi$$$$d\\omega = \\frac{dA}{r^2} = sin\\theta d\\theta d\\phi \\tag{solid angles}$$ IrradianceIrradiance is the power per(perpendicular/projected)unit area incident on a surface point. $$E(x) = \\frac{d\\Phi(x)}{dA} [\\frac{W}{m^2}][\\frac{lm}{m^2} = lux]\\tag{irradiance}$$ Do you still rememeber that in the Lighting-Shading-Texture article, we mentioned the empirical model Blinn phong model, and there’s diffuse component, when we calculate the diffuse component, we just assume the point’s worldposition,the point’s worldnormal etc. Actually,we assume this point is not really a point but a very small surface which we call differential area, denoted as $dA$.Now,what we actually consider as well is the amount of light falling on the surface of this very small area around P.Light that falls at P is not reduced to a single light ray since we are not interested in singular point but the small region $dA$ around that point. Light that falls on this region is contained within a small volume perpendicular to P. In the empirical model, the diffuse component is calculated like this:$$c_{diff} = c_{light} \\cdot m_{diff} \\cdot max(0,n \\cdot l)\\tag{blinn-phong-diffuse}$$as before, n is the surface normal and l is a unit vector that points towards the light source. The factor $m_{diff}$ is the material’s diffuse color, which is the value that most people think of when they think of the “color” of an object. The diffuse material color often comes from a texture map. The diffuse color of the light source is $c_{light}$. 1fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); The diffuse component obeys Lambert’s Cosine Law,here the Irradiance also obeys Lambert’s Cosine Law: RadianceRadiance(luminance) is the power emitted,reflected, transmitted or received by a surface, per unit solid angle, per projected unit area.Radiance is the fundamental field quantity that describes the distribution of light in an environment Radiance is the quantity associated with a ray Rendering is all about computing radiance $$L(p,\\omega) = \\frac{d^2\\Phi(p,\\omega)}{d\\omega dAcos\\theta} [\\frac{W}{sr m^2}][\\frac{cd}{m^2} = \\frac{lm}{sr m^2} = nit]\\tag{radiance}$$ Incident Radiance Incident radiance is the irradiance per unit solid angle arriving at the surface Exiting Radiance Exiting surface radiance is the intensity per unit projected area leaving the surface. Those concepts are really hard to remember, but don’t feel frustrated. Keep going. Read more. If you cannot recall the concepts, turn back, read again. BRDFFinally BRDF. Actualy the Radiometry step is for BRDF.As mentioned above what Phong essentially used to simulate the appearance of shiny materials is a function. This function (which includes a specular and a diffuse compute). This function contains a certain number of parameters such as n that can be tweaked to change the appearance of the material, but more importantly, it actually depends on two variables, the incident light direction (which is used to compute both the diffuse and specular component) and the view direction (which is used to compute the specular component only). We could essential write this function as: $$f_R(\\omega_o,\\omega_i)$$Where $\\omega_o$ and $\\omega_i$ are the angle between the surface normal (N) and the view direction (V) and the surface normal and the light direction (I) respectively.The subscript o stands for outgoing. In computer graphics, this function is given the fancy name of Bidirectional Reflectance Distribution Function or in short BRDF. A BRDF is nothing else than a function that returns the amount of light reflected in the view direction for a given incident light direction: $$BRDF(\\omega_o,\\omega_i)$$ In the following pic,BRDF represents how much light is reflected into each outgoing direction $\\omega_r$ from each incoming direction: $$f_r(\\omega_i \\rightarrow \\omega_r) = \\frac{dL_r(\\omega_r)}{dE_i(\\omega_i)} = \\frac{dL_r(\\omega_r)}{L_i(\\omega_i)cos\\theta_i d\\omega_i}\\tag{BRDF}$$ One thing that I want to mention, why camera can see the objects? It’s because when one light beam hits the surface, it will reflect in all directions among the half hemisphere, but only the reflected light enter the viewer’s eye make sense. So we need to calculate the output reflected light. So here come’s The Reflection Equation. $$L_r(p,\\omega_r) = \\int_{H^2}f_r(p,\\omega_i \\rightarrow \\omega_r)L_i(p,\\omega_i)cos\\theta_i d\\omega_i\\tag{The-Reflection-Equation}$$ Please make sure you understand the meaning of each symbol in the above formulas. Here come’s The Rendering Equation. $$L_o(p,\\omega_o) = L_e(p,\\omega_o) + \\int_{\\Omega^+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot\\omega_i)d\\omega_i\\tag{The-Rendering-Equation}$$ $L_e(p,\\omega_o)$ means the object’point p emit the emissive light to the $\\omega_o$ direction(remeber the $\\omega_o$ direction should be the view direction), and $\\int_{\\Omega^+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot\\omega_i)d\\omega_i$ means the reflected light from $L_i(p,\\omega_i)$. Summary global illuminationPATH TRACINGAppendixMonte Carlo MethodsBefore we start, one question, do u have the background in Probability and Statistics Theory? If the answer is no, please go to learn something and later come back. What is Monte Carlo?Assume that we want to know the average height of all adults in one city, the most common method is to sample some adults and calculate the average height of them to approximate the true result. Here’s the formula: $$Approximation(Average(X)) = \\frac{1}{N}\\sum_{n=1}^{N}x_{n}$$ We generally denote random variables with upper case letters,the height of a population would be called a random variable,so the letter X is used.The formula can be read as,the approximation of the average value of the random variable X(the height of the adult population of the given country),is equal to sum of the height of N adults randomly chosen from that population(the samples),divided by the number N(the sample size). This in essence, is called a Monte Carlo approximation. In statistics,the average of the random variable X is called an expectation and is written E(X). So To summarize, Monte Carlo approximation (which is one of the MC methods) is a technique to approximate the expectation of random variables, using samples. Monte Carlo raytracing References: [1]GAMES [2]scratchapixel [3]3D Math Primer for Graphics and Game Development 2nd Edition. [4]Fundamentals of Computer Graphics and 3rd Edition. [5]Unity+Shader入门精要 [6]Unity3d Mannual [7]VSCode Document [8]基于物理着色：BRDF [9]如何正确理解 BRDF","link":"/Graphics/Rendering/Rendering-First-Met-With-RayTracing/"},{"title":"address-opereator","text":"Keywords: address This is for the readers who have basic c++ background. Those days I have noticed that there’s char ‘&amp;’ in front of function. So decided to record it. I suggest an online c++ compiler : (You can test your simple programs on it.) 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;memory&gt;using namespace std;int&amp; fun(int &amp;a){ cout &lt;&lt; \"aa \" &lt;&lt; &amp;a &lt;&lt;endl; return a;}int main(){ int b; cout &lt;&lt; \"b: \" &lt;&lt; &amp;b &lt;&lt;endl; b = 4; cout &lt;&lt; \"fun: \"&lt;&lt; &amp;fun(b) &lt;&lt; endl; int a; cout &lt;&lt; \"a: \" &lt;&lt; &amp;a &lt;&lt; endl; a = fun(b); cout &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; &amp;a &lt;&lt; endl;} The result is : 1234567b: 0x7ffe5ec356b8fun: aa 0x7ffe5ec356b80x7ffe5ec356b8a: 0x7ffe5ec356bcaa 0x7ffe5ec356b840x7ffe5ec356bc To be honest, &amp; and * are too hard in c++.","link":"/Language/Advanced-CPP/STL-Address-Opereator/"},{"title":"Rendering-Graphics-Pipeline-Overview","text":"Keywords: Rendering Pipeline, Deduce Projection Matrix, Rotation Matrix Just look at the middle part of Fig1,the working flow is followed. Step1.Setting up the sceneBefore we begin rendering,we must set several options that apply to the entire scene. For example,we need to set up the camera, to be more specifically,that means,pick a point of view in the scene from which to render it, and choose where on the screen to render it. We also need to select lighting and fog options, and prepare the depth buffer. If you have used Unity,then it is easy to understand, you put the camera in the proper place and set the lighting properties,also change the aspect ratio. Step2.Visibility determinationOnce we have a camera in place,we must then decide which objects in the scene are visible. In unity this means that you can tick the box on the Inspector panel to determine the object visible or not. Step3.Setting object-level rendering statesEach object may have its own rendering options. We must install these options into the rendering context before rendering any primitives associated with the object. The most basic property associated with an object is material that describes the surface propertis of the object. In unity,the material defines how the surface should be rendered,by including referencse to the texutres it uses,tiling information,color tints and so on. The avaliable options for a material depend on which shader the material is using. Step4.Geometry generation/deliveryThe geometry is actually submitted to the rendering API.Typically,the data is delivered in the form of triangles;either as individual triangles,or an indexed triangle mesh,triangle strip,or some other form. If you have heard about 3D Max or Maya,then you can get it.The artists create the model in the form of .obj file, we programmers load the model to the RAM, then we got the triangles data. You can achieve the obj_loader.h. Then you can get the triangles data like this: 123456789101112131415161718std::vector&lt;Triangle*&gt; TriangleList;objl::Loader Loader;bool loadout = Loader.LoadFile(obj_path);for(auto mesh:Loader.LoadedMeshes){ for(int i=0;i&lt;mesh.Vertices.size();i+=3) { Triangle* t = new Triangle(); for(int j=0;j&lt;3;j++) { t-&gt;setVertex(j,Vector4f(mesh.Vertices[i+j].Position.X,mesh.Vertices[i+j].Position.Y,mesh.Vertices[i+j].Position.Z,1.0)); t-&gt;setNormal(j,Vector3f(mesh.Vertices[i+j].Normal.X,mesh.Vertices[i+j].Normal.Y,mesh.Vertices[i+j].Normal.Z)); t-&gt;setTexCoord(j,Vector2f(mesh.Vertices[i+j].TextureCoordinate.X, mesh.Vertices[i+j].TextureCoordinate.Y)); } TriangleList.push_back(t); }}draw(TriangleList); In unity, this is done by the powerful engine. Step5.Vertex-level operationsOnce we have the geometry in some triangulated format,a number of various operations are performed at the vertex level. The most important operation is the transformation of vertex positions from modeling space into camera space/clip space. In unity, this operation is performed by a user-supplied microprogram called vertex shader. Like this: 1234567891011121314151617181920212223struct a2v{ float4 vertex : POSITION; float3 normal : NORMAL; float4 tangent : TANGENT; float4 texcoord : TEXCOORD0;};struct v2f{ float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float3 lightDir: TEXCOORD1; float3 viewDir : TEXCOORD2;};v2f vert(a2v v){ v2f o; //transform the vertex positions from modeling space into clip space o.pos = UnityObjectToClipPos(v.vertex); //... return o;} Though Unity has encapsulated the transformation function for us,there exists lots things to write.  We all know that the models that artists give us is in the model space,then how to transform them to the world space/camera space(view space)/clip space/screen space? How to deduce the matrixs(mvp)? What is the coordinates difference among OpenGL,DirectX and Unity? I will describe those in the Appendix :) Actually,the details have confused me for a long time,if you have the same feeling,don’t worry.Just go ahead. After we transformed the triangles to the camera space, any portion of a triangle outside the view frustum is removed, by the process known as clipping. Here the mvp matrix have ended. Once we have a clipped polygon in 3D clip space, we then project the vertices of that polygon,mapping them to 2D screen-space coordinates of the output window, here the viewport matrix is used. Step6.RasterizationOnce we have a clipped polygon in screen space,it is rasterized. Rasterization refers to the process of selecting which pixels on the screen should be drawn for a particular triangle; interpolating texture coordinates, colors, and lighting values that were computed at the vetex level across the face for each pixel; and passing these down to the next stage for pixel(fragment) shading. The pseudo-code is as follows: 123456789101112131415161718192021222324252627...got the TriangleListfor (const auto&amp; t:TriangleList){ //...mvp //...viewport rasterize(t);}rasterize(){ //...get the triangle bounding box for(x = x_min; x &lt; x_max+1; x++&gt; { for(y = y_min; y &lt; y_max+1; y++) { //...if the pixel(x,y) is the triangle t //...interpolate the depth buffer/color/normal/texcoords/shadingcoords. if(depth_buffer &lt; z_buffer[]) means visible { //...compute the color(texture/lighting...)(pixel shading) setpixel(x,y,color); } } }} Attention: in the code above, why we need the shadingcoords. That’s because, variable x,y is in the screen space, but the shading process should be done in the world space/view space/clip space. In unity, rasterization is mostly done by the powerful engine, but we can control the process of viewport to adjust the game to different resolution platforms and control the shader part to get more amazing effects. Step7.Pixel(fragment) shadingWe compute a color for the pixel,a process known as shading. The innocuous phrase “compute a color” is the heart of computer graphics! In unity, we write the fragment shader to compute the pixel colors under different lighting models. code-snippet as follows,from [3]. 1234567891011121314151617181920212223fixed4 frag(v2f i) : SV_Target { fixed3 tangentLightDir = normalize(i.lightDir); fixed3 tangentViewDir = normalize(i.viewDir); // Get the texel in the normal map fixed4 packedNormal = tex2D(_BumpMap, i.uv.zw); fixed3 tangentNormal; // If the texture is not marked as \"Normal map\" //tangentNormal.xy = (packedNormal.xy * 2 - 1) * _BumpScale; //tangentNormal.z = sqrt(1.0 - saturate(dot(tangentNormal.xy, tangentNormal.xy))); // Or mark the texture as \"Normal map\", and use the built-in funciton tangentNormal = UnpackNormal(packedNormal); tangentNormal.xy *= _BumpScale; tangentNormal.z = sqrt(1.0 - saturate(dot(tangentNormal.xy, tangentNormal.xy))); fixed3 albedo = tex2D(_MainTex, i.uv).rgb * _Color.rgb; fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(tangentNormal, tangentLightDir)); fixed3 halfDir = normalize(tangentLightDir + tangentViewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(tangentNormal, halfDir)), _Gloss); return fixed4(ambient + diffuse + specular, 1.0);} From the code we can see that the fragment shader computes the ambient,diffuse,specular,the MainTex &amp; BumpMap texture controls the coefficients value of lighting model formulas. The lighting model is much more than you see. There are many physical formulas. But they are not hard to understand. You can get the details from the reference books[1][2][3]. Step8.Blending and OutputFinally! At the bottom of the render pipeline, we have produced a color,opacity, and depth value. The depth value is tested against the depth buffer for per-pixel visibility determination to ensure that an object farther away from the camera doesn’t obscure one closer to the camera. Pixels with an opacity that is too low are rejected, and the output color is then combined with the previous color in the frame buffer in a process known as alpha blending. SUMMARYOK! Now the 8 steps have all been listed. You may want to overview the rough processes. The pseudocode summarizes the simplified rendering pipeline outlined above, from[1]. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// First , figure how to view the scenesetupTheCamera ( ) ;// Clear the zbufferclearZBuffer ( ) ;// Setup environmental lighting and fogsetGlobalLightingAndFog ( ) ;// get a l i s t of objects that are potentially visiblepotentiallyVisibleObjectList = highLevelVisibilityDetermination ( scene ) ;// Render everything we found to be potentially visiblefor ( all objects in potentiallyVisibleObjectList ) { // Perform lower−level VSD using bounding volume test i f (! object . isBoundingVolumeVisible ( ) ) continue ; // Fetch or procedurally generate the geometry triMesh = object . getGeometry ( ) // Clip and render the faces for ( each triangle in the geometry ) { // Transform the vertices to clip space , and perform // vertex−level calculations (run the vertex shader ) clipSpaceTriangle = transformAndLighting ( triangle ) ; // Clip the triangle to the view volume clippedTriangle = clipToViewVolume ( clipSpaceTriangle ) ; i f ( clippedTriangle . isEmpty ( ) ) continue ; // Project the triangle onto screen space screenSpaceTriangle = clippedTriangle . projectToScreenSpace ( ) ; // Is the triangle backfacing ? i f ( screenSpaceTriangle . isBackFacing ( ) ) continue ; // Rasterize the triangle for ( each pixel in the triangle ) { // Scissor the pixel here ( i f triangle was // not completely clipped to the frustum ) i f ( pixel is off−screen ) continue ; // Interpolate color , zbuffer value , // and texture mapping coords // The pixel shader takes interpolated values // and computes a color and alpha value color = shadePixel ( ) ; // Perform zbuffering i f (! zbufferTest ( ) ) continue ; // Alpha test to ignore pixels that are ”too // transparent” i f (! alphaTest ( ) ) continue ; // Write to the frame buffer and zbuffer writePixel ( color , interpolatedZ ) ; // Move on to the next pixel in this triangle } // Move on to the next triangle in this object }// Move on to the next potentially visible object} AppendixSince we referred to Coordinates Transformation in Step5. I guess you may not very clear about the internal matrixs and the workflow. Come on baby! Time to overcome the difficulties! Model,World,Camera Space,Clip SpaceThe geometry of an object is initially described in object space,which is a coordinate space local to the object. The information described usually consisits of vertex positions and surface normals. Object space = Model space = Local spaceForm the model space,the vertices are transformed into world space. The transformation from modeling space to world space is often called model transform. Typically,lighting for the scene is specified in world space,but it doesn’t matter what coordinate space is used to perform the lighting calculations provided that the geometry and the lights can be expressed in the same space. So it is not weird that you see lighting calculations in the world space,or view space,or tangent space,or clip space in unity shader file. From world space,vertices are transformed into camera sapce. Camera space is a 3D coordinate space in which the origin is at the center of projection,one is axis parallel to the direction the camera is facing(perpendicullar to the projection plane),one axis is the intersection of the top and bottom clip planes,and the other axis is the intersection of the left and right clip planes. Camera space = View space = Eye spaceHere we should be alert to the difference between left-handed world and right-handed world,as shown in Fig2.In the left-handed world,the most common convention is to point +z in the direction that the camera is facing,with +x and +y pointing “right” and “up”.This is fairly intuitive,as shown in Fig3.The typical right-handed convention is to have -z point in the direction that the camera is facing. From camera space,vertices are transformed once again into clip space. The matrix that transforms vertices from camera space into clip space is called the clip matrix. clip space = canonical view volume space clip matrix = projection matrixMore Learning on Transformation and Matrix &gt;&gt; References:[1]3D Math Primer for Graphics and Game Development 2nd Edition.[2]Fundamentals of Computer Graphics and 3rd Edition.[3]Unity+Shader入门精要[4]Unity3d Mannual[5]GAMES","link":"/Graphics/Rendering/Rendering-Graphics-Pipeline/"},{"title":"Function-Template","text":"Keywords: FunctionTemplate This is for the readers who have basic c++ background. If you want to get the rough sketch of funtion template of c++, then there it is. I suggest an online c++ compiler : http://coliru.stacked-crooked.com/ (You can test your simple programs on it.) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;iostream&gt;#include &lt;functional&gt;#include &lt;array&gt;using namespace std;std::function&lt;int(int)&gt; Functional;int TestFunc(int a){ return a;}auto lambdaFn = [](float a)-&gt;float{ return a; };class Functor{ public: int operator()(int a){return a;}};class TestClass{ public: int ClassMember(int a) { return a; } static int StaticMember(int a) { return a; }};int main(){ Functional = TestFunc; int result = Functional(10); //10 cout &lt;&lt; \"TestFunc：\"&lt;&lt; result &lt;&lt; endl; Functional = lambdaFn; result = Functional(22.00); //22 cout &lt;&lt; \"Lambda：\"&lt;&lt; result &lt;&lt; endl; Functor testFunctor; Functional = testFunctor; result = Functional(30); //30 cout &lt;&lt; \"Functor：\"&lt;&lt; result &lt;&lt; endl; TestClass testObj; Functional = std::bind(&amp;TestClass::ClassMember, testObj, std::placeholders::_1); result = Functional(40); //40 cout &lt;&lt; \"TestClass：\"&lt;&lt; result &lt;&lt; endl; Functional = TestClass::StaticMember; result = Functional(50); //50 cout &lt;&lt; \"TestClass(static)：\"&lt;&lt; result &lt;&lt; endl; return 0;} function is a kind of wrapper,which provides a way to handle several funtion-like forms uniformly. If you know polymorphism, then this is easy to understand. Let’s see another code segment. 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;functional&gt;#include &lt;map&gt;using namespace std;int add(int x,int y){return x+y;}struct divide{ int operator()(int denominator,int divisor) { return denominator/divisor; }};int main(){ map&lt;char,function&lt;int(int,int)&gt;&gt; op = { {'+',add}, {'/',divide()}, {'-',[](int i,int j){return i-j;}} }; cout &lt;&lt; op['+'](1, 2) &lt;&lt; endl; //3 cout &lt;&lt; op['-'](1, 2) &lt;&lt; endl; //-1 cout &lt;&lt; op['/'](1, 2) &lt;&lt; endl; //0} Reference: https://www.cnblogs.com/reboost/p/11076511.html","link":"/Language/Advanced-CPP/STL-FunctionTemplate/"},{"title":"Variation-First-Understanding-of-Variation","text":"Keywords: Newbie, Variation This is the translation of this article(partly) https://zhuanlan.zhihu.com/p/139018146. Basic TheorySuppose we have two points $(a,p), (b,q)$, any curves joints them satisfies the bouding condition as follows: $$y(a) = p, y(b) = q\\tag{1}$$ Now we consider the definite Integral: $$I = \\int_a^b f(y,y’)dx\\tag{2}$$ More about Why $f(y,y’)$ &gt;&gt; $f(y,y’)$ is a function about $y(x)$ and it’s first-order derivative $y’(x)$, we expect a specific $y(x)$ lets $I$ has extrema. Recall what we have leanred about extrema in common function ? More about Extrema &gt;&gt; So $I$ is a functional（泛函） to $y(x)$ —- $y(x)$ changes, so as $I$, when $y(x) = ?$, I reaches the extrema. if y(x) has any small changes $\\delta y(x)$, we call it the variation of $y(x)$. (In basic calculus, we denote the samll changes of $x$ is $\\Delta x$ or $dx$) So the changes of $f(y,y’)$ is:$$\\delta f = \\frac{\\partial f}{\\partial y} \\delta y + \\frac{\\partial f}{\\partial y’}\\delta y’\\tag{3}$$ More about Partial Calculus &gt;&gt; The corresponding change of $I$ is : $$\\begin{aligned}\\delta I &amp;= \\int_a^b (\\delta f) dx \\\\&amp;= \\int_a^b [\\frac{\\partial f}{\\partial y} \\delta y + \\frac{\\partial f}{\\partial y’}\\delta y’] dx \\\\&amp;= \\int_a^b \\frac{\\partial f}{\\partial y} \\delta y dx + \\int_a^b \\frac{\\partial f}{\\partial y’}\\delta y’ dx\\end{aligned}\\tag{4}$$ the second term of equation(4) can be written as$$\\frac{\\partial f}{\\partial y’}\\delta y’ = \\frac{\\partial f}{\\partial y’} \\frac{d(\\delta y)}{dx}$$so, we are going to simplify equation(4), we firstly integrate the second term: $$\\begin{aligned}\\int_a^b \\frac{\\partial f}{\\partial y’}\\delta y’ dx &amp;= \\int_a^b \\frac{\\partial f}{\\partial y’} \\frac{d(\\delta y)}{dx} dx\\\\&amp;= \\int_a^b \\frac{\\partial f}{\\partial y’} d(\\delta y)\\\\&amp;= \\left. \\frac{\\partial f}{\\partial y’} \\delta y \\right|_a^b - \\int_a^b \\delta y d(\\frac{\\partial f}{\\partial y’})\\\\&amp;= \\left. \\frac{\\partial f}{\\partial y’} \\delta y \\right|_a^b - \\int_a^b \\delta y \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’}) dx\\\\\\end{aligned}\\tag{4.2}$$ More about Integral Formulas &gt;&gt; Since the function $y(x)$ has constant bouding value, which is $y(a) = p, y(b) = q$, so that$$\\delta y(a) = 0, \\delta y(b) = 0$$ thus, the first term in Equation(4.2), $\\left. \\frac{\\partial f}{\\partial y’} \\delta y \\right|_a^b = 0$. Substitue (4.2) to (4), we get $$\\begin{aligned}\\delta I &amp;= \\int_a^b \\frac{\\partial f}{\\partial y} \\delta y dx - \\int_a^b \\delta y \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’}) dx \\\\&amp;= \\int_a^b \\left[ \\frac{\\partial f}{\\partial y} - \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’})\\right] \\delta y(x) dx\\end{aligned}\\tag{5}$$ If $I$ has any extrama, then for any $\\delta y(x)$ satisfying the bounding contitions, there must have $\\delta I = 0$, which means: $$\\frac{\\partial f}{\\partial y} - \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’}) = 0\\tag{6}$$ This is Euler-Lagrange Equation, which is the basic Theorem in Variation. With it, we can find the extremal funtion $y(x)$. Of couse, Equation(6) is a second-order Differential function. More about Second-Order Differential Functions &gt;&gt; ExamplesThe shortest path between two pointsGive two points in the xy-plane, which is the shortest path(curve) that joints them? Solution: Example2Example3","link":"/Math/Variation/Variation-First-Understanding-of-Variation/"},{"title":"something-from-vector-implementation","text":"Keywords: STL This article is for the readers who have a basic background of math, graphics, c++. Although I have written the code about Vector2,Vector3 by c++, but as time goes by, all the principles were forgotten. So I decided to write something about the simple program to get much more strong memory.If you search the implementation of vector in github, you can find lots examples. Here’s an example vector.hpp, actually it’s too hard for me to read. Today’s vector.hpp is about the really basic syntax in c++ such as operator overloading, friends, inline etc. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#pragma once#include &lt;cmath&gt;#include &lt;iostream&gt;class Vector3f{public: Vector3f() : x(0) , y(0) , z(0) {} Vector3f(float xx) : x(xx) , y(xx) , z(xx) {} Vector3f(float xx, float yy, float zz) : x(xx) , y(yy) , z(zz) {} Vector3f operator*(const float&amp; r) const { return Vector3f(x * r, y * r, z * r); } Vector3f operator/(const float&amp; r) const { return Vector3f(x / r, y / r, z / r); } Vector3f operator*(const Vector3f&amp; v) const { return Vector3f(x * v.x, y * v.y, z * v.z); } Vector3f operator-(const Vector3f&amp; v) const { return Vector3f(x - v.x, y - v.y, z - v.z); } Vector3f operator+(const Vector3f&amp; v) const { return Vector3f(x + v.x, y + v.y, z + v.z); } Vector3f operator-() const { return Vector3f(-x, -y, -z); } Vector3f&amp; operator+=(const Vector3f&amp; v) { x += v.x, y += v.y, z += v.z; return *this; } friend Vector3f operator*(const float&amp; r, const Vector3f&amp; v) { return Vector3f(v.x * r, v.y * r, v.z * r); } friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const Vector3f&amp; v) { return os &lt;&lt; v.x &lt;&lt; \", \" &lt;&lt; v.y &lt;&lt; \", \" &lt;&lt; v.z; } float x, y, z;};class Vector2f{public: Vector2f() : x(0) , y(0) {} Vector2f(float xx) : x(xx) , y(xx) {} Vector2f(float xx, float yy) : x(xx) , y(yy) {} Vector2f operator*(const float&amp; r) const { return Vector2f(x * r, y * r); } Vector2f operator+(const Vector2f&amp; v) const { return Vector2f(x + v.x, y + v.y); } float x, y;};inline Vector3f lerp(const Vector3f&amp; a, const Vector3f&amp; b, const float&amp; t){ return a * (1 - t) + b * t;}inline Vector3f normalize(const Vector3f&amp; v){ float mag2 = v.x * v.x + v.y * v.y + v.z * v.z; if (mag2 &gt; 0) { float invMag = 1 / sqrtf(mag2); return Vector3f(v.x * invMag, v.y * invMag, v.z * invMag); } return v;}inline float dotProduct(const Vector3f&amp; a, const Vector3f&amp; b){ return a.x * b.x + a.y * b.y + a.z * b.z;}inline Vector3f crossProduct(const Vector3f&amp; a, const Vector3f&amp; b){ return Vector3f(a.y * b.z - a.z * b.y, a.z * b.x - a.x * b.z, a.x * b.y - a.y * b.x);} Scan the program above, here’s some tips: Inline Functions Reference Variables Operator Overloading Friends Inline Functions Inline functions are a C++ enhancement designed to speed up programs. The primary distinction between normal functions and inline functions is not in how you code them but in how the C++ compiler incorporates them into a program.To understand the distinction between inline functions and normal functions,you need to peer more deeply into a program’s innards.Let’s do that now. How do normal function calls work?The final product of the compilation process is an executable program,which consists of a set of machine language instructions.When you start a program,the operating system loads these instructions into the computer’s memory so that each instruction has a particular memory address.The computer then goes through these instructions step-by-step.Sometimes,as when you have a loop or a branching statement,program execution skips over instructions,jumping backward or forward to a particular address. Normal function calls also involve having a program jump to another address (the function’s address) and then jump back when the function terminates.Let’s look at a typical implementation of that process in a little more detail.When a program reaches the function call instruction, the program stores the memory address of the instruction immediately following the function call,copies function arguments to the stack (a block of memory reserved for that purpose),jumps to the memory location that marks the beginning of the function,executes the function code (perhaps placing a return value in a register),and then jumps back to the instruction whose address it saved. Jumping back and forth and keeping track of where to jump means that there is an overhead in elapsed time to using functions. How do inline functions work?C++ inline functions provide an alternative.In an inline function,the compiled code is “in line” with the other code in the program.That is,the compiler replaces the function call with the corresponding function code.With inline code,the program doesn’t have to jump to another location to execute the code and then jump back. Inline functions thus run a little faster than regular functions,but they come with a memory penalty.If a program calls an inline function at ten separate locations,then the program winds up with ten copies of the function inserted into the code. When use inline?You should be selective about using inline functions.If the time needed to execute the function code is long compared to the time needed to handle the function call mechanism,then the time saved is a relatively small portion of the entire process.If the code execution time is short,then an inline call can save a large portion of the time used by the non-inline call.On the other hand,you are now saving a large portion of a relatively quick process,so the absolute time savings may not be that great unless the function is called frequently. Inline versus MacrosThe inline facility is an addition to C++. C uses the preprocessor #define statement to provide macros, which are crude implementations of inline code. For example, here’s a macro for squaring a number: 1#define SQUARE(X) X*X This works not by passing arguments but through text substitution, with the X acting as a symbolic label for the “argument”: 123a = SQUARE(5.0); is replaced by a = 5.0*5.0;b = SQUARE(4.5 + 7.5); is replaced by b = 4.5 + 7.5 * 4.5 + 7.5;d = SQUARE(c++); is replaced by d = c++*c++; Only the first example here works properly. You can improve matters with a liberal application of parentheses: 1#define SQUARE(X) ((X)*(X)) Still, the problem remains that macros don’t pass by value. Even with this new definition, SQUARE(c++) increments c twice, but the inline square() function evaluates c , passes that value to be squared, and then increments c once. The intent here is not to show you how to write C macros. Rather, it is to suggest that if you have been using C macros to perform function-like services, you should consider converting them to C++ inline functions. Reference VariablesThe main use for a reference variable is as a formal argument to a function.If you use a reference as an argument,the function works with the original data instead of with a copy.References provide a convenient alternative to pointers for processing large structures with a function,and they are essential for designing classes. Creating a reference variable‘&amp;’ in c++ has two functions: to indicate the address of a variable to declare references 12int rats;int &amp; rodents = rats; // makes rodents an alias for rats In this context, &amp; is not the address operator.Instead,it serves as part of the type identifier. Just as char * in a declaration means pointer-to-char , int &amp; means reference-to-int .The reference declaration allows you to use rats and rodents interchangeably;both refer to the same value and the same memory location. Difference between pointer and reference?123int rats = 101;int &amp; rodents = rats; // rodents a referenceint * prats = &amp;rats; // prats a pointer Then you could use the expressions rodents and *prats interchangeably with rats and use the expressions &amp;rodents and prats interchangeably with &amp;rats .From this standpoint,a reference looks a lot like a pointer in disguised notation in which the * dereferencing operator is understood implicitly.And,in fact,that’s more or less what a reference is.But there are differences besides those of notation.For one,it is necessary to initialize the reference when you declare it;you can’t declare the reference and then assign it a value later the way you can with a pointer: 123int rat;int &amp; rodent;rodent = rat; // No, you can't do this. You should initialize a reference variable when you declare it. A reference is rather like a const pointer;you have to initialize it when you create it, and when a reference pledges its allegiance to a particular variable,it sticks to its pledge. That is, int &amp; rodents = rats; (equals to) int * const pr = &amp; rats; References as Function Parameterscode from[1] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;iostream&gt;using namespace std;void swapr(int &amp; a, int &amp; b); // a, b are aliases for intsvoid swapp(int * p, int * q); // p, q are addresses of intsvoid swapv(int a, int b); // a, b are new variablesint main(){ int wallet1 = 300; int wallet2 = 350; cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; cout &lt;&lt; \"Using references to swap contents:\\n\"; swapr(wallet1, wallet2); // pass variables cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; cout &lt;&lt; \"Using pointers to swap contents again:\\n\"; swapp(&amp;wallet1, &amp;wallet2); // pass addresses of variables cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; cout &lt;&lt; \"Trying to use passing by value:\\n\"; swapv(wallet1, wallet2); // pass values of variables cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; return 0;}void swapr(int &amp; a, int &amp; b) // use references{ int temp; temp = a; // use a, b for values of variables a = b; b = temp;}void swapp(int * p, int * q) // use pointers{ int temp; temp = *p; // use *p, *q for values of variables *p = *q; *q = temp;}void swapv(int a, int b) // try using values{ int temp; temp = a; // use a, b for values of variables a = b; b = temp;} The result is : 1234567wallet1 = $300 wallet2 = $350Using references to swap contents:wallet1 = $350 wallet2 = $300Using pointers to swap contents again:wallet1 = $300 wallet2 = $350Trying to use passing by value:wallet1 = $300 wallet2 = $350 The reference and pointer methods both successfully swap the contents of the two wallets,whereas the passing by value method fails. Operator OverloadingOperator overloading is a technique for giving object operations a prettier look. Operator overloading is an example of C++ polymorphism. 123456int main(){ Vector3f v (2,3,4); Vector3f w (1,2,1); std::cout &lt;&lt; v + w &lt;&lt; std::endl;} FriendsAs you’ve seen,C++ controls access to the private portions of a class object.Usually,public class methods serve as the only access,but sometimes this restriction is too rigid to fit particular programming problems.In such cases,C++ provides another form of access:the friend.Friends come in three varieties: Friend functions Friend classes Friend member functions Why we need friends?Often,overloading a binary operator (that is,an operator with two arguments) for a class generates a need for friends Multiplying a Time object by a real number provides just such a situation,so let’s study that case. 1A = B * 2.75; Remember,the left operand is the invoking object. Translates to the following member function call: 1A = B.operator*(2.75); But what about the following statement? 1A = 2.75 * B; // cannot correspond to a member function Conceptually, 2.75 * B should be the same as B * 2.75 ,but the first expression cannot correspond to a member function because 2.75 is not a type Time object.Remember, the left operand is the invoking object,but 2.75 is not an object.So the compiler cannot replace the expression with a member function call. One way around this difficulty is to tell everyone (and to remember yourself) that you can only write B * 2.75 but never write 2.75 * B .This is a server-friendly,client-beware solution,and that’s not what OOP is about. However,there is another possibility—using a nonmember function.(Remember,most operators can be overloaded using either member or nonmember functions.) A nonmember function is not invoked by an object;instead,any values it uses,including objects,are explicit arguments.Thus,the compiler could match the expression 1Time operator*(double m, const Time &amp; t); Using a nonmember function solves the problem of getting the operands in the desired order (first double and then Time ),but it raises a new problem:Nonmember functions can’t directly access private data in a class.Well,at least ordinary nonmember functions lack access.But there is a special category of nonmember functions,called friends,that can access private members of a class. Creating friendsThe first step toward creating a friend function is to place a prototype in the class declaration and prefix the declaration with the keyword friend . Like the code we showed in the beginning. 1234friend Vector3f operator*(const float&amp; r, const Vector3f&amp; v){ return Vector3f(v.x * r, v.y * r, v.z * r);} This prototype has two implications: Although the operator*() function is declared in the class declaration,it is not a member function.So it isn’t invoked by using the membership operator. Although the operator*() function is not a member function,it has the same access rights as a member function. Are friends unfaithful to OOP?At first glance, it might seem that friends violate the OOP principle of data hiding because the friend mechanism allows nonmember functions to access private data. However, that’s an overly narrow view. Instead, you should think of friend functions as part of an extended interface for a class. For example, from a conceptual point of view, multiplying a double by a Time value is pretty much the same as multiplying a Time value by a double . That the first requires a friend function whereas the second can be done with a member function is the result of C++ syntax, not of a deep conceptual difference. By using both a friend function and a class method, you can express either operation with the same user interface. Also keep in mind that only a class declaration can decide which functions are friends, so the class declaration still controls which functions access private data. In short, class methods and friends are simply two different mechanisms for expressing a class interface. A common kind of friend: Overloading the &lt;&lt; OperatorFrom the code in the begining,we see that : 1234friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const Vector3f&amp; v){ return os &lt;&lt; v.x &lt;&lt; \", \" &lt;&lt; v.y &lt;&lt; \", \" &lt;&lt; v.z;} In its most basic incarnation,the &lt;&lt; operator is one of C and C++’s bit manipulation operators;it shifts bits left in a value.But the ostream class overloads the operator,converting it into an output tool.Recall that cout is an ostream object and that it is smart enough to recognize all the basic C++ types.That’s because the ostream class declaration includes an overloaded operator&lt;&lt;() definition for each of the basic types.That is,one definition uses an int argument,one uses a double argument,and so on.So one way to teach cout to recognize a Vector3f object is to add a new function operator definition to the ostream class declaration.But it’s a dangerous idea to alter the iostream file and mess around with a standard interface. Instead,use the Vector3f class declaration to teach the Vector3f class how to use cout. So this is the basic syntax, anyway, there’s still lots deeper knowledge there… if I have met much more advanced code, I will write this article again. References:[1]C++ Primer Plus.","link":"/Language/Advanced-CPP/STL-Vector/"},{"title":"Rendering-Lighting-Shading-Texture","text":"Keywords: Surface Shading, Texture mapping How to Understand Shading?How to understand shading? Just like everything does in the real world, all the scenes we see are under different ligths : the sun, the lamp, the flash light etc. Shading is kind of a technique that has a significant relationship with lights and will present the colorful drawings to us at last. Surface shading means the surface is ‘painted’ with lights, it’s a process of applying a material to an object. The Standard Local Lighting ModelI bet that you have heard the term: BRDF(Bidirectional Reflectance Distribution Function) which seems to have inextricable linkes to the Standard Lighting Model. Yes,BRDF will appear in PBS(Physically Based Shading) which is used to make more realistic modeling presenting the reactions between lights and materials. As for Standard Local Lighting Model, they can be considered as simplified versions of PBS, they are empirical models, but they are easier to understand. The Standard Lighting Equation OverviewThe Standard Lighting Model only cares about direct light (direct reflection).Lights are special entities without any corresponding geometry,and are simulated as if the light were emitting from a sight point. The rendering equation is an equation for the radiance outgoing from a point in any particular direction,the only outgoing direction that mattered in those days were the directions that pointed to the eye. Why say that? Because you know the real world doesn’t work like this where the light may reflect for dozens of times and the process is really complicated, the cost is also a luxury that could not yet be afforded. The basic idea is to classify coming into the eye into four distinct categories, each of which has a unique method for calculating its contribution. The four categories are : Emissive contribution,denoted as $c_{emis}$. It tells the amount of radiance emitted directly from the surface in the given direction. Note that without global illumination techniques,these surfaces do not actually light up anything(except themselves). Specular contribution,denoted as $c_{spc}$. It accounts for light incident directly from the light source that is scattered preferentially in the direction of a perfect “mirror bounce”. Diffuse contribution,denoted as $c_{diff}$. It accounts for light incident directly from the light source that is scattered in every direction evenly. Ambient contribution,denoted as $c_{amb}$. It is a fudge factor to account for all indirect light. The Ambient and Emmisive ComponentsTo model light that is reflected more than one time before it enters the eye,we can use a very crude approximation known as “ambient light”. The ambient portion of the lighting equation depends only on the properties of materials and an ambient lighting value,which is often a global value used for the entire scene.$$c_{amb} = g_{amb} \\cdot m_{amb}\\tag{1}$$The factor $m_{amb}$ is the material’s “ambient color”. This is almost always the same as the diffuse color (which is often defined using texture map). The other factor,$g_{amb}$,is the ambient light value. Somtimes a ray of light travels directly from the light source to the eye,without striking any surface in between. The standard lighting equation accounts for such rays by assigning a material an emissive color. For example,when we render the surface of the light bulb,this surface will probably appear very bright,even if there’s no other light in the scene,because the light bulb is emitting light. In many situations,the emissive contribution doesn’t depend on environmental factor; it is simply the emissive color of the material.$$c_{emis} = m_{emis}\\tag{2}$$ The Diffuse ComponentFor diffuse lighting, the location of the viewer is not relevant,since the reflections are scattered randomly, and no matter where we position the camera,it is equally likely that a ray will be sent our way. But the direction if incidence l,which is dictated by the position of the light source relative to the surface, is important. Diffuse lighting obeys Lambert’s law: the intensity of the reflected light is proportional to the cosine of the angle between the surface normal and the rays of light. We calculate the diffuse component according to Lambert’s Law:$$c_{diff} = c_{light} \\cdot m_{diff} \\cdot max(0,n \\cdot l)\\tag{3}$$as before, n is the surface normal and l is a unit vector that points towards the light source. The factor $m_{diff}$ is the material’s diffuse color, which is the value that most people think of when they think of the “color” of an object. The diffuse material color often comes from a texture map. The diffuse color of the light source is $c_{light}$. On thing needs attention, that is the max(), because we need to prevent the dot result of normal and light negative, we use $max(0,n \\cdot l)$, so the object won’t be lighted by the rays from it’s back. The Specular ComponentThe specular component is what gives surfaces a “shiny” appearance. If you don’t understand what a specular is, think about the professional term in animes: Now let’s see how the standard model calculates the specular contribution. For convenience,we assume that all of these vectors are unit vectors. n is the local outward-pointing surface normal v points towards the viewer. l points towards the light source. r is the reflection vector, which is the direction of a “perfect mirror bounce.” It’s the result of reflecting l about n. $\\theta$ is the angle between r and v. Of the four vectors, you can see the reflection vector can be computed by The Phong Model for specular reflection is :$$c_{spec} = c_{light} \\cdot m_{spec} \\cdot max(0,v \\cdot r)^{m_{gls}}\\tag{4}$$$$r = 2(n \\cdot l)n-l$$ $m_{gls}$ means the glossiness of the material,also known as the Phong exponent, specular exponent, or just as the material shininess. This controls how wide the “hotspot” is - a smaller $m_{gls}$ produces a larger, more gradual falloff from the hotspot,and a larger $m_{gls}$ produces a tight hotspot with sharp falloff. $m_{spec}$ is related to “shininess”, it represents the material’s specular color. While $m_{gls}$ controls the size of the hotspot, $m_{spec}$ controls its intensity and color. $c_{light}$ is essentially the “color” of the light, which contains both its color and intensity. But!!We usually use Blinn Phong Model instead of Phong Model. The Blinn phong model can be faster to implement in hardware than the Phong model, if the viewer and light source are far enough away from the object to be considered a constant,since then h is a constant and only needs to be computed once. But when v or l may not be considered constant, the Phong model calculation might be faster.The Blinn Phong Model for specualr reflection is :$$c_{spec} = c_{light} \\cdot m_{spec} \\cdot max(0,n \\cdot h)^{m_{gls}}\\tag{4}$$$$h = \\frac{v + l}{|v + l|}$$ In real coding, vector in the above (1)(2)(3)(4) should be unit vector Limitations of the Standard ModelWhy learn about this ancient history? First, it isn’t exactly ancient history, it’s alive and well. Second,the current local lighting model is one that content creators can understand and use. A final reason to learn the standard lighting model is becausemany newer models bear similarities to the standard model, and you cannotknow when to use more advanced lighting models without understandingthe old standard. Since Blinn Phong Model contains all the components above,so we call the it Blinn-Phong. Actually, there are several important physical phenomena not properly captured by the Blinn-Phong model. Such as Fresnel reflectance. (:) We’ll discuss this PBS part in the Appendix). Flat &amp; Gouraud ShadingThis part is about the Shading Frequencies. Are you confused? If not, it’s impossible. Because I’m confused at the first time learning and the second time learning. But now, I got it. So come with me. On modern shader-based hardware, lighting calculations are usually done on a per-pixel basis. By this we mean that for each pixel, we determine a surface normal (whether by interpolating the vertex normal across the face or by fetching it from a bump map), and then we perform the full lighting equation using this surface normal. This is per-pixel lighting, and the technique of interpolating vertex normals across the face is sometimes called Phong shading, not to be confused with the Phong calculation for specular reflection. The alternative to Phong shading is to perform the lighting equation less frequently (per face, or per vertex). These two techniques are known as flat shading and Gouraud shading, respectively. Flat shading is almost never used in practice except in software rendering. This is because most modern methods of sending geometry efficiently to the hardware do not provide any face-level data whatsoever. Gouraud shading, in contrast, still has some limited use on some platforms. Some important general principles can be gleaned from studying these methods, so let’s examine their results. Phong shading ≠ Phong Reflection Model ≠ Blinn Phong Reflection Model The table below can list differences among them. per-pixel lighting per-vertex lighting per-face lighting Phong shading Gouraud shading Flat shading Interpolate normal vectors across each triangle Interpolate colors from vertices across triangle Triangle face is flat — one normal vector Compute full shading model at each pixel Each vertex has a normal vector Not good for smooth surfaces # Mostly used is phong shading Talk is cheap, show me the code. Here’s a code (from[3]) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667//per-pixel lightingShader \"Unity Shaders Book/Chapter 6/Blinn-Phong Use Built-in Functions\" { Properties { _Diffuse (\"Diffuse\", Color) = (1, 1, 1, 1) _Specular (\"Specular\", Color) = (1, 1, 1, 1) _Gloss (\"Gloss\", Range(1.0, 500)) = 20 } SubShader { Pass { Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float4 worldPos : TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); // Use the build-in funtion to compute the normal in world space o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex); return o; } fixed4 frag(v2f i) : SV_Target { fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; fixed3 worldNormal = normalize(i.worldNormal); // Use the build-in funtion to compute the light direction in world space // Remember to normalize the result fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); // Use the build-in funtion to compute the view direction in world space // Remember to normalize the result fixed3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); return fixed4(ambient + diffuse + specular, 1.0); } ENDCG } } FallBack \"Specular\"} The result is : Light SourcesIf you have used Unity, you won’t forget different kinds of lights. Standard Abstract Light Types A point light source represents light that emanates from a single point outward in all directions. Point lights are also called omni lights (short for “omnidirectional”) or spherical lights.A point light has a position and color, which controls not only the hue of the light, but also its intensity. Point lights can be used to represent many common light sources, such as light bulbs, lamps, fires, and so forth. point light = omni light = spherical light A spot light is used to represent light from a specific location in a specific direction. These are used for lights such as flashlights, headlights, and of course, spot lights~ As for A conical spot light, it has a circular “bottom”, the width of the cone is defined by a falloff angle(not to be confused with the falloff distance). Also, there is an inner angle that measures the size of the hotspot. A directional light represents light emanating from a point in space sufficiently far away that all the rays of light involved in lighting the scene (or at least the object we are currently considering) can be considered as parallel. Directional lights usually do not have a position, at least as far as lighting calculations are concerned, and they usually do not attenuate. Like the sun and moon in our real world. Directional light = parallel light = distant light An area light is only useful in bake. So we don’t talke about it here. Here’s intuitional effects among the lights. Light AttenuationIn the real world, the intensity of a light is inversely proportional to the square of the distance between the light and the object, as$$\\frac{i_1}{i_2} = (\\frac{d_2}{d_1})^2\\tag{1}$$where i is the radiant flux (the radiant power per unit area) and d is the distance. This part will be mentioned again in the RayTracing article, it’s about Radiometry. Here you just need to know that the final amount of emitted light is obtained by multiplying the light color by its intensity: light amount = light color * light intensity Actually,I haven’t used light-falloff in my coding. Also this blog is for the novices, so let’s continue with a simple practice and finish this part. Just rememeber that this is the very primary part. Talk is cheap, show me the code.(frome[3]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161Shader \"Unity Shaders Book/Chapter 9/Forward Rendering\" { Properties { _Diffuse (\"Diffuse\", Color) = (1, 1, 1, 1) _Specular (\"Specular\", Color) = (1, 1, 1, 1) _Gloss (\"Gloss\", Range(8.0, 256)) = 20 } SubShader { Tags { \"RenderType\"=\"Opaque\" } Pass { // Pass for ambient light &amp; first pixel light (directional light) Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM // Apparently need to add this declaration //该指令可以保证我们的shader中使用的光照衰减等光照变量可以被正确赋值 #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag(v2f i) : SV_Target { fixed3 worldNormal = normalize(i.worldNormal); fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); fixed atten = 1.0; return fixed4(ambient + (diffuse + specular) * atten, 1.0); } ENDCG } Pass { // Pass for other pixel lights Tags { \"LightMode\"=\"ForwardAdd\" } Blend One One CGPROGRAM // Apparently need to add this declaration //该指令保证我们在additional pass 中访问到正确的光照变量 #pragma multi_compile_fwdadd #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" #include \"AutoLight.cginc\" fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag(v2f i) : SV_Target { fixed3 worldNormal = normalize(i.worldNormal); //如果当前处理的光源类型是平行光,因为平行光没有固定的位置 #ifdef USING_DIRECTIONAL_LIGHT fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); //如果是点光源或者聚光灯，他们的位置是 #else fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos.xyz); #endif fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); //如果是平行光，衰减值为1 #ifdef USING_DIRECTIONAL_LIGHT fixed atten = 1.0; #else //if point light,transform the vertex position from world spact to light space //sample the texture to get the Attenuation value //如果是点光源，把顶点坐标从世界空间到光线空间 #if defined (POINT) float3 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)).xyz; fixed atten = tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; //if spot light #elif defined (SPOT) float4 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)); fixed atten = (lightCoord.z &gt; 0) * tex2D(_LightTexture0, lightCoord.xy / lightCoord.w + 0.5).w * tex2D(_LightTextureB0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; #else fixed atten = 1.0; #endif #endif return fixed4((diffuse + specular) * atten, 1.0); } ENDCG } } FallBack \"Specular\"} If you want to know the rendering order,you can use the Frame Debug, this tool is really useful. I think as the study goes further, this part will be mentioned again. Also this is my learning curve, maybe it matches you too. So go on with my articles. Texture MappingFinally, here comes the texture mapping!I am already gearing up and eager to try. Cuz I really want to overview the shadow mapping and opacity blend again. And there are too many things cannot learn forward without the knowledge of texture. What is a Texture?There is much more to the appearance of an object than its shape. Different objects are different colors and have different patterns on their surface. One simple yet powerful way to capture these qualities is through texture mapping. A texture map is a bitmap image that is “pasted” to the surface of an object. bitmap image is pixel-image, on the contrary, vector-image So a texture map is just a regular bitmap that is applied onto the surface of a model. Exactly how does this work? The key idea is that, at each point on the surface of the mesh, we can obtain texture-mapping coordinates, which define the 2D location in the texture map that corresponds to this 3D location. Traditionally, these coordinates are assigned the variables (u,v), where u is the horizontal coordinate and v is the vertical coordinate; thus texture-mapping coordinates are often called UV coordinates or simply UVs. On thing needs attention : The origin is in the upper left-hand corner of the image, which is the DirectX-style convention, or in the lower left-hand corner, the OpenGL conventions.In unity, the powerful engine has solved the problem for us, unity use uniform left-hand corner as OpenGL. Although bitmaps come in different sizes, UV coordinates are normalized such that the mapping space ranges from 0 to 1 over the entire width(u) or height (v) of the image, rather than depending on the image dimensions. We typically compute or assign UV coordinates only at the vertex level, and the UV coordinates at an arbitratry interior position on a face are obtained through interpolation (:) See in Appendix) So the pseudo-code of UV mapping should be: 12345//c++for each rasterized screen sample (x,y): //sample (x,y)-usually a pixel's center (u,v) = evaluate texture coordinate at (x,y) //using barycentric coordinates texcolor = texture.sample(u,v); set sample’s color to texcolor; //usually the diffuse albedo Kd(recall the Blinn-Phong reflectance model) Texture MagnificationUV coordinates outside of the range [0,1] are allowed, and in fact are quite useful. Such coordinates are interpreted in a variety of ways. The most common addressing modes (Wrap Mode) are repeat (also known as tile or wrap) and clamp. When repeating is used, the integer portion is discarded and only the fractional portion is used, causing the texture to repeat. Under clamping, when a coordinate outside the range [0,1] is used to access a bitmap, it is clamped in range. This has the effect of streaking the edge pixels of the bitmap outwards. The mesh in both cases is identical: a single polygon with four vertices. And the meshes have identical UV coordinates. The only difference is how coordinates outside the [0,1] range are interpreted. See Fig17. If you have used Unity, this is not strange to you. See the example below(from[3]). The shader code on the materail of the Quad is : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354Shader \"Unity Shaders Book/Chapter 7/Texture Properties\" { Properties { _MainTex (\"Main Tex\", 2D) = \"white\" {} } SubShader { Pass { Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" sampler2D _MainTex; float4 _MainTex_ST; struct a2v { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; }; struct v2f { float4 position : SV_POSITION; float2 uv : TEXCOORD0; }; v2f vert(a2v v) { v2f o; // Transform the vertex from object space to projection space o.position = UnityObjectToClipPos(v.vertex); //注意这里的v是模型空间中就定义好的纹理坐标 //v.texcoord应该是0-1之间,如果在面板上更改缩放值和偏移量，就会改动o.uv不在0-1之间 //o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; o.uv = TRANSFORM_TEX(v.texcoord, _MainTex); return o; } fixed4 frag(v2f i) : SV_Target { //根据uv值对纹理进行采样 fixed4 c = tex2D(_MainTex, i.uv); return fixed4(c.rgb, 1.0); } ENDCG } } FallBack \"Diffuse\"} I think you have noticed the code //o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; Look at the gif below, _MainTex_ST.xy means Tiling, _MainTex_ST.zw means offset. Also, I think you have noticed that there is Mipmap &amp; FilterMode properties in the panel of Fig17-1, what’s the meaning of these? You see that in Unity, the png is 512*512, it matches the Quad just in time. What if the texture(png) is too small? It’s easy to imagine, that you have an image, but the object is too giant, you need some methods to let the texture ‘pasted’ on the object surface without low resolution/distortion. Here I want to infer Bilinear Interpolation (:) see it in the Appendix) Then What if the texture(png) is too large? Here comes Mipmap (This part is a little hard for me. so Jump over it and later back..) Different Types Texture MappingThere are too many types of texture mapping. Bump MappingBump mapping is a general term that can refer to at least two different methods of controlling the surface normal per texel. A height map is a grayscale map, in which the intensity indicates the local “elevation” of the surface. Lighter colors indicate portions of the surface that are “bumped out,” and darker colors are areas where the surface is “bumped in.” Height maps are attractive because they are very easy to author, but they are not ideal for real-time purposes because the normal is not directly available; instead, it must be calculated from the intensity gradient. (We wil talk about it in Displacement Mapping) A bump map, which is very common nowadays, is Normal Mapping. Normal MappingIn a normal map, the coordinates of the surface normal are directly encoded in the map. How could a bump map save the surface normal of the object? Of course, the color. The most basic way is to encode x, y, and z in the red, green, and blue channels. Since the normal vector is bounded in [-1,1],and the color channel component is bounded in [0,1], so there should be a principle:$$pixel = \\frac{normal + 1}{2}$$Seems easy~ The bump map storse the normal vectors in model space in terms of pixels(rgb). Voila! If only it were that easy. Real-world objects exhibit a great deal of symmetry and self-similarity, and patterns are often repeated. For example, a box often has similar bumps and notches on more than one side. Because of this, it is currently a more efficient use of the same amount of memory (and artist time) to increase the resolution of the map and reuse the same normal map (or perhaps just portions of it) on multiple models (or perhaps just on multiple places in the same model). Of course, the same principle applies to any sort of texture map, not just normal maps. But normal maps are different in that they cannot be arbitrarily rotated or mirrored because they encode a vector. Imagine using the same normal map on all six sides of a cube. While shading a point on the surface of the cube, we will fetch a texel from the map and decode it into a 3D vector. A particular normal map texel on the top will produce a surface normal that points in the same direction as that same texel on the bottom of the cube, when they should be opposites! We need some other kind of information to tell us how to interpret the normal we get from the texture, and this extra bit of information is stored in the basis vectors. So there comes the Tangent Space.In tangent space, +z points out from the surface; the +z basis vector is actually just the surface normal n. The x basis vector is known as the tangent vector, which we’ll denote t, and it points in the direction of increasing t in texture space. Similarly, the y basis vector, known as the binormal and denoted here as b, corresponds to the direction of increasing b, although whether this motion is “up” or “down” in the texture space depends on the conventions for the origin in (t,b) space, which can differ, as we discussed earlier. The coordinates for the tangent and binormal are given in model space. And how to calculate basis vectors as the average of adjacent triangle normals?Here’s the formula &amp; code(from[1])We are given a triangle with vertex positions $p_0 = (x_0 ,y_0 ,z_0 ), p_1 = (x_1 ,y_1 ,z_1 ), and p_2 = (x_2 ,y_2 ,z_2),$ and at those vertices we have the UV coordinates $(u_0 ,v_0 ), (u_1 ,v_1 ), and (u_2 ,v_2 ).$$$q_1 = p_1 − p_0 , s_1 = u_1 − u_0 , t_1 = v_1 − v_0$$$$q_2 = p 2 − p_0 , s_2 = u_2 − u_0 , t_2 = v_2 − v_0.$$$$tangent = t_2q_1 - t_1q_2 , binormal = -s_2q_1 + s_1q_2$$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071//c++struct Vertex { Vector3 pos ; float u, v ; Vector3 normal ; Vector3 tangent ; float det ; // determinant of tangent transform. (−1 i f mirrored )};struct Triangle { int vertexIndex [3];};struct TriangleMesh { int vertexCount ; Vertex ∗vertexList ; int triangleCount ; Triangle ∗ triangleList ; void computeBasisVectors ( ) { // Note: we assume vertex normals are valid Vector3 ∗tempTangent = new Vector3 [ vertexCount ]; Vector3 ∗tempBinormal = new Vector3 [ vertexCount ]; // F i r s t clear out the accumulators for ( int i = 0 ; i &lt; vertexCount ; ++i ) { tempTangent [i].zero ( ) ; tempBinormal [i].zero ( ) ; } // Average in the basis vectors for each face // into i t s neighboring vertices for ( int i = 0 ; i &lt; triangleCount ; ++i ) { // Get shortcuts const Triangle &amp;tri = triangleList [ i ]; const Vertex &amp;v0 = vertexList [ tri.vertexIndex [0]]; const Vertex &amp;v1 = vertexList [ tri.vertexIndex [1]]; const Vertex &amp;v2 = vertexList [ tri.vertexIndex [2]]; // Compute intermediate values Vector3 q1 = v1.pos − v0.pos ; Vector3 q2 = v2.pos − v0.pos ; float s1 = v1.u − v0.u; float s2 = v2.u − v0.u; float t1 = v1.v − v0.v ; float t2 = v2.v − v0.v ; // Compute basis vectors for this triangle Vector3 tangent = t2∗q1 − t1∗q2; tangent.normalize ( ) ; Vector3 binormal = −s2∗q1 + s1∗q2; binormal.normalize ( ) ; // Add them into the running totals for neighboring verts for ( int j = 0 ; j &lt; 3 ; ++j ) { tempTangent [ tri.vertexIndex [ j ]] += tangent ; tempBinormal [ tri.vertexIndex [ j ]] += binormal ; } } // Now f i l l in the values into the vertices for ( int i = 0 ; i &lt; vertexCount ; ++i ) { Vertex &amp;v = vertexList [ i ]; Vector3 t = tempTangent [ i ]; // Ensure tangent is perpendicular to the normal. // (Gram−Schmit ) , then keep normalized version t −= v.normal ∗ dot (t,v.normal ) ; t.normalize ( ) ; v.tangent = t ; // Figure out i f we’ re mirrored if ( dot ( cross ( v.normal , t ) , tempBinormal [ i ]) &lt; 0.0 f ) { v.det = −1.0f ; // we’ re mirrored } else { v.det = +1.0 f ; // not mirrored } } // Clean up delete [] tempTangent ; delete [] tempBinormal ; }}; In unity, you can calculate the lighting model in the world space with bump textures.Here an example.(from[3]) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697Shader \"Unity Shaders Book/Chapter 7/Normal Map In World Space\" { Properties { _Color (\"Color Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Main Tex\", 2D) = \"white\" {} _BumpMap (\"Normal Map\", 2D) = \"bump\" {} _BumpScale (\"Bump Scale\", Float) = 1.0 _Specular (\"Specular\", Color) = (1, 1, 1, 1) _Gloss (\"Gloss\", Range(8.0, 256)) = 20 } SubShader { Pass { Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" fixed4 _Color; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpMap; float4 _BumpMap_ST; float _BumpScale; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; float4 tangent : TANGENT; float4 texcoord : TEXCOORD0; }; struct v2f { float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float4 TtoW0 : TEXCOORD1; float4 TtoW1 : TEXCOORD2; float4 TtoW2 : TEXCOORD3; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; o.uv.zw = v.texcoord.xy * _BumpMap_ST.xy + _BumpMap_ST.zw; float3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; fixed3 worldNormal = UnityObjectToWorldNormal(v.normal); fixed3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); fixed3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; // Compute the matrix that transform directions from tangent space to world space // Put the world position in w component for optimization o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); return o; } fixed4 frag(v2f i) : SV_Target { // Get the position in world space float3 worldPos = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); // Compute the light and view dir in world space fixed3 lightDir = normalize(UnityWorldSpaceLightDir(worldPos)); fixed3 viewDir = normalize(UnityWorldSpaceViewDir(worldPos)); // Get the normal in tangent space fixed3 bump = UnpackNormal(tex2D(_BumpMap, i.uv.zw)); bump.xy *= _BumpScale; bump.z = sqrt(1.0 - saturate(dot(bump.xy, bump.xy))); // Transform the narmal from tangent space to world space bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump))); fixed3 albedo = tex2D(_MainTex, i.uv).rgb * _Color.rgb; fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(bump, lightDir)); fixed3 halfDir = normalize(lightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(bump, halfDir)), _Gloss); return fixed4(ambient + diffuse + specular, 1.0); } ENDCG } } FallBack \"Specular\"} Displacement MappingA height map (or true displacement map) can be easily painted in Photoshop; Since normal map is clear , displacement is not hard for you.A displacement map actually changes the geometry using a texture. A common simplification is that the displacement will be in the direction of the surface normal. $$p\\prime = p + f(p)n.$$ Environment MappingOften we want to have a texture-mapped background and for objects to have specular reflections of that background. This can be accomplished using environment maps; There are many ways to store environment maps. Here is the most common method cube map. If you have used Unity, then you’ll be familiar with cube map, yes, the sky box~ In ideal cases, we want to generate the corresponding cube map for the objects of different positions in the scene. So the smart way is to write scripts. Here’s an example 123456789101112131415161718192021222324252627282930313233using UnityEngine;using UnityEditor;using System.Collections;public class RenderCubemapWizard : ScriptableWizard { public Transform renderFromPosition; public Cubemap cubemap; void OnWizardUpdate () { helpString = \"Select transform to render from and cubemap to render into\"; isValid = (renderFromPosition != null) &amp;&amp; (cubemap != null); } void OnWizardCreate () { // create temporary camera for rendering GameObject go = new GameObject( \"CubemapCamera\"); go.AddComponent&lt;Camera&gt;(); // place it on the object go.transform.position = renderFromPosition.position; // render into cubemap go.GetComponent&lt;Camera&gt;().RenderToCubemap(cubemap); // destroy temporary camera DestroyImmediate( go ); } [MenuItem(\"GameObject/Render into Cubemap\")] static void RenderCubemap () { ScriptableWizard.DisplayWizard&lt;RenderCubemapWizard&gt;( \"Render cubemap\", \"Render!\"); }} Shadow MapsHere comes the shadow map. Opacity Blending Appendix:PBSThis part will be explained in First-Met-With-RayTracing. InterpolationBefore learning CG, I couldn’t understand the term interpolation. Now it’s time write something about it.There are many interpolation methods, today I want to introduce a common method, called Barycentric Coordinates - used in Interpolation Across Triangles. If you have read the above the paragraphs carefully, you can see the barycentric coordinates method has appeared before. Why do we want interplate? Specify values at vertices Obtain smoothly varying values across triangles What do we want to interpolate? Texture coordinates, colors, normal vectors, … Barycentric Coordinates: Formulas $$\\alpha = \\frac{-(x-x_B)(y_C - y_B) + (y-y_B)(x_C-x_B)}{-(x_A-x_B)(y_C-y_B) + (y_A-y_B)(x_C-x_B)}\\tag{Barycentric Coordinates: Formulas}$$$$\\beta = \\frac{-(x-x_C)(y_A-y_C) + (y-y_C)(x_A-x_C)}{-(x_B-x_C)(y_A-y_C) + (y_B-y_C)(x_A-x_C)}$$$$\\gamma = 1 - \\alpha - \\beta$$ Using Barycentric Coordinates talk is cheap, show me the code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//c++static std::tuple&lt;float, float, float&gt; computeBarycentric2D(float x, float y, const Vector4f* v){ float alpha = (x*(v[1].y() - v[2].y()) + (v[2].x() - v[1].x())*y + v[1].x()*v[2].y() - v[2].x()*v[1].y()) / (v[0].x()*(v[1].y() - v[2].y()) + (v[2].x() - v[1].x())*v[0].y() + v[1].x()*v[2].y() - v[2].x()*v[1].y()); float beta = (x*(v[2].y() - v[0].y()) + (v[0].x() - v[2].x())*y + v[2].x()*v[0].y() - v[0].x()*v[2].y()) / (v[1].x()*(v[2].y() - v[0].y()) + (v[0].x() - v[2].x())*v[1].y() + v[2].x()*v[0].y() - v[0].x()*v[2].y()); float gamma = (x*(v[0].y() - v[1].y()) + (v[1].x() - v[0].x())*y + v[0].x()*v[1].y() - v[1].x()*v[0].y()) / (v[2].x()*(v[0].y() - v[1].y()) + (v[1].x() - v[0].x())*v[2].y() + v[0].x()*v[1].y() - v[1].x()*v[0].y()); return {alpha,beta,gamma};}//we all know that color,vertex position,normal are Vector3fstatic Eigen::Vector3f interpolate(float alpha, float beta, float gamma, const Eigen::Vector3f&amp; vert1, const Eigen::Vector3f&amp; vert2, const Eigen::Vector3f&amp; vert3, float weight){ return (alpha * vert1 + beta * vert2 + gamma * vert3) / weight;}//uv coordinates are Vector2fstatic Eigen::Vector2f interpolate(float alpha, float beta, float gamma, const Eigen::Vector2f&amp; vert1, const Eigen::Vector2f&amp; vert2, const Eigen::Vector2f&amp; vert3, float weight){ auto u = (alpha * vert1[0] + beta * vert2[0] + gamma * vert3[0]); auto v = (alpha * vert1[1] + beta * vert2[1] + gamma * vert3[1]); u /= weight; v /= weight; return Eigen::Vector2f(u, v);}//here's the rasterization processvoid rasterization(Triangle &amp;t){ ...find the bounding box of t for(int x = int(x_min); x &lt; int(x_max)+1; x++) { for(int y = int(y_min); y&lt; int(y_max)+1;y++) { if(insideTriangle(float(x) + 0.5, float(y) + 0.5, t)) { //get alpha,beta,gamma auto[alpha, beta, gamma] = computeBarycentric2D(x, y, t.v); float Z = 1.0 / (alpha / v[0].w() + beta / v[1].w() + gamma / v[2].w()); //interpolate depth float zp = alpha * v[0].z() / v[0].w() + beta * v[1].z() / v[1].w() + gamma * v[2].z() / v[2].w(); zp *= Z; //if pass the depth test auto interpolated_color = interpolate(alpha,beta,gamma,t.color[0],t.color[1],t.color[2],1); auto interpolated_normal = interpolate(alpha,beta,gamma,t.normal[0],t.normal[1],t.normal[2],1); auto interpolated_texcoords = interpolate(alpha,beta,gamma,t.tex_coords[0],t.tex_coords[1],t.tex_coords[2],1); ... } } }} Bilinear InterpolationSince we mentioned bilinear interpolation in the texture magnificient part. So let’s go straight. Step1. We want to sample texture f(x,y) at red point, black points indicate texture sample locations. Step2. Take 4 nearest sample locations, with texture values as labeled. Step3. Calculate fractional offsets,(s,t) Step4. $$lerp(x,v_0,v_1) = v_0 + x(v_1 - v_0)\\tag{Linear interpolation (1D)}$$$$u_0 = lerp(s,u_{00},u_{10})$$$$u_1 = lerp(s,u_{01},u_{11})\\tag{Two helper lerps}$$$$f(x,y) = lerp(t,u_0,u_1)\\tag{Final vertical lerp, to get result}$$ talk is cheap, show me the code 1234567891011121314151617181920212223242526272829303132333435//c++/opencvEigen::Vector3f getColor(float u, float v) { auto u_img = u * (width-1); auto v_img = (1 - v) * (height-1); auto color = image_data.at&lt;cv::Vec3b&gt;(v_img, u_img); return Eigen::Vector3f(color[0], color[1], color[2]); } //if the texture image is low-pixels, then u_img &amp; v_img will not be int(ideally case). Eigen::Vector3f getColorBilinear(float u,float v) { auto u_img = u * (width-1); auto v_img = v * (height-1); Eigen::Vector2f u00(std::floor(u_img)*1.f,std::floor(v_img)*1.f); Eigen::Vector2f u10(std::ceil(u_img)*1.f,std::floor(v_img)*1.f); Eigen::Vector2f u01(std::floor(u_img)*1.f,std::ceil(v_img)*1.f); Eigen::Vector2f u11(std::ceil(u_img)*1.f,std::ceil(v_img)*1.f); float s = (u_img - u00.x()); float t = (v_img - u00.y()); Eigen::Vector3f u0 = lerp(s,getColor(u00.x()/width,u00.y()/height),getColor(u10.x()/width,u10.y()/height)); Eigen::Vector3f u1 = lerp(s,getColor(u01.x()/width,u01.y()/height),getColor(u11.x()/width,u11.y()/height)); Eigen::Vector3f color = lerp(t,u0,u1); return color; } Eigen::Vector3f lerp(float coefficient,Eigen::Vector3f a,Eigen::Vector3f b) { //return (coefficient * a + (1-coefficient) * b); return (a + coefficient*(b-a)); } For the code above, I have a few words to add: In opencv:Mat image;image.at&lt;&gt;(i,j) i–&gt;y j–&gt;xcolor order : BGRthe origin is at the upper-left corner Another one, Since have learned that one pixel can be seen as a little square. and the center of the pixel is (x + 0.5,y + 0.5); I tried this experiment using OpenCV, and found that : the result shows that they represent the same pixel. 12window.at&lt;cv::Vec3b&gt;(1,1)[1] = 255window.at&lt;cv::Vec3b&gt;(1,1.9)[1] = 255 References:[1]3D Math Primer for Graphics and Game Development 2nd Edition.[2]Fundamentals of Computer Graphics and 3rd Edition.[3]Unity+Shader入门精要[4]Unity3d Mannual[5]GAMES[6]scratchapixel","link":"/Graphics/Rendering/Rendering-Lighting-Shading-Texture/"},{"title":"PatterRecognition-C12-Continuous-Latent-Variables","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C12-Continuous-Latent-Variables/"},{"title":"PatterRecognition-C1-Introduction","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C1-Introduction/"},{"title":"PatterRecognition-C10-Approximate-Inference","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C10-Approximate-Inference/"},{"title":"PatterRecognition-C13-Sequential-Data","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C13-Sequential-Data/"},{"title":"PatterRecognition-C14-Combining-Models","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C14-Combining-Models/"},{"title":"PatterRecognition-C11-Sampling-Methods","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C11-Sampling-Methods/"},{"title":"PatterRecognition-C4-Linear-Models-For-Classification","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C4-Linear-Models-For-Classification/"},{"title":"PatterRecognition-C5-Neural-Networks","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C5-Neural-Networks/"},{"title":"PatterRecognition-C2-Probability-Distributions","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C2-Probability-Distributions/"},{"title":"PatterRecognition-C7-Sparse-Kernel-Machines","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C7-Sparse-Kernel-Machines/"},{"title":"PatterRecognition-C9-Mixture-Models-and-EM","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C9-Mixture-Models-and-EM/"},{"title":"PatterRecognition-C8-Graphical-Models","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C8-Graphical-Models/"},{"title":"PatterRecognition-C6-Kernel-Methods","text":"Keywords: Linear Model, Bayesian Model, Evidence Approximation, Bias-Variance Decomposition This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C6-Kernel-Methods/"}],"tags":[{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Geometry","slug":"Geometry","link":"/tags/Geometry/"},{"name":"MotionCaptureAnimation","slug":"MotionCaptureAnimation","link":"/tags/MotionCaptureAnimation/"},{"name":"Calculus","slug":"Calculus","link":"/tags/Calculus/"},{"name":"GameProgramming","slug":"GameProgramming","link":"/tags/GameProgramming/"},{"name":"Numerical Analysis","slug":"Numerical-Analysis","link":"/tags/Numerical-Analysis/"},{"name":"Regression","slug":"Regression","link":"/tags/Regression/"},{"name":"Sketh Facical Generation","slug":"Sketh-Facical-Generation","link":"/tags/Sketh-Facical-Generation/"},{"name":"Sketh Consolidatoin","slug":"Sketh-Consolidatoin","link":"/tags/Sketh-Consolidatoin/"},{"name":"CurveFitting of Sketches","slug":"CurveFitting-of-Sketches","link":"/tags/CurveFitting-of-Sketches/"},{"name":"Probability","slug":"Probability","link":"/tags/Probability/"},{"name":"RandomForest","slug":"RandomForest","link":"/tags/RandomForest/"},{"name":"Rendering","slug":"Rendering","link":"/tags/Rendering/"},{"name":"CPP","slug":"CPP","link":"/tags/CPP/"},{"name":"Variation","slug":"Variation","link":"/tags/Variation/"}],"categories":[{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"Graphics","slug":"Graphics","link":"/categories/Graphics/"},{"name":"Linear Algebra","slug":"Math/Linear-Algebra","link":"/categories/Math/Linear-Algebra/"},{"name":"Calculus","slug":"Math/Calculus","link":"/categories/Math/Calculus/"},{"name":"Geometry","slug":"Math/Geometry","link":"/categories/Math/Geometry/"},{"name":"Animation","slug":"Graphics/Animation","link":"/categories/Graphics/Animation/"},{"name":"GameProgramming","slug":"GameProgramming","link":"/categories/GameProgramming/"},{"name":"Numerical Analysis","slug":"Math/Numerical-Analysis","link":"/categories/Math/Numerical-Analysis/"},{"name":"System","slug":"GameProgramming/System","link":"/categories/GameProgramming/System/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/categories/MachineLearning/"},{"name":"PatternRecognition","slug":"MachineLearning/PatternRecognition","link":"/categories/MachineLearning/PatternRecognition/"},{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"Probability","slug":"Math/Probability","link":"/categories/Math/Probability/"},{"name":"Sketch Generation","slug":"Paper/Sketch-Generation","link":"/categories/Paper/Sketch-Generation/"},{"name":"Rendering","slug":"Graphics/Rendering","link":"/categories/Graphics/Rendering/"},{"name":"Language","slug":"Language","link":"/categories/Language/"},{"name":"Sketch Consolidation","slug":"Paper/Sketch-Consolidation","link":"/categories/Paper/Sketch-Consolidation/"},{"name":"Variation","slug":"Math/Variation","link":"/categories/Math/Variation/"},{"name":"Advanced CPP","slug":"Language/Advanced-CPP","link":"/categories/Language/Advanced-CPP/"}]}