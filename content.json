{"pages":[{"title":"","text":"AnimationTrack3DCharacterAnimation_Foundamental [Code]language : python(Numpy,Pytorch,Pytorch3d,Scipy)framework : [Games105]function: FK，IK(CCD, Jacobian), Motion Retargeting(A-Pose / T-Pose, same skeleton), Motion Interpolation, Motion Blend, State Machine, Motion Matching, Skinning(LBS), PD Control AI4Animation_Sebastian Starke [Code]language : c#, pythonframework : Unity, pytorchfunction: interact-animation learning from data SMPL [Video][Page]language : c#, pythonframework : Unity, Maya, UE, pytorchfunction: pose shape blending(with LBS,DQS skinning) / s3D pose and shape estimation from pictures Simulation and Animation Seminar GAMES Webinar 300 姚贺源-复杂动作控制策略的学习与生成(Learning and Generating Control Policies for Complex Actions) https://www.bilibili.com/video/BV1tz4y1A71z/?vd_source=12c3d0a1622797f9f65e6fd88ed2ec9c 徐霈-复杂动作控制策略的学习与生成 https://www.bilibili.com/video/BV1Mu4y1P7pc/?spm_id_from=333.999.0.0&amp;vd_source=12c3d0a1622797f9f65e6fd88ed2ec9c GAMES Webinar 272 敖腾隆-高质量数字人建模与驱动 https://www.bilibili.com/video/BV1qo4y187ex/?spm_id_from=333.999.0.0&amp;vd_source=12c3d0a1622797f9f65e6fd88ed2ec9c GAMES Webinar 272 张启煊-高质量数字人建模与驱动 https://www.bilibili.com/video/BV1as4y1P79P/?spm_id_from=333.999.0.0&amp;vd_source=12c3d0a1622797f9f65e6fd88ed2ec9c GAMES Webinar 243-模拟与动画专题-Fangzhou Hong-基于物理的角色动画与跨模态角色生成与动画 https://www.bilibili.com/video/BV1Xg41167Yo/?spm_id_from=333.999.0.0&amp;vd_source=12c3d0a1622797f9f65e6fd88ed2ec9c Others UnrealEngine_AnimationModule[C++]http://www.andreasaristidou.com/publications/papers/IK_survey.pdf\\UE_5.1\\Engine\\Source\\Runtime\\AnimationCore\\Private\\CCDIK.cpp\\UE_5.1\\Engine\\Source\\Runtime\\AnimationCore\\Private\\FABRIK.cpp \\UE_5.1\\Engine\\Plugins\\Experimental\\FullBodyIK\\Source\\FullBodyIK\\Private\\RigUnit_FullbodyIK.cpp\\UE_5.1\\Engine\\Plugins\\Experimental\\FullBodyIK\\Source\\FullBodyIK\\Private\\JacobianIK.cpp\\UE_5.1\\Engine\\Plugins\\Experimental\\FullBodyIK\\Source\\FullBodyIK\\Private\\JacobianSolver.cpp\\UE_5.1\\Engine\\Plugins\\Experimental\\FullBodyIK\\Source\\FullBodyIK\\Public\\FBIKConstraint.h https://skinning.org/https://rodolphe-vaillant.fr/entry/146/unreal-engine-skeletal-mesh-doc-sheet \\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Private\\SkeletalRenderCPUSkin.cpp\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Private\\GPUSkinCache.cpp\\UE_5.1\\Engine\\Shaders\\Private\\GpuSkinCacheComputeShader.usf(compute shader)\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Private\\GPUSkinVertexFactory.cpp\\UE_5.1\\Engine\\Shaders\\Private\\GpuSkinVertexFactory.ush(vertex shader) \\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Public\\SkeletalRenderPublic.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Classes\\Components\\SkinnedMeshComponent.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Classes\\Components\\SkeletalMeshComponent.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Classes\\Engine\\SkeletalMesh.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Public\\ReferenceSkeleton.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Public\\Rendering\\SkeletalMeshModel.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Public\\Rendering\\SkeletalMeshLODModel.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Public\\Rendering\\SkeletalMeshRenderData.h\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Public\\Rendering\\SkeletalMeshLODRenderData.h \\UE_5.1\\Engine\\Source\\Runtime\\Core\\Public\\Math\\DualQuat.h https://www.youtube.com/watch?v=ykQzmA6vs5chttps://www.youtube.com/watch?v=_sLnCqBaElI \\UE_5.1\\Engine\\Plugins\\Animation\\IKRig\\Source\\IKRigEditor\\Private\\RetargetEditor\\IKRetargetBatchOperation.cpp\\UE_5.1\\Engine\\Plugins\\Animation\\IKRig\\Source\\IKRig\\Private\\Retargeter\\IKRetargetProcessor.cpp\\UE_5.1\\Engine\\Plugins\\Animation\\IKRig\\Source\\IKRig\\Public\\Retargeter\\IKRetargeter.h MotionMatching2016: https://www.gdcvault.com/play/1023280/Motion%20Matching%20and%20The%20RoadMotionMathcing2020: https://www.youtube.com/watch?v=lN9pXZzR3YsMotionMathcingUE5: https://www.youtube.com/watch?v=rLEWEQjTOb8 \\UE_5.1\\Engine\\Plugins\\Experimental\\Animation\\PoseSearch\\Source\\Runtime\\Private\\AnimNode_MotionMatching.cpp\\UE_5.1\\Engine\\Plugins\\Experimental\\Animation\\PoseSearch\\Source\\Runtime\\Private\\PoseSearchLibrary.cpp\\UE_5.1\\Engine\\Plugins\\Experimental\\Animation\\PoseSearch\\Source\\Runtime\\Private\\PoseSearch.cpp\\Epic Games\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Private\\Animation\\AnimNode_Inertialization.cpp MotionBlendingUE: https://www.youtube.com/watch?v=DN6B7rJ5Mps&amp;t=270s\\UE_5.1\\Engine\\Source\\Runtime\\Engine\\Classes\\Animation\\BlendSpace.h","link":"/Animation/index.html"},{"title":"About Me","text":"Currently I’m a Game Client Programmer. I have worked in Garena, participating in developing FreeFire(2021.11-2022.9) and Tencent, participating in developing Arena of Valor(2020.9-2021.1,Intern). I have received my Master’s degree in Computer Science and Technology from Beijing Forestry University. My research interest includes Computer Animation, Machine Learning. You can find my CV here: Anna’s Curriculum Vitae. Email / Github / CSDN","link":"/about/index.html"},{"title":"","text":"","link":"/CV/index.html"},{"title":"","text":"Useful Animation Videos or Materials: Books: 3D Math Primer for Graphics and Game Development 2nd Edition Computer Animation Algorithms and Techniques Useful MachineLearning Materials: Books: A First Course in Probability Linear Algebra and Its Application Pattern Recognition and Machine Learning Useful CS Materials: Books: C++ Primer [&gt;&gt;Reading Notes] Introduction to Algorithms Useful CG Websites: https://www.3dgep.com/ Useful Latex Websites: https://garsia.math.yorku.ca/MPWP/LATEXmath/latexsym.html","link":"/extra/index.html"},{"title":"","text":"","link":"/work/index.html"},{"title":"","text":"Study of Video-driven 2D Character Animation Generation Method Abstract Video-driven animation has always been a hot and challenging topic in the field of computer animation. We propose a method of mapping a sequence of human skeletal keypoints in a video onto a two-dimensional character to generate 2D character animation. For a given two-dimensional character picture, we extract the motion of real human in video data, driving the character deformation. We analyze common two-dimensional human body movements, classify the basic posture of the human body, realize the recognition of skeleton posture based on back propagation network, capture human body motion by automatically tracking the position of the human skeleton keypoints coordinates in the video and redirect the motion data to a 2D character. Compared with the traditional method, our work is less affected by video data illumination and background complexity. We calibrate human body motion in videos to a 2D character according to the skeleton topology to avoid motion distortion caused by the difference in skeleton size and ratio. The experimental results show that the proposed algorithm can generate the motion of two-dimensional characters based on the motion of human characters in video data. The animation is natural and smooth, and the algorithm has strong robustness. Conclution and Future work This paper proposes a method of mapping human motion data in a video onto a two-dimensional character to generate character animation. We statistically analyze common two-dimensional human body movements, classify the basic posture of the human body; design and implement the method of human body posture recognition based on the skeleton information on images and videos; propose a geometric calibration method based on the tree structure to correct motion reorientation of the bones, obtaining a good skeleton-driven deformation effect, and generating high quality animation in the same posture. This method can be used to auto-produced animation, which need fewer user interaction. Although our method maps the sequence of human skeletal postures in the video onto a two-dimensional character, resulting in a high quality animation, there is some room for improvement: 1) Scope of application. Our method is only for 2D human characters, cannot be extended to other types of images such as animals and plants; 2) Physical simulation. We didn’t take some phenomena such as hair, clothes into consideration, thus these deformation was not natural enough, as shown in Fig.14(e), the man’s hair deforms in a stiff way. In the future, we will try to improve the issues discussed above, an approach for animals and plants animation will be adapted by this method, physical simulation will be used to get higher quality results. On the other hand, we will also integrate all methods into one interactive animation system.","link":"/projects/index.html"},{"title":"Life Photo","text":"Cooking garlic fried chicken korean fried chicken spicy boston lobster braised crucian carp spicy crayfish beer braised duck sweet and sour pork(guo bao rou) fried rice with egg golden enoki mushroom beef soup fried fensi durian melaleuca bean sprouts(get-lean diet)","link":"/life/index.html"},{"title":"","text":"Video-Driven 2D Character Animation Qinran YIN & Weiqun CAO Chinese Journal of Electronics, vol. 30, no. 6, pp. 1038-1048, 2021 [Project page][Paper] [Code]","link":"/publication/index.html"}],"posts":[{"title":"Algebra-C2-Matrix-Algebra","text":"Keywords: Inverse, LU Factorization, Homogeneous Coordinates, Perspective Projections, Rank This is the Chapter2 ReadingNotes from book Linear Algebra and its Application. Matrix Operation$$\\begin{aligned}&amp;AB \\neq BA\\\\&amp;AB = AC,it’s \\space not \\space true \\space B = C\\\\&amp;AB = 0, it’s \\space not \\space true \\space A = 0 \\space or \\space B = 0\\\\&amp;(A^T)^T = A\\\\&amp;(A+B)^T = A^T+B^T\\\\&amp;(AB)^T = B^TA^T\\\\&amp;A(BC) = (AB)C\\end{aligned}$$ The fastest way to obtain $AB$ on a computer depends on the way in which the computer stores matrices in its memory. The standard high-performance algorithms, such as in LAPACK, calculate $AB$ by columns, as in our definition of the product. (A version of LAPACK written in C++ calculates $AB$ by rows.) The definition of $AB$ lends itself well to parallel processing on a computer. The columns of $B$ are assigned individually or in groups to different processors, which independently and hence simultaneously compute the corresponding columns of $AB$. The Inverse of MatrixAn $n \\times n$ matrix $A$ is said to be invertible if there is an $n \\times n$ matrix $C$ such that $$CA = I \\space and \\space AC = I$$ A matrix that is not invertible is sometimes called a singular matrix, and an invertible matrix is called a nonsingular matrix.一个不可逆的矩阵有时被称为奇异矩阵，一个可逆的矩阵被称为非奇异矩阵 $$\\begin{aligned}&amp; A = \\begin{bmatrix}a &amp; b \\\\ c &amp; d\\end{bmatrix}. \\spaceif \\space ad - bc \\neq 0, A \\space is\\space invertible\\\\&amp; A^{-1} = \\frac{1}{ad-bc}\\begin{bmatrix}d &amp; -b \\\\ -c &amp; a\\end{bmatrix}\\\\a&amp; d - bc = 0 , A\\space is \\space not \\space invertible\\\\&amp; det A = ad - bc(行列式)\\end{aligned}$$ If $A$ is an invertible $n \\times n$ matrix, then for each $\\vec{b}$ in $R^n$, the equation $A\\vec{x} = \\vec{b}$ has the unique solution $\\vec{x} = A^{-1}\\vec{b}$. $$(A^{-1})^{-1} = A\\\\(AB)^{-1} = B^{-1}A^{-1}\\\\(A^T)^{-1} = (A^{-1})^T\\\\$$ An $n \\times n$ matrix $A$ is invertible if and only if $A$ is row equivalent to $I_n$, and in this case, any sequence of elementary row operations that reduces A to $I_n$ also transforms $I_n$ into $A^{-1}$. For example: transform $E_1$ to $I$, get $I$ to $E_1^{-1}$:$$E_1 = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -4 &amp; 0 &amp; 1\\end{bmatrix},E_1^{-1} = \\begin{bmatrix}1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 4 &amp; 0 &amp; 1\\end{bmatrix}$$ An algorithm for Finding $A^{-1}$Row reduce the augmented matrix $\\begin{bmatrix}A &amp; I\\end{bmatrix}$. If $A$ is row equivalent to $I$, then $\\begin{bmatrix}A &amp; I\\end{bmatrix}$ is row equivalent to $\\begin{bmatrix}I &amp; A^{-1}\\end{bmatrix}$. Other wise, A does not have an inverse. For example: Find the Inverse of the matrix A = $\\begin{bmatrix}0 &amp; 1 &amp; 2\\\\1 &amp; 0 &amp; 3\\\\4 &amp; -3 &amp; 8\\end{bmatrix}$, if it exists. $$\\begin{bmatrix}A &amp; I\\end{bmatrix} =\\begin{bmatrix}0 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; 3 &amp; 0 &amp; 1 &amp; 0\\\\4 &amp; -3 &amp; 8 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; \\frac{-9}{2} &amp; 7 &amp; \\frac{-3}{2}\\\\0 &amp; 1 &amp; 0 &amp; -2 &amp; 4 &amp; -1\\\\0 &amp; 0 &amp; 1 &amp; \\frac{3}{2} &amp; -2 &amp; \\frac{1}{2}\\end{bmatrix}\\longrightarrowA^{-1} = \\begin{bmatrix}\\frac{-9}{2} &amp; 7 &amp; \\frac{-3}{2}\\\\ -2 &amp; 4 &amp; -1\\\\ \\frac{3}{2} &amp; -2 &amp; \\frac{1}{2}\\end{bmatrix}$$ Characterizations of Inversible Matrix注意，这些性质的应用必须是n $\\times$ n的矩阵 Let $A$ be a square $n \\times n$ matrix. Then the following statements are equivalent. That is, for a given $A$, the statements are either all true or all false.a. $A$ is an invertible matrix.b. $A$ is row equivalent to the $n \\times n$ identity matrix.c. $A$ has $n$ pivot positions.d. The equation $Ax = 0$ has only the trivial solution.e. The columns of $A$ form a linearly independent set.f. The linear transformation $x \\rightarrow Ax$ is one-to-one.g. The equation $Ax = b$ has at least one solution for each $b$ in $R^n$.h. The columns of $A$ span $R^n$.i. The linear transformation $x \\rightarrow Ax$ maps $R^n$ onto $R^n$.j. There is an $n \\times n$ matrix $C$ such that $CA = I$.k. There is an $n \\times n$ matrix $D$ such that $AD = I$.l. $A^T$ is an invertible matrix. Invertible Linear TransformationsLet $T :R^n\\rightarrow R^n$ be a linear transformation and let $A$ be the standard matrix for $T$. Then $T$ is invertible if and only if $A$ is an invertible matrix. In that case, the linear transformation $S$ given by $S(x) = A^{-1}x $ is the unique function satisfying equations (1) and (2) $$S(T(x)) = x, x \\in R^n\\tag{1}$$$$T(S(x)) = x, x \\in R^n\\tag{2}$$ 由于计算机的精度问题，you might occasionally encounter a “nearly singular” or illconditioned matrix—an invertible matrix, that can become singular if some of its entries are changed ever so slightly.In this case, row reduction may produce fewer than $n$ pivot positions, as a result of roundoff error.Also, roundoff error can sometimes make a singular matrix appear to be invertible. Partitioned Matrices$$A =\\left[ \\begin{array}{ccc|cc|c} 3 &amp; 0 &amp; -1 &amp; 5 &amp; 9 &amp; -2 \\\\ -5 &amp; 2 &amp; 4 &amp; 0 &amp; -3 &amp; 1 \\\\ \\hline -8 &amp; -6 &amp; 3 &amp; 1 &amp; 7 &amp; -4 \\end{array}\\right]$$can also be written as the $2 \\times 3$ partitioned (or block) matrix $$A = \\begin{bmatrix}A_{11} &amp; A_{12} &amp; A_{13} \\\\A_{21} &amp; A_{22} &amp; A_{23}\\end{bmatrix}$$ Multiplication of Partitioned Matrices$$A : m \\times n, B : n \\times p\\\\AB = \\begin{bmatrix}col_1(A) &amp; col_2(A) &amp; \\cdots &amp; col_n(A)\\end{bmatrix}\\begin{bmatrix}row_1(B) \\\\ row_1(B) \\\\ \\cdots \\\\ row_n(B)\\end{bmatrix}\\tag{1}$$ Inverse of Partitioned Matrices$$assume\\space A_{11}: p\\times p, A_{22}: q\\times q, A \\space invertible.A = \\begin{bmatrix}A_{11} &amp; A_{12}\\\\0 &amp; A_{22}\\end{bmatrix}\\tag{bolck upper triangular}$$ $$求逆过程如下：\\\\A_{11}B_{11} + A_{12}B_{21} = I_p\\tag{1}$$$$A_{11}B_{12} + A_{12}B_{22} = 0\\tag{2}$$$$A_{22}B_{21} = 0\\tag{3}$$$$A_{22}B_{22} = I_{q}\\tag{4}$$$$(1)(2)(3)(4)\\rightarrowA^{-1} = \\begin{bmatrix}A_{11} &amp; A_{12}\\\\0 &amp; A_{22}\\end{bmatrix} ^ {-1}=\\begin{bmatrix}A_{11}^{-1} &amp;-A_{11}^{-1}A_{12}A_{22}^{-1}\\\\0 &amp; A_{22}^{-1}\\end{bmatrix}$$ 分块矩阵提高计算机计算效率 When matrices are too large to fit in a computer’s high-speed memory, partitioning permits the computer to work with only two or three submatrices at a time. Matrix Factorizations 矩阵分解The LU FactorizationAsume that $A$ is an $m \\times n$ matrix that can be row reduced to echelon form, without row interchanges. Then $A$ can be written in the form $A = LU$ , where $L$ is an $m \\times m$ lower triangular matrix with $1’s$ on the diagonal and $U$ is an $m \\times n$ echelon form of $A$. The matrix $L$ is invertible and is called a unit lower triangular matrix. 单位下三角矩阵$$A = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ * &amp; 1 &amp; 0 &amp; 0\\\\ * &amp; * &amp; 1 &amp; 0\\\\ * &amp; * &amp; * &amp; 1\\end{bmatrix}\\begin{bmatrix}\\blacksquare &amp; * &amp; * &amp; * &amp; *\\\\0 &amp; \\blacksquare &amp; * &amp; * &amp; *\\\\0 &amp; 0 &amp; 0 &amp; \\blacksquare &amp; *\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ 思考，这样分解有什么好处？ $$A\\vec{x} = \\vec{b}\\longrightarrowL(U\\vec{x}) = \\vec{b},\\\\let\\space \\vec{y} = U\\vec{x}\\Rightarrow\\\\\\begin{cases}L\\vec{y} = \\vec{b}\\\\U\\vec{x} = \\vec{y}\\tag{2}\\end{cases}$$ 分解后好求解，因为LU都是三角矩阵 For example： $$A =\\begin{bmatrix}3 &amp; -7 &amp; -2 &amp; 2\\\\-3 &amp; 5 &amp; 1 &amp; 0\\\\6 &amp; -4 &amp; 0 &amp; -5\\\\-9 &amp; 5 &amp; -5 &amp; 12\\end{bmatrix}=\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\-1 &amp; 1 &amp; 0 &amp; 0\\\\2 &amp; -5 &amp; 1 &amp; 0\\\\-3 &amp; 8 &amp; 3 &amp; 1\\end{bmatrix}\\begin{bmatrix}3 &amp; -7 &amp; -2 &amp; 2\\\\0 &amp; -2 &amp; -1 &amp; 2\\\\0 &amp; 0 &amp; -1 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; -1\\end{bmatrix}= LU,Solve A\\vec{x} = \\vec{b}, where\\spaceb =\\begin{bmatrix}-9 \\\\5 \\\\7 \\\\11\\end{bmatrix}$$Solution：$$\\begin{bmatrix}L &amp; \\vec{b}\\end{bmatrix} =\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; -9\\\\-1 &amp; 1 &amp; 0 &amp; 0 &amp; 5\\\\2 &amp; -5 &amp; 1 &amp; 0 &amp; 7\\\\-3 &amp; 8 &amp; 3 &amp; 1 &amp; 11\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; -9\\\\0 &amp; 1 &amp; 0 &amp; 0 &amp; -4\\\\0 &amp; 0 &amp; 1 &amp; 0 &amp; 5\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; 1\\end{bmatrix} =\\begin{bmatrix}I &amp; \\vec{y}\\end{bmatrix}\\\\\\begin{bmatrix}U &amp; \\vec{y}\\end{bmatrix} =\\begin{bmatrix}3 &amp; -7 &amp; -2 &amp; 2 &amp; -9\\\\0 &amp; -2 &amp; -1 &amp; 2 &amp; -4\\\\0 &amp; 0 &amp; -1 &amp; 1 &amp; 5\\\\0 &amp; 0 &amp; 0 &amp; -1 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 3\\\\0 &amp; 1 &amp; 0 &amp; 0 &amp; 4\\\\0 &amp; 0 &amp; 1 &amp; 0 &amp; -6\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; -1\\end{bmatrix} =\\begin{bmatrix}I &amp; \\vec{x}\\end{bmatrix}$$ The LU Factorization AlgorithmSuppose $A$ can be reduced to an echelon form $U$ using only row replacements that add a multiple of one row to another row below it. In this case, there exist unit lower triangular elementary matrices $E_1,\\cdots,E_p$ such that: $$E_p\\cdots E_1A = U\\tag{3}$$Then$$A = (E_p\\cdots E_1)^{-1}U = LU$$where$$L = (E_p\\cdots E_1)^{-1}\\tag{4}$$ 从推导过程可以看出，$A$经历了哪些初等行变换，$L$也同时经历，最终$A$变成了$U$，$L$变成了$I$ It can be shown that products and inverses of unit lower triangular matrices are also unit lower triangular.Thus $L$ is unit lower triangular. For example:Find the $LU$ factorization of $A$.$$A =\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\-4 &amp; -5 &amp; 3 &amp; -8 &amp; 1\\\\2 &amp; -5 &amp; -4 &amp; 1 &amp; 8\\\\-6 &amp; 0 &amp; 7 &amp; -3 &amp; 1\\end{bmatrix}$$ Solution：$$A = \\begin{bmatrix}\\bbox[border:2px solid red]{2} &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\\\bbox[border:2px solid red]{-4} &amp; -5 &amp; 3 &amp; -8 &amp; 1\\\\\\bbox[border:2px solid red]2 &amp; -5 &amp; -4 &amp; 1 &amp; 8\\\\\\bbox[border:2px solid red]{-6} &amp; 0 &amp; 7 &amp; -3 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\0 &amp; \\bbox[border:2px solid red]3 &amp; 1 &amp; 2 &amp; -3\\\\0 &amp; \\bbox[border:2px solid red]{-9} &amp; -3 &amp; -4 &amp; 10\\\\0 &amp; \\bbox[border:2px solid red]{12} &amp; 4 &amp; 12 &amp; -5\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\0 &amp; 3 &amp; 1 &amp; 2 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; \\bbox[border:2px solid red]2 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; \\bbox[border:2px solid red]4 &amp; 7\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; 4 &amp; -1 &amp; 5 &amp; -2\\\\0 &amp; 3 &amp; 1 &amp; 2 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; 2 &amp; 1\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp;\\bbox[border:2px solid red]5\\end{bmatrix} = U$$$$\\begin{bmatrix}2\\\\-4\\\\2\\\\6\\end{bmatrix}\\begin{bmatrix}\\\\3\\\\-9\\\\12\\end{bmatrix}\\begin{bmatrix}\\\\\\\\2\\\\4\\end{bmatrix}\\begin{bmatrix}\\\\\\\\\\\\5\\end{bmatrix}\\\\\\div 2\\downarrow\\div 3\\downarrow\\div 2\\downarrow\\div 5\\downarrow\\\\\\begin{bmatrix}1 &amp; &amp; &amp; \\\\-2 &amp; 1 &amp; &amp; \\\\1 &amp; -3 &amp; 1 &amp; \\\\3 &amp; 4 &amp; 2 &amp; 1\\end{bmatrix} = L$$ More About QR Factorization &gt;&gt; The Leontief Input–Output Model(Omit)Applications to Computer GraphicsHomogeneous CoordinatesThe mathematics of computer graphics is intimately connected with matrix multiplication. Unfortunately, translating an object on a screen does not correspond directly to matrix multiplication because translation is not a linear transformation. The standard way to avoid this difficulty is to introduce what are called homogeneous coordinates. $$translation : (x,y)\\mapsto(x + h, y + k)\\\\homogeneous-translation: (x,y,1)\\mapsto(x + h, y + k, 1)\\\\matrix-multiplication:\\begin{bmatrix}1 &amp; 0 &amp; h\\\\0 &amp; 1 &amp; k\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix} x\\\\y\\\\1\\end{bmatrix}=\\begin{bmatrix}x + h\\\\y+k\\\\1\\end{bmatrix}$$ Any linear transformation on $R^2$ is represented with respect to homogeneous coordinates by a partitioned matrix of the form $\\begin{bmatrix}A &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix}$, where $A$ is a $2 \\times 2$ matrix. Typical examples are: $$\\begin{bmatrix}\\cos\\psi &amp; -\\sin\\psi &amp; 0\\\\\\sin\\psi &amp; \\cos\\psi &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix},\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix},\\begin{bmatrix}s &amp; 0 &amp; 0\\\\0 &amp; t &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}$$ Homogeneous 3D Coordinates$(x,y,z,1)$ are homogeneous coordinates for the point $(x,y,z)$ in $R^3$. In general, $(X,Y,Z,H)$ are homogeneous coordinates for $(x,y,z)$ if $H \\neq 0$ $$x = \\frac{X}{H}, y = \\frac{Y}{H}, z = \\frac{Z}{H}$$ For example: Rotation about the y-axis through an angle of 30&deg; Solution: First, construct the $3\\times 3$ matrix for the rotation. The vector $\\vec{e_1}$ rotates down toward the negative $z-axis$, stopping at $(\\cos 30, 0, -\\sin 30) = (\\frac{\\sqrt{3}}{2}, 0 , -0.5)$. The vector $\\vec{e_2}$ on the y-axis does not move. The vector $\\vec{e_3}$ on the $z-axis$ rotates down toward the positive $x-axis$, stopping at $(\\sin 30, 0, \\cos 30) = (0.5, 0, \\frac{\\sqrt{3}}{2})$. So the rotation matrix is$$\\begin{bmatrix}\\frac{\\sqrt{3}}{2} &amp; 0 &amp; 0.5\\\\0 &amp; 1 &amp; 0\\\\-0.5 &amp; 0 &amp; \\frac{\\sqrt{3}}{2}\\end{bmatrix}$$ Perspective ProjectionsFor example: let $xy-plane$ represent the computer screen, and imagine that the eye of a viewer is along the positive $z-axis$, at point $(0,0,d)$.A perspective projection maps each point $(x,y,z)$ onto an image point $(x^{\\ast}, y^{\\ast}, 0)$. Solution: induce projection-matrix as follows： $$\\frac{x^{\\ast}}{d} = \\frac{x}{d-z}\\rightarrowx^{\\ast} = \\frac{dx}{d-z} = \\frac{x}{1-\\frac{z}{d}}$$$$Similarly\\rightarrow y^{\\ast} = \\frac{y}{1-\\frac{z}{d}}$$$$(x,y,z,1)\\xrightarrow{projection} (\\frac{x}{1-\\frac{z}{d}}, \\frac{y}{1-\\frac{z}{d}}, 0 ,1)\\sim(x,y,0,1-\\frac{z}{d})$$$$p\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}=\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; -1/d &amp; 1\\end{bmatrix}\\begin{bmatrix}x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}=\\begin{bmatrix}x \\\\ y \\\\ 0 \\\\ 1-\\frac{z}{d}\\end{bmatrix}$$ More about Rotation matrix &amp; Perspective projection &gt;&gt; SubSpace of $R^n$ A subspace of $R^n$ is any set $H$ in $R^n$ that has three properties: The Zero Vector is in $H$. For each $\\vec{u}$ and $\\vec{v}$ in $H$, the sum $\\vec{u} + \\vec{v}$ is in $H$. For each $\\vec{u}$ in $H$ and each scalar $c$, the vector $c\\vec{u}$ is in $H$. Column Space and Null Space of a MatrixThe column space of a matrix $A$ is the set $ColA$ of all linear combinations of the columns of $A$. if $A = \\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}$, with the columns in $R^m$, then $Col A$ is the same as $Span \\set{\\vec{a_1} \\cdots \\vec{a_n}}$. For example: Let A = $\\begin{bmatrix}1 &amp; -3 &amp; -4 \\\\-4 &amp; 6 &amp; -2 \\\\-3 &amp; 7 &amp; 6\\end{bmatrix}$ and b = $\\begin{bmatrix}3 \\\\3 \\\\-4\\end{bmatrix}$. Determine whether $\\vec{b}$ is in the column space of $A$. Solution: $$\\begin{bmatrix}1 &amp; -3 &amp; -4 &amp; 3\\\\-4 &amp; 6 &amp; -2 &amp; 3\\\\-3 &amp; 7 &amp; 6 &amp; -4\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -3 &amp; -4 &amp; 3\\\\0 &amp; -6 &amp; -18 &amp; 15\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\\\有解，意味着b是A列向量的线性组合\\thereforeb \\subseteq Col A$$ Basis for a Subspace A basis for a subspace $H$ of $R^n$ is a linearly independent set in $H$ that spans $H$. $$\\vec{e_1} = \\begin{bmatrix}1 \\\\ 0 \\\\ \\cdots \\\\ 0\\end{bmatrix},\\vec{e_2} = \\begin{bmatrix}0 \\\\ 1 \\\\ \\cdots \\\\ 0\\end{bmatrix},\\cdots,\\vec{e_n} = \\begin{bmatrix}0 \\\\ 0 \\\\ \\cdots \\\\ 1\\end{bmatrix}$$ the set {$\\vec{e_1}, \\cdots, \\vec{e_n}$} is called the standard basis for $R^n$. For example: Find a basis for the null space of the matrix. $$A = \\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4\\end{bmatrix}$$ 找到矩阵零空间的一组基如下：$$\\begin{bmatrix}A &amp; \\vec{0}\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -2 &amp; 0 &amp; -1 &amp; 3 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix},\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\\\x_4\\\\x_5\\end{bmatrix}=\\begin{bmatrix}2x_2 + x_4 - 3x_5\\\\x_2\\\\-2x_4 + 2x_5\\\\x_4\\\\x_5\\end{bmatrix}=x_2\\begin{bmatrix}2\\\\1\\\\0\\\\0\\\\0\\\\\\end{bmatrix} + x4\\begin{bmatrix}1\\\\0\\\\-2\\\\1\\\\0\\end{bmatrix} + x_5\\begin{bmatrix}-3\\\\0\\\\2\\\\0\\\\1\\end{bmatrix}= x_2\\vec{u} + x_4\\vec{v} + x_5\\vec{w}\\\\{\\vec{u}, \\vec{v}, \\vec{w}} 是基向量$$ Dimension and RankMore About Dimension and Rank is in C4 &gt;&gt; Coordinate SystemsSuppose the set $\\beta = \\set{\\vec{b_1},\\cdots\\cdots, \\vec{b_p}}$ is a basis for a SubSpace $H$. For each $\\vec{x}$ in $H$, the coordinates of $\\vec{x}$ relative to the basis $\\beta$ are the weights $c_1, \\cdots, c_p$ such that $\\vec{x} = c_1\\vec{b_1} + \\cdots + c_p\\vec{b_p}$, and the vector in $R^p$$$[\\vec{x}]_\\beta =\\begin{bmatrix}c_1\\\\\\cdots\\\\c_p\\end{bmatrix}$$is called the coordinate vector of $\\vec{x}$(relative to $\\beta$) or the $\\beta-$ coordinate vector of $\\vec{x}$ The Dimension of a Subspace The rank of a matrix $A$, denoted by $rank A$, is the dimension of the column space of $A$ Determin the rank of the matrix$$A\\sim\\begin{bmatrix}2 &amp; 5 &amp; -3 &amp; -4 &amp; 8\\\\0 &amp; -3 &amp; 2 &amp; 5 &amp; -7\\\\0 &amp; 0 &amp; 0 &amp; 4 &amp; -6\\\\\\end{bmatrix}矩阵有3个pivot-columns，所以rankA = 3$$ If a matrix $A$ has $n$ columns, then $rank A + dim Nul A = n$. Rank and the Invertible Matrix Theorem Let $A$ be an $n \\times n$ matrix. Then the following statements are each equivalent to the statement that $A$ is an invertible matrix.m. The columns of $A$ form a basis of $R^n$:n. $Col A = R^n$o. $dim Col A = n$p. $rank A = n$q. $Nul A = {0}$r. $dim Nul A = 0$","link":"/Math/Linear-Algebra/Algebra-C2-Matrix-Algebra/"},{"title":"Algebra-C3-Determinants","text":"Keywords: Determinants, Linear Transformation, Area or Volume, Cramer’s Rule This is the Chapter3 ReadingNotes from book Linear Algebra and its Application. Introduction to Determinants If $A$ is a triangular matrix, then det $A$ is the product of the entries on the main diagonal of $A$. The determinant of an $n \\times n$ matrix $A$ can be computed by a cofactor expansion across any row or down any column. The expansion across the $i-th$ row using the cofactors is $$\\begin{aligned}det A &amp;= a_{i1}C_{i1} + a_{i2}C_{i2} + \\cdots + a_{in}C_{in}\\\\C_{ij} &amp;= (-1)^{i+j}detA_{i,j}\\end{aligned}$$ 在计算机中是怎么计算行列式的？Yet it would be impossible to calculate a $25 \\times 25$ determinant by cofactor expansion.In general, a cofactor expansion requires more than $n!$ multiplications, and $25!$ is approximately $1.5 * 10 ^ {25}$ If a computer performs one trillion multiplications per second, it would have to run for more than $500,000 years$ to compute a $25 \\times 25$ determinant by this method. Fortunately, there are faster methods, see the next chapter. Properties of Deternimants Let $A$ be a square matrix.a. If a multiple of one row of $A$ is added to another row to produce a matrix $B$, then $det B = det A$.b. If two rows of $A$ are interchanged to produce $B$, then $det B = - det A$.c. If one row of $A$ is multiplied by $k$ to produce $B$, then $det B = k * det A$. For example: Compute $det A$, where $A = \\begin{bmatrix}2 &amp; -8 &amp; 6 &amp; 8\\\\ 3 &amp; -9 &amp; 5 &amp; 10\\\\ -3 &amp; 0 &amp; 1 &amp; -2\\\\1 &amp; -4 &amp; 0 &amp; 6\\end{bmatrix}$ Solution: 求行列式如下：$$det A = 2\\left|\\begin{array}{} 1 &amp; -4 &amp; 3 &amp; 4 \\\\ 3 &amp; -9 &amp; 5 &amp; 10\\\\ -3 &amp; 0 &amp; 1 &amp; -2\\\\ 1 &amp; -4 &amp; 0 &amp; 6\\end{array}\\right|=2\\left|\\begin{array}{} 1 &amp; -4 &amp; 3 &amp; 4 \\\\ 0 &amp; 3 &amp; -4 &amp; -2\\\\ 0 &amp; 0 &amp; -6 &amp; 2\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right|=2\\cdot (1)\\cdot(3)\\cdot(-6)\\cdot(1)= -36$$ A square matrix $A$ is invertible if and only if $det A \\neq$ 0 (因为阶梯式下，对角线上如果有0，矩阵肯定不可逆) $$det A = \\begin{cases}(-1)^r \\cdot (product\\space of \\space pivots\\space in \\space U) &amp; when \\space A \\space is\\space invertible\\\\0 &amp; when \\space A \\space is\\space not \\space invertible\\end{cases}\\tag{1}\\\\U = \\begin{bmatrix}\\blacksquare &amp; * &amp; * &amp; * \\\\0 &amp; \\blacksquare &amp; * &amp; * \\\\0 &amp; 0 &amp; \\blacksquare &amp; * \\\\0 &amp; 0 &amp; 0 &amp; \\blacksquare\\end{bmatrix}$$ $r$: interchange operations in the process of matrix $A \\rightarrow U$ . Most computer programs that compute $det A$ for a general matrix $A$ use the method of formula (1) above. It can be shown that evaluation of an $n \\times n$ determinant using row operations requires about $2n^3 / 3$ arithmetic operations. Any modern microcomputer can calculate a $25 \\times 25$ determinant in a fraction of a second, since only about $10,000$ operations are required. Column Operations If $A$ is an $n \\times n$ matrix, then $det A^T = det A$ Determinants and Matrix Products If $A$ and $B$ are $n \\times n$ matrices, then $det AB = detA \\cdot detB$. A Linearity Property of the Determinant Function$$A = \\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{a_{j-1}} &amp; \\vec{x}&amp; \\vec{a_{j+1}} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}\\\\T(\\vec{x}) = det \\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{a_{j-1}} &amp; \\vec{x}&amp; \\vec{a_{j+1}} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}\\\\T(c\\vec{x}) = cT(\\vec{x})\\\\T(\\vec{u}+\\vec{v}) = T(\\vec{u}) + T(\\vec{v})$$ Cramer’s Rule, Volume, Linear TransformationCramer’s Rule（克拉默法则）Let A be an invertible $n \\times n$ matrix. For any $\\vec{b}$ in $R^n$, the unique solution $\\vec{x}$ of $A\\vec{x} = \\vec{b}$ has entries given by $$\\vec{x_i} = \\frac{det A_i(\\vec{b})}{det A}, i = 1, 2, \\cdots, n$$$$det A_i(\\vec{b}) =\\begin{bmatrix}\\vec{a_1} &amp; \\cdots &amp; \\vec{b} &amp; \\cdots &amp; \\vec{a_n}\\end{bmatrix}第i列的向量改成了\\vec{b}向量$$ Application to EngineeringLaplace transforms: This approach converts an appropriate system of linear differential equations into a system of linear algebraic equations whose coefficients involve a parameter.（只是提一嘴拉普拉斯变换） For example: Consider the following system in which $s$ is an unspecified parameter. Determine the values of $s$ for which the system has a unique solution, and use Cramer’s rule to describe the solution. $$\\begin{cases}3s x_1 - 2x_1 = 4\\\\-6x_1 + sx_2 = 1\\end{cases}$$ Solution：$$A = \\begin{bmatrix}3s &amp; -2\\\\-6 &amp; s\\end{bmatrix},A_1(\\vec{b}) = \\begin{bmatrix}4 &amp; -2\\\\1 &amp; s\\end{bmatrix},A_2(\\vec{b}) = \\begin{bmatrix}3s &amp; 4\\\\-6 &amp; 1\\end{bmatrix}$$Since,$$det A = 3s^2-12 = 3(s+2)(s-2)$$so,$$unique-solution: s \\neq \\pm2\\\\x_1 = \\frac{det A_1(\\vec{b})}{det A} = \\frac{4s+2}{3(s+2)(s-2)},x_1 = \\frac{det A_2(\\vec{b})}{det A} = \\frac{s+8}{(s+2)(s-2)}$$ A Formula for $A^{–1}$let $A$ be an invertible $n \\times n$ matrix. Then$$A^{-1} = \\frac{1}{det A} adj A$$The adjugate matrix is the transpose of the matrix of cofactors For example: Find the inverse matrix of $A = \\begin{bmatrix}2 &amp; 1 &amp; 3\\\\1 &amp; -1 &amp; 1\\\\1 &amp; 4 &amp; 2\\end{bmatrix}$ Solution: $$C_{11} = + \\left|\\begin{array}{} -1 &amp; 1 \\\\ 4 &amp; -2\\end{array}\\right| = -2 , C_{12} = 3, C_{13} = 5,C_{21} = 14, C_{22} = -7,C_{23} = -7$$$$adj A =\\begin{bmatrix}-2 &amp; 14 &amp; 4\\\\3 &amp; -7 &amp; 1\\\\5 &amp; -7 &amp; 3\\end{bmatrix}接下来套公式即可$$ 在计算机中，For a larger $n \\times n$ matrix (real or complex), Cramer’s rule is hopelessly inefficient.Computing just one determinant takes about as much work as solving $Ax = b$ by row reduction. Determinants as Area or Volume If $A$ is a $2 \\times 2$ matrix, the area of the parallelogram（平行四边形） determined by the columns of $A$ is $|detA|$.If $A$ is a $3 \\times 3$ matrix, the volume of the parallelepiped（平行六面体） determined by the columns of $A$ is $|detA|$. $$\\left|{}det\\begin{bmatrix}a &amp; 0 \\\\ 0 &amp; d\\end{bmatrix}\\right|=\\left|ad\\right| = {area \\space of \\space rectangle}$$ For example: Calculate the area of the parallelogram determined by the points $(-2,-2),(0,3),(4,-1)and (6,4)$ Solution: 求平行四边形面积如下：$$1、平移(如下图),新点：(0,0), (2,5),(6,1),(8,6)$$$$2、建立矩阵列向量,A =\\begin{bmatrix}2 &amp; 5\\\\5 &amp; 1\\end{bmatrix},\\left|detA\\right| = \\left|-28\\right|，面积是28$$ Linear TransformationsLet $T:R^2\\rightarrow R^2$ be the linear transformation determined by a $2 \\times 2$ matrix $A$. If $S$ is a parallelogram(or other finite area) in $R^2$, then $$(area-of-T(S)) = \\left|detA\\right| \\cdot (area-of-S)$$ If $T$ is determined by a $3 \\times 3$ matrix $A$, and if $S$ is a parallelepiped(or other finite volume) in $R^3$, then $$(volume-of-T(S)) = \\left|detA\\right| \\cdot (volume-of-S)$$ for example: Let $a$ and $b$ be positive numbers. Find the area of the region $E$ bounded by the ellipse whose equation is $$\\frac{x_1^2}{a^2} + \\frac{x_2^2}{b^2} = 1$$We claim that $E$ is the image of the unit disk $D$ under the linear transformation $T$ determined by the matrix $A = \\begin{bmatrix}a &amp; 0\\\\ 0 &amp; b\\end{bmatrix}$, because if $\\vec{u} =\\begin{bmatrix}u_1\\\\u_2\\end{bmatrix}$ ,$\\vec{x} = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$,and $\\vec{x} = A\\vec{u}$, then $$u_1 = \\frac{x_1}{a}, u_2 = \\frac{x_2}{b}$$ it follows that $\\vec{u}$ is in the unit disk, with $u_1^2 + u_2^2 \\leqslant 1$, if and only if $\\vec{x}$ is in E, with $(x_1/a)^2 + (x_2/b)^2 \\leqslant 1$, so the area of ellipse is :$$\\begin{aligned}area \\space of \\space ellipse &amp;= area \\space of \\space T(D)\\\\&amp;= \\left|detA\\right| \\cdot area \\space of \\space D\\\\&amp;= ab \\cdot \\pi (1)^2 \\\\&amp;= \\pi ab\\end{aligned}$$","link":"/Math/Linear-Algebra/Algebra-C3-Determinants/"},{"title":"Algebra-C1-Linear-Equation-in-Linear-Algebra","text":"Keywords: Aumented matrix, Linear Independence, Linear Transformation This is the Chapter1 ReadingNotes from book Linear Algebra and its Application. System of Linear EquationsLinear Equation:$$a_1 x_1 + a_2 x_2 + … + a_n x_n = b$$ NonLinear Equation:$$4 x_1 - 5 x_2 = x_1x_2$$ Linear System$$\\begin{cases}&amp;2 x_1 - x_2 + 1.5 x_3 = 8 \\\\&amp;x_1 - 4 x_3 = -7\\end{cases}\\tag{2}$$ Solution setThe set of all possible solutions is called the solution set of the linear system. e.g. (5, 6.5, 3) is a solution of system(2) Two linear systems are called equivalent if they have the same solution set.A system of linear equations has no solution, or exactly one solution, or infinitely many solutions Augmented Matrix$$\\begin{cases}x_1 - 2 x_2 + x_3 = 0\\\\2x_2 - 8x_3 = 8 \\\\5x_1 - 5x_3 = 10\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; -2 &amp; 1 &amp; 0 \\\\0 &amp; 2 &amp; -8 &amp; 8\\\\5 &amp; 0 &amp; -5 &amp; 10\\end{bmatrix}$$ Solving a Linear SystemThe procedure can also be called simplify the augmented matrix(化简增广矩阵的过程就是在求线性方程组的解) $$\\begin{cases}x_1 = 1, \\\\x_2 = 0, \\\\x_3 = -1\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 1 \\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; -1\\end{bmatrix}$$ ELEMENTARY ROW OPERATIONS (Replacement) Replace one row by the sum of itself and a multiple of another row (Interchange) Interchange two rows. (Scaling) Multiply all entries in a row by a nonzero constant Existence and Uniqueness Questionsthis system solution is unique, this system is consistent:$$\\begin{cases}x_1 - 2 x_ 2 + x_3 = 0\\\\x_2 - 4x_3 = 4\\\\x_3 = -1\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; -2 &amp; 1 &amp; 0 \\\\0 &amp; 1 &amp; -4 &amp; 4\\\\0 &amp; 0 &amp; 1 &amp; -1\\end{bmatrix}$$ Row Reduction and Echelon Formsa nonzero row or columna row or column that contains at least one nonzero entry leading entrya leading entry of a row refers to the leftmost nonzero entry (in a nonzero row) A rectangular matrix is in echelon form (or row echelon form) if it has the following three properties: All nonzero rows are above any rows of all zeros. Each leading entry of a row is in a column to the right of the leading entry of the row above it. All entries in a column below a leading entry are zeros.If a matrix in echelon form satisfies the following additional conditions, then it is in reduced echelon form (or reduced row echelon form): The leading entry in each nonzero row is 1. Each leading 1 is the only nonzero entry in its column. 行阶梯矩阵 row echelon form$$\\begin{bmatrix}2 &amp; -3 &amp; 2 &amp; 1 \\\\0 &amp; 1 &amp; -4 &amp; 8\\\\0 &amp; 0 &amp; 0 &amp; 5/2\\end{bmatrix}$$ 最简阶梯矩阵 reduced echelon form$$\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 29 \\\\0 &amp; 1 &amp; 0 &amp; 16\\\\0 &amp; 0 &amp; 1 &amp; 3\\end{bmatrix}$$ Pivot PositonsA pivot position in a matrix A is a location in A that corresponds to a leading 1in the reduced echelon form of A. A pivot column is a column of A that containsa pivot position I think the leading entry is pivot postion.只是pivot postions是在最简阶梯矩阵的语境下的 Solutions of Linear SystemsThe variables $x_1$ and $x_2$ corresponding to pivot columns in the matrix are called basic variables.The other variable, $x_3$, is called a free variable. $$\\begin{cases}x_1 - 5 x_ 3 = 1 \\\\x_2 + x_3 = 4\\\\0 = 0\\end{cases}\\rightarrow\\begin{bmatrix}1 &amp; 0 &amp; -5 &amp; 1 \\\\0 &amp; 1 &amp; 1 &amp; 4\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ Augumented matrix has solution Existence and Uniqueness TheoremA linear system is consistent if and only if the rightmost column of the augmented matrix is not a pivot column—that is, if and only if an echelon form of the augmented matrix has no row of the form$$\\begin{bmatrix}0 &amp; \\cdots &amp;0 &amp; b\\end{bmatrix} with \\space b\\space nonzero$$If a linear system is consistent, then the solution set contains either (i) a unique solution, when there are no free variables, or (ii) infinitely many solutions, when there is at least one free variable. Vector EquationsVector in $R^n$if $n$ is a positive integer, $R^n$ denotes the collection of all lists (or ordered n-tuples) of $n$ real numbers, usually written as $n \\times 1$ column matrics, such as $$\\vec{a} = \\begin{bmatrix} u_1\\\\u_2\\\\\\cdots\\\\u_3\\end{bmatrix}$$ Linear CombinationsGiven vectors $\\vec{v_1}, \\vec{v_2}, \\cdots, \\vec{v_p}$ in $R^n$ and given scalars $c_1, c_2, \\cdots, c_p$, the vector $\\vec{y}$ defined by $$\\vec{y} = c_1\\vec{v_1} + \\cdots + c_p\\vec{v_p}$$ is called a linear combination of $\\vec{v_1}, \\cdots, \\vec{v_p}$ with weights $c_1, \\cdots, c_p$ Linear Combinations &amp; MatricsA vector equation $$x_1\\vec{a_1} + x_2\\vec{a_2} + \\cdots + x_n\\vec{a_n} = \\vec{b}$$ has the same solution set as the linear system whose augmented matrix is $$\\begin{bmatrix}\\vec{a_1} &amp; \\vec{a_2} &amp; \\cdots &amp; \\vec{a_n} &amp; \\vec{b}\\end{bmatrix}\\tag{5}$$ In particular, $\\vec{b}$ can be generated by a linear combination of $\\vec{a_1}\\cdots\\vec{a_n}$ if and only if there exists a solution to the linear system corresponding to the matrix(5) Span{$\\vec{v_1}, \\cdots, \\vec{v_p}$}Asking whether a vector $\\vec{b}$ is in Span{$\\vec{v_1}, \\cdots, \\vec{v_p}$} amounts to asking whether the vector equation $$x_1\\vec{v_1} + \\cdots + x_p\\vec{v_p} = \\vec{b}$$ has a solution, or, equivalently, asking whether the linear system with augmented matrix $$\\begin{bmatrix}\\vec{v_1} &amp; \\cdots &amp; \\vec{v_p} &amp; \\vec{b}\\end{bmatrix}$$ has a solution A Geometric Description of Span{$\\vec{v}$} and Span{$\\vec{u}, \\vec{v}$}Let $\\vec{v}$ be a nonzero vector in $R^3$. Then Span{$\\vec{v}$} is the set of all scalar multiples of $\\vec{v}$, which is the set of points on the line in $R^3$ through $\\vec{v}$ and $\\vec{0}$. See Figure 10. If $\\vec{u}$ and $\\vec{v}$ are nonzero vectors in $R^3$, with $\\vec{v}$ not a multiple of $\\vec{u}$, then Span{$\\vec{u},\\vec{v}$}is the plane in $R^3$ that contains $\\vec{u}$, $\\vec{v}$, and $\\vec{0}$. In particular, Span{$\\vec{u},\\vec{v}$} contains the line in $R^3$ through $\\vec{u}$ and $\\vec{0}$ and the line through $\\vec{v}$ and $\\vec{0}$. See Figure 11. The Matrix Equation $Ax = b$if $A$ is an $m \\times n$ matrix, with columns $\\vec{a_1}, \\cdots, \\vec{a_n}$, and if $\\vec{x}$ is in $R^n$, then the product of $A$ and $\\vec{x}$, denoted by $A\\vec{x}$, is the linear combination of the columns of $A$ using the corresponding entries in $\\vec{x}$ as weights; that is, $$A\\vec{x} =\\begin{bmatrix}\\vec{a_1} &amp; \\vec{a_2} &amp; \\cdots \\vec{a_n}\\end{bmatrix}\\begin{bmatrix}x_1\\\\ \\cdots \\\\ x_n\\end{bmatrix}=x_1\\vec{a_1}+x_2\\vec{a_2}+\\cdots+x_n\\vec{a_n}$$ 以下四种写法是等价的，全部转化为求增广矩阵的Solution： $$\\begin{cases}x_1 + 2x_2 - x_3 = 4\\\\-5x_2 + 3x_3 = 1 \\tag{1}\\end{cases}$$ $$x_1\\begin{bmatrix}1\\\\0\\end{bmatrix} +x_2\\begin{bmatrix}2\\\\-5\\end{bmatrix} +x_3\\begin{bmatrix}-1\\\\3\\end{bmatrix} =\\begin{bmatrix}4\\\\1\\end{bmatrix} \\tag{2}$$ $$\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -5 &amp; 3\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}4 \\\\ 1\\end{bmatrix} \\tag{3}$$ $$\\begin{bmatrix}1 &amp; 2 &amp; -1 &amp; 4\\\\0 &amp; -5 &amp; 3 &amp; 1\\end{bmatrix} \\tag{4}$$ 计算机存储矩阵，用连续的空间存储提高效率To optimize a computer algorithm to compute $Ax$, the sequence of calculationsshould involve data stored in contiguous memory locations. The most widelyused professional algorithms for matrix computations are written in Fortran, alanguage that stores a matrix as a set of columns. Such algorithms compute $Ax$ asa linear combination of the columns of $A$. In contrast, if a program is written inthe popular language C, which stores matrices by rows, Ax should be computedvia the alternative rule that uses the rows of $A$ Solution sets of linear systemsHomogeneous Linear SystemsA system of linear equations is said to be homogeneous if it can be written in the form $A\\vec{x} = 0 $, where A is an $m \\times n$ matrix and $\\vec{0}$ is the zero vector in $R^m$. $\\vec{x} = \\vec{0}$ is a trivial solution. The homogeneous equation $A\\vec{x} = 0 $ has a nontrivial solution if and only if the equation has at least one free variable. For Example:$$\\begin{cases}3x_1 + 5x_2 - 4x_3 = 0\\\\-3x_1 - 2x_2 + 4x_3 = 0\\\\6x_1 + x_2 - 8x_3 = 0 \\tag{1}\\end{cases}$$Solution:$$\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 0\\\\-3 &amp; 2 &amp; 4 &amp; 0\\\\6 &amp; 1 &amp; -8 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 0\\\\0 &amp; 3 &amp; 0 &amp; 0\\\\0 &amp; -9 &amp; 0 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 0\\\\0 &amp; 3 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$ Since $x_3$ is a free variable, $A\\vec{x} = \\vec{0}$ has nontrivial solutions, continue the row reduction of $\\begin{bmatrix}A &amp; \\vec{0}\\end{bmatrix}$ to reduced echelon form: $$\\begin{bmatrix}1 &amp; 0 &amp; -4/3 &amp; 0\\\\0 &amp; 3 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\rightarrow\\begin{matrix}x_1-\\frac{4}{3}x_3 = 0\\\\x_2 = 0\\\\0 = 0\\end{matrix}$$ the general solution of $A\\vec{x} = \\vec{0}$ has the form: $$\\vec{x} = \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}\\frac{4}{3}x_3\\\\0\\\\x_3\\end{bmatrix}=x_3\\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix}=x_3\\vec{v},where\\space \\vec{v} = \\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix}$$ Geometrically, the solution set is a line through $\\vec{0}$ in $R^3$. See Figure1. Also, form as $\\vec{x} = x_3\\vec{v}$, we say that the solution is in parametric vector form. Solutions of Nonhomogeneous Linear SystemsFor example:$$\\begin{bmatrix}3 &amp; 5 &amp; -4 &amp; 7\\\\-3 &amp; -2 &amp; 4 &amp; -1\\\\6 &amp; 1 &amp; -8 &amp; -4\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; -4/3 &amp; -1\\\\0 &amp; 1 &amp; 0 &amp; 2\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\tag{2}$$the general solution has the form:$$\\vec{x} = \\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}-1+\\frac{4}{3}x_3\\\\2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}-1\\\\2\\\\0\\end{bmatrix} + x_3\\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix}=\\vec{p} + x_3\\vec{v},where\\space \\vec{v} = \\begin{bmatrix}\\frac{4}{3}\\\\0\\\\1\\end{bmatrix},\\vec{p} = \\begin{bmatrix}-1\\\\2\\\\0\\end{bmatrix}$$ Summary： $$Homogeneous: \\vec{x} = t\\vec{v}\\\\Nonhomogeneous: \\vec{x} = \\vec{p} + t\\vec{v}\\\\From \\space Geometry：translation$$ the solution set of $A\\vec{x} = \\vec{b}$ is a line through $\\vec{p}$ parallel to the solution set of $A\\vec{x} = \\vec{0}$. Figure 6 illustrates the case in which there are two free variables, the solution set is a plane, not a line. Application of linear systemsNetwork FlowFor Example: The network in Figure 2 shows the traffic flow (in vehicles per hour) over several one-way streets in downtown Baltimore during a typical early afternoon.Determine the general flow pattern for the network. Solution: Intersection Flow in Flow out A 300+500 = $x_1 + x2$ B $x_2 + x4$ = 300 + $x_3$ C 100 + 400 = $x_4 + x5$ D $x_1 + x5$ = 600 $$列方程，解方程如下：\\begin{matrix}x_1 + x_2 = 800\\\\x_2 - x_3 + x4 = 300\\\\x_4 + x_5 = 500\\\\x1 + x5 = 600\\\\x_3 = 400\\end{matrix}\\sim\\begin{cases}x_1 = 600 - x_5 \\\\x_2 = 200 + x_5 \\\\x_3 = 400 \\\\x_4 = 500 - x_5\\\\x_5 is free\\end{cases}$$ Linear independence线性相关（有非零解）和线性无关（只有零解） An indexed set of vectors {${\\vec{v_1}, \\cdots, \\vec{v_p}}$} in $R^n$ is said to be linearly independent if the vector equation$$x_1\\vec{v_1} + x_2\\vec{v_2} + \\cdots + x_p\\vec{v_p} = \\vec{0}$$has only the trivial solution. The set {${\\vec{v_1}, \\cdots, \\vec{v_p}}$} is said to be linealy dependent if there exist weights $c_1, \\cdots, c_p$, not all zeros, such that$$c_1\\vec{v_1} + c_2\\vec{v_2} + \\cdots + c_p\\vec{v_p} = \\vec{0}$$ The columns of a matrix $A$ are linearly independent if and only if the equation $A\\vec{x} = \\vec{0}$has only the trivial solution, 当然可以通过观察发现两组向量是否线性相关,比如:$$\\vec{v_1} = \\begin{bmatrix}3\\\\1\\end{bmatrix},\\space\\vec{v_2} = \\begin{bmatrix}6\\\\2\\end{bmatrix}\\rightarrow\\vec{v_2} = 2\\vec{v_1},linear\\space dependent$$ A set of two vectors {$\\vec{v_1},\\vec{v_2}$} is linearly dependent if at least one of the vectors is a multiple of the other. The set is linearly independent if and only if neither of the vectors is a multiple of the other. Sets of Two or More Vectors 如果至少能找到一个向量是其他向量的线性组合，那么这个向量集就是线性相关的 If a set contains more vectors than there are entries in each vector, then the set is linearly dependent. That is, any set {$\\vec{v_1}, \\cdots, \\vec{v_p}$} in $R^n$ is linearly dependent if p &gt; n $$n= 3, p = 5,\\overbrace{\\begin{bmatrix}* &amp; * &amp; * &amp; * &amp; * \\\\* &amp; * &amp; * &amp; * &amp; * \\\\* &amp; * &amp; * &amp; * &amp; * \\\\\\end{bmatrix}}^{p}$$ $$\\begin{bmatrix}2\\\\1\\end{bmatrix},\\begin{bmatrix}4\\\\-1\\end{bmatrix},\\begin{bmatrix}-2\\\\2\\end{bmatrix}linearly-dependent, cause\\space three\\space vectors,but\\space two\\space entries\\space in \\space each \\space vector$$ Introduction to Linear TransformationIn Computer Graphics, $A\\vec{x}$ is not related to linear combination. We think the matrix $A$ as an object that “acts” on a vector $\\vec{v}$ by multiplication to produce a new vector called $A\\vec{x}$. Matrix Transformation$$\\vec{x}\\mapsto A\\vec{x}$$ A projection transformation if $A = \\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0\\end{bmatrix}$, then the transformation $\\vec{x}\\mapsto A\\vec{x}$ projects points in $R^3$ onto the $x_1x_2-plane$, because$$\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}\\mapsto\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix}=\\begin{bmatrix}x_1\\\\x_2\\\\0\\end{bmatrix}$$ A shear transformation if $A = \\begin{bmatrix}1 &amp; 3 \\\\0 &amp; 1 \\\\\\end{bmatrix}$, the transformation $T:R^2\\rightarrow R^2$ defined by $T(\\vec{x}) = A\\vec{x}$ is called shear transformation, because $$\\begin{bmatrix}0\\\\2\\\\\\end{bmatrix}\\mapsto\\begin{bmatrix}1 &amp; 3 \\\\0 &amp; 1 \\\\\\end{bmatrix}\\begin{bmatrix}0\\\\2\\\\\\end{bmatrix}=\\begin{bmatrix}6\\\\2\\end{bmatrix}，\\begin{bmatrix}2\\\\2\\end{bmatrix}\\mapsto\\begin{bmatrix}1 &amp; 3 \\\\0 &amp; 1\\end{bmatrix}\\begin{bmatrix}2\\\\2\\end{bmatrix}=\\begin{bmatrix}8\\\\2\\end{bmatrix}$$ A transformation (or mapping) $T$ is linear if:(i) $T(\\vec{u} + \\vec{v}) = T(\\vec{u}) + T(\\vec{v})$ for all $\\vec{u},\\vec{v}$ in the domain of $T$.(ii) $T(c\\vec{u}) = cT(\\vec{u})$ for all scalars $c$ and all $\\vec{u}$ in the domain of $T$. Linear transformations preserve the operations of vector addition and scalar multiplication. The Matrix of Linear TransformationLet $T: R^n \\rightarrow R^m$ be a linear transformation. Then there exists a unique matrix $A$ such that $$T(\\vec{x}) = A\\vec{x},for\\space all\\space x \\space in R^n$$ In fact, $A$ is the $m \\times n$ matrix whose $j-th$ column is the vector $T(\\vec{e_j})$, where $\\vec{e_j}$ is the $j-th$ column of the identity matrix in $R^n$: $$A = \\begin{bmatrix} T(\\vec{e_1}) \\cdots T(\\vec{e_n})\\end{bmatrix}\\tag{3}$$ the matrxi in (3) is called the standard matrix for the linear transformation T. For example: The columns of $I_2 = \\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; 1\\end{bmatrix}$ are $\\vec{e_1} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ and $\\vec{e_2} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$. Suppose T is a linear transformation from $R^2$ into $R^3$ such that $$T(\\vec{e_1}) = \\begin{bmatrix}5 \\\\ -7 \\\\ 2\\end{bmatrix}and \\spaceT(\\vec{e_2}) = \\begin{bmatrix}-3 \\\\ 8 \\\\ 0\\end{bmatrix}$$ with no additional information, find a formula for the image of an arbitrary $\\vec{x}$ in $R^2$ Solution：$$\\vec{x} = \\begin{bmatrix}x_1 \\\\ x_2 \\end{bmatrix}= x_1\\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + x_2\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}= x_1\\vec{e_1} + x_2\\vec{e_2}\\\\T(\\vec{x}) = x_1T(\\vec{e_1}) + x_2T(\\vec{e_2})= \\begin{bmatrix}5x_1 - 3x_2 \\\\ -7x_1+8x_2 \\\\ 2x_1+0\\end{bmatrix}\\\\T(\\vec{x}) = \\begin{bmatrix}T(\\vec{e_1}) &amp; T(\\vec{e_2})\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}= A\\vec{x}$$ A Rotation transformation 根据上文公式可以快速推导出2D旋转矩阵 $\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$ rotates into $\\begin{bmatrix}\\cos\\psi \\\\ \\sin\\psi\\end{bmatrix}$, and $\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$ rotates into $\\begin{bmatrix}-\\sin\\psi \\ \\cos\\psi\\end{bmatrix}$ so the rotation matrix is $\\begin{bmatrix}\\cos\\psi &amp; -\\sin\\psi\\\\ \\sin\\psi &amp; \\cos\\psi\\end{bmatrix}$ 同理，其他如Reflection， expansion…变换都可以根据基向量进行推导 Let $T: R^n \\rightarrow R^m$ be a linear transformation. Then $T$ is one-to-one if and only if the equation $T(\\vec{x}) = \\vec{0}$ has only the trivial solution. Linear models in business,science, and engineeringDifference Equations(差分方程)Several features of the system are each measured at discrete time intervals, producing a sequence of vectors $\\vec{x_0},\\vec{x_1},\\vec{x_2},\\cdots,\\vec{x_k}$ The entries in $\\vec{x_k}$ provide information about the state of the system at the time of the $k-th$ measurement. If there is a matrix A such that $\\vec{x_1} = A\\vec{x_0}, \\vec{x_2} = A\\vec{x_1}$ and, in general, $$\\vec{x_{k+1}} = A\\vec{x_k}, k = 0,1,2,…\\tag{5}$$ then (5) is called a linear difference equation (or recurrence relation). 比如, 一个地区的人口增长可以用这种模型简单的表示 思考：矩阵快速幂怎么求？首先考虑普通的快速幂: $$\\begin{aligned}4 ^ {11} &amp;= 4^{(1011)_2}\\\\&amp;= 4^{2^0}\\cdot4^{2^1}\\cdot4^{2^3}\\\\4^{2^1} &amp;= 4^{2^0} \\cdot 4^{2^0}\\\\4^{2^2} &amp;= 4^{2^1} \\cdot 4^{2^1}\\\\4^{2^3} &amp;= 4^{2^2} \\cdot 4^{2^2}\\end{aligned}$$ the c++ code is: 12345678910111213//get a^k mod pint qmi(int a, int k, int p){ int res = 1 % p; int t = a; //此时相当于a^(2^0) while(k) { if(k &amp; 1) res = (LL)res * t % p; t = (LL)t * t % p; k &gt;&gt;= 1; } return res;} 接下来考虑矩阵快速幂： for example: 如何快速计算Fibonacci的f(1000)？ $$Fibonacci\\\\f(0) = 0, f(1) = 1\\\\f(n) = f(n-1) + f(n-2), n &gt; 1\\\\建立矩阵方程如下：let \\space F(n) = \\begin{bmatrix}f(n)\\\\f(n+1)\\end{bmatrix}\\\\AF(n) = F(n+1)\\rightarrowA\\begin{bmatrix}f(n)\\\\ f(n+1)\\end{bmatrix} =\\begin{bmatrix}f(n+1)\\\\ f(n+2)\\end{bmatrix}\\rightarrowA = \\begin{bmatrix}0 &amp; 1\\\\1 &amp; 1\\end{bmatrix}\\RightarrowF(n) = A^nF(0), F(0) = \\begin{bmatrix}0\\\\1\\end{bmatrix}$$ 1234567891011121314//get A^k * F_0 % p//示例使用左乘int qmi(Matrix A, int k, int p, Vector F_0){ Vector res = F_0 % p; Matrix t = A; //A^(2^0) while(k) { if(k &amp; 1) res = t * res % p; t = t * t % p; k &gt;&gt;=1; } return res;}","link":"/Math/Linear-Algebra/Algebra-C1-Linear-Equation-in-Linear-Algebra/"},{"title":"Algebra-Summary","text":"Keywords: invertible $n \\times n$ matrix invertible properties:Let $A$ be a square $n \\times n$ matrix. Then the following statements are equivalent. That is, for a given $A$, the statements are either all true or all false.a. $A$ is an invertible matrix.b. $A$ is row equivalent to the $n \\times n$ identity matrix.c. A has $n$ pivot positions.d. The equation $Ax = 0$ has only the trivial solution.e. The columns of $A$ form a linearly independent set.f. The linear transformation $x \\rightarrow Ax$ is one-to-one.g. The equation $Ax = b$ has at least one solution for each $b$ in $R^n$.h. The columns of $A$ span $R^n$.i. The linear transformation $x \\rightarrow Ax$ maps $R^n$ onto $R^n$.j. There is an $n \\times n$ matrix $C$ such that $CA = I$.k. There is an $n \\times n$ matrix $D$ such that $AD = I$.l. $A^T$ is an invertible matrix.m. The columns of $A$ form a basis of $R^n$:n. $Col A = R^n$o. $dim Col A = n$p. $rank A = n$q. $Nul A = {0}$r. $dim Nul A = 0$s. The number $0$ is not an eigenvalue of $A$.t. The determinant of $A$ is not zero.u. $(Col A)^\\perp = {0}$.v. $(Null A)^\\perp = R^n$.w. $RowA = R^n$.x. $A$ has $n$ nonzero singular values.","link":"/Math/Linear-Algebra/Algebra-Summary/"},{"title":"Algebra-C5-EigenValues-And-EigenVectors","text":"Keywords: EigenVectors, Diagonalization, Characteristic Equation, Quaternion, Differential Equation This is the Chapter5 ReadingNotes from book Linear Algebra and its Application. EigenVectors And EigenValues An eigenvector of an $n \\times n$ matrix $A$ is a nonzero vector $\\vec{x}$ such that $A\\vec{x} = \\lambda\\vec{x}$ for some scalar $\\lambda$.A scalar $\\lambda$ is called an eigenvalue of $A$ if there is a nontrivial solution $\\vec{x}$ of $A\\vec{x} = \\lambda\\vec{x}$, such an $x$ is called an eigenvector corresponding to $\\lambda$. For Example: Let A = $\\begin{bmatrix}4 &amp; -1 &amp; 6\\\\ 2 &amp; 1 &amp; 6\\\\ 2 &amp; -1 &amp; 8 \\end{bmatrix}$.An eigenvalue of $A$ is 2. Find a basis for the corresponding eigenspace. $$A\\vec{x} = 2\\vec{x}\\rightarrow(A - 2I)\\vec{x} = \\vec{0}$$$$\\begin{bmatrix}2 &amp; -1 &amp; 6 &amp; 0\\\\2 &amp; -1 &amp; 6 &amp; 0\\\\2 &amp; -1 &amp; 6 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}2 &amp; -1 &amp; 6 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}$$general solution is:$$\\begin{bmatrix}x_1\\\\x_2\\\\x_3\\end{bmatrix} =x_2\\begin{bmatrix}1/2\\\\1\\\\0\\end{bmatrix} +x_3\\begin{bmatrix}-3\\\\0\\\\1\\end{bmatrix},x_2 and x_3 free$$ The eigenspace, shown in Figure 3, is a two-dimensional subspace of $R^3$. A basis is$$\\lbrace\\begin{bmatrix}1\\\\2\\\\0\\end{bmatrix}\\begin{bmatrix}-3\\\\0\\\\1\\end{bmatrix}\\rbrace$$ 个人理解就是所有特征向量组成的空间就是特征空间 The eigenvalues of a triangular matrix are the entries on its main diagonal.Let$$A = \\begin{bmatrix}3 &amp; 6 &amp; -8\\\\0 &amp; 0 &amp; 6\\\\0 &amp; 0 &amp; 2\\end{bmatrix},B = \\begin{bmatrix}4 &amp; 0 &amp; 0\\\\-2 &amp; 1 &amp; 0\\\\5 &amp; 3 &amp; 4\\end{bmatrix}$$The eigenvalues of $A$ are $3, 0, 2$. The eigenvalues of $B$ are $4, 1$. If $\\vec{v_1},\\cdots, \\vec{v_r}$ are eigenvectors that correspond to distinct eigenvalues $\\lambda_1, \\cdots, \\lambda_2$ of an $n \\times n$ matrix $A$, then the set $\\lbrace \\vec{v_1}, \\cdots , \\vec{v_r}\\rbrace$ is linearly independent. Eigenvectors and Difference Equations (差分方程)This section concludes by showing how to construct solutions of the first-order difference equation discussed in the chapter introductory example: $$\\vec{x_{k+1}} = A\\vec{x_{k}}\\tag{8}$$ If A is an $n \\times n$ matrix, then (8) is a recursive description of a sequence $\\lbrace \\vec{x_k}\\rbrace$ in $R^n$. A solution of (8) is an explicit description of $\\lbrace \\vec{x_k}\\rbrace$ whose formula for each $\\vec{x_k}$ does not depend directly on $A$ or on the preceding terms in the sequence other than the initial term $\\vec{x_0}$. The simplest way to build a solution of (8) is to take an eigenvector $\\vec{x_0}$ and its corresponding eigenvalue $\\lambda$and let $$\\vec{x_k} = \\lambda ^k \\vec{x_0}$$ This sequence is a solution because $$\\begin{aligned}A\\vec{x_k} &amp;= A(\\lambda ^k \\vec{x_0})\\\\&amp;=\\lambda ^k A\\vec{x_0} \\\\&amp;= \\lambda ^k (\\lambda\\vec{x_0})\\\\&amp;=\\vec{x_{k+1}}\\end{aligned}$$ The Characteristic Equation（特征方程）For Example: Find the eigenvalues of $A = \\begin{bmatrix}2 &amp; 3 \\\\ 3 &amp; -6 \\end{bmatrix}$. Solution: We must find all scalars $\\lambda$ such that the matrix equation$$(A - \\lambda I)\\vec{x} = \\vec{0}$$has a nontrivial solution（非零解）. This problem is equivalent to finding all$\\lambda$such that the matrix $A - \\lambda I$ is not invertible（不可逆）, where $$A - \\lambda I =\\begin{bmatrix}2 - \\lambda &amp; 3\\\\3 &amp; -6-\\lambda\\end{bmatrix}\\longrightarrowdet(A - \\lambda I) = (\\lambda - 3)(\\lambda + 7) = 0$$so the eigenvalues of $A$ are 3 and -7. Determinants Let $A$ be an $n\\times n$ matrix. Then A is invertible if and only if:s. The number $0$ is not an eigenvalue of $A$.t. The determinant of $A$ is not zero. Let $A$ and $B$ be $n \\times n$ matrices.a. $A$ is invertible if and only if det A 不等于 0.b. $det AB = (det A)(det B)$.c. $det A^T = det A$.d. If $A$ is triangular, then $det A$ is the product of the entries on the main diagonal of $A$.e. A row replacement operation on $A$ does not change the determinant.A row interchange changes the sign of the determinant.A row scaling also scales the determinant by the same scalar factor. The Characteristic EquationThe scalar equation $det(A - \\lambda I) = 0$ is called the characteristic equation of $A$. A scalar $\\lambda$ is an eigenvalue of an $n \\times n$ matrix $A$ if and only if $\\lambda$ satisfies the characteristic equation$$det(A - \\lambda I) = 0$$ For Example: Find the characteristic equation of$$A =\\begin{bmatrix}5 &amp; -2 &amp; 6 &amp; -1\\\\0 &amp; 3 &amp; -8 &amp; 0\\\\0 &amp; 0 &amp; 5 &amp; 4\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}$$ Solution: $$det(A - \\lambda I) = (5-\\lambda)(3-\\lambda)(5-\\lambda)(1-\\lambda)$$The characteristic equation is$$(5-\\lambda)^2(3-\\lambda)(1-\\lambda) = 0$$Expanding the product, we can also write $$\\lambda^4 - 14\\lambda^3 + 68\\lambda^2 - 130\\lambda + 75 = 0$$ then $det(A - \\lambda I)$ is a polynomial of degree $n$ called the characteristic polynomial（特征多项式） of $A$. The eigenvalue 5 in Example above is said to have multiplicity 2 because $\\lambda - 5$ occurs two times as a factor of the characteristic polynomial. Similarity$A$ is similar to $B$ if there is an invertible matrix $P$ such that $P^{-1}AP = B$, or, equivalently, $PBP^{-1} = A$. Changing $A$ into $P^{-1}AP = B$ is called a similarity transformation. If $n \\times n$ matrices $A$ and $B$ are similar, then they have the same characteristic polynomial and hence the same eigenvalues (with the same multiplicities). Warning: The matrices$$\\begin{bmatrix}2 &amp; 1\\\\0 &amp; 2\\end{bmatrix}and\\begin{bmatrix}2 &amp; 0\\\\0 &amp; 2\\end{bmatrix}$$are not similar even though they have the same eigenvalues. 2.Similarity is not the same as row equivalence. (If $A$ is row equivalent to $B$, then $B = EA$ for some invertible matrix $E$.) Row operations on a matrix usually change its eigenvalues. Application to Dynamical SystemsFor Example: Let $A = \\begin{bmatrix} 0.95 &amp; 0.03\\\\ 0.05 &amp; 0.97\\end{bmatrix}$. Analyze the long-term behavior of the dynamical system defined by $\\vec{x_{k+1}} = A\\vec{x_k}$, with $\\vec{x_0} = \\begin{bmatrix}0.6\\\\0.4\\end{bmatrix}$. Solution: The first step is to find the eigenvalues of $A$ and a basis for each eigenspace. The characteristic equation for $A$ is $$0 = det\\begin{bmatrix}0.95-\\lambda &amp; 0.03\\\\0.05 &amp; 0.97-\\lambda\\end{bmatrix} =\\lambda^2 - 1.92\\lambda + 0.92$$By the quadratic formula $$\\lambda = 1 or \\space 0.92$$It is readily checked that eigenvectors corresponding to $\\lambda = 1$ and $\\lambda = 0.92$ are multiples of $$\\vec{v_1} = \\begin{bmatrix}3\\\\5\\end{bmatrix}and \\space\\vec{v_2} = \\begin{bmatrix}1\\\\-1\\end{bmatrix}$$respectively.The next step is to write the given $\\vec{x_0}$ in terms of $\\vec{v_1}$ and $\\vec{v_2}$. This can be done because $\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ is obviously a basis for $R^2$: $$\\vec{x_0} = c_1\\vec{v_1} + c_2\\vec{v_2} =\\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2}\\end{bmatrix}\\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix}\\\\\\longrightarrow\\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix} =\\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2}\\end{bmatrix} ^ {-1}\\vec{x_0}=\\begin{bmatrix}0.125 \\\\ 0.225\\end{bmatrix}$$Because $\\vec{v_1}$ and $\\vec{v_2}$ are eigenvectors of $A$, with $A\\vec{v_1} = \\vec{v_1}$ and $A\\vec{v_2} = 0.92\\vec{v_2}$, we easily compute each $\\vec{x_k}$: $$\\vec{x_1} = A\\vec{x_0} = c_1\\vec{v_1}+c_2(0.92)\\vec{v_2}\\\\\\vec{x_2} = A\\vec{x_1} = c_1\\vec{v_1}+c_2(0.92)^2\\vec{v_2}\\\\\\cdots\\vec{x_k} = c_1\\vec{v_1}+c_2(0.92)^k\\vec{v_2}=0.125\\begin{bmatrix}3\\\\5\\end{bmatrix} + 0.225(0.92)^k\\begin{bmatrix}1\\\\-1\\end{bmatrix}\\\\\\lim_{x \\to \\infty} \\vec{x_k} = 0.125\\vec{v_1}$$ DiagonalizationFor Example: Let $A = \\begin{bmatrix} 7 &amp; 2\\\\-4 &amp; 1\\end{bmatrix}$. Find a formula for $A^k$, given that $A = PDP^{-1}$, where$$P =\\begin{bmatrix}1 &amp; 1\\\\-1 &amp; -2\\end{bmatrix}andD =\\begin{bmatrix}5 &amp; 0\\\\0 &amp; 3\\end{bmatrix}$$ Solution: $$A^2 = PDP^{-1}PDP^{-1} = PD^2P^{-1}\\\\\\RightarrowA^k = PD^kP^{-1}$$A square matrix $A$ is said to be diagonalizable if $A$ is similar to a diagonal matrix, that is, if $A = PDP^{-1}$ for some invertible matrix $P$ and some diagonal matrix $D$. An $n \\times n$ matrix $A$ is diagonalizable if and only if $A$ has $n$ linearly independent eigenvectors.In fact, $A = PDP^{-1}$, with $D$ a diagonal matrix, if and only if the columns of $P$ are $n$ linearly independent eigenvectors of $A$. In this case, the diagonal entries of $D$ are eigenvalues of $A$ that correspond, respectively, to the eigenvectors in $P$.In other words, $A$ is diagonalizable if and only if there are enough eigenvectors to form a basis of $R^n$. We call such a basis an eigenvector basis of $R^n$. Diagonalizing MatricesFor Example: Diagonalize the following matrix, if possible.$$A = \\begin{bmatrix}1 &amp; 3 &amp; 3\\\\-3 &amp; -5 &amp; -3\\\\3 &amp; 3 &amp; 1\\end{bmatrix}$$That is, find an invertible matrix $P$ and a diagonal matrix $D$ such that $A = PDP^{-1}$. Solution: Step1. Find the eigenvalues of A.$$det(A - \\lambda I) = -(\\lambda-1)(\\lambda+2)^2$$The eigenvalues are $\\lambda = 1 , \\space \\lambda = -2$ Step2. Find three linearly independent eigenvectors of A. $$Basis \\space for \\space \\lambda = 1: \\vec{v_1} = \\begin{bmatrix}1 \\\\ -1 \\\\ 1\\end{bmatrix}\\Basis \\space for \\space \\lambda = -2: \\vec{v_2} = \\begin{bmatrix}-1 \\\\ 1 \\\\ 0\\end{bmatrix} and \\space\\vec{v_3} = \\begin{bmatrix}-1 \\\\ 0 \\\\ 1\\end{bmatrix}$$Step3. Construct P from the vectors in step 2.$$P = \\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2} &amp; \\vec{v_3}\\end{bmatrix} =\\begin{bmatrix}1 &amp; -1 &amp; -1\\\\-1 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 1\\end{bmatrix}$$Step4. Construct D from the corresponding eigenvalues.$$D =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; -2\\end{bmatrix}$$Step5. Verify.$$AP = PD$$ An $n \\times n$ matrix with $n$ distinct eigenvalues is diagonalizable. Matrices Whose Eigenvalues Are Not DistinctLet $A$ be an $n \\times n$ matrix whose distinct eigenvalues are $\\lambda_1, \\cdots, \\lambda_p$. a. For $1 \\le k \\le p$, the dimension of the eigenspace for $\\lambda_k$ is less than or equal to the multiplicity of the eigenvalue $\\lambda_k$. b. The matrix $A$ is diagonalizable if and only if the sum of the dimensions of the eigenspaces equals $n$, and this happens if and only if (i) the characteristic polynomial factors completely into linear factors and (ii) the dimension of the eigenspace for each $\\lambda_k$ equals the multiplicity of $\\lambda_k$. c. If $A$ is diagonalizable and $\\beta_k$ is a basis for the eigenspace corresponding to $\\lambda_k$ for each $k$, then the total collection of vectors in the sets $\\beta_1, \\cdots, \\beta_p$ forms an eigenvector basis for $R^n$. EigenVectors And Linear TransformationsThe Matrix of a Linear TransformationLet $V$ be an $n$-dimensional vector space, let $W$ be an $m$-dimensional vector space, and let $T$ be any linear transformation from $V$ to $W$ . To associate a matrix with $T$, choose (ordered) bases $\\beta$ and $\\gamma$ for $V$ and $W$ , respectively. Given any $\\vec{x}$ in $V$ , the coordinate vector $\\begin{bmatrix}\\vec{x}\\end{bmatrix}_\\beta$ is in $R^n$ and the coordinate vector of its image, $\\begin{bmatrix}T(\\vec{x})\\end{bmatrix}_\\gamma$, is in $R^m$, as shown in Figure 1. The connection between $\\begin{bmatrix}\\vec{x}\\end{bmatrix}_\\beta$ and $\\begin{bmatrix}T(\\vec{x})\\end{bmatrix}_\\gamma$ is easy to find. Let $\\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$ be the basis $\\beta$ for $V$ . If $\\vec{x} = r_1\\vec{b_1} + \\cdots + r_n\\vec{b_n}$, then$$\\begin{bmatrix} \\vec{x}_\\beta\\end{bmatrix}=\\begin{bmatrix}r_1 \\\\ \\cdots \\\\r_n\\end{bmatrix}$$and$$T(\\vec{x}) = T(r_1\\vec{b_1} + \\cdots + r_n\\vec{b_n})= r_1T(\\vec{b_1}) + \\cdots + r_nT(\\vec{b_n})\\tag{1}$$because $T$ is linear. Now, since the coordinate mapping from $W$ to $R^m$ is linear, equation (1) leads to$$\\begin{bmatrix} T(\\vec{x}) \\end{bmatrix}_\\gamma = r_1\\begin{bmatrix} T(\\vec{b_1}) \\end{bmatrix}_\\gamma\\ +\\cdots + r_n\\begin{bmatrix} T(\\vec{b_n}) \\end{bmatrix}_\\gamma\\tag{2}$$Since $C$-coordinate vectors are in $R^m$, the vector equation (2) can be written as a matrix equation, namely,$$\\begin{bmatrix}T(\\vec{x})\\end{bmatrix}_\\gamma =M\\begin{bmatrix}\\vec{x}\\end{bmatrix}_\\beta\\tag{3}$$where $$M = \\begin{bmatrix}\\begin{bmatrix}(T(\\vec{b_1}))\\end{bmatrix}_\\gamma&amp;\\begin{bmatrix}(T(\\vec{b_2}))\\end{bmatrix}_\\gamma&amp;\\cdots&amp;\\begin{bmatrix}(T(\\vec{b_n}))\\end{bmatrix}_\\gamma\\end{bmatrix}\\tag{4}$$The matrix $M$ is a matrix representation of $T$ , called the matrix for $T$ relative to the bases $\\beta$ and $\\gamma$. See Figure 2. For Example: Suppose $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$ is a basis for $V$ and $\\gamma = \\lbrace \\vec{c_1}, \\vec{c_2}, \\vec{c_3}\\rbrace$ is a basis for $W$. Let $T: V \\rightarrow W$ be a linear transformation with the property that $$T(\\vec{b_1}) = 3\\vec{c_1} - 2\\vec{c_2} + 5\\vec{c_3}\\\\T(\\vec{b_2}) = 4\\vec{c_1} + 7\\vec{c_2} - \\vec{c_3}$$Find the matrix $M$ for $T$ relative to $\\beta$ and $\\gamma$. Solution: The $\\gamma$-coordinate vectors of the images of $\\vec{b_1}$ and $\\vec{b_2}$ are $$\\begin{bmatrix}T(\\vec{b_1})\\end{bmatrix}_\\gamma =\\begin{bmatrix}3 \\\\ -2 \\\\ 5\\end{bmatrix}\\\\begin{bmatrix}T(\\vec{b_2})\\end{bmatrix}_\\gamma =\\begin{bmatrix}4 \\\\ 7 \\\\ -1\\end{bmatrix}$$Hence,$$M =\\begin{bmatrix}3 &amp; 4\\\\ -2 &amp; 7\\\\ 5 &amp; -1\\end{bmatrix}$$ Linear Transformations on $R^n$ Diagonal Matrix RepresentationSuppose $A = PDP^{-1}$, where $D$ is a diagonal $n \\times n$ matrix. If $\\beta$ is the basis for $R^n$ formed from the columns of $P$ , then $D$ is the $\\beta-matrix$ for the transformation $x \\mapsto Ax$. For example: Define $T: R^2\\mapsto R^2$ by $T(\\vec{x}) = A\\vec{x}$, where $A = \\begin{bmatrix} 7 &amp; 2\\\\-4 &amp; 1\\end{bmatrix}$. Find a basis $\\beta$ for $R^2$ with the property that the $\\beta-matrix$ for $T$ is a diagonal matrix. Solution: $$A = PDP^{-1}, P = \\begin{bmatrix}1 &amp; 1 \\\\ -1 &amp; -2\\end{bmatrix}, D = \\begin{bmatrix}5 &amp; 0 \\\\ 0 &amp; 3\\end{bmatrix}$$The columns of $P$ , call them $\\vec{b_1}$ and $\\vec{b_2}$, are eigenvectors of $A$. Thus, $D$ is the $B-matrix$ for $T$ when $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2} \\rbrace$. The mappings$\\vec{x} \\mapsto A\\vec{x}$ and $\\vec{u} \\mapsto D\\vec{u}$ describe the same linear transformation, relative to different bases. Complex EigenValues$C^n$: a space of $n$-tuples of complex numbers. For Example: If $A = \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix}$, then the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ on $R^2$ rotates the plane counterclockwise through a quarter-turn.The action of $A$ is periodic, since after four quarter-turns, a vector is back where it started. Obviously, no nonzero vector is mapped into a multiple of itself, so $A$ has no eigenvectors in $R^2$ and hence no real eigenvalues. In fact, the characteristic equation of $A$ is $$\\lambda^2 + 1 = 0\\rightarrow\\lambda = i, \\lambda = -i;$$If we permit $A$ to act on $C^2$, then $$\\begin{bmatrix}0 &amp; -1\\\\1 &amp; 0\\end{bmatrix}\\begin{bmatrix}1 \\\\ -i\\end{bmatrix}=i\\begin{bmatrix}1 \\\\ -i\\end{bmatrix}\\\\\\begin{bmatrix}0 &amp; -1\\\\1 &amp; 0\\end{bmatrix}\\begin{bmatrix}1 \\\\ i\\end{bmatrix}=-i\\begin{bmatrix}1 \\\\ i\\end{bmatrix}$$Thus, $i,-i$ are eigenvalues, with $\\begin{bmatrix}1 \\\\ -i\\end{bmatrix}$ and $\\begin{bmatrix}1 \\\\ i\\end{bmatrix}$ as corresponding eigenvectors. For Example: Let $A = \\begin{bmatrix} 0.5 &amp; -0.6 \\\\ 0.75 &amp; 1.1 \\end{bmatrix}$. Find the eigenvalues of $A$, and find a basis for each eigenspace. Solution: $$0 = det\\begin{bmatrix}0.5-\\lambda &amp; -0.6 \\\\ 0.75 &amp; 1.1-\\lambda\\end{bmatrix}=\\lambda^2 - 1.6\\lambda + 1\\\\\\Rightarrow\\lambda = 0.8 \\pm 0.6i$$for $\\lambda = 0.8 - 0.6i$, construct $$A - (0.8-0.6i)I =\\begin{bmatrix}-0.3 + 0.6i &amp; -0.6\\\\0.75 &amp; 0.3 + 0.6i\\end{bmatrix}\\tag{2}$$The second equation in (2) leads to $$0.75x_1 = (-0.3-0.6i)x_2\\\\x_1 = (-0.4-0.8i)x_2$$Choose $x_2 = 5$ to eliminate the decimals, and obtain $x_1 = -2 - 4i$. $$\\vec{v_1} = \\begin{bmatrix}-2-4i\\\\5\\end{bmatrix}$$Analogous calculations for $\\lambda = 0.8 + 0.6i$ produce the eigenvector $$\\vec{v_2} = \\begin{bmatrix}-2 + 4i\\\\5\\end{bmatrix}$$the matrix $A$ determines a transformation $\\vec{x} \\mapsto A\\vec{x}$ that is essentially a rotation. See Figure1. Real and Imaginary Parts of VectorsFor Example: If $\\vec{x} = \\begin{bmatrix}3-i\\\\i\\\\2+5i\\end{bmatrix}$ = $\\begin{bmatrix}3\\\\0\\\\2\\end{bmatrix}$ + i$\\begin{bmatrix}-1\\\\1\\\\5\\end{bmatrix}$, then $$Re(Real)\\vec{x} = \\begin{bmatrix}3 \\\\ 0 \\\\ 2\\end{bmatrix}\\\\Im(Imaginary)\\vec{x} = \\begin{bmatrix}-1 \\\\ 1 \\\\ 5\\end{bmatrix}, and \\overline{\\vec{x}} = \\begin{bmatrix}3\\\\0\\\\2\\end{bmatrix} - i\\begin{bmatrix}-1\\\\1\\\\5\\end{bmatrix}$$ Eigenvalues and Eigenvectors of a Real Matrix That Acts on $C^n$For Example: If $C = \\begin{bmatrix}a &amp; -b \\\\ b &amp; a\\end{bmatrix}$, where $a$ and $b$ are real and not both zero, then the eigenvalues of $C$ are $\\lambda = a \\pm bi$. Also, if $r = |\\lambda| = \\sqrt{a^2 + b^2}$, then $$C = r\\begin{bmatrix}a/r &amp; -b/r\\\\b/r &amp; a/r\\end{bmatrix} =\\begin{bmatrix}r &amp; 0\\\\0 &amp; r\\end{bmatrix}\\begin{bmatrix}\\cos\\psi &amp; -\\sin\\psi\\\\\\sin\\psi &amp; \\cos\\psi\\end{bmatrix}$$where $\\psi$ is the angle between the positive $x-axis$ and the ray from $(0,0)$ through $(a,b)$. The angle $\\psi$ is called the argument of $\\lambda = a + bi$. Thus the transformation $\\vec{x}\\mapsto C\\vec{x}$ may be viewed as the composition of a rotation through the angle $\\psi$ and a scaling by $|\\lambda|$ (see Figure 3). Let $A$ be a real $2 \\times 2$ matrix with a complex eigenvalue $\\lambda = a - bi, b \\neq 0$ and an associated eigenvector $\\vec{v}$ in $C^2$. Then $$A = PCP^{-1}, where \\space P = \\begin{bmatrix} Re\\vec{v} &amp; Im\\vec{v}\\end{bmatrix}and \\space C = \\begin{bmatrix}a &amp; -b\\\\ b &amp; a\\end{bmatrix}$$For Example: The matrix $A = \\begin{bmatrix}0.8 &amp; -0.6 &amp; 0\\\\0.6 &amp; 0.8 &amp; 0\\\\ 0 &amp; 0 &amp; 1.07\\end{bmatrix}$ has eigenvalues $0.8\\pm 0.6i$ and $1.07$. Any vector $\\vec{w_0}$ in the $x_1x_2-plane$ (with third coordinate 0) is rotated by $A$ into another point in the plane. Any vector $\\vec{x_0}$ not in the plane has its $x_3-coordinate$ multiplied by $1.07$. The iterates of the points $\\vec{w_0} = (2,0,0)$ and $\\vec{x_0} = (2,0,1)$ under multiplication by $A$ are shown in Figure 5. More about Complex EigenValues and Quaternions &gt;&gt; Discrete Dynamical Systems(Omitted)Applications to Differential Equations 微分方程$$\\vec{x(t)’} = A\\vec{x(t)}$$ where,$$\\vec{x(t)} =\\begin{bmatrix}x_1(t)\\\\\\cdots\\\\x_n(t)\\end{bmatrix},\\vec{x’(t)} =\\begin{bmatrix}x_1’(t)\\\\\\cdots\\\\x_n’(t)\\end{bmatrix},A =\\begin{bmatrix}a_{11} &amp; \\cdots &amp; a_{1n}\\\\\\cdots&amp; &amp; \\cdots\\\\a_{n1} &amp; \\cdots &amp; a_{nn}\\end{bmatrix}$$$x_1, x_2, \\cdots, x_n$ are differentiable functions of $t$. First-order Differential Equation &gt;&gt; For example : $$\\begin{bmatrix}x_1’(t)\\\\x_2’(t)\\end{bmatrix}=\\begin{bmatrix}3 &amp; 0\\\\0 &amp; -5\\end{bmatrix}\\begin{bmatrix}x_1(t)\\\\x_2(t)\\end{bmatrix}\\tag{2}$$ $$\\begin{cases}x_1’(t) = 3x_1(t)\\\\x_2’(t) = -5x_2(t)\\end{cases}\\tag{3}$$ The system (2) is said to be decoupled because each derivative of a function depends only on the function itself, not on some combination or “coupling” of both $x_1(t)$ and $x_2(t)$. From calculus, the solutions of (3) are $x_1(t) = c_1e^{3t}$ and $x_2(t) = c_2e^{-5t}$, for any constants $c_1$ and $c_2$. Each solution of equation (2) can be written in the form $$\\begin{bmatrix}x_1(t)\\\\x_2(t)\\end{bmatrix}=\\begin{bmatrix}c_1e^{3t}\\\\c_2e^{-5t}\\end{bmatrix}=c_1\\begin{bmatrix}1 \\\\0\\end{bmatrix}e^{3t}+c_2\\begin{bmatrix}0 \\\\1\\end{bmatrix}e^{-5t}$$This example suggests that for the general equation $\\vec{x}’ = A\\vec{x}$, a solution might be a linear combination of functions of the form $$\\vec{x}(t) = \\vec{v} e^{\\lambda t}\\tag{4}$$ Thus,$$\\begin{cases}\\vec{x’}(t) = \\lambda \\vec{v} e^{\\lambda t}\\\\A\\vec{x}(t) = A\\vec{v} e^{\\lambda t}\\end{cases}\\RightarrowA\\vec{v} = \\lambda\\vec{v}$$ that means, $\\lambda$ is an eigenvalue of $A$ and $\\vec{v}$ is a corresponding eigenvector. Eigenfunctions provide the key to solving systems of differential equations. For example: Suppose a particle is moving in a planar force field and its position vector $\\vec{x}$ satisfies $\\vec{x’} = A\\vec{x}$ and $\\vec{x}(0) = \\vec{x_0}$, where $$A =\\begin{bmatrix}4 &amp; -5\\\\-2 &amp; 1\\end{bmatrix},\\vec{x_0} =\\begin{bmatrix}2.9\\\\2.6\\end{bmatrix}$$Solve this initial value problem for $t \\geq 0$ and sketch the trajectory of the particle. Solution: the eigenvalues of A is $\\lambda_1 = 6, \\lambda_2 = -1$, with corresponding eigenvectors $\\vec{v_1} = (-5,2), \\vec{v_2} = (1,1)$. So the function is $$\\begin{aligned}\\vec{x}(t) &amp;= c_1\\vec{v_1}e^{\\lambda_1t} + c_2\\vec{v_2}e^{\\lambda_2t}\\\\&amp;= c_1\\begin{bmatrix}-5 \\\\ 2\\end{bmatrix} e^{6t} + c_2\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} e^{-t}\\end{aligned}$$is the solution of $\\vec{x’} = A\\vec{x}$. $$\\vec{x}(0) = \\vec{x_0} \\longrightarrowc_1\\begin{bmatrix}-5 \\\\ 2\\end{bmatrix} + c_2\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} =\\begin{bmatrix}2.9\\\\2.6\\end{bmatrix}$$Thus,$$\\vec{x}(t) = \\frac{-3}{70}\\begin{bmatrix}-5 \\\\ 2\\end{bmatrix} e^{6t} + \\frac{188}{70}\\begin{bmatrix}1 \\\\ 1\\end{bmatrix} e^{-t}$$ See Figure3, the origin is called a saddle point of the dynamical system because some trajectories approach the origin at first and then change direction and move away from the origin. A saddle point arises whenever the matrix $A$ has both positive and negative eigenvalues. The direction of greatest repulsion is the line through $\\vec{v_1}$ and $\\vec{0}$, corresponding to the positive eigenvalue. The direction of greatest attraction is the line through $\\vec{v_2}$ and $\\vec{0}$, corresponding to the negative eigenvalue. Iterative Estimates for Eigenvalues 迭代估计特征值(Numerical Analysis)The Power MethodThe power method applies to an $n \\times n$ matrix $A$ with a strictly dominant eigenvalue $\\lambda_1$, which means that $\\lambda_1$ must be larger in absolute value than all the other eigenvalues. The Power Method For Estimating A Strictly Dominant EigenValue Select an initial vector $\\vec{x_0}$ whose largest entry is 1. For k = 0, 1 …a. Compute $A\\vec{x_k}$.b. Let $\\mu_k$ be an entry in $A\\vec{x_k}$ whose absolute value is as large as possible.c. Compute $\\vec{x_{k+1}} = (\\frac{1}{\\mu_k})A\\vec{x_k}$. For almost all choices of $\\vec{x_0}$, the sequence $\\lbrace \\mu_k \\rbrace$ approaches the dominant eigenvalue, and the sequence $\\lbrace \\vec{x_k} \\rbrace$ approaches a corresponding eigenvector. The Inverse Power MethodThis method provides an approximation for any eigenvalue, provided a good initial estimate $\\alpha$ of the eigenvalue $\\lambda$ is known. The Inverse Power Method For Estimating An EigenValue $\\lambda$ of $A$ Select an initial estimate $\\alpha$ sufficiently close to $\\lambda$. Select an initial vector $\\vec{x_0}$ whose largest entry is 1. For k = 0, 1 …a. Solve $(A - \\alpha I)\\vec{y_k} = \\vec{x_k}$ for $\\vec{y_k}$.b. Let $\\mu_k$ be an entry in $\\vec{y_k}$ whose absolute value is as large as possible.c. Compute $v_{k} = \\alpha + (1/\\mu_k)$.d. Compute $\\vec{x_{k+1}} = (1/\\mu_k)\\vec{y_k}$ For almost all choices of $\\vec{x_0}$, the sequence $\\lbrace v_k \\rbrace$ approaches the eigenvalue $\\lambda$ of $A$, and the sequence $\\lbrace \\vec{x_k} \\rbrace$ approaches a corresponding eigenvector. More about Calculate EigenValues by Numerical Analysis &gt;&gt;","link":"/Math/Linear-Algebra/Algebra-C5-EigenValues-And-EigenVectors/"},{"title":"Algebra-C4-Vector-Spaces","text":"Keywords: SubSpaces, NullSpace and ColSpace, Coordinates Mapping, Dimension, Rank, Difference Equation, Markov Chains This is the Chapter4 ReadingNotes from book Linear Algebra and its Application. Vector Spaces and SubSpaces A subspace of a vector space $V$ is a subset $H$ of $V$ that has three properties:a. The zero vector of $V$ is in $H$.b. $H$ is closed under vector addition. That is, for each $\\vec{u}$ and $\\vec{v}$ in $H$, the sum $\\vec{u} + \\vec{v}$ is in $H$.c. $H$ is closed under multiplication by scalars. That is, for each $\\vec{u}$ in $H$ and each scalar $c$, the vector $c\\vec{u}$ is in $H$. if $\\vec{v_1} \\cdots \\vec{v_p}$ are in a vector space $V$, then Span{$\\vec{v_1} \\cdots \\vec{v_p}$} is a subspace of $V$. For Example: Let $H$ be the set of all vectors of the form $(a - 3b, b - a, a, b)$, where $a$ and $b$ are arbitrary scalars. That is, let $H = \\lbrace a - 3b, b - a, a, b \\rbrace$, $a$ and $b$ in $R$. Show that $H$ is a subspace of $R^4$. Proof： Write the vectors in $H$ as column vectors. Then the arbitrary vector in $H$ has the form $$\\begin{bmatrix}a-3b \\\\ b-a \\\\ a \\\\ b\\end{bmatrix}=a\\begin{bmatrix}1 \\\\ -1 \\\\ 1 \\\\ 0\\end{bmatrix} +b\\begin{bmatrix}-3 \\\\ 1 \\\\ 0 \\\\ 1\\end{bmatrix}$$ This calculation shows that H = Span{$\\vec{v_1}, \\vec{v_2}$}, where $\\vec{v_1}$ and $\\vec{v_2}$ are the vectors indicated above. Thus $H$ is a subspace of $R^4$. Null Spaces, Column Spaces, And Linear TransformationsThe Null Space of a MatrixThe null space of an $m \\times n$ matrix $A$, written as $Nul A$, is the set of all solutions of the homogeneous equation $A\\vec{x} = \\vec{0}$. In set notation,$$Nul A = \\lbrace\\vec{x} : \\vec{x} \\space in\\space R^n and A\\vec{x} = 0\\rbrace$$ The null space of an $m \\times n$ matrix $A$ is a subspace of $R^n$. An Explicit Description of Nul AFor Example: Find a spanning set for the null space of the matrix$$A =\\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4\\end{bmatrix}$$ Solution: $$\\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7 &amp; 0\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1 &amp; 0\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -2 &amp; 0 &amp; -1 &amp; 3 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3 \\\\x_4 \\\\x_5\\end{bmatrix} =\\begin{bmatrix}2x_2+x_4-3x_5 \\\\ x_2 \\\\ -2x_4+2x_5 \\\\x_4 \\\\x_5\\end{bmatrix} =x_2\\begin{bmatrix}2 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{bmatrix} +x_4\\begin{bmatrix}1 \\\\ 0 \\\\ -2 \\\\ 1 \\\\ 0\\end{bmatrix} +x_5\\begin{bmatrix}-3 \\\\ 0 \\\\ 2 \\\\ 0 \\\\ 1\\end{bmatrix}= x_2\\vec{u} + x_4\\vec{v} + x_5\\vec{w}$$ Every linear combination of $\\vec{u},\\vec{v},\\vec{w}$ is an element of $Nul A$ and vice versa. Thus $\\lbrace\\vec{u},\\vec{v},\\vec{w}\\rbrace$ is a spanning set for $Nul A$. The Column Space of a MatrixThe column space of an $m \\times n$ matrix A, written as $Col A$, is the set of all linear combinations of the columns of A. If $A = \\left[\\vec{a_1}, \\cdots, \\vec{a_n}\\right]$, then$$ColA = Span\\lbrace\\vec{a_1}, \\cdots, \\vec{a_n}\\rbrace$$ The column space of an $m \\times n$ matrix A is a subspace of $R^m$. Note that a typical vector in $Col A$ can be written as $A\\vec{x}$ for some $\\vec{x}$ because the notation $A\\vec{x}$ stands for a linear combination of the columns of $A$. That is, $$Col A = \\lbrace \\vec{b} : \\vec{b} = A \\vec{x} for \\space some\\space x \\space in \\space R^n\\rbrace$$ The Contrast Between Nul A and Col AContrast Between $NulA$ and $ColA$ for an $m \\times n$ Matrix $A$ $NulA$ $ColA$ 1. $NulA$ is a subspace of $R^n$. 1.$ColA$ is a subspace of $R^m$. 2.$NulA$ is implicitly defined; that is, you are given only a condition $A\\vec{x}=\\vec{0}$that vectors in $NulA$ must satisfy. 2.$ColA$ is explicitly defined; that is, you are told how to build vectors in $ColA$. 3. it takes time to find vectors in $NulA$. Row operations on $\\begin{bmatrix}A &amp; \\vec{0} \\end{bmatrix}$ are required. 3. It is easy to find vectors in $ColA$. The columns of $A$ are displayed; others are formed from them 4. There is no obvious relation between $NulA$ and the entries in $A$. 4. There is an obvious relation between $ColA$ and the entries in $A$, since each column of $A$ is in $ColA$. 5. A typical vector $\\vec{v}$ in $NulA$ has the property that $A\\vec{v} = \\vec{0}$. 5. A typical vector $\\vec{v}$ in$ColA$has the property that the equation $A\\vec{x} = \\vec{v}$ is consistent. 6. Given a specific vector $\\vec{v}$, it is easy to tell if $\\vec{v}$ is in $NulA$. Just compute $A\\vec{v}$. 6. Given a specific vector $\\vec{v}$, it may take time to tell if $\\vec{v}$ is in $ColA$. Row operations on are required. 7. $NulA = \\lbrace\\vec{0}\\rbrace$ if and only if the equation $A\\vec{x} = \\vec{0}$ has only the trivial solution. 7. $ColA = R^m$ if and only if the equation $A\\vec{x} = \\vec{b}$ has a solution for every $\\vec{b}$ in $R^m$. 8. $NulA = \\lbrace\\vec{0}\\rbrace$ if and only if the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ is one-to-one. 8.$ColA = R^m$ if and only if the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ maps $R^n$ onto $R^m$. Linearly Independent Sets; Bases An indexed set $\\lbrace \\vec{v_1}, \\cdots, \\vec{v_p} \\rbrace$ of two or more vectors, with $\\vec{v_1} \\neq \\vec{0}$, is linearly depentdent if and only if some $\\vec{v_j}$(with $j &gt; 1$) is a linear combination if the preceding vectors $\\vec{v_1}, \\cdots, \\vec{v_{j-1}}$. Let $H$ be a subspace of a vector space $V$. An indexed set of vectors $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_p}\\rbrace$ in $V$ is a basis for $H$ if(i) $\\beta$ is a linearly independent set, and(ii) the subspace spanned by $\\beta$ coincides with $H$; that is $$H = Span \\lbrace \\vec{b_1}, \\cdots, \\vec{b_p} \\rbrace$$ For Example: Let $S = \\lbrace 1, t, t^2, \\cdots, t^n \\rbrace$. Verify that $S$ is a basis for $P_n$. This basis is called the standard basis for $P_n$. Solution: Certainly $S$ spans $P_n$. To show that S is linearly independent, suppose that $c_0, \\cdots, c_n$ satisfy $$c_0 \\cdot 1 + c_1 \\cdot t + c_2 \\cdot t^2 + \\cdots + c_n \\cdot t^n = \\vec{0_t}\\tag{2}$$ This equality means that the polynomial on the left has the same values as the zero polynomial on the right. A fundamental theorem in algebra says that the only polynomial in $P_n$ with more than $n$ zeros is the zero polynomial. That is, equation (2) holds for all $t$ only if $c_0 = \\cdots = c_n = 0$. This proves that $S$ is linearly independent and hence is a basis for $P_n$. The Spanning Set Theorem Let $S = \\lbrace \\vec{v_1}, \\cdots, \\vec{v_p} \\rbrace$ be a set in $V$, and let $H = \\lbrace \\vec{v_1}, \\cdots, \\vec{v_p} \\rbrace$.a. If one of the vectors in $S$-say, $\\vec{v_k}$- is a linear combination of the remaining vectors in $S$, then the set formed from $S$ by removing $\\vec{v_k}$ still spans $H$.b. If $H \\neq \\lbrace \\vec{0}\\rbrace$, some subset of $S$ is a basis for $H$. Bases for Nul A and Col A The pivot columns of a matrix A form a basis for Col A. Two Views of a BasisFor Example: The following three sets in $R^3$ show how a linearly independent set can be enlarged to a basis and how further enlargement destroys the linear independence of the set. Also, a spanning set can be shrunk to a basis, but further shrinking destroys the spanning property. Linearly independent,but does not span $R^3$:$$\\lbrace\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}2\\\\3\\\\0\\end{bmatrix}\\rbrace$$A basis for $R^3$:$$\\lbrace\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}2\\\\3\\\\0\\end{bmatrix},\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix}\\rbrace$$Spans $R^3$ but is linearly dependent:$$\\lbrace\\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix},\\begin{bmatrix}2\\\\3\\\\0\\end{bmatrix},\\begin{bmatrix}4\\\\5\\\\6\\end{bmatrix},\\begin{bmatrix}7\\\\8\\\\9\\end{bmatrix}\\rbrace$$ Coordinate SystemsLet $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$ be a basis for a vector space $V$. Then for each $\\vec{x}$ in $V$, there exists a unique set of scalars $c_1, \\cdots, c_n$ such that$$\\vec{x} = c_1\\vec{b_1} + \\cdots + c_n \\vec{b_n}$$ Suppose $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$ is a basis for $V$ and $\\vec{x}$ is in $V$. The coordinates of x relative to the basis $\\beta$ (or the $\\beta $-coordinates of x) are the weights $c_1, \\cdots, c_n$ such that$$\\vec{x} = c_1\\vec{b_1} + \\cdots + c_n\\vec{b_n}$$If $c_1, \\cdots, c_n$ are the $\\beta$-coordinates of $\\vec{x}$, then the vector in $R^n$$$[\\vec{x}]_\\beta =\\begin{bmatrix}c_1 \\\\ \\cdots \\\\c_n\\end{bmatrix}$$is the coordinate vector of $\\vec{x}$ (relative to $\\beta$), the mapping $\\vec{x} \\mapsto [\\vec{x}]_\\beta$ is the coordinate mapping (determined by $\\beta$). A Graphical Interpretation of Coordinates1 unit in the $\\vec{e_1}$ direction, 6 units in the $\\vec{e_2}$ direction:$$\\vec{x} =\\begin{bmatrix}1 \\\\ 6\\end{bmatrix}$$2 units in the $\\vec{b_1}$ direction, 3 units in the $\\vec{b_2}$ direction:$$[\\vec{x}]_\\beta =\\begin{bmatrix}-2 \\\\ 3\\end{bmatrix}$$ Coordinates in $R^n$For Example: Let $\\vec{b_1} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$,$\\vec{b_2} = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix}$,$\\vec{x} = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}$, and $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$. Find the coordinate vector $[\\vec{x}]_\\beta$ of $\\vec{x}$ relative to $\\beta$. Solution: The $\\beta$-coordinates $c_1, c_2$ of $\\vec{x}$ satisfy $$c_1\\begin{bmatrix}2 \\\\ 1\\end{bmatrix} + c_2\\begin{bmatrix}-1 \\\\ 1\\end{bmatrix}= \\begin{bmatrix}4 \\\\ 5\\end{bmatrix},\\begin{bmatrix}2 &amp; -1 \\\\ 1 &amp; 1\\end{bmatrix} \\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix}= \\begin{bmatrix}4 \\\\ 5\\end{bmatrix}\\tag{3}$$ This equation can be solved by row operations on an augmented matrix or by using the inverse of the matrix on the left. In any case, the solution is$$c_1 = 3, c_2 = 2 \\\\\\vec{x} = 3\\vec{b_1} + 2\\vec{b_2}, and \\space[\\vec{x}]_\\beta = \\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix}=\\begin{bmatrix}3 \\\\ 2\\end{bmatrix}$$ The matrix in (3) changes the $\\beta$ -coordinates of a vector $\\vec{x}$ into the standard coordinates for $\\vec{x}$. An analogous change of coordinates can be carried out in $R^n$ for a basis $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$. Let $$P_\\beta =\\begin{bmatrix}\\vec{b_1} &amp; \\vec{b_2} &amp; \\cdots &amp; \\vec{b_n}\\end{bmatrix}$$Then the vector equation $$\\vec{x} = c_1\\vec{b_1} + c_2\\vec{b_2} + \\cdots + c_n\\vec{b_n}$$is equivalent to$$\\vec{x} = P_\\beta[\\vec{x}]_\\beta$$ We call $P_\\beta$ the change-of-coordinates matrix from $\\beta$ to the standard basis in $R^n$. Left-multiplication by $P_\\beta$ transforms the coordinate vector $[\\vec{x}]_\\beta$ into $\\vec{x}$. Since the columns of $P_\\beta$ form a basis for $R^n$, $P_\\beta$ is invertible. Left-multiplication by $P_\\beta^{-1}$converts $\\vec{x}$ into its $\\beta$-coordinate vector: $$P_\\beta^{-1}\\vec{x} = [\\vec{x}]_\\beta$$ The Coordinate MappingChoosing a basis $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$ for a vector space $V$ introduces a coordinate system in $V$. The coordinate mapping $\\vec{x} \\mapsto [\\vec{x}]_\\beta$ connects the possibly unfamiliar space $V$ to the familiar space $R^n$: Let $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n} \\rbrace$ be a basis for a vector space $V$. Then the coordinate mapping $\\vec{x} \\mapsto [\\vec{x}]_\\beta$ is a one-to-one linear transformation from $V$ onto $R^n$.这里一定要注意，beta是向量空间V的基向量，x也是属于V空间的向量 Proof: two typical vectors in $V$:$$\\vec{u} = c_1\\vec{b_1} + \\cdots + c_n\\vec{b_n},\\vec{w} = d_1\\vec{b_1} + \\cdots + d_n\\vec{b_n}\\stackrel{addition-operation}\\longrightarrow\\vec{u} + \\vec{w} = (c_1 + d_1)\\vec{b_1} + \\cdots + (c_n + d_n)\\vec{b_n}\\longrightarrow\\\\[\\vec{u} + \\vec{w}]_\\beta =\\begin{bmatrix}c_1 + d_1 \\\\\\cdots \\\\c_n + d_n\\end{bmatrix} =\\begin{bmatrix}c_1 \\\\\\cdots \\\\c_n\\end{bmatrix} +\\begin{bmatrix}d_1 \\\\\\cdots \\\\d_n\\end{bmatrix} =[\\vec{u}]_\\beta + [\\vec{w}]_\\beta\\\\常量乘法也一样性质，略过$$ Thus the coordinate mapping also preserves scalar multiplication and hence is a linear transformation. In general, a one-to-one linear transformation from a vector space $V$ onto a vector space$W$ is called an isomorphism from $V$ onto $W$. For Example: Let $\\beta$ be the standard basis of the space $P_3$ of polynomials; that is, let $\\beta = \\lbrace \\vec{1}, \\vec{t}, \\vec{t^2}, \\vec{t^3}\\rbrace$. A typical element $p$ of $P_3$ has the form: $$\\vec{p(t)} = a_0 + a_1\\vec{t} + a_2\\vec{t^2} + a_3\\vec{t^3}$$Since $\\vec{p}$is already displayed as a linear combination of the standard basis vectors, weconclude that:$$[\\vec{p}]_\\beta =\\begin{bmatrix}a_0\\\\a_1\\\\a_2\\\\a_3\\end{bmatrix}$$ Thus the coordinate mapping $\\vec{p} \\mapsto [\\vec{p}]_\\beta$is an isomorphism from$P_3$ onto $R_4$. The Dimension of A Vector Space If a vector space $V$ has a basis $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$, then any set in $V$ containing more than $n$ vectors must be linearly dependent. If a vector space $V$ has a basis of $n$ vectors, then every basis of $V$ must consist of exactly $n$ vectors. If $V$ is spanned by a finite set, then $V$ is said to be finite-dimensional, and the dimension of $V$ , written as $dim V$ , is the number of vectors in a basis for $V$. The dimension of the zero vector space ${\\vec{0}}$ is defined to be zero. If $V$ is not spanned by a finite set, then V is said to be infinite-dimensional. For Example: Find the dimension of the subspace:$H = \\lbrace \\begin{bmatrix} a - 3b + 6c\\\\5a + 4d\\\\ b - 2c - d\\\\5d\\end{bmatrix} a,b,c,d \\space in R \\rbrace$ Solution: $H$ is the set of all linear combinations of the vectors:$$\\vec{v_1} =\\begin{bmatrix}1\\\\5\\\\0\\\\0\\end{bmatrix},\\vec{v_2} =\\begin{bmatrix}-3\\\\0\\\\1\\\\0\\end{bmatrix},\\vec{v_3} =\\begin{bmatrix}6\\\\0\\\\-2\\\\0\\end{bmatrix},\\vec{v_4} =\\begin{bmatrix}0\\\\4\\\\-1\\\\5\\end{bmatrix}$$ $\\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_4}\\rbrace$ is linearly independent and hence is a basis for $H$. Thus $dim H = 3$. Subspaces of a Finite-Dimensional SpaceLet $H$ be a subspace of a finite-dimensional vector space $V$ . Any linearly independent set in $H$ can be expanded, if necessary, to a basis for $H$ . Also, $H$ is finite-dimensional and $$dim H \\leqslant dim V$$ The Dimensions of $Nul A$ and $Col A$ The dimension of $Nul A$ is the number of free variables in the equation $A\\vec{x} = \\vec{0}$, and the dimension of $Col A$ is the number of pivot columns in $A$. For Example: Find the dimensions of the null space and the column space of $A = \\begin{bmatrix}-3 &amp; 6 &amp; -1 &amp; 1 &amp; -7\\\\1 &amp; -2 &amp; 2 &amp; 3 &amp; -1\\\\2 &amp; -4 &amp; 5 &amp; 8 &amp; -4\\end{bmatrix}$ $$\\begin{bmatrix}1 &amp; -2 &amp; 2 &amp; 3 &amp; -1 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 2 &amp; -2 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\x_2, x_4, x_5: free\\space variables\\\\\\longrightarrow dimNulA = 3, dimColA = 2.$$ RankThe Row Space If two matrices $A$ and $B$ are row equivalent, then their row spaces are the same. If $B$ is in echelon form, the nonzero rows of $B$ form a basis for the row space of $A$ as well as for that of $B$. For Example: Find bases for the row space, the column space, and the null space of the matrix $\\begin{bmatrix}-2 &amp; -5 &amp; 8 &amp; 0 &amp; -17\\\\1 &amp; 3 &amp; -5 &amp; 1 &amp; 5\\\\3 &amp; 11 &amp; -19 &amp; 7 &amp; 1\\\\1 &amp; 7 &amp; -13 &amp; 5 &amp; -3\\end{bmatrix}$. $$A \\sim B =\\begin{bmatrix}1 &amp; 3 &amp; -5 &amp; 1 &amp; 5\\\\0 &amp; 1 &amp; -2 &amp; 2 &amp; -7\\\\0 &amp; 0 &amp; 0 &amp; -4 &amp; 20\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\sim C =\\begin{bmatrix}1 &amp; 0 &amp; 1 &amp; 0 &amp; 1\\\\0 &amp; 1 &amp; -2 &amp; 0 &amp; 3\\\\0 &amp; 0 &amp; 0 &amp; 1 &amp; -5\\\\0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}\\\\（只需看前几行非零）Basis-for-RowA:\\lbrace(1, 3, 5, 1, 5),(0, 1, 2, 2, 7),(0, 0, 0, 4, 20)\\rbrace\\\\（要看pivot \\space columns）Basis-for-ColA:\\lbrace\\begin{bmatrix}-2\\\\ 1\\\\ 3\\\\ 1\\end{bmatrix},\\begin{bmatrix}-5\\\\ 3\\\\ 11\\\\ 7\\end{bmatrix},\\begin{bmatrix}0\\\\ 1\\\\ 7\\\\ 5\\end{bmatrix}\\rbrace\\\\（求解，有两个自由变量）Basis-for-NullA:\\lbrace\\begin{bmatrix}-1\\\\ 2\\\\ 1\\\\ 0\\\\ 0\\end{bmatrix},\\begin{bmatrix}-1\\\\ -3\\\\ 0\\\\ 5\\\\ 1\\end{bmatrix}\\rbrace$$ The Rank Theorem The rank of $A$ is the dimension of the column space of $A$. The dimensions of the column space and the row space of an $m \\times n$ matrix $A$ are equal. This common dimension, the rank of $A$, also equals the number of pivot positions in $A$ and satisfies the equation $$rank A + dim NullA = n\\\\其实就是：\\\\\\lbrace number-of-pivot-columns\\rbrace +\\lbrace number-of-nonpivot-columns\\rbrace =\\lbrace number-of-columns\\rbrace$$ Applications to Systems of EquationsFor Example: A scientist has found two solutions to a homogeneous system of 40 equations in 42 variables. The two solutions are not multiples, and all other solutions can be constructed by adding together appropriate multiples of these two solutions. Can the scientist be certain that an associated nonhomogeneous system (with the same coefficients) has a solution? Solution: Yes. Let $A$ be the $40 \\times 42$ coefficient matrix of the system. The given information implies that the two solutions are linearly independent and span $Nul A$.So $dim Nul A = 2$. By the Rank Theorem, $dim Col A = 42 - 2 = 40$. Since $R^{40}$ is the only subspace of $R^{40}$ whose dimension is $40$, $Col A$ must be all of $R^{40}$. This means that every nonhomogeneous equation $Ax = b$ has a solution. Rank and the Invertible Matrix Theorem The Invertible Matrix TheoremLet $A$ be an $n \\times n$ matrix. Then the following statements are each equivalent to the statement that $A$ is an invertible matrix.m.The columns of $A$ form a basis of $R^n$.n. $Col A = R^n$o. $dim Col A = n$p. $rank A = n$q. $Nul A = \\lbrace 0 \\rbrace $r. $dim Nul A = 0$ Change of BasisFor Example: Consider two bases $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$ and $\\gamma = \\lbrace \\vec{c_1}, \\vec{c_2}\\rbrace$ for a vector space $V$, such that $$\\vec{b_1} = 4 \\vec{c_1} + \\vec{c_2} \\space and \\space \\vec{b_2} = -6 \\vec{c_1} + \\vec{c_2}\\tag{1}$$ Suppose $$\\vec{x} = 3\\vec{b_1} + \\vec{b_2}\\tag{2}$$ That is, suppose $[\\vec{x}]_\\beta = \\begin{bmatrix}3\\\\1\\end{bmatrix}$. Find $[\\vec{x}]_\\gamma$. Solution: Apply the coordinate mapping determined by $\\gamma$ to $\\vec{x}$ in (2). Since the coordinate mapping is a linear transformation. $$[\\vec{x}]_\\gamma = [3\\vec{b_1}+\\vec{b_2}]_\\gamma= 3[\\vec{b_1}]_\\gamma+[\\vec{b_2}]_\\gamma$$ We can write this vector equation as a matrix equation, using the vectors in the linear combination as the columns of a matrix: $$[\\vec{x}]_\\gamma =\\begin{bmatrix}[\\vec{b_1}]_\\gamma &amp; [\\vec{b_2}]_\\gamma\\end{bmatrix}\\begin{bmatrix}3\\\\ 1\\end{bmatrix}$$ From (1), $$[\\vec{b_1}]_\\gamma =\\begin{bmatrix}4\\\\ 1\\end{bmatrix},[\\vec{b_2}]_\\gamma =\\begin{bmatrix}-6\\\\ 1\\end{bmatrix}$$ Thus, $$[\\vec{x}]_\\gamma =\\begin{bmatrix}4 &amp; -6\\\\1 &amp; 1\\end{bmatrix}\\begin{bmatrix}3\\\\ 1\\end{bmatrix}=\\begin{bmatrix}6\\\\ 4\\end{bmatrix}$$ Let $\\beta = \\lbrace \\vec{b_1}, \\cdots, \\vec{b_n}\\rbrace$ and $\\gamma = \\lbrace \\vec{c_1}, \\cdots, \\vec{c_n}\\rbrace$ be bases of a vector space $V$. Then there is a unique $n \\times n$ matrix $\\gamma \\stackrel{P}\\leftarrow \\beta$ such that $$[\\vec{x}]_\\gamma =\\gamma \\stackrel{P}\\leftarrow \\beta [\\vec{x}]_\\beta$$ The columns of $\\gamma \\stackrel{P}\\leftarrow \\beta$ are the $\\gamma$-coordinate vectors of the vectors in the basis $\\beta$. That is, $$\\gamma \\stackrel{P}\\leftarrow \\beta =\\begin{bmatrix}[\\vec{b_1}]\\gamma &amp; [\\vec{b_2}]\\gamma \\cdots [\\vec{b_n}]\\gamma\\end{bmatrix}$$ $\\gamma \\stackrel{P}\\leftarrow \\beta$ is called the change-of-coordinates matrix from $\\beta$ to $\\gamma$.Multiplication by $\\gamma \\stackrel{P}\\leftarrow \\beta$ converts $\\beta$-coordinates into $\\gamma$-coordinates. Change of Basis in $R^n$For Example: Let $\\vec{b_1} = \\begin{bmatrix}-9\\\\1\\end{bmatrix}$,$\\vec{b_2} = \\begin{bmatrix}-5\\\\-1\\end{bmatrix}$,$\\vec{c_1} = \\begin{bmatrix}1\\\\-4\\end{bmatrix}$,$\\vec{c_2} = \\begin{bmatrix}3\\\\-5\\end{bmatrix}$, and consider the bases for $R^2$ given by $\\beta = \\lbrace \\vec{b_1}, \\vec{b_2}\\rbrace$ and $\\gamma = \\lbrace \\vec{c_1}, \\vec{c_2}\\rbrace$. Find the change-of-coordinates matrix from $\\beta$ to $\\gamma$. Solution: The matrix $\\gamma \\stackrel{P}\\leftarrow \\beta$ involves the $\\gamma$-coordinate vectors of $\\vec{b_1}$ and $\\vec{b_2}$. Let $[\\vec{b_1}_\\gamma] = \\begin{bmatrix}x_1\\\\x_2\\end{bmatrix}$ and $[\\vec{b_2}_\\gamma] = \\begin{bmatrix}y_1\\\\y_2\\end{bmatrix}$. Then, by definition, $$\\begin{bmatrix}\\vec{c_1} &amp; \\vec{c_2}\\end{bmatrix}\\begin{bmatrix}\\vec{x_1} \\\\ \\vec{x_2}\\end{bmatrix}=\\vec{b_1} \\space and \\space\\begin{bmatrix}\\vec{c_1} &amp; \\vec{c_2}\\end{bmatrix}\\begin{bmatrix}\\vec{y_1} \\\\ \\vec{y_2}\\end{bmatrix}=\\vec{b_2}$$ To solve both systems simultaneously, augment the coefficient matrix with $\\vec{b1}$ and $\\vec{b2}$, and row reduce: $$\\left[ \\begin{array}{cc|c} \\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{b_1} &amp; \\vec{b_2}\\end{array}\\right] =\\left[ \\begin{array}{cc|cc} -1 &amp; 3 &amp; -9 &amp; -5 \\\\ -4 &amp; -5 &amp; 1 &amp; -1\\end{array}\\right]\\sim\\left[ \\begin{array}{cc|cc} 1 &amp; 0 &amp; 6 &amp; 4 \\\\ 0 &amp; 1 &amp; -5 &amp; -3\\end{array}\\right]$$ Thus $$[\\vec{b_1}]_\\gamma =\\begin{bmatrix}6 \\\\ -5\\end{bmatrix}\\space and \\space[\\vec{b_2}]_\\gamma =\\begin{bmatrix}4 \\\\ -3\\end{bmatrix}$$ The desired change-of-coordinates matrix is therefore $$\\gamma \\stackrel{P}\\leftarrow \\beta=\\begin{bmatrix}[\\vec{b_1}]_\\gamma &amp; [\\vec{b_2}]_\\gamma\\end{bmatrix}=\\begin{bmatrix}6 &amp; 4 \\\\ -5 &amp; -3\\end{bmatrix}$$ An analogous procedure works for finding the change-of-coordinates matrix between any two bases in $R^n$: $$\\left[ \\begin{array}{cc|c} \\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{b_1} &amp; \\vec{b_2}\\end{array}\\right]\\sim\\left[ \\begin{array}{c|c} I &amp; \\gamma \\stackrel{P}\\leftarrow \\beta \\end{array}\\right]$$ the change-ofcoordinate matrices $P_\\beta$ and $P_\\gamma$ that convert $\\beta$-coordinates and $\\gamma$-coordinates, respectively, into standard coordinates. $$P_\\beta[\\vec{x}]_\\beta = \\vec{x}, \\spaceP_\\gamma[\\vec{x}]_\\gamma = \\vec{x}, \\space and \\space[\\vec{x}]_\\gamma = P_\\gamma^{-1}\\vec{x}$$ Thus, $$[\\vec{x}]_\\gamma = P_\\gamma^{-1}\\vec{x} = P_\\gamma^{-1}P_\\beta[\\vec{x}]_\\beta\\\\\\Rightarrow\\\\\\gamma \\stackrel{P}\\leftarrow \\beta = P_\\gamma^{-1}P_\\beta$$ More about Coordinates Space and Transformations in Graphics &gt;&gt; Applications to Difference Equation差分方程Linear Independence in the Space $S$ of Signalswe consider a set of only three signals in $S$, say, $\\lbrace u_k \\rbrace,\\lbrace v_k \\rbrace,\\lbrace w_k \\rbrace$.They are linearly independent precisely when the equation $$c_1 u_k + c_2 v_k + c_3 w_k = 0 for\\space all \\space k\\tag{1}$$ implies that $c1 = 0, c2 = 0, c3 = 0$.Then equation (1) holds for any three consecutive values of $k$, say, $k, k + 1, \\space and \\space k + 2$.Hence $c1, c2, c3$ satisfy $$\\begin{bmatrix}u_k &amp; v_k &amp; w_k\\\\u_{k+1} &amp; v_{k+1} &amp; w_{k+1}\\\\u_{k+2} &amp; v_{k+2} &amp; w_{k+2}\\end{bmatrix}\\begin{bmatrix}c_1\\\\c_2\\\\c_3\\\\\\end{bmatrix}=\\begin{bmatrix}0\\\\0\\\\0\\end{bmatrix}\\tag{2}$$ The coefficient matrix in this system is called the Casorati matrix of the signals, and the determinant of the matrix is called the Casoratian of $u_k, v_k, w_k$. If the Casorati matrix is invertible for at least one value of k, then (2) will imply that $c1 = c2 = c3 = 0$, which will prove that the three signals are linearly independent. For Example: Verify that $1^k, (-2)^k, and \\space 3^k$ are linearly independent signals. Solution: The Casorati matrix is: $$\\begin{bmatrix}1^k &amp; (-2)^k &amp; 3^k\\\\1^{k+1} &amp; (-2){k+1} &amp; 3^{k+1}\\\\1^{k+2} &amp; (-2)^{k+2} &amp; 3^{k+2}\\end{bmatrix}$$ The Casorati matrix is invertible for $k = 0$. So $1^k, (-2)^k, 3^k$ are linearly independent. Linear Difference Equations（线性差分方程）Given scalars $a_0,\\cdots, a_n$, with $a_0$ and $a_n$ nonzero, and given a signal $\\lbrace z_k \\rbrace$, the equation $$a_0y_{k+n} + a_1y_{k+n-1} + \\cdots + a_ny_k = z_k for\\space all\\space k \\tag{3}$$ is called a linear difference equation (or linear recurrence relation) of order $n$. For simplicity, $a_0$ is often taken equal to 1. If $\\lbrace z_k \\rbrace$ is the zero sequence, the equation is homogeneous; otherwise, the equation is nonhomogeneous. 在第一章节的笔记中，有提一嘴Fibonacci数列是差分方程，具体形式如下,为什么看起来是差分方程的定义不太一样？（接着看下面的内容） $$\\vec{x_{k+1}} = A\\vec{x_k}, k = 0,1,2,…\\tag{5}$$ $$Fibonacci\\\\f(0) = 0, f(1) = 1\\\\f(n) = f(n-1) + f(n-2), n &gt; 1$$建立矩阵方程如下：$$let \\space F(n) = \\begin{bmatrix}f(n) \\\\ f(n+1)\\end{bmatrix}\\\\AF(n) = F(n+1)\\rightarrowA\\begin{bmatrix}f(n) \\\\ f(n+1)\\end{bmatrix} =\\begin{bmatrix}f(n+1) \\\\ f(n+2)\\end{bmatrix}\\\\\\rightarrowA = \\begin{bmatrix}0 &amp; 1 \\\\ 1 &amp; 1\\end{bmatrix}\\\\\\RightarrowF(n) = A^nF(0), F(0) = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix}$$ For Example: In digital signal processing, a difference equation such as (3) describes a linear filter, and $a_0,\\cdots, a_n$ are called the filter coefficients. If $\\lbrace y_k \\rbrace$ is treated as the input and $\\lbrace z_k \\rbrace$ as the output, then the solutions of the associated homogeneous equation are the signals that are filtered out and transformed into the zero signal. Let us feed two different signals into the filter $$0.35y_{k+2} + 0.5y_{k+1} + 0.35y_k = z_k$$ Here $0.35$ is an abbreviation for $\\sqrt2/4$. The first signal is created by sampling the continuous signal $y = \\cos(\\pi t / 4)$at integer values of $t$, as in Figure3(a). The discrete signal is $$\\lbrace y_k\\rbrace = \\lbrace \\cdots, \\cos(0), \\cos(\\pi/4), cos(2\\pi/4), cos(3\\pi/4), \\cdots\\rbrace$$ For simplicity, write $\\pm0.7$ in place of $\\pm\\sqrt2/2$, so that $$\\lbrace y_k\\rbrace = \\lbrace \\cdots, 1, 0.7, 0, -0.7,-1,-0.7,0,0.7,1,0.7,0 \\cdots\\rbrace$$ Table 1 shows a calculation of the output sequence $\\lbrace z_k \\rbrace$, where $0.35\\cdot0.7$ is an abbreviation for $(\\sqrt2/4)(\\sqrt2/2)=0.25$. The output is $\\lbrace y_k\\rbrace$, shifted by one term. A different input signal is produced from the higher frequency signal $y = \\cos(3\\pi/4)$, shown in Figure 3(b). Sampling at the same rate as before produces a new input sequence: $$\\lbrace w_k\\rbrace = \\lbrace \\cdots, 1, -0.7, 0, 0.7, -1, 0.7, 0 \\cdots\\rbrace$$ When $\\lbrace w_k\\rbrace$ is fed into the filter, the output is the zero sequence. The filter, called a low-pass filter（低通滤波器）, lets $\\lbrace y_k \\rbrace$ pass through, but stops the higher frequency $\\lbrace w_k \\rbrace$. In many applications, a sequence $\\lbrace z_k \\rbrace$ is specified for the right side of a difference equation (3), and a $\\lbrace y_k \\rbrace$ that satisfies (3) is called a solution of the equation. The next example shows how to find solutions for a homogeneous equation. （比如机器学习就类似于如此，给定输入和输出，学习最优权重） For Example: Solutions of a homogeneous difference equation often have the form $y_k = r^k$ for some $r$. Find some solutions of the equation $$y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0 , for\\space all\\space k$$ Solution: Substitute $r^k$ for $y_k$ in the equation and factor the left side: $$r^{k+3} - 2r^{k+2} - 5r^{k+1} + 6r^{k} = 0\\rightarrowr^k(r-1)(r+2)(r-3) = 0$$ Thus, $(1)^k, (-2)^k, (3)^k$ are all solutions. In general, a nonzero signal $r^k$ satisfies the homogeneous difference equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_ny_k = 0 for\\space all\\space k $$ if and only if $r$ is a root of the auxiliary equation（辅助方程） $$r^n + a_1r^{n-1} + \\cdots + a_{n-1}r + a_n = 0$$ Solution Sets of Linear Difference Equationsif $a_n \\neq 0$ and if $\\lbrace z_k \\rbrace$ is given, the equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = z_k, for\\space all\\space k$$ has a unique solution whenever $y_0, \\cdots, y_{n-1}$ are specified. The set $H$ of all solutions of the nth-order homogeneous linear difference equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = 0, for\\space all\\space k$$ is an $n$-dimensional vector space. Nonhomogeneous Equations（非齐次差分方程）The general solution of the nonhomogeneous difference equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = z_k, for\\space all\\space k$$ For Example: Verify that the signal $y_k = k^2$ satisfies the difference equation $$y_{k+2} - 4y_{k+1} + 3y_k = -4k, for\\space all\\space k\\tag{12}$$ Then find a description of all solutions of this equation. Solution: Substitute $k^2$ for $y_k$ on the left side of (12): $$(k+2)^2-4(k+1)^2+3k^2 = -4k$$ so $k^2$is indeed a solution of (12). The next step is to solve the homogeneous equation $$y_{k+2} - 4y_{k+1} + 3y_k = 0, for\\space all\\space k\\tag{13}$$ The auxiliary equation is $$r^2 -4r + 3 = (r-1)(r+3) = 0$$ The roots are $r = 1,3$.So two solutions of the homogeneous difference equation are $1^k$ and $3^k$. They are obviously not multiples of each other, so they are linearly independent signals. the solution space is two-dimensional, so 3k and 1k form a basis for the set of solutions of equation (13).Translating that set by a particular solution of the nonhomogeneous equation (12), we obtain the general solution of (12): $$k^2 + c_1 1^k + c_2 3^k, or \\space k^2 + c_1 + c_23^k$$ Figure 4 gives a geometric visualization of the two solution sets. Each point in the figure corresponds to one signal in S. Reduction to Systems of First-Order Equations（约简到一阶方程组） 接下来可以解释Fibonacci数列是差分方程 $$AF(n) = F(n+1)\\rightarrowA\\begin{bmatrix}f(n)\\\\f(n+1)\\end{bmatrix} =\\begin{bmatrix}f(n+1)\\\\f(n+2)\\end{bmatrix}\\$$ A modern way to study a homogeneous $nth$-order linear difference equation is to replace it by an equivalent system of first-order difference equations, written in the form $$\\vec{x_{k+1}} = A\\vec{x_{k}}, for \\space all\\space k$$ where the vectors $\\vec{x_{k}}$ are in $R^n$ and $A$ is an $n \\times n$ matrix. For Example: Write the following difference equation as a first-order system: $$y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0, for \\space all\\space k$$ Solution: for each k, set $$\\vec{x_k} =\\begin{bmatrix}y_k\\\\y_{k+1}\\\\y_{k+2}\\end{bmatrix}$$ The difference equation says that $y_{k+3} - 2y_{k+2} - 5y_{k+1} + 6y_k = 0$, so $$\\vec{x_{k+1}} =\\begin{bmatrix}y_{k+1} \\\\ y_{k+2} \\\\ y_{k+3}\\end{bmatrix} =\\begin{bmatrix}0 + y_{k+1} + 0\\\\0 + 0 + y_{k+2}\\\\-6y_k + 5y_{k+1} + 2y_{k+2}\\end{bmatrix} =\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\\\-6 &amp; 5 &amp; 2\\end{bmatrix}\\begin{bmatrix}y_{k}\\\\y_{k+1}\\\\y_{k+2}\\end{bmatrix}$$ That is, $$\\vec{x_{k+1}} = A\\vec{x_{k}}, for \\space all\\space k , whereA =\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\\\-6 &amp; 5 &amp; 2\\end{bmatrix}$$ In general, the equation $$y_{k+n} + a_1y_{k+n-1} + \\cdots + a_{n-1}y_{k+1} + a_ny_k = 0, for\\space all\\space k$$ can be written as $\\vec{x_{k+1}} = A\\vec{x_{k}}, for \\space all\\space k$, where $$\\vec{x_{k}} = \\begin{bmatrix}y_{k}\\\\ y_{k+1} \\\\ \\cdots \\\\ y_{k+n-1} \\end{bmatrix},A = \\begin{bmatrix}0 &amp; 1 &amp; 0 &amp; \\cdots &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; \\cdots &amp; 0\\\\\\cdots\\\\0 &amp; 0 &amp; 0 &amp; \\cdots &amp; 1\\\\-a_n &amp; -a_{n-1} &amp; -a_{n-2} &amp; \\cdots &amp; -a_1\\end{bmatrix}$$ Applications to Markov ChainsFor example, if the population of a city and its suburbs were measured each year, then a vector such as $$\\vec{x_0} =\\begin{bmatrix}0.60\\\\0.40\\end{bmatrix}\\tag{1}$$ could indicate that 60% of the population lives in the city and 40% in the suburbs. The decimals in $\\vec{x_0}$ add up to 1 because they account for the entire population of the region.Percentages are more convenient for our purposes here than population totals. A vector with nonnegative entries that add up to 1 is called a probability vector（概率向量）. A stochastic matrix（随机矩阵） is a square matrix whose columns are probability vectors. A Markov chain（马尔科夫链） is a sequence of probability vectors $\\vec{x_0}, \\vec{x_1}, \\vec{x_2}$ together with a stochastic matrix $P$ , such that $$\\vec{x_1} = P\\vec{x_0}, \\vec{x_2} = P\\vec{x_1}, \\vec{x_3} = P\\vec{x_2}, \\cdots\\\\\\longrightarrow\\vec{x_{k+1}} = P\\vec{x_k}$$ $\\vec{x_k}$ is often called a state vector（状态向量). Predicting the Distant FutureFor Example: Let $P = \\begin{bmatrix}0.5 &amp; 0.2 &amp; 0.3\\\\0.3 &amp; 0.8 &amp; 0.3\\\\0.2 &amp; 0 &amp; 0.4\\end{bmatrix}$ and $\\vec{x_0} = \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix}$. Consider a system whose state is described by the Markov chain $\\vec{x_{k+1}} = P\\vec{x_{k}}, for k = 0,1,\\cdots$ What happens to the system as time passes? Compute the state vectors $\\vec{x_1},\\cdots, \\vec{x_{15}}$to find out. Solution:$$\\vec{x_1} = P\\vec{x_0} = \\begin{bmatrix}0.5\\\\0.3\\\\0.2\\end{bmatrix},\\\\\\\\\\cdots\\\\\\vec{x_{14}} = P\\vec{x_{13}} = \\begin{bmatrix}0.3001\\\\0.59996\\\\0.10002\\end{bmatrix},\\\\\\vec{x_{15}} = P\\vec{x_{14}} = \\begin{bmatrix}0.3001\\\\0.59998\\\\0.10001\\end{bmatrix}$$These vectors seem to be approaching $\\vec{q} = \\begin{bmatrix}0.3\\\\0.6\\\\0.1\\end{bmatrix}$The probabilities are hardly changing from one value of $k$ to the next. $$P\\vec{q} = \\begin{bmatrix}0.5 &amp; 0.2 &amp; 0.3\\\\0.3 &amp; 0.8 &amp; 0.3\\\\0.2 &amp; 0 &amp; 0.4\\end{bmatrix}\\begin{bmatrix}0.3\\\\0.6\\\\0.1\\end{bmatrix}=\\begin{bmatrix}0.3\\\\0.6\\\\0.1\\end{bmatrix}=\\vec{q}$$When the system is in state $q$, there is no change in the system from one measurement to the next. Steady-State VectorsIf $P$ is a stochastic matrix, then a steady-state vector (or equilibrium vector（平衡向量）) for $P$ is a probability vector $\\vec{q}$ such that $$P\\vec{q} = \\vec{q}$$ For Example: Let $P = \\begin{bmatrix} 0.6 &amp; 0.3 \\\\ 0.4 &amp; 0.7\\end{bmatrix}$. Find a steady-state vector for $P$. Solution:$$P\\vec{x} = \\vec{x}\\rightarrow (P-I)\\vec{x} = \\vec{0}\\\\longrightarrow\\begin{bmatrix}-0.4 &amp; 0.3 &amp; 0 \\\\0.4 &amp; -0.3 &amp; 0\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -3/4 &amp; 0 \\\\0 &amp; 0 &amp; 0\\end{bmatrix}$$$$\\vec{x} = x_2\\begin{bmatrix}3/4 \\\\ 1\\end{bmatrix}$$ Obviously, One obvious choice is $\\begin{bmatrix}3/4 \\\\ 1\\end{bmatrix}$,but a better choice with no fractions is $\\vec{w} = \\begin{bmatrix}3 \\\\ 4\\end{bmatrix}, (x_2 = 4)$, since every solution is a multiple of the solution$\\vec{w}$ above. Divide $\\vec{w}$ by the sum of its entries and obtain $$\\vec{q} = \\begin{bmatrix}3/7 \\\\ 4/7\\end{bmatrix}$$ if $P$ is an $n\\times n$ regular stochastic matrix, then $P$ has a unique steady-state vector $\\vec{q}$. Further, if $\\vec{x_0}$ is any initial state and $\\vec{x_{k+1}} = P \\vec{x_k}$, then the Markov chain $\\vec{x_k}$ converges to $\\vec{q}$ as $k \\rightarrow \\infty$. Also, $\\vec{q}$ is the eigenvector of $P$. See C5 ReadingNotes.","link":"/Math/Linear-Algebra/Algebra-C4-Vector-Spaces/"},{"title":"Algebra-C8-The-Geometry-of-Vector-Spaces","text":"Keywords: Affine Combinations, Barycentric Coordinates,Bézier Curves This is the Chapter8 ReadingNotes from book Linear Algebra and its Application. Affine CombinationsAn affine combination of vectors is a special kind of linear combination.Given vectors (or “points”) $v_1, v_2, \\cdots, v_p$ in $R^n$ and scalars $c_1, \\cdots, c_p$, an affine combination of $v_1, v_2, \\cdots, v_p$ is a linear combination $$c_1v_1 + \\cdots + c_pv_p,\\\\ c_1 + \\cdots + c_p = 1$$ A point $y$ in $R^n$ is an affine combination of $v_1, \\cdots v_p$ in $R^n$ if and only if $(y - v_1)$ is a linear combination of the translated points $v_2 - v_1, \\cdots, v_p - v_1$. Affine Independence An indexed set of points $v_1, … v_p$ in $R^n$ is affinely dependent if there exist real numbers $c_1, .. c_p$, not all zero, such that$$c_1 + \\cdots + c_p = 0 \\space and \\space c_1v_1 + \\cdots + c_pv_p = 0$$Otherwise, the set is affinely independent. Given an indexed set $S = \\lbrace v_1, \\cdots, v_p \\rbrace$ in $R^n$, with $p \\geq 2$,the following statements are logically equivalent. That is, either they are all true statements or they are all false.a. $S$ is affinely dependent.b. One of the points in $S$ is an affine combination of the other points in $S$.c. The set $\\lbrace v_2 - v_1, \\cdots v_p - v_1\\rbrace$ in $R^n$ is linearly dependent.d. The set $\\widetilde{v_1}, \\cdots, \\widetilde{v_p}$ of homogeneous forms in $R^{n+1}$ is linearly dependent. For example: let $\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 3 \\\\ 7\\end{bmatrix}, \\vec{v_2} = \\begin{bmatrix} 2 \\\\ 7 \\\\ 6.5\\end{bmatrix}, \\vec{v_3} = \\begin{bmatrix} 0 \\\\ 4 \\\\ 7\\end{bmatrix}, \\vec{v_4} = \\begin{bmatrix} 0 \\\\ 14 \\\\ 6\\end{bmatrix}$, and $S = \\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_3}, \\vec{v_4} \\rbrace$. Determine whether $S$ is affinely independent. Solution: $$\\vec{v_2} -\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 4 \\\\ -0.5\\end{bmatrix},\\vec{v_3} -\\vec{v_1} = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0\\end{bmatrix},\\vec{v_4} -\\vec{v_1} = \\begin{bmatrix} -1 \\\\ 11 \\\\ -1\\end{bmatrix}\\Rightarrow\\begin{bmatrix}1 &amp; -1 &amp; -1\\\\4 &amp; 1 &amp; 11\\\\-0.5 &amp; 0 &amp; -1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; -1 &amp; -1\\\\0 &amp; 5 &amp; 15\\\\0 &amp; 0 &amp; 0\\end{bmatrix}$$the columns are linearly dependent because not every column is a pivot column, so $v_2 - v_1, v_3 - v_1, v_4 - v_1$ are linearly dependent. Thus, $v_1, v_2, v_3, v_4$ is affinely dependent. $$\\vec{v_4} - \\vec{v_1} = 2(\\vec{v_2} - \\vec{v_1}) + 3(\\vec{v_3} - \\vec{v_1})\\\\\\vec{v_4} = -4\\vec{v_1} + 2\\vec{v_2} + 3\\vec{v_3}$$ Barycentric Coordinates let $S = \\lbrace \\vec{v_1},\\cdots, \\vec{v_k}\\rbrace$ be an affinely independent set in $R^n$. Then each $\\vec{p}$ in aff S has a unique representation as an affine combination of $\\vec{v_1},\\cdots,\\vec{v_k}$. That is, for each $\\vec{p}$ there exists a unique set of scalars $c_1, \\cdots, c_k$ such that$$\\vec{p} = c_1\\vec{v_1} + \\cdots + c_k\\vec{v_k}, and \\space c_1 + \\cdots + c_k = 1 \\tag{7}$$the coefficients $c_1 \\cdots c_k$ in the unique representation (7) of $\\vec{p}$ are called the barycentric (or, sometimes, affine) coordinates of $\\vec{p}$ Observe that (7) is equivalent to the single equation $$\\begin{bmatrix}\\vec{p}\\\\1\\end{bmatrix} = c_1\\begin{bmatrix} \\vec{v_1}\\\\1 \\end{bmatrix} + \\cdots + c_k\\begin{bmatrix} \\vec{v_k }\\\\1\\end{bmatrix}\\tag{8}$$ involving the homogeneous forms of the points. Row reduction of the augmented matrix $\\begin{bmatrix} \\widetilde{\\vec{v_1}} &amp; \\cdots \\widetilde{\\vec{v_k}} &amp; \\widetilde{\\vec{p}}\\end{bmatrix}$ for (8) produces the barycentric coordinates of $\\vec{p}$. For Example: Let $\\vec{a} = \\begin{bmatrix} 1 \\\\ 7\\end{bmatrix}, \\vec{b} = \\begin{bmatrix} 3 \\\\ 0\\end{bmatrix}, \\vec{c} = \\begin{bmatrix} 9 \\\\ 3\\end{bmatrix}, and \\space \\vec{p} = \\begin{bmatrix} 5 \\\\ 3\\end{bmatrix}$ Find the barycentric coordinates of $\\vec{p}$ determined by the affinely independent set $\\lbrace \\vec{a}, \\vec{b}, \\vec{c}\\rbrace$. Solution: $$\\begin{bmatrix}\\widetilde{\\vec{a}} &amp; \\widetilde{\\vec{b}} &amp; \\widetilde{\\vec{c}} &amp; \\widetilde{\\vec{p}}\\end{bmatrix}=\\begin{bmatrix}1 &amp; 3 &amp; 9 &amp; 5\\\\7 &amp; 0 &amp; 3 &amp; 3\\\\1 &amp; 1 &amp; 1 &amp; 1\\end{bmatrix}\\sim\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; \\frac{1}{4}\\\\7 &amp; 0 &amp; 3 &amp; \\frac{1}{3}\\\\1 &amp; 1 &amp; 1 &amp; \\frac{5}{12}\\end{bmatrix}$$so, $\\vec{p} = \\frac{1}{4}\\vec{a} + \\frac{1}{3}\\vec{b} + \\frac{5}{12}\\vec{c}$ Barycentric Coordinates in Computer GraphicsBarycentric coordinates provide the tool for smoothly interpolating the vertex information over the interior of a triangle. The color and other information displayed in the pixel on the screen should come from the object that the ray first intersects. See Figure 7. When the objects in the graphics scene are approximated by wire frames with triangular patches, the hidden surface problem can be solved using barycentric coordinates. For example: Let$$\\vec{v_1} =\\begin{bmatrix}1\\\\1\\\\-6\\end{bmatrix},\\vec{v_2} =\\begin{bmatrix}8\\\\1\\\\-4\\end{bmatrix},\\vec{v_3} =\\begin{bmatrix}5\\\\11\\\\-2\\end{bmatrix},\\vec{a} =\\begin{bmatrix}0\\\\0\\\\10\\end{bmatrix},\\vec{b} =\\begin{bmatrix}0.7\\\\0.4\\\\-3\\end{bmatrix}$$and $x(t) = a + tb$ for $t \\geq 0$. Find the point where the ray $x(t)$ intersects the plane that contains the triangle with vertices $v_1, v_2, and v_3$. Is this point inside the triangle? Solution: The plane is $aff \\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_3} \\rbrace$. The ray $x(t)$ intersects the plane when $c_2, c_3$, and $t$ satisfy $$(1-c_2-c_3)\\vec{v_1} + c_2\\vec{v_2} + c_3\\vec{v_3} = \\vec{a} + t\\vec{b} \\mapsto\\\\\\begin{bmatrix}\\vec{v_2} - \\vec{v_1} &amp; \\vec{v_3} - \\vec{v_1} &amp; -\\vec{b}\\end{bmatrix}\\begin{bmatrix}c_2 \\\\ c_3 \\\\ t\\end{bmatrix}=\\vec{a} - \\vec{v_1}$$ thus, get the solution and the intersection point is $$\\vec{x(5)} = \\vec{a} + 5\\vec{b} = \\begin{bmatrix} 3.5 \\\\ 2.0 \\\\ -5.0\\end{bmatrix}$$ The intersection point is inside the triangle because the barycentric weights for $\\vec{x(5)}$ are all positive. Convex Combinations（凸包） A convex combination of points $\\vec{v_1}, \\vec{v_2},\\cdots, \\vec{v_k}$ in $R^n$ is a linear combination of the form$$c_1\\vec{v_1} + c_2\\vec{v_2} + \\cdots + c_k\\vec{v_k}$$such that $c_1 + c_2 + \\cdots + c_k = 1$ and $c_i \\geq 0$ for all $i$. The set of all convex combinations of points in a set $S$ is called the convex hull of $S$, denoted by $conv S$. the point in $cov \\lbrace \\vec{v_1}, \\vec{v_2} \\rbrace$ may be written as $$\\vec{y} = (1-t)\\vec{v_1} + t\\vec{v_2} , 0 \\leq t \\leq 1$$which is a line segment between $\\vec{v_1}$ and $\\vec{v_2}$, hereafter denoted by $\\overline{\\vec{v_1}\\vec{v_2}}$ A set $S$ is convex if for each $\\vec{p}, \\vec{q} \\in S$, the line segment $\\overline{\\vec{p}\\vec{q}}$ is contained in $S$. Intuitively, a set $S$ is convex if every two points in the set can “see” each other without the line of sight leaving the set. Figure 1 illustrates this idea. consider a set $S$ that lies inside some large rectangle in $R^2$, and imagine stretching a rubber band around the outside of $S$. As the rubber band contracts around $S$, it outlines the boundary of the convex hull of $S$. Or to use another analogy, the convex hull of $S$ fills in all the holes in the inside of $S$ and fills out all the dents in the boundary of $S$. A set $S$ is convex if and only if every convex combination of points of $S$ lies in $S$. That is, $S$ is convex if and only if $S$ = $conv S$. More about ConvexHull Algorithms&gt;&gt; Hyperplanes（超平面）The key to working with hyperplanes is to use simple implicit descriptions, rather than the explicit or parametric representations of lines and planes used in the earlier work with affine sets. An implicit equation of a line in $R^2$ has the form $ax + by = d$. An implicit equation of a plane in $R^3$ has the form $ax + by + cz = d$. Both equations describe the line or plane as the set of all points at which a linear expression (also called a linear functional（线性泛函）) has a fixed value, d. A linear functional on $R^n$ is a linear transformation $f$ from $R^n$ into $R$. For eachscalar $d$ in $R$, the symbol $[f:d]$ denotes the set of all $\\vec{x}$ in $R^n$ at which the valueof $f$ is $d$. That is,$$[f:d] \\space is \\space the \\space set \\lbrace \\vec{x} \\in R^n : f(\\vec{x}) = d\\rbrace$$line: $x - 4y = 13$, is a hyperplane in $R^2$, it is the set of points at which the linear functional $f(x,y) = x - 4y$ has the value 13, the line is the set $[f:13]$ If $f$ is a linear functional on $R^n$, then the standard matrix of this linear transformation $f$ is a $1 \\times n$ matrix $A$, say $A = \\begin{bmatrix}a_1 &amp; a_2 &amp; \\cdots &amp; a_n \\end{bmatrix}$. So$$[f:0] \\Leftrightarrow \\lbrace \\vec{x} \\in R^n: A\\vec{x} = \\vec{0}\\rbrace \\Leftrightarrow NullA$$We know the relation between $A\\vec{x} = \\vec{0}$ and $A\\vec{x} = \\vec{b}$, thus $$[f:d] = [f:0] + \\vec{p}, \\vec{p} \\in [f:d]$$ $$[f:0] = \\lbrace \\vec{x} \\in R^n: \\vec{n} \\cdot \\vec{x} = 0\\rbrace$$ $\\vec{n}$ is called a normal vector（法向量） to $[f:0]$, sometimes it’s also called gradient（梯度）. For example: In $R^2$, give an explicit description of the line $x-4y = 13$ in parametric vector form. Solution: $$\\begin{aligned}\\vec{x} &amp;= \\begin{bmatrix}x \\\\ y\\end{bmatrix}= \\begin{bmatrix}13 + 4y \\\\ y\\end{bmatrix}\\\\&amp;= \\begin{bmatrix} 13 \\\\ 0\\end{bmatrix} + y \\begin{bmatrix} 4 \\\\ 1\\end{bmatrix} = \\vec{p} + y\\vec{q}\\end{aligned}$$ For example: Let $\\vec{v_1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1\\end{bmatrix}, \\vec{v_2} = \\begin{bmatrix} 2 \\\\ -1 \\\\ 4\\end{bmatrix}, \\vec{v_3} = \\begin{bmatrix} 3 \\\\ 1 \\\\ 2\\end{bmatrix}$. Find an implicit description $[f : d]$ of the plane $H_1$ that passes through $\\vec{v_1}, \\vec{v_2}, \\vec{v_3}$. Solution: $H_1$ is parallel to a plane $H_0$ through the origin that contains the translated points $$\\vec{v_2} - \\vec{v_1} =\\begin{bmatrix}1 \\\\ -2 \\\\ 3\\end{bmatrix},\\vec{v_3} - \\vec{v_1} =\\begin{bmatrix}2 \\\\ 0 \\\\ 1\\end{bmatrix}$$ Since these two points are linearly independent, $H_0 = Span \\lbrace \\vec{v_2} - \\vec{v_1}, \\vec{v_3} - \\vec{v_1} \\rbrace$. Let $\\vec{n} = \\begin{bmatrix} a \\\\ b \\\\ c\\end{bmatrix}$ be the normal to $H_0$. $$\\begin{bmatrix} 1 &amp; -2 &amp; 3\\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c\\end{bmatrix} = 0,\\begin{bmatrix} 2 &amp; 0 &amp; 1\\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c\\end{bmatrix} = 0\\Longrightarrow \\\\\\begin{bmatrix} 1 &amp; -2 &amp; 3 &amp; 0\\\\ 2 &amp; 0 &amp; 1 &amp; 0\\end{bmatrix}\\Longrightarrow \\\\\\vec{n} = \\begin{bmatrix} -2 \\\\ 5 \\\\ 4\\end{bmatrix}$$or$$\\begin{aligned}\\vec{n} &amp;= (\\vec{v_2} - \\vec{v_1}) \\times (\\vec{v_3} - \\vec{v_1})\\\\&amp;= \\left|\\begin{array}{cccc} 1 &amp; 2 &amp; i \\\\ -2 &amp; 0 &amp; j \\\\ 3 &amp; 1 &amp; k\\end{array}\\right|\\\\&amp;= -2i + 5j + 4k\\end{aligned}$$Thus,$$f(x) = -2x_1 + 5x_2 + 4x_3 \\\\d = f(\\vec{v_1}) = 7\\LongrightarrowH_1 : [f: d]$$ A subset $H$ of $R^n$ is a hyperplane if and only if $H = [f:d]$ for some nonzero linear functional $f$ and some scalar $d$ in $R$. Thus, if $H$ is a hyperplane, there exist a nonzero vector $\\vec{n}$ and a real number $d$ such that $H = \\lbrace \\vec{x} : \\vec{n} \\cdot \\vec{x} = d\\rbrace$. Topology in $R^n$For any point $p$ in $R^n$ and any real $\\delta &gt; 0$, the open ball $B(p,\\delta)$ with center $p$ and radius $\\delta$ is given by$$B(p,\\delta) = \\lbrace x : ||x - p|| &lt; \\delta\\rbrace$$Given a set in $R^n$,A set is open if it contains none of its boundary points.A set is closed if it contains all of its boundary points.A set $S$ is bounded if there exists a $\\delta &gt; 0$ such that $S \\subset B(0, \\delta)$.A set in $R^n$ is compact if it is closed and bounded. Polytopes（多面体） Let $S$ be a compact convex subset of $R^n$. A nonempty subset $F$ of $S$ is called a (proper) face of $S$ if $F \\neq S$ and there exists a hyperplane $H = [f:d]$ such that $F = S \\cap H$ and either $f(S) \\leq d$ or $f(S) &gt; \\geq d$. The hyperplane $H$ is called a supporting hyperplane to $S$. If the dimension of $F$ is $k$, then $F$ is called a $k-face$ of $S$.If $P$ is a polytope of dimension $k$, then $P$ is called a $k-polytope$. A $0-face$ of $P$ is called a vertex (plural: vertices), a $1-face$ is an edge, and a $(k-1)-dimensional$ face is a facet of $S$. SimplexHyperCube（超立方体）Curves and SurfacesBézier Curves quadratic and cubic Bézier curves:$$\\begin{aligned}&amp;\\vec{w(t)} = (1-t^2)\\vec{p_0} + 2t(1-t)\\vec{p_1} + t^2\\vec{p_2}\\\\&amp;\\vec{x(t)} = (1-t^3)\\vec{p_0} + 3t(1-t)^2\\vec{p_1} + 3t^2(1-t)\\vec{p_2} + t^3\\vec{p_3}\\\\\\end{aligned}$$Bézier curves are useful in computer graphics because their essential properties are preserved under the action of linear transformations and translations. $$\\begin{aligned}A\\vec{x(t)} &amp;= A[(1-t^3)\\vec{p_0} + 3t(1-t)^2\\vec{p_1} + 3t^2(1-t)\\vec{p_2} + t^3\\vec{p_3}]\\\\&amp;= (1-t^3)A\\vec{p_0} + 3t(1-t)^2A\\vec{p_1} + 3t^2(1-t)A\\vec{p_2} + t^3A\\vec{p_3}\\end{aligned}$$the new control points are $A\\vec{p_0}, \\cdots, A\\vec{p_3}$. Connecting Two Bézier CurvesThe combined curve is said to have $G^0$ geometric continuity (at $p_2$) because the two segments join at $p_2$. To avoid a sharp bend, it usually suffices to adjust the curves to have what is called $G^1$ geometric continuity, where both tangent vectors at $p_2$ point in the same direction. When the tangent vectors are actually equal at $p_2$, the tangent vector is continuous at $p_2$, and the combined curve is said to have $C^1$ continuity, or $C^1$ parametric continuity. Figure 4 shows $C^1$ continuity for two cubic Bézier curves. Notice how the point joining the two segments lies in the middle of the line segment between the adjacent control points. Two curves have $C^2$ (parametric) continuity when they have $C^1$ continuity and the second derivatives are equal. Another class of cubic curves, called B-splines, always have $C^2$ continuity because each pair of curves share three control points rather than one. Matrix Equations for Bézier CurvesSince a Bézier curve is a linear combination of control points using polynomials as weights, the formula for $\\vec{x(t)}$ may be written as $$\\begin{aligned}\\vec{x(t)} &amp;= \\begin{bmatrix}\\vec{p_0} &amp; \\vec{p_1} &amp; \\vec{p_2} &amp; \\vec{p_3}\\end{bmatrix}\\begin{bmatrix}(1-t)^3 \\\\ 3t(1-t)^2 \\\\ 3t^2(1-t) \\\\ t^3\\end{bmatrix}\\\\&amp;=\\underbrace{\\begin{bmatrix}\\vec{p_0} &amp; \\vec{p_1} &amp; \\vec{p_2} &amp; \\vec{p_3}\\end{bmatrix}}_{Geometry-matrix: G}\\begin{bmatrix}1 &amp; -3 &amp; 3 &amp; -1\\\\0 &amp; 3 &amp; -6 &amp; 3\\\\0 &amp; 0 &amp; 3 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\\\end{aligned}$$ $$\\Longrightarrow\\vec{x(t)} = GM_B\\vec{u(t)},M_B : Bezier-basis-matrix\\tag{4}$$A Hermite cubic curve arises when the matrix $M_B$ is replaced by a Hermite basis matrix. Bézier SurfacesThe Bézier curve in equation (4) can also be “factored” in another way, to be used in the discussion of Bézier surfaces.$$\\begin{aligned}\\vec{x(s)} &amp;= \\vec{u(s)}^TM_B^T\\begin{bmatrix}\\vec{p_0} \\\\ \\vec{p_1} \\\\ \\vec{p_2} \\\\ \\vec{p_3}\\end{bmatrix}\\\\&amp;=\\begin{bmatrix}1 &amp; s &amp; s^2 &amp; s^3\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\-3 &amp; 3 &amp; 0 &amp; 0\\\\3 &amp; -6 &amp; 3 &amp; 0\\\\-1 &amp; 3 &amp; -3 &amp; 1\\end{bmatrix}\\begin{bmatrix}\\vec{p_0} \\\\ \\vec{p_1} \\\\ \\vec{p_2} \\\\ \\vec{p_3}\\end{bmatrix}\\\\&amp;=\\begin{bmatrix} (1-s)^3 &amp; 3s(1-s)^2 &amp; 3s^2(1-s) &amp; s^3\\end{bmatrix}\\underbrace{\\begin{bmatrix}\\vec{p_0} \\\\ \\vec{p_1} \\\\ \\vec{p_2} \\\\ \\vec{p_3}\\end{bmatrix}}_{Geometry-vector}\\end{aligned}$$ Approximations to Curves and SurfacesThe basic idea for approximating a Bézier curve or surface is to divide the curve or surface into smaller pieces, with more and more control points. Recursive Subdivision of Bézier Curves and Surfaces $$x(t) = (1-3t+3t^2-t^3)p_0 + (3t-6t^2+3t^3)p_1 + (3t^2-3t^3)p_2 + t^3p_3, 0 \\leq t \\leq 1$$Thus, the new control points $q_3$ and $r_0$ are given by$$q_3 = r_0 = x(0.5) = \\frac{1}{8}(p_0 + 3p_1 + 3p_2 + p_3)$$the tangent vector to a parameterized curve $x(t)$ is the derivative $x(t)’$.$$x(t)’ = (-3 + 6t -3t^2)p_0 + (3 - 12t + 9t^2)p_1 + (6t-9t^2)p_2 + 3t^2p_3, 0 \\leq t \\leq 1$$In particular,$$x’(0) = 3(p_1 - p_0)\\\\x’(1) = 3(p_3 - p_2)\\\\x’(0.5) = \\frac{3}{4}(-p_0 - p_1 + p_2 + p_3)$$ Let $y(t)$ be the Bézier curve determined by $q_0, \\cdots, q_3$, and let $z(t)$ be the Bézier curve determined by $r_0, \\cdots, r_3$. Since $y(t)$ traverses the same path as $x(t)$ but only gets to $x(0.5)$ as $t$ goes from $0$ to $1$,$$y(t) = x(0.5t), 0 \\leq t \\leq 1$$ Similarly$$z(t) = x(0.5 + 0.5t), 0 \\leq t \\leq 1$$ By chain rule: $$y’(t) = 0.5x’(0.5t),\\\\z’(t) = 0.5x’(0.5 + 0.5t)$$ the control points for $y(t)$ satisfy $$3(q_1-q_0) = y’(0) = 0.5x’(0) = \\frac{3}{2}(p_1-p_0)\\\\3(q_3-q_2) = y’(1) = 0.5x’(0.5) = \\frac{3}{8}(-p_0-p_1+p_2+p_3)$$ Once the subdivision completely stops, the endpoints of each curve are joined by line segments, and the scene is ready for the next step in the final image preparation. More about Curves in Graphics&gt;&gt;","link":"/Math/Linear-Algebra/Algebra-C8-The-Geometry-of-Vector-Spaces/"},{"title":"Algebra-C7-Symmetric-Matrices-And-Quadratic-Forms","text":"Keywords: Symmetric Matrix, Quadratic Forms, Constrained Optimization, Singular Value Decomposition, ImageProcessing, Statistics, PCA This is the Chapter7 ReadingNotes from book Linear Algebra and its Application. Diagonalization of Symmetric Matrices（对称矩阵的对角化） If $A$ is symmetric, then any two eigenvectors from different eigenspaces are orthogonal. An $n \\times n$ matrix $A$ is orthogonally diagonalizable if and only if $A$ is a symmetric matrix. For Example: Orthogonally diagonalize the matrix $A = \\begin{bmatrix}3 &amp; -2 &amp; 4\\\\-2 &amp; 6 &amp; 2\\\\4 &amp; 2 &amp; 3\\end{bmatrix}$, whose characteristic equation is $$0 = -\\lambda^3 + 12\\lambda^2 -21\\lambda - 98 = -(\\lambda - 7)^2(\\lambda + 2)$$ Solution: $$\\lambda = 7, \\vec{v_1} = \\begin{bmatrix}1 \\\\ 0 \\\\ 1\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-1/2 \\\\ 1 \\\\ 0\\end{bmatrix};\\\\\\lambda = -2,\\vec{v_3} = \\begin{bmatrix}-1 \\\\ -1/2 \\\\ 1\\end{bmatrix}$$Although $\\vec{v_1}$ and $\\vec{v_2}$ are linearly independent, they are not orthogonal. the component of $\\vec{v_2}$ orthogonal to $\\vec{v_1}$ is $$\\vec{z_2} = \\vec{v_2} - \\frac{\\vec{v_2} \\cdot \\vec{v_1}}{\\vec{v_1} \\cdot \\vec{v_1}}\\vec{v_1}= \\begin{bmatrix}-1/4 \\\\ 1 \\\\ 1/4\\end{bmatrix}$$Then $\\lbrace \\vec{v_1}, \\vec{z_2}\\rbrace$ is an orthogonal set in the eigenspace for $\\lambda = 7$. Normalize $\\vec{v_1}$ and $\\vec{z_2}$ to obtain the following orthonormal basis for the eigenspace for$\\lambda = 7$: $$\\vec{u_1} = \\begin{bmatrix} 1/\\sqrt{2} \\\\ 0 \\\\ 1/\\sqrt{2}\\end{bmatrix},\\vec{u_2} = \\begin{bmatrix} -1/\\sqrt{18} \\\\ 4/\\sqrt{18} \\\\ 1/\\sqrt{18}\\end{bmatrix},also, \\vec{u_3} = \\begin{bmatrix} -2/3 \\\\ -1/3 \\\\ 2/3\\end{bmatrix}$$Hence $\\vec{u_1},\\vec{u_2},\\vec{u_3}$ is an orthonormal set. Let $$P = \\begin{bmatrix}\\vec{u_1} &amp; \\vec{u_2} &amp; \\vec{u_3}\\end{bmatrix}=\\begin{bmatrix}1/\\sqrt{2} &amp; -1/\\sqrt{18} &amp; -2/3 \\\\ 0 &amp; 4/\\sqrt{18} &amp; -1/3 \\\\ 1/\\sqrt{2} &amp; 1/\\sqrt{18} &amp; 2/3\\end{bmatrix},D =\\begin{bmatrix}7 &amp; 0 &amp; 0 \\\\ 0 &amp; 7 &amp; 0 \\\\ 0 &amp; 0 &amp; -2\\end{bmatrix}$$Then $P$ orthogonally diagonalizes $A$, and $A = PDP^{-1}$. Spectral DecompositionSince $P$ is orthogonal matrix（正交矩阵, orthonormal columns）, $P^{-1} = P^T$. So $$A = PDP^T =\\begin{bmatrix}\\vec{u_1} &amp; \\cdots &amp; \\vec{u_n}\\end{bmatrix}\\begin{bmatrix}\\lambda_1 &amp; &amp; 0\\\\&amp;\\ddots\\\\0 &amp; &amp; \\lambda_n\\end{bmatrix}\\begin{bmatrix}\\vec{u_1}^T \\\\ \\cdots \\\\ \\vec{u_n}^T\\end{bmatrix}\\\\\\RightarrowA = \\lambda_1\\vec{u_1}\\vec{u_1}^T + \\lambda_2\\vec{u_2}\\vec{u_2}^T +\\cdots + \\lambda_n\\vec{u_n}\\vec{u_n}^T$$ Quadratic Forms（二次型）A quadratic form on $R^n$ is a function $Q$ defined on $R^n$ whose value at a vector $\\vec{x}$ in $R^n$ can be computed by an expression of the form $Q(\\vec{x}) = \\vec{x}^TA\\vec{x}$, where $A$ is an $n \\times n$ symmetric matrix. The matrix $A$ is called the matrix of the quadratic form. For Example: For $\\vec{x}$ in $R^3$, let $Q(\\vec{x}) = 5x_1^2 + 3x_2^2 + 2x_3^2 - x_1x_2 + 8x_2x_3$. Write this quadratic form as $\\vec{x}^TA\\vec{x}$. Solution: $$Q(\\vec{x}) = \\vec{x}^TA\\vec{x} =\\begin{bmatrix}x_1 &amp; x_2 &amp; x_3\\end{bmatrix}\\begin{bmatrix}5 &amp; -1/2 &amp; 0\\\\-1/2 &amp; 3 &amp; 4\\\\0 &amp; 4 &amp; 2\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2 \\\\ x_3\\end{bmatrix}$$ Change of Variable in a Quadratic FormLet A be an $n \\times n$ symmetric matrix. Then there is an orthogonal change of variable, $\\vec{x} = P\\vec{y}$, that transforms the quadratic form $\\vec{x}^TA\\vec{x}$ into a quadratic form $\\vec{y}^TD\\vec{y}$ with no cross-product term. The columns of $P$ in the theorem are called the principal axes of the quadratic form（二次型的主轴） $\\vec{x}^TA\\vec{x}$. The vector $\\vec{y}$ is the coordinate vector of $\\vec{x}$ relative to the orthonormal basis of $R^n$ given by these principal axes. For Example : Make a change of variable that transforms the quadratic form $Q(\\vec{x}) = x_1^2 - 8x_1x_2 - 5x_2^2$ into a quadratic form with no cross-product term. Solution: The matrix of the quadratic form is$$A =\\begin{bmatrix}1 &amp; -4\\\\-4 &amp; -5\\end{bmatrix}$$The first step is to orthogonally diagonalize $A$. $$\\lambda = 3 :\\begin{bmatrix}2/\\sqrt{5}\\\\-1/\\sqrt{5}\\end{bmatrix};$$$$\\lambda = -7 :\\begin{bmatrix}1/\\sqrt{5}\\\\2/\\sqrt{5}\\end{bmatrix}$$Let$$P = \\begin{bmatrix}2/\\sqrt{5} &amp; 1/\\sqrt{5}\\\\-1/\\sqrt{5} &amp; 2/\\sqrt{5}\\end{bmatrix},D = \\begin{bmatrix}3 &amp; 0\\\\0 &amp; -7\\end{bmatrix}$$Then $A = PDP^{-1}$ and $D = P^{-1}AP = P^TAP$, as pointed out earlier. A suitable change of variable is $$\\vec{x} = P\\vec{y},where \\space \\vec{x} =\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}and\\vec{y} =\\begin{bmatrix}y_1\\\\y_2\\end{bmatrix}$$Then, $$x_1^2 - 8x_1x_2 - 5x_2^2= \\vec{x}^TA\\vec{x}= (P\\vec{y})^TA(P\\vec{y})\\\\= \\vec{y}^TP^TAP\\vec{y}= \\vec{y}^TD\\vec{y}= 3y_1^2-7y_2^2$$ To illustrate the meaning of the equality of quadratic forms, we can compute $Q(\\vec{x})$ for $\\vec{x} = (2,-2)$ using the new quadratic form. First, since $\\vec{x} = P\\vec{y}$, $$\\vec{y} = P^{-1}\\vec{x} = P^T\\vec{x}$$so, $$\\vec{y} =\\begin{bmatrix}2/\\sqrt{5} &amp; -1/\\sqrt{5}\\\\1/\\sqrt{5} &amp; 2/\\sqrt{5}\\end{bmatrix}\\begin{bmatrix}2\\\\-2\\end{bmatrix}=\\begin{bmatrix}6/\\sqrt{5} \\\\-2/\\sqrt{5}\\end{bmatrix}$$Hence, $3y_1^2-7y_2^2 = 16$ A Geometric View of Principal Axes$$\\vec{x}^TA\\vec{x} = c, c : constant, A:symmetric \\tag{3}$$If $A$ is a diagonal matrix（对角矩阵）, the graph is in standard position, such as in Figure 2. If $A$ is not a diagonal matrix（不是对角矩阵）, the graph of equation (3) is rotated out of standard position, as in Figure 3. Finding the principal axes (determined by the eigenvectors of $A$) amounts to finding a new coordinate system with respect to which the graph is in standard position. The hyperbola in Figure 3(b) is the graph of the equation $\\vec{x}^TA\\vec{x} = 16$, where $A$ is the matrix in above Example$$A =\\begin{bmatrix}1 &amp; -4\\\\-4 &amp; -5\\end{bmatrix}$$The positive $y_1-axis$ in Figure 3(b) is in the direction of the first column of the matrix $P$, and the positive $y_2-axis$ is in the direction of the second column of $P$ .$$P =\\begin{bmatrix}2/\\sqrt{5} &amp; 1/\\sqrt{5}\\\\-1/\\sqrt{5} &amp; 2/\\sqrt{5}\\end{bmatrix}$$ Classifying Quadratic Forms A quadratic form $Q$ is:a. positive definite（正定） if $Q(\\vec{x}) &gt; 0$ for all $\\vec{x} \\neq \\vec{0}$b. negative definite（负定） if $Q(\\vec{x}) &lt; 0$ for all $\\vec{x} \\neq \\vec{0}$c. indefinite if $Q(\\vec{x})$ assumes both positive and negative values. $Q(\\vec{x})$ is said to be positive semidefinite if $Q(\\vec{x}) \\geq 0$ for all $\\vec{x}$, and to be negative semidefinite if$Q(\\vec{x}) \\leq 0$ for all $\\vec{x}$. Let $A$ be an $n \\times n$ symmetric matrix. Then a quadratic form $\\vec{x}^TA\\vec{x}$ is:a. positive definite if and only if the eigenvalues of $A$ are all positive,b. negative definite if and only if the eigenvalues of $A$ are all negative, orc. indefinite if and only if $A$ has both positive and negative eigenvalues. Constrained Optimization（有约束的优化）When a quadratic form $Q$ has no cross-product terms, it is easy to find the maximum and minimum of $Q(\\vec{x})$ for $\\vec{x}^T\\vec{x} = 1$. Example1:Find the maximum and minimum values of $Q(\\vec{x}) = 9x_1^2 + 4x_2^2 + 3x_3^2$ subject to the constraint $\\vec{x}^T\\vec{x} = 1$. Solution: $$\\vec{x}^T\\vec{x} = 1\\iff ||\\vec{x}|| = 1 \\iff ||\\vec{x}||^2 = 1 \\iff x:unit-vector$$ Since $x_2^2$ and $x_3^2$ are nonnegative, note that $$4x_2^2 \\leq 9x_2^2, and \\space 3x_3^2 \\leq 9x_3^2$$hence, $$\\begin{aligned}Q(\\vec{x}) &amp;= 9x_1^2 + 4x_2^2 + 3x_3^2 \\\\ &amp;\\leq 9x_1^2 + 9x_2^2 + 9x_3^2 \\\\ &amp;= 9\\end{aligned}$$So the maximum value of $Q(\\vec{x})$ cannot exceed $9$ when $\\vec{x}$ is a unit vector. To find the minimum value of $Q(\\vec{x})$, observe that $$9x_1^2 \\geq 3x_1^2, and \\space 4x_2^2 \\geq 3x_2^2$$hence, $$\\begin{aligned}Q(\\vec{x}) \\geq 3x_1^2 + 3x_2^2 + 3x_3^2 = 3\\end{aligned}$$ that the matrix of the quadratic form $Q(\\vec{x})$ has eigenvalues $9, 4, and 3$ and that the greatest and least eigenvalues equal, respectively, the (constrained) maximum and minimum of $Q(\\vec{x})$. The same holds true for any quadratic form, as we shall see.（最值和特征值相关） Example2:Let $A = \\begin{bmatrix} 3 &amp; 0 \\\\0 &amp; 7\\end{bmatrix}$, and let $Q(\\vec{x}) = \\vec{x}^TA\\vec{x}$ for $\\vec{x}$ in $R^2$. Figure 1 displays the graph of $Q$. Figure 2 shows only the portion of the graph inside a cylinder; the intersection of the cylinder with the surface is the set of points $(x_1,x_2,z)$such that $z = Q(x_1,x_2)$ and $x_1^2 + x_2^2 = 1$. Geometrically, the constrained optimization problem is to locate the highest and lowest points on the intersection curve. The two highest points on the curve are $7$ units above the $x_1x_2-plane$, occurring where $x_1 = 0$ and $x_2 = \\pm 1$. These points correspond to the eigenvalue $7$ of $A$ and the eigenvectors $\\vec{x} = (0, 1) and -\\vec{x} = (0,-1)$. Similarly, the two lowest points on the curve are $3$ units above the $x_1x_2-plane$. They correspond to the eigenvalue $3$ and the eigenvectors $(1, 0) and (-1,0)$. let$$m = min\\lbrace \\vec{x}^TA\\vec{x} : ||\\vec{x}|| = 1\\rbrace\\\\M = max\\lbrace \\vec{x}^TA\\vec{x} : ||\\vec{x}|| = 1\\rbrace\\tag{2}$$Let $A$ be a symmetric matrix, and define $m$ and $M$ as in (2). Then $M$ is the greatest eigenvalue $\\lambda_1$ of $A$ and $m$ is the least eigenvalue of $A$. The value of $\\vec{x}^TA\\vec{x}$ is $M$ when $\\vec{x}$ is a unit eigenvector $\\vec{u_1}$ corresponding to $M$. The value of $\\vec{x}^TA\\vec{x}$ is $m$ when $\\vec{x}$ is a unit eigenvector corresponding to $m$. Then the maximum value of $\\vec{x}^TA\\vec{x}$ subject to the constraints$$\\vec{x}^T\\vec{x} = 1, \\vec{x}^T\\vec{u_1} = 0$$is the second greatest eigenvalue, $\\lambda_2$, and this maximum is attained when $\\vec{x}$ is an eigenvector $\\vec{u_2}$ corresponding to $\\lambda_2$. Let $A$ be a symmetric $n \\times n$ matrix with an orthogonal diagonalization $A = PDP^{-1}$, where the entries on the diagonal of $D$ are arranged so that $\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\lambda_n$ and where the columns of $P$ are corresponding unit eigenvectors $\\vec{u_1}, \\cdots, \\vec{u_n}$. Then for $k = 2, \\cdots, n$, the maximum value of $x^TAx$ subject to the constraints$$\\vec{x}^T\\vec{x} = 1, \\vec{x}^T\\vec{u_1} = 0, \\cdots, \\vec{x}^T\\vec{u_{k-1}} = 0$$is the eigenvalue $\\lambda_k$, and this maximum is attained at $\\vec{x} = \\vec{u_k}$. The Singular Value DecompositionUnfortunately, as we know, not all matrices can be factored as $A = PDP^{-1}$ with $D$ diagonal. However, a factorization $A = QDP^{-1}$ is possible for any $m \\times n$ matrix $A$. A special factorization of this type, called the singular value decomposition, is one of the most useful matrix factorizations in applied linear algebra. The absolute values of the eigenvalues of a symmetric matrix A measure the amounts that A stretches or shrinks certain vectors (the eigenvectors). For a symmetric $A$, if $A\\vec{x} = \\lambda \\vec{x}$ and $||\\vec{x}|| = 1$, then $$||A\\vec{x}|| = ||\\lambda \\vec{x}|| = |\\lambda| ||\\vec{x}|| = |\\lambda|\\tag{1}$$If $\\lambda_1$ is the eigenvalue with the greatest magnitude, then a corresponding unit eigenvector $\\vec{v_1}$ identifies a direction in which the stretching effect of $A$ is greatest. That is, the length of $A\\vec{x}$ is maximized when $\\vec{x} = \\vec{v_1}$, and $A\\vec{v_1} = \\lambda_1$, by (1). For example: If $A = \\begin{bmatrix}4 &amp; 11 &amp; 14\\\\8 &amp; 7 &amp; -2\\end{bmatrix}$, then the linear transformation $\\vec{x} \\mapsto A\\vec{x}$ maps the unit sphere $\\lbrace \\vec{x} : ||\\vec{x}|| = 1\\rbrace$ in $R^3$ onto an ellipse in $R^2$, shown in Figure 1. Find a unit vector $\\vec{v}$ at which the length $||A\\vec{x}||$ is maximized, and compute this maximum length. Solution: $$||A\\vec{x}||^2 = (A\\vec{x})^T(A\\vec{x}) = \\vec{x}^T(A^TA)\\vec{x}$$$A^TA$ is a symmetric matrix, So the problem now is to maximize the quadratic form $\\vec{x}^T(A^TA)\\vec{x}$ subject to the constraint $||\\vec{x}|| = 1$. The eigenvalues of $A^TA$ are $\\lambda_1 = 360, \\lambda_2 = 90, \\lambda_3 = 0$. Corresponding unit eigenvectors are, respectively: $$\\vec{v_1} = \\begin{bmatrix}1/3 \\\\ 2/3 \\\\ 2/3\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-2/3 \\\\ -1/3 \\\\ 2/3\\end{bmatrix},\\vec{v_1} = \\begin{bmatrix}2/3 \\\\ -2/3 \\\\ 1/3\\end{bmatrix}$$The maximum value of $||A\\vec{x}||^2$ is 360, The vector $A\\vec{v_1}$ is a point on the ellipse in Figure 1 farthest from the origin, namely. $$A\\vec{v_1} = \\begin{bmatrix}18\\\\6\\end{bmatrix}$$the maximum value of $A||\\vec{x}||$ is $||A\\vec{v_1}|| = \\sqrt 360 = 6\\sqrt 10$. The Singular Values of an $m \\times n$ Matrix Let A be an $m \\times n$ matrix. Then $A^TA$ is symmetric and can be orthogonally diagonalized. Let $\\lbrace \\vec{v_1}, \\cdots, \\vec{v_n}\\rbrace$ be an orthonormal basis for $R^n$ consisting of eigenvectors of $A^TA$, and let $\\lambda_1, \\cdots, \\lambda_n$ be the associated eigenvalues of $A^TA$. Then, for $1 \\leq i \\leq n$,$$\\begin{aligned}||A\\vec{v_i}||^2 &amp;= (A\\vec{v_i})^TA\\vec{v_i} = \\vec{v_i}^TA^TA\\vec{v_i}\\\\&amp;= \\vec{v_i}^T(\\lambda_i\\vec{v_i})\\\\&amp;= \\lambda_i\\end{aligned}\\tag{2}$$The singular values of $A$ are the square roots of the eigenvalues of $A^TA$, the singular values of $A$ are the lengths of the vectors $A\\vec{v_1}, \\cdots, A\\vec{v_n}$. The Singular Value Decomposition（奇异值分解）The decomposition of $A$ involves an $m \\times n$ “diagonal” matrix $\\Sigma$ of the form$$\\Sigma = \\begin{bmatrix}D &amp; 0\\\\0 &amp; 0\\end{bmatrix}\\tag{3}$$where $D$ is an $r \\times r$ diagonal matrix. Let $A$ be an $m \\times n$ matrix with rank $r$. Then there exists an $m \\times n$ matrix $\\Sigma$ as in (3) for which the diagonal entries in $D$ are the first $r$ singular values of $A$, $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\geq \\sigma_r &gt; 0$, and there exist an $m \\times m$ orthogonal matrix（正交矩阵） $U$ and an$n \\times n$ orthogonal matrix（正交矩阵） $V$ such that$$A = U \\Sigma V^T$$ but the diagonal entries of $\\Sigma$ are necessarily the singular values of $A$.The columns of $U$ in such a decomposition are called left singular vectors of $A$, and the columns of $V$ are called right singular vectors of A. For example: Construct a singular value decomposition of $A = \\begin{bmatrix}4 &amp; 11 &amp; 14\\\\8 &amp; 7 &amp; -2\\end{bmatrix}$ Solution: Step1. Find an orthogonal diagonalization of $A^TA$.（find the eigenvalues of $A^TA$ and a corresponding orthonormal set of eigenvectors.） Step2. Set up $V$ and $\\Sigma$. Arrange the eigenvalues of $A^TA$ in decreasing order. The corresponding unit eigenvectors, $\\vec{v_1}, \\vec{v_2}, \\vec{v_3}$, are the right singular vectors of $A$. $$V = \\begin{bmatrix}\\vec{v_1} &amp; \\vec{v_2} &amp; \\vec{v_3}\\end{bmatrix}=\\begin{bmatrix}1/3 &amp; -2/3 &amp; 2/3\\\\2/3 &amp; -1/3 &amp; -2/3\\\\2/3 &amp; 2/3 &amp; 1/3\\end{bmatrix}$$The square roots of the eigenvalues are the singular values. $$\\sigma_1 = 6\\sqrt{10},\\sigma_2 = 3\\sqrt{10},\\sigma_3 = 0$$ The nonzero singular values are the diagonal entries of $D$. $$D = \\begin{bmatrix}6\\sqrt 10 &amp; 0\\\\0 &amp; 3\\sqrt 10\\end{bmatrix}\\longrightarrow\\Sigma = \\begin{bmatrix} D &amp; 0\\end{bmatrix}= \\begin{bmatrix}6\\sqrt 10 &amp; 0 &amp; 0\\\\0 &amp; 3\\sqrt 10 &amp; 0\\end{bmatrix}$$ Step3. Construct $U$. When A has rank $r$, the first $r$ columns of $U$ are the normalized vectors obtained from $A\\vec{v_1}, \\cdots, A\\vec{v_r}$. In this example, $A$ has two nonzero singular values, so $rank A = 2$. Recall from equation (2), $||A\\vec{v_1}|| = \\sigma_1, ||A\\vec{v_2}|| = \\sigma_2$. Thus $$\\vec{u_1} = \\frac{1}{\\sigma_1}A\\vec{v_1} =\\begin{bmatrix}3/\\sqrt 10\\\\1/\\sqrt10\\end{bmatrix},\\vec{u_2} = \\frac{1}{\\sigma_2}A\\vec{v_2} =\\begin{bmatrix}1/\\sqrt 10\\\\-3/\\sqrt 10\\end{bmatrix}$$ Thus, $$A =\\begin{bmatrix}3/\\sqrt 10 &amp; 1/\\sqrt 10\\\\ 1/\\sqrt10 &amp; -3/\\sqrt 10\\end{bmatrix}\\begin{bmatrix}6\\sqrt 10 &amp; 0 &amp; 0\\\\0 &amp; 3\\sqrt 10 &amp; 0\\end{bmatrix}\\begin{bmatrix}1/3 &amp; -2/3 &amp; 2/3\\\\2/3 &amp; -1/3 &amp; -2/3\\\\2/3 &amp; 2/3 &amp; 1/3\\end{bmatrix}$$ Applications of the Singular Value Decomposition The Invertible Matrix Theorem (concluded)Let $A$ be an $n \\times n$ matrix. Then the following statements are each equivalent to the statement that $A$ is an invertible matrix.u. $(Col A)^\\perp = {0}$.v. $(Null A)^\\perp = R^n$.w. $RowA = R^n$.x. $A$ has $n$ nonzero singular values. For example:Given the equation $Ax = b$, A is a $m \\times n$ matrix, use the pseudoinverse of $A$ to define$$\\hat{\\vec{x}} = A^{\\dagger}\\vec{b} = V_rD^{-1}U_R^T\\vec{b}$$where$$r = rank(A)$$ More about SVD Decomposition by Numerical Analysis &gt;&gt; Applications to Image Processing and Statisticsfor example: An example of two-dimensional data is given by a set of weights and heights of $N$ college students. Let $X_j$ denote the observation vector in $R^2$ that lists the weight and height of the $j th$ student. If $w$ denotes weight and $h$ height, then the matrix of observations has the form $$\\begin{matrix}w_1 &amp; w_2 &amp; \\cdots &amp; w_N\\\\h_1 &amp; h_2 &amp; \\cdots &amp; h_N\\\\\\uparrow &amp; \\uparrow &amp; &amp;\\uparrow\\\\X_1 &amp; X_2 &amp; &amp; X_N\\end{matrix}$$The set of observation vectors can be visualized as a two-dimensional scatter plot. See Figure 1. Mean and Covariance（均值和协方差）let $\\begin{bmatrix} X_1 \\cdots X_N\\end{bmatrix}$ be a $p \\times N$ matrix of observations, such as described above. The sample mean, $M$, of the observation vectors $X_1 \\cdots X_N$ is given by $$M = \\frac{1}{N}(X_1 + \\cdots + X_n)$$ for $k = 1, \\cdots, N$, let$$\\hat{X_k} = X_k - M$$ The columns of the $p \\times N$ matrix$$B =\\begin{bmatrix}\\hat{X_1} \\hat{X_2} \\cdots \\hat{X_N}\\end{bmatrix}$$have a zero sample mean, and B is said to be in mean-deviation form. The (sample) covariance matrix is the $p \\times p$ matrix $S$ defined by$$S = \\frac{1}{N-1}BB^T$$Since any matrix of the form $BB^T$ is positive semidefinite, so is $S$. For example:Three measurements are made on each of four individuals in a random sample from a population. The observation vectors are$$X_1 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 1\\end{bmatrix},X_2 = \\begin{bmatrix} 4 \\\\ 2 \\\\ 13\\end{bmatrix},X_3 = \\begin{bmatrix} 7 \\\\ 8 \\\\ 1\\end{bmatrix},X_4 = \\begin{bmatrix} 8 \\\\ 4 \\\\ 5\\end{bmatrix}$$Compute the sample mean and the covariance matrix. Solution:$$M = \\begin{bmatrix}5 \\\\ 4 \\\\ 5\\end{bmatrix},B = \\begin{bmatrix}4 &amp; -1 &amp; 2 &amp; 3 \\\\ -2 &amp; -2 &amp; 4 &amp; 0 \\\\ -4 &amp; 8 &amp; -4 &amp; 0\\end{bmatrix},S = \\begin{bmatrix}10 &amp; 6 &amp; 0 \\\\ 6 &amp; 8 &amp; -8 \\\\ 0 &amp; -8 &amp; 32\\end{bmatrix}$$To discuss the entries in $S = [S_{ij}]$, denote the coordinates of $X$ by $x_1, \\cdots, x_p$. For each $j = 1, \\cdots, p$, the diagonal entry $s_{jj}$ in $S$ is called the variance of $x_j$. The variance of $x_j$ measures the spread of the values of $x_j$ . The total variance of the data is the sum of the variances on the diagonal of $S$.$$\\lbrace{total \\space variance\\rbrace} = tr(S)$$ The entry $s_{ij}$ in $S$ for $i \\neq j$ is called the covariance of $x_i$ and $x_j$. (1,3)-entry in $S$ is $0$. Statisticians say that $x_1$ and $x_3$ are uncorrelated. Principal Component Analysis（主成分分析）Suppose the matrix $[X_1, \\cdots, X_N]$ is already in mean-deviation form. The goal of principal component analysis is to find an orthogonal $p \\times p$ matrix(正交矩阵) $P = [u_1 \\cdots u_p]$ that determines a change of variable, $X = PY$, or $$\\begin{bmatrix}x_1 \\\\x_2\\\\\\cdots \\\\x_p\\end{bmatrix} =\\begin{bmatrix}u_1 &amp; u_2 &amp; \\cdots &amp; u_p\\end{bmatrix}\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\cdots \\\\ y_p\\end{bmatrix}$$with the property that the new variables $y_1, \\cdots, y_p$ are uncorrelated and are arranged in order of decreasing variance. Recall the Quadratic form we learned above, $\\vec{x} = P\\vec{y}$, the columns of $P$ means the correspongding unit eigen vectors to eigen values(decreased) of $A$, so that, $Q = x^TAx = y^TDy, A = PDP^{-1} = PDP^T$, similar, Right? Notice that $Y_k$ is the coordinate vector of $X_k$ with respect to the columns of $P$, and $Y_k = P^{-1}X_k = P^T X_k$ for $k = 1, \\cdots, N$. Thus, the covariance matrix of $Y_1, \\cdots, Y_N$ is $D = P^TSP$. $D$ is a diagonal matrix. The unit eigenvectors $u_1, \\cdots, u_p$ of the covariance matrix $S$ are called the principal components of the data (in the matrix of observations). The first principal component is the eigenvector corresponding to the largest eigenvalue of $S$. Thus $y_1$ is a linear combination of the original variables $x_1, \\cdots, x_p$, using the entries in the eigenvector $u_1$ as weights. $$y_1 = u_1^TX = c_1x_1 + c_2x_2 + \\cdots + c_px_p$$ For example: The initial data for the multispectral image of Railroad Valley consisted of 4 million vectors in $R^3$. The associated covariance matrix is $$S = \\begin{bmatrix}2382.78 &amp; 2611.84 &amp; 2136.20\\\\2611.84 &amp; 3106.47 &amp; 2553.90\\\\2136.20 &amp; 2553.90 &amp; 2650.71\\end{bmatrix}$$Find the principal components of the data, and list the new variable determined by the first principal component. Solution: $$\\lambda_1 = 7614.23, \\lambda_1 = 427.63, \\lambda_1 = 98.10, \\\\$$$$\\vec{u_1} = \\begin{bmatrix} 0.5417 \\\\ 0.6295 \\\\ 0.5570\\end{bmatrix},\\vec{u_2} = \\begin{bmatrix} -0.4894 \\\\ -0.3026\\\\ 0.8179\\end{bmatrix},\\vec{u_3} = \\begin{bmatrix} 0.6834 \\\\ -0.7157 \\\\ 0.1441\\end{bmatrix}$$the variable for the first principal component is $$y_1 = 0.54x_1 + 0.63x_2 + 0.56x_3$$ This equation was used to create photograph (d) as follows. The variables $x_1, x_2, and x_3$ are the signal intensities in the three spectral bands. At each pixel in photograph (d), the gray scale value is computed from $y_1$, a weighted linear combination of $x_1, x_2, and x_3$. In this sense, photograph (d) “displays” the first principal component of the data. (Sensors aboard the satellite acquire seven simultaneous images of any region on earth to be studied. The sensors record energy from separate wavelength bands— three in the visible light spectrum and four in infrared and thermal bands.) As we shall see, this fact will permit us to view the data as essentially one-dimensional rather than three-dimensional. Reducing the Dimension of Multivariate DataIt can be shown that an orthogonal change of variables, $X = PY$, does not change the total variance of the data. (Roughly speaking, this is true because left-multiplication by $P$ does not change the lengths of vectors or the angles between them.)(正交矩阵的性质) $$\\lbrace total \\space variance \\space of \\space x_1, \\cdots, x_p\\rbrace=\\lbrace total \\space variance \\space of \\space y_1, \\cdots, y_p\\rbrace=\\lambda_1 + \\lambda_2 + \\cdots + \\lambda_p$$ In a sense, 93.5%(7614.23/(7614.23+427.63+98.10)) of the information collected by Landsat for the Railroad Valley region is displayed in photograph (d), with 5.3% in (e) and only 1.2% remaining for (f). which means that the points lie approximately along a line, and the data are essentially one-dimensional. Characterizations of Principal Component VariablesThe singular value decomposition is the main tool for performing principal component analysis in practical applications. if $B$ is a $p \\times N$ matrix of observations in mean-deviation form, and if $A = (1/\\sqrt{N-1})B^T$, then $A^TA$ is the covariance matrix $S$. The squares of the singular values of $A$ are the $p$ eigenvalues of $S$. The right singular vectors of $A$ are the principal components of the data. Iterative calculation of the $SVD$ of $A$ is faster and more accurate than an eigenvalue decomposition of $S$.","link":"/Math/Linear-Algebra/Algebra-C7-Symmetric-Matrices-And-Quadratic-Forms/"},{"title":"Animation-C12-Special-Models-for-Animation","text":"Keywords: Inverse kinematics, Forward kinematics, Hierarchy This is the Chapter5 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012).","link":"/Graphics/Animation/Animation-C12-Special-Models-for-Animation/"},{"title":"Algorithms-C1-Foundations","text":"Keywords: InsertSort, MergeSort, Divide and Conquer, C++ Getting StartedInsertionSort123456789INSERTION-SORT(A)for j = 2 to A.length key = A[j] // Insert A[j] into the sorte sequence A[1...j-1]. i = j - 1 while i &gt; 0 and A[i] &gt; key A[i + 1] = A[i] i = i - 1 A[i + 1] = key conventions in our pseudocode: Indentation indicates block structure. The looping constructs while, for, and repeat-until and the if-else conditional construct have interpretations similar to those in C, C++, Java, Python, and Pascal. We typically organize compound data into objects, which are composed of attributes. A.length MergeSort1234567891011121314151617181920212223242526MERGE(A,p,q,r) n_1 = q - p + 1 n_2 = r - q let L[1..n_1+1] and R[1..n_2+1] be new arrays for i = 1 to n_1 L[i] = A[p + i - 1] for j = 1 to n_2 R[j] = A[q + j] L[n_1 + 1] = infty R[n_2 + 1] = infty i = 1 j = 1 for k = p to r if L[i] &lt;= R[j] A[k] = L[i] i = i + 1 else A[k] = R[j] j = j + 1MERGE-SORT(A,p,r) if p &lt; r q = floor (p + r)/2 MERGE-SORT(A, p, q) MERGE-SORT(A, q + 1, r) MERGE(A, p, q, r) Divide-and-Conquerthe maximum-subarray problem123456789101112131415161718192021222324252627282930FIND-MAX-CROSSING-SUBARRAY(A,low,mid,high) left-sum = -infty sum = 0 for i = mid downto low sum = sum + A[i] if sum &gt; left-sum left-sum = sum max-left = i right-sum = -infty sum = 0 for j = mid+1 to high sum = sum + A[j] if sum &gt; right-sum right-sum = sum max-right = j return (max-left, max-right, left-sum+right-sum)FIND-MAXIMUM-SUBARRAY(A,low,high) if high == low return (low, high, A[low]) else mid = (low + high) / 2 (left-low, left-high, left-sum) = FIND-MAXIMUM-SUBARRAY(A, low, mid) (right-low, right-high, right-sum) = FIND-MAXIMUM-SUBARRAY(A, mid+1, high) (cross-low, cross-high, cross-sum) = FIND-MAX-CROSSING-SUBARRAY(A, low, mid, high) if left-sum &gt;= right-sum and left-sum &gt;= cross-sum return(left-low, left-high, left-sum) else if right-sum &gt;= left-sum and right-sum &gt;= cross-sum return(right-low, right-high, right-sum) else return (cross-low, cross-high, cross-sum) Strassen’s algorithm for matrix multiplication123456789SQUARE-MATRIX-MULTIPLY(A,B) n = A.rows Let C be a new n x n matrix for i = 1 to n for j = 1 to n c_ij = 0 for k = 1 to n c_ij = c_ij + a_ik b_kjreturn C this simple divide-and-conquer approach is no faster than the straightforward SQUARE-MATRIX-MULTIPLY procedure. 123456789101112SQUARE-MATRIX-MULTIPLY-RECURSIVE(A,B) n = A.rows Let C be a new n x n matrix if n == 1 c_11 = a_11 b_11 else partition A,B,and C as in equations(4.9) C_11 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_11,B_11) + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_12,B_21) C_12 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_11,B_12) + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_12,B_22) C_21 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_21,B_11) + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_22,B_21) C_22 = SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_21,B_12) + SQUARE-MATRIX-MULTIPLY-RECURSIVE(A_22,B_22)return C $$A =\\begin{bmatrix}A_{11} &amp; A_{12}\\\\A_{21} &amp; A_{22}\\end{bmatrix},B =\\begin{bmatrix}B_{11} &amp; B_{12}\\\\B_{21} &amp; B_{22}\\end{bmatrix},$$ $$C =\\begin{bmatrix}C_{11} &amp; C_{12}\\\\C_{21} &amp; C_{22}\\end{bmatrix}=\\begin{bmatrix}A_{11} &amp; A_{12}\\\\A_{21} &amp; A_{22}\\end{bmatrix}\\begin{bmatrix}B_{11} &amp; B_{12}\\\\B_{21} &amp; B_{22}\\end{bmatrix}\\tag{4.9}$$ Probabilistic Analysis and Randomized Algorithmsthe hiring problem1234567HIRE-ASSISTANT(n) best = 0 //candidate 0 is a least-qualified dummy candidate for i = 1 to n interview candidate i if candidate i is better than candidate best best = i hire candidate i","link":"/CS/Algorithms/Algorithms-C1-Foundations/"},{"title":"Algebra-C6-Orthogonality-And-Least-Squares","text":"Keywords: Orthogonal Projections, Gram-Schmidt Process, Least-Squares Problems, Linear Model This is the Chapter6 ReadingNotes from book Linear Algebra and its Application. The Inner Productinner product is dot product. $$\\vec{u}^T\\vec{v} = \\vec{u} \\cdot \\vec{v}$$ The Length of a VectorThe length (or norm) of $\\vec{v}$ is the nonnegative scalar $||\\vec{v}||$ defined by$$||\\vec{v}|| = \\sqrt{\\vec{v} \\cdot \\vec{v}} = \\sqrt{v_1^2 + \\cdots + v_n^2}$$ Distance in $R^n$$$dis(\\vec{u}, \\vec{v}) = ||\\vec{u} - \\vec{v}||$$ Orthogonal Vectors$$\\vec{u} \\cdot \\vec{v} = 0\\Leftrightarrow||\\vec{u}+\\vec{v}||^2 = ||\\vec{u}||^2 + ||\\vec{v}||^2$$ Orthogonal ComplementsLet $A$ be an $m \\times n$ matrix. The orthogonal complement of the row space of $A$ is the null space of $A$, and the orthogonal complement of the column space of $A$ is the null space of $A^T$ : $$(Row A)^\\bot = NullA\\\\(Col A)^\\bot = NulA^T$$ Angles in $R^2$ and $R^3$$$\\vec{u} \\cdot \\vec{v} = ||\\vec{u}|| ||\\vec{v}|| \\cos \\theta$$ Orthogonal SetsLet $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ be an orthogonal basis(orthogonal $\\neq$ orthonormal) for a subspace $W$ of $R^n$. For each $\\vec{y}$ in $W$, the weights in the linear combination$$\\vec{y} = c_1\\vec{u_1} + \\cdots + c_p\\vec{u_p}$$are give by$$c_j = \\frac{\\vec{y} \\cdot \\vec{u_j}}{\\vec{u_j} \\cdot \\vec{u_j}}$$ An Orthogonal ProjectionGiven a nonzero vector $\\vec{u}$ in $R^n$, consider the problem of decomposing a vector $\\vec{y}$ in $R^n$ into the sum of two vectors, one a multiple of $u$ and the other orthogonal to $u$. We wish to write $$\\vec{y} = \\hat{\\vec{y}} + \\vec{z}$$where $\\hat{\\vec{y}} = \\alpha \\vec{u}$ for some scalar $\\alpha$˛ and $\\vec{z}$ is some vector orthogonal to $\\vec{u}$. $$\\vec{z} \\cdot \\vec{u} = 0\\Rightarrow(\\vec{y} - \\alpha \\vec{u}) \\cdot \\vec{u} = 0\\\\\\Rightarrow\\alpha = \\frac{\\vec{y}\\cdot \\vec{u}}{\\vec{u}\\cdot\\vec{u}},\\hat{\\vec{y}} = \\frac{\\vec{y}\\cdot \\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\vec{u}$$that means:subspace $L$ spanned by $\\vec{u}$ (the line through $\\vec{u}$ and $\\vec{0}$)$$\\hat{\\vec{y}} = proj_L\\vec{y} = \\frac{\\vec{y}\\cdot \\vec{u}}{\\vec{u}\\cdot\\vec{u}}\\vec{u}$$ Orthonormal SetsA set $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ is an orthonormal set if it is an orthogonal set of unit vectors. If $W$ is the subspace spanned by such a set, then $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ is an orthonormal basis for $W$ , since the set is automatically linearly independent. （正交矩阵）An $m \\times n$ matrix $U$ has orthonormal columns if and only if $U^TU = I$. Let $U$ be an $m \\times n$ matrix with orthonormal columns, and let $\\vec{x}$ and $\\vec{y}$ be in $R^n$. Thena.$||U\\vec{x}|| = ||\\vec{x}||$转换保留长度和正交性b.$(U\\vec{x})\\cdot (U\\vec{y}) = \\vec{x}\\cdot \\vec{y}$c.$(U\\vec{x})\\cdot (U\\vec{y}) = 0 ,if \\space and \\space only \\space if,\\vec{x}\\cdot \\vec{y} = 0$ Orthogonal ProjectionsLet $W$ be a subspace of $R^n$, let $\\vec{y}$ be any vector in $R^n$, and let $\\hat{\\vec{y}}$ be the orthogonal projection of $\\vec{y}$ onto $W$ . Then $\\hat{\\vec{y}}$ is the closest point in $W$ to $\\vec{y}$, in the sense that $$||\\vec{y}-\\hat{\\vec{y}}|| &lt; ||\\vec{y}-\\vec{v}||$$for all $\\vec{v}$ in $W$ distinct from $\\hat{\\vec{y}}$. If $\\lbrace \\vec{u_1}, \\cdots, \\vec{u_p}\\rbrace$ is an orthonormal basis for a subspace $W$ of $R^n$, then $$proj_W\\vec{y} = (\\vec{y} \\cdot \\vec{u_1})\\vec{u_1} + \\cdots + (\\vec{y} \\cdot \\vec{u_p})\\vec{u_p}$$if $U = \\begin{bmatrix} \\vec{u_1} &amp; \\vec{u_2} &amp; \\vec{u_p} \\end{bmatrix}$, then $$proj_W\\vec{y} = UU^T\\vec{y}$$ The Gram–Schmidt Process（构建标准正交基）For Example: Let $\\vec{x_1} = \\begin{bmatrix}1 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{x_2} = \\begin{bmatrix}0 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{x_3} = \\begin{bmatrix}0 \\\\ 0 \\\\1 \\\\1\\end{bmatrix}$. Then $\\lbrace \\vec{x_1}, \\vec{x_2}, \\vec{x_3}\\rbrace$ is clearly linearly independent and thus is a basis for a subspace $W$ of $R^4$. Construct an orthogonal basis for $W$ . Solution: Step 1. Let $\\vec{v_1} = \\vec{x_1}$ and $W_1 = Span \\lbrace \\vec{x_1} \\rbrace =Span \\lbrace \\vec{v_1} \\rbrace$. Step 2. Let $\\vec{v_2}$ be the vector produced by subtracting from $\\vec{x_2}$ its projection onto the subspace $W_1$. That is, let $$\\vec{v_2} = \\vec{x_2} - proj_{W_1}\\vec{x_2}\\\\= \\vec{x_2} - \\frac{\\vec{x_2} \\cdot \\vec{v_1}}{\\vec{v_1} \\cdot \\vec{v_1}} \\vec{v_1}\\\\=\\begin{bmatrix}-\\frac{3}{4}\\\\\\frac{1}{4}\\\\\\frac{1}{4}\\\\\\frac{1}{4}\\end{bmatrix}$$$\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ is an orthogonal basis for the subspace $W_2$ spanned by $\\vec{x_1}$ and $\\vec{x_2}$ . Step 2 (optional). If appropriate, scale $\\vec{v_2}$ to simplify later computations. Since $\\vec{v_2}$ has fractional entries, it is convenient to scale it by a factor of $4 $and replace $\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ by the orthogonal basis: $$\\vec{v_1} = \\begin{bmatrix}1 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-3 \\\\ 1 \\\\1 \\\\1\\end{bmatrix}$$ Step 3. Let $\\vec{v_3}$ be the vector produced by subtracting from $\\vec{v_3}$ its projection onto the subspace $W_2$. Use the orthogonal basis $\\lbrace \\vec{v_1}, \\vec{v_2}\\rbrace$ to compute this projection onto $W_2$: $$proj_{W_2}\\vec{x_3} =\\frac{\\vec{x_3} \\cdot \\vec{v_1}}{\\vec{v_1} \\cdot \\vec{v_1}} \\vec{v_1} +\\frac{\\vec{x_3} \\cdot \\vec{v_2}}{\\vec{v_2} \\cdot \\vec{v_2}} \\vec{v_2},\\vec{v_3} = \\vec{x_3} - proj_{W_2}\\vec{x_3}=\\begin{bmatrix}0\\\\\\frac{-2}{3}\\\\\\frac{1}{3}\\\\\\frac{1}{3}\\end{bmatrix}$$$\\lbrace \\vec{v_1}, \\vec{v_2}, \\vec{v_3}\\rbrace$ is an orthogonal basis for $W$ . QR Factorization of Matrices If $A$ is an $m \\times n$ matrix with linearly independent columns, then $A$ can be factored as $A = QR$, where $Q$ is an $m \\times n$ matrix whose columns form an orthonormal basis for $Col A$ and $R$ is an $n \\times n$ upper triangular invertible matrix with positive entries on its diagonal. For Example: Find a $QR$ factorization of $A = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\1 &amp; 1 &amp; 0\\\\1 &amp; 1 &amp; 1\\\\1 &amp; 1 &amp; 1 \\end{bmatrix}$. Solution: An orthogonal basis for $ColA = Span\\lbrace \\vec{x_1}, \\vec{x_2}, \\vec{x_3}\\rbrace$ was found by Gram–Schmidt Process:$$\\vec{v_1} = \\begin{bmatrix}1 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{v_2} = \\begin{bmatrix}-3 \\\\ 1 \\\\1 \\\\1\\end{bmatrix},\\vec{v_3} =\\begin{bmatrix}0\\\\\\frac{-2}{3}\\\\\\frac{1}{3}\\\\\\frac{1}{3}\\end{bmatrix}$$Then normalize the three vectors to obtain $\\vec{u_1}, \\vec{u_2}, \\vec{u_3}$, and use these vectors as the columns of $Q$: $$Q =\\begin{bmatrix}\\frac{1}{2} &amp; \\frac{-3}{\\sqrt{12}} &amp; 0 \\\\\\frac{1}{2} &amp; \\frac{1}{\\sqrt{12}} &amp; \\frac{-2}{\\sqrt{6}}\\\\\\frac{1}{2} &amp; \\frac{1}{\\sqrt{12}} &amp; \\frac{1}{\\sqrt{6}}\\\\\\frac{1}{2} &amp; \\frac{1}{\\sqrt{12}} &amp; \\frac{1}{\\sqrt{6}}\\end{bmatrix}$$because: $$Q^TA = Q^T(QR) = R$$thus: $$R = Q^TA =\\begin{bmatrix}2 &amp; \\frac{3}{2} &amp; 1 \\\\0 &amp; \\frac{3}{\\sqrt{12}} &amp; \\frac{2}{\\sqrt{12}} \\\\0 &amp; 0 &amp; \\frac{2}{\\sqrt{6}} \\\\\\end{bmatrix}$$ More About LU Factorization &gt;&gt; Least-Squares Problems（最小二乘问题）For $A\\vec{x} = \\vec{b}$, when a solution is demanded and none exists, the best one can do is to find an $\\vec{x}$ that makes $A\\vec{x}$ as close as possible to $\\vec{b}$. The general least-squares problem is to find an $\\vec{x}$ that makes $||\\vec{b} - A\\vec{x}||$ as small as possible. The adjective “least-squares” arises from the fact that $||\\vec{b} - A\\vec{x}||$ is the square root of a sum of squares（平方和的平方根）. If A is $m \\times n$ and $\\vec{b}$ is in $R^m$, a least-squares solution of $A\\vec{x} = \\vec{b}$ is an $\\hat{x}$ in $R^n$such that$$||\\vec{b} - A\\hat{\\vec{x}}|| \\leq ||\\vec{b} - A\\vec{x}||$$for all $\\vec{x}$ in $R^n$. The most important aspect of the least-squares problem is that no matter what $\\vec{x}$ we select, the vector $A\\vec{x}$ will necessarily be in the column space $ColA$. So we seek an $\\vec{x}$ that makes $A\\vec{x}$ the closest point in $Col A$ to $\\vec{b}$. See Figure 1. (Of course, if $\\vec{b}$ happens to be in $Col A$, then $\\vec{b}$ is $A\\vec{x}$ for some $\\vec{x}$, and such an $\\vec{x}$ is a “least-squares solution.”) Solution of the General Least-Squares Problem $$\\hat{\\vec{b}} = proj_{ColA}\\vec{b}$$Because $\\hat{\\vec{b}}$ is in the column space of $A$, the equation $A\\vec{x} = \\hat{\\vec{b}}$ is consistent, and there is an $\\hat{\\vec{x}}$ in $R^n$ such that $$A\\hat{\\vec{x}} = \\hat{\\vec{b}}$$Since $\\hat{\\vec{b}}$ is the closest point in $Col A$ to $\\vec{b}$, a vector $\\hat{\\vec{x}}$ is a least-squares solution of $A\\vec{x} = \\vec{b}$. Suppose $\\hat{\\vec{x}}$ satisfies$A\\hat{\\vec{x}} = \\hat{\\vec{b}}$. The projection $\\hat{\\vec{b}}$ has the property that $\\vec{b} - \\hat{\\vec{b}}$ is orthogonal to $Col A$. so $\\vec{b} - \\hat{\\vec{b}}$ is orthogonal to each column of $A$. If $\\vec{a_j}$ is any column of $A$, then $\\vec{a_j}\\cdot (\\vec{b} - A\\hat{\\vec{x}}) = 0$, and $\\vec{a_j}^T(\\vec{b} - A\\hat{\\vec{x}}) = 0$（one is vector dot, one is matrix multiplication）. Since each $\\vec{a_j}^T$ is a row of $A^T$, $$A^T(\\vec{b} - A\\hat{\\vec{x}}) = \\vec{0}\\tag{2}\\RightarrowA^T\\vec{b} - A^TA\\hat{\\vec{x}} = \\vec{0}\\RightarrowA^T\\vec{b} = A^TA\\hat{\\vec{x}}$$These calculations show that each least-squares solution of $A\\vec{x} = \\vec{b}$ satisfies the equation $$A^TA\\vec{x} = A^T\\vec{b}\\tag{3}$$The matrix equation (3) represents a system of equations called the normal equations（正规方程） for $A\\vec{x} = \\vec{b}$. A solution of (3) is often denoted by $\\hat{\\vec{x}}$. For Example: Find a least-squares solution of $A\\vec{x} = \\vec{b}$ for$$A =\\begin{bmatrix}1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 1 &amp; 0 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 1 &amp; 0\\\\1 &amp; 0 &amp; 0 &amp; 1\\\\1 &amp; 0 &amp; 0 &amp; 1\\\\\\end{bmatrix},\\vec{b} =\\begin{bmatrix}-3\\\\-1\\\\0\\\\2\\\\5\\\\1\\end{bmatrix}$$Solution:$$A^TA =\\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2\\\\2 &amp; 2 &amp; 0 &amp; 0\\\\2 &amp; 0 &amp; 2 &amp; 0\\\\2 &amp; 0 &amp; 0 &amp; 2\\\\\\end{bmatrix},A^T\\vec{b} =\\begin{bmatrix}4\\\\-4\\\\2\\\\6\\end{bmatrix}$$The augmented matrix for $A^TA\\vec{x} = A^T\\vec{b}$ is$$\\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2 &amp; 4\\\\2 &amp; 2 &amp; 0 &amp; 0 &amp; -4\\\\2 &amp; 0 &amp; 2 &amp; 0 &amp; 2\\\\2 &amp; 0 &amp; 0 &amp; 2 &amp; 6\\\\\\end{bmatrix}\\sim\\begin{bmatrix}6 &amp; 2 &amp; 2 &amp; 2 &amp; 4\\\\2 &amp; 2 &amp; 0 &amp; 0 &amp; -4\\\\2 &amp; 0 &amp; 2 &amp; 0 &amp; 2\\\\2 &amp; 0 &amp; 0 &amp; 2 &amp; 6\\\\\\end{bmatrix}\\Rightarrow\\hat{\\vec{x}} =\\begin{bmatrix}3\\\\-5\\\\-2\\\\0\\end{bmatrix} + x_4\\begin{bmatrix}-1\\\\1\\\\1\\\\1\\end{bmatrix}$$ Let A be an $m \\times n$ matrix. The following statements are logically equivalent:a. The equation $A\\vec{x} = \\vec{b}$ has a unique least-squares solution for each $\\vec{b}$ in $R^m$.b. The columns of $A$ are linearly independent.c. The matrix $A^TA$ is invertible.When these statements are true, the least-squares solution $\\hat{\\vec{x}}$ is given by$$\\hat{\\vec{x}} = (A^TA)^{-1}A^T\\vec{b}$$ Alternative Calculations of Least-Squares SolutionsGiven an $m \\times n$ matrix $A$ with linearly independent columns, let $A = QR$ be a $QR$ factorization of $A$. Then, for each $\\vec{b}$ in $R^m$, the equation $A\\vec{x} = \\vec{b}$ has a unique least-squares solution, given by $$\\hat{\\vec{x}} = R^{-1}Q^T\\vec{b}$$ Applications to Linear ModelsInstead of $A\\vec{x} = \\vec{b}$, we write $X\\vec{\\beta} = \\vec{y}$ and refer to $X$ as the design matrix（模型的输入）, $\\vec{\\beta}$ as the parameter vector（模型的参数）, and $\\vec{y}$ as the observation vector（模型的输出，俗称观测值）. Least-Squares LinesThe simplest relation between two variables $x$ and $y$ is the linear equation $y = \\beta_0 + \\beta_1x$. Experimental data often produce points $(x_1, y_1), \\cdots, (x_n, y_n)$ that, when graphed, seem to lie close to a line. We want to determine the parameters $\\beta_0$ and $\\beta_1$ that make the line as “close” to the points as possible. This line is also called a line of regression of $\\vec{y}$ on $\\vec{x}$, because any errors in the data are assumed to be only in the $y-coordinates$. Computing the least-squares solution of $X\\vec{\\beta} = \\vec{y}$ is equivalent to finding the $\\vec{\\beta}$ that determines the least-squares line in Figure 1. The General Linear ModelStatisticians usually introduce a residual vector $\\vec{\\epsilon}$ defined by $\\vec{\\epsilon} = \\vec{y} - X\\vec{\\beta}$, and write $$\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}$$Any equation of this form is referred to as a linear model. Once $X$ and $\\vec{y}$ are determined, the goal is to minimize the length of $\\vec{\\epsilon}$, which amounts to finding a least-squares solution of $X\\vec{\\beta} = \\vec{y}$. In each case, the least-squares solution $\\widehat{\\vec{\\beta}}$ is a solution of the normal equations: $$X^TX\\vec{\\beta} = X^T\\vec{y}$$ Least-Squares Fitting of Other CurvesThe most common example is curve fitting. like $$y = \\beta_0 + \\beta_1 x + \\beta_2 x^2\\tag{3}$$ The general form is $$y = \\beta_0 f_0(x) + \\beta_1f_1(x) + \\cdots + \\beta_kf_k(x)\\tag{2}$$where $f_0, \\cdots, f_k$ are known functions and $\\beta_0, \\cdots, \\beta_k$ are parameters that must be determined. As we will see, equation (2) describes a linear model because it is linear in the unknown parameters. See Figure5, it shows fitting over a cubic curve.$$\\begin{bmatrix}y_1\\\\y_2\\\\ \\cdots \\\\y_n\\end{bmatrix}=\\begin{bmatrix}1 &amp; x_1 &amp; x_1^2\\\\1 &amp; x_2 &amp; x_2^2\\\\&amp; \\cdots \\\\1 &amp; x_n &amp; x_n^2\\end{bmatrix}\\begin{bmatrix}\\beta_0\\\\\\beta_1\\\\\\beta_2\\end{bmatrix}+\\begin{bmatrix}\\epsilon_1\\\\\\epsilon_2\\\\\\cdots\\\\\\epsilon_n\\\\\\end{bmatrix}\\\\\\Longleftrightarrow\\\\\\vec{y} = X\\vec{\\beta} + \\vec{\\epsilon}$$ Multiple RegressionLeast-squares plane. $$y = \\beta_0 f_0(u,v) + \\beta_1f_1(u,v) + \\cdots + \\beta_kf_k(u,v)$$ where $f_0, \\cdots, f_k$ are known functions and $\\beta_0, \\cdots, \\beta_k$ are parameters that must be determined. More about Linear Models and Least-Squares Problems in Machine Learning&gt;&gt; (求最大似然函数 = 最小二乘 = 最小误差值 = 问题的最优解) Inner Product Spaces An inner product on a vector space $V$ is a function that, to each pair of vectors $\\vec{u}$ and $\\vec{v}$ in $V$ , associates a real number $\\langle \\vec{u}, \\vec{v}\\rangle$ and satisfies the following axioms, for all $\\vec{u}, \\vec{v}, \\vec{w}$ in $V$ and all scalars $c$: $\\langle\\vec{u}, \\vec{v}\\rangle = \\langle\\vec{v}, \\vec{u}\\rangle$ $\\langle\\vec{u + v}, \\vec{w}\\rangle = \\langle\\vec{u}, \\vec{w}\\rangle + \\langle\\vec{v}, \\vec{w}\\rangle$ $\\langle\\vec{u}, \\vec{v}\\rangle = c\\langle\\vec{u}, \\vec{v}\\rangle$ $\\langle\\vec{u}, \\vec{u}\\rangle \\geq 0$ and $\\langle\\vec{u}, \\vec{u}\\rangle = 0$ if and only if $\\vec{u} = \\vec{0}$A vector space with an inner product is called an inner product space. For example:Let $t_0, \\cdots, t_n$ be distinct real numbers. For $p$ and $q$ in $P_n$, define $$\\langle p,q\\rangle = p(t_0)q(t_0) + \\cdots + p(t_n)q(t_n)\\\\\\langle p,p\\rangle = [p(t_0)]^2 + \\cdots + [p(t_n)]^2$$ Lengths, Distances, and Orthogonality$$||\\vec{v}|| = \\sqrt{\\langle \\vec{v}, \\vec{v} \\rangle}$$ The Gram–Schmidt Processcannot understand, to be added…$$ $$ Best Approximation in Inner Product SpacesTwo InequalitiesAn Inner Product for $C[a,b]$ (Calculus required)Applications of Inner Product SpacesWeighted Least-SquaresTrend Analysis of DataFourier Series (Calculus required)","link":"/Math/Linear-Algebra/Algebra-C6-Orthogonality-And-Least-Squares/"},{"title":"Animation-C10-Facial-Animation","text":"Keywords: Blend shape, Lip-sync animation This is the Chapter10 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). The human faceAnatomic structureThe facial action coding systemFacial modelsCreating a continuous surface modelTexturesAnimating the faceParameterized modelsBlend shapesMuscle modelsExpressionsSummaryLip-sync animationArticulators of speechPhonemesCoarticulationProsodyChapter summary","link":"/Graphics/Animation/Animation-C10-Facial-Animation/"},{"title":"Animation-C11-Behavioral-Animation","text":"Keywords: intelligent behavior， Crowds This is the Chapter11 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). Animation Technique types: In key-frame animations, for example, the animator precisely specifies values for animation parameters as specific frames and selects an interpolation procedure to fill in the remaining values for the in-between frames. There is not much mystery as to what the parametric values will be or what the motion will look like. In physically based animation, such as rigid body dynamics, the animator might specify starting values for some of physical simulation parameters, for example, initial position and velocity. The animator generally knows what the animation will look like but does not have a good idea of the intermediate parametric values as an object falls, collides, and bounces. This is an example of model-based animation where there is an underlying model controlling the motion. In this case the model is physics—or at least some approximation to it. Behavioral animation is another type of model-based animation in which it is the cognitive processes that are being modeled. The term cognitive process refers to any behavior that can be attributed to mental activity, voluntary or involuntary, that is reacting to the environment—basically non-physically based procedural motion. The cognitive processes can be as simple as moving toward a light or as complex as balancing self-preservation, mood swings, and personality disorders. Synthetic characters with built-in rules of behavior are often referred to as actors or intelligence agents. When a figure is meant to be the embodiment of a user in a virtual environment, the term avatar is often used. Cognitive modelingA common approach is to use a database of simple actions (e.g., simple SIMS games) as a library of behaviors and have appropriate behavior kick in when conditions are satisfied. A more sophisticated approach is to blend from one activity to another with either precomputed transitions or transitions computed on the fly. 状态机 Aggregate behaviorOften behavioral animation is concerned with a large number of characters. Adding more intelligence to the members in a group results in more interesting individual behaviors, which is sometimes referred to as autonomous behavior. Primitive behaviorsPrimitive behavior, for purposes of this discussion, is characterized by being immediately reactive to the sensed environment as opposed to any temporally extended reasoning process. Flocking behaviorLocal controlPerceptionInteracting with other membersA member interacts with other members of the flock to maintain separation without collision while trying to maintain membership in the flock. Interacting with the environmentThe main interaction between a flock member and the environment is collision avoidance, for which various approaches can be used. Force fields are the simplest to implement and give good results in simple cases. Global controlThere is usually a global goal that is used to control and direct the flock. Flock leaderTo simulate the behavior of actual flocks, the animator can have the leader change periodically. Negotiating the motionIn producing the motion, three low-level controllers are commonly used. They are, in order of priority, collision avoidance, velocity matching, and flock centering. Each of these controllers produces a directive that indicates desired speed and direction (a velocity vector). The task is to negotiate a resultant velocity vector given the various desires. Collision avoidanceSplitting and rejoiningModeling flightPrey-predator behaviorKnowledge of the environmentBehavioral animation is all about cognitive interaction with the environment combined with the limitations imposed by simulation of physical constraints. VisionMemoryIn work by Noser et al. [24], a quadtree data structure is used to partition space and construct an occupancy map from the vision information. As more of the environment is seen, more knowledge is accumulated. Modeling intelligent behaviorAutonomous behaviorExpressions and gesturesCrowdsCrowd behaviorsInternal structureCrowd controlManaging n-squared complexityAppearanceChapter summary","link":"/Graphics/Animation/Animation-C11-Behavioral-Animation/"},{"title":"Animation-C13-Learning-based-Animation","text":"Keywords: Deep learning, Animation, Latest Method, python This is the ReadingNotes from video games105. We have learned several animation method: interpolation animation(keyframes, in-betweens) kinematics animation(FK, IK) data-based animation(motion capture) physical-based animation this chapter focus on: learning based animation(Deep learning &amp; Reinforcement learning) State Machine, Motion Matching is using the existing mocap motion(mocap from professional actors), but the mocap motion is limited, we want to generate new natural motions. So the problems becomes how to generate new natrual motions based on the motions we already have? PCA Gaussian Mixture Model Autogressive Model Neural Network VAE GAN Normalizing Flow Diffusion Model 扩散模型","link":"/Graphics/Animation/Animation-C13-Learning-based-Animation/"},{"title":"Animation-C3-Interpolating-Values","text":"Keywords: Interpolation, Quaternion, pipeline This is the Chapter3 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). InterpolationThe appropriate functionInterpolation versus approximationCommonly used interpolating functions are the Hermite formulation and the Catmull-Rom spline. ComplexityThe simpler the underlying equations of the interpolating function, the faster its evaluation. In practice, polynomials are easy to compute, and piecewise cubic polynomials are the lowest degree polynomials that provide sufficient smoothness while still allowing enough flexibility to satisfy other constraints such as beginning and ending positions and tangents. ContinuityZero-order continuity refers to the continuity of values of the curve itself. If the same can be said of the first derivative of the function (the instantaneous change in values of the curve), then the function has first-order, or tangential, continuity. Second-order continuity refers to continuous curvature or instantaneous change of the tangent vector. Global versus local controlA formulation in which control points have a limited effect on the curve is referred to as providing local control. If repositioning one control point redefines the entire curve, however slightly, then the formulation provides only global control. Curves in Geometry&gt;&gt; Controlling the motion of a point along a curvefor a given value of $u$, will produce a value that is a point in space, that is, $\\pmb{p} = \\pmb{P(u)}$.for any specified time, $u$, a position $(X, Y, Z)$ can be produced as $(P_x(u), P_y(u), P_z(u)) = P(u)$. If positions are being interpolated by varying $u$ at a constant rate, the positions that are generated will not necessarily, in fact will seldom, represent a constant speed (e.g., see Figure 3.4). To ensure a constant speed for the interpolated value, the interpolating function has to be parameterized by arc length, that is, distance along the curve of interpolation. Computing arc lengthAlternatively, instead of specifying time constraints, the animator might want to specify the relative velocities that an object should have along the curve. It is important to remember that in animation the path swept out by the curve in space is not the only important thing. Equally important is how the path is swept out over time. Arc Length in Calculus &gt;&gt; The analytic approach to computing arc lengthThe length of the curve from a point $\\pmb{P}(u_a)$ to any other point $\\pmb{P}(u_b)$ can be found by evaluating the arc length integral [4] by$$s = \\int_{u_a}^{u_b} |dP/du| du\\tag{3.3}$$where the derivative of the space curve with respect to the parameterizing variable is defined to bethat shown in Equations 3.4 and 3.5.$$d\\pmb{P} / du = ((dx(u)/(du)), (dy(u)/(du)), (dz(u)/(du)))\\tag{3.4}$$$$|d\\pmb{P} / du| = \\sqrt{(dx(u)/du)^2 + (dy(u)/du)^2 + (dz(u)/du)^2}\\tag{3.5}$$ Space Arc Length in Calculus &gt;&gt; Estimating arc length by forward differencingAs mentioned previously, analytic techniques are not tractable for many curves of interest to animation. The easiest and conceptually simplest strategy for establishing the correspondence between parameter value and arc length is to sample the curve at a multitude of parametric values.  for example:the user wants to know the distance (arc length) from the beginning of the curve to the point on the curve corresponding to a parametric value of 0.73. Solution: The parametric entry closest to 0.73 must be located in the table. Since the parametric entries are evenly spaced, the distance between parametric entries is$$d = 0.05$$thus, the idex $i$,$$\\begin{aligned}i &amp;= \\lfloor \\frac{v}{d} + 0.5 \\rfloor \\\\&amp;= \\lfloor \\frac{0.73}{0.05} + 0.5 \\rfloor \\\\ &amp;= 15\\end{aligned}$$ A better approximation can be obtained by interpolating between the arc lengths corresponding to entries on either side of the given parametric value. $$i = \\lfloor \\frac{v}{d} \\rfloor = 14$$ thus,$$\\begin{aligned}s &amp;= G[i] + \\frac{v - V[i]}{V[i+1]-V[i]}(G[i+1]-G[i])\\\\&amp;= 0.944 + \\frac{0.73-0.70}{0.75-0.70}(0.959-0.944)\\\\&amp;= 0.953\\end{aligned}$$ What if given the arc length, I want to know the corresponding parameter value?  for example: Estimate the location of the point on the curve that is 0.75 unit of arc length from the beginning of the curve. Solution: the value of 0.75 is three-eighths of the way between the table values 0.72 and 0.80. An estimate of the parametric value would be calculated as $$u = 0.40 + \\frac{3}{8}(0.45 - 0.40) = 0.41875$$ Advantage: easy to implement, intuitive, fast to compute.Disadvantage: errors. The curve can be supersampled to help reduce errors in forming the table. Instead of linear interpolation, higher-degree interpolation procedures can be used in computing the parametric value. Adaptive approachThe adaptive approach begins by initializing the table with an entry for the first point of the curve, $&lt;0.0, P(0)&gt;$, and initializing the list of segments to be tested with the entire curve, $&lt;0.0,1.0&gt;$. The segment’s midpoint is computed by evaluating the curve at the middle of the range of its parametric value. The curve is also evaluated at the endpoint of the segment; the position of the start of the segment is already in the table and can be retrieved from there. $$|||P(0.0) - P(1.0)|| - (||P(0.0) - P(0.5)|| + ||P(0.5) - P(1.0)||)| &lt; \\epsilon$$ If the difference between these two values is above some user-specified threshold, then both halves, in order, are added to the list of segments to be tested along with their error tolerance ($\\epsilon/2^n$ where $n$ is the level of subdivision). If the values are within tolerance, then the parametric value of the midpoint is recorded in the table along with the arc length of the first point of the segment plus the distance from the first point to the midpoint. Also added to the table is the last parametric value of the segment along with the arc length to the midpoint plus the distance from the midpoint to the last point. When the list of segments to be tested becomes empty, a list of $&lt;$parametric value, arc length$&gt;$ has been generated for the entire curve. Estimating the arc length integral numericallyMany numerical integration techniques approximate the integral of a function with the weighted sum of values of the function at various points in the interval of integration. Techniques such as Simpson’s and trapezoidal integration use evenly spaced sample intervals. Gaussian quadrature [1] uses unevenly spaced intervals in an attempt to get the greatest accuracy using the smallest number of function evaluations. Numerical Differentiation and Integration &gt;&gt; $$\\begin{aligned}&amp;\\int_{-1}^{1} f(u) = \\sum_i w_i f(u_i)\\\\&amp;t = f(u) = \\frac{(b-a)}{2}u + \\frac{b+a}{2}\\\\&amp;\\int_a^b g(t) dt = \\int_{-1}^{1}g(\\frac{(b-a)}{2}u + \\frac{b+a}{2})du\\end{aligned}$$ Adaptive Gaussian integrationAdaptive Gaussian integration is similar to the previously discussed adaptive forward differencing. Find a point that is a given distance along a curveSpeed controlEase-in/ease-outSine interpolationUsing sinusoidal pieces for acceleration and decelerationSingle cubic polynomial ease-in/ease-outConstant acceleration: parabolic ease-in/ease-out General distance-time functionsCurve fitting to position-time pairsInterpolation of orientationsInterpolating quaternionsWorking with pathsPath followingOrientation along a pathSmoothing a pathDetermining a path along a surfacePath findingAppendix B.5","link":"/Graphics/Animation/Animation-C3-Interpolating-Values/"},{"title":"Animation-C4-Interpolation-Based-Animation","text":"Keywords: Interpolation, Quaternion, pipeline This is the Chapter4 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). Key-frame systems It is common for such systems to provide an interactive interface with which the animator can specify the key values and can control the interpolation curve by manipulating tangents or interior control points. How to interpolate shapes(curves) between key frames? If both curves were generated with the same type of interpolation information, for example, each is a single, cubic Bezier curve, then intermediate curves could be generated by interpolating the control points and reapplying the Bezier interpolation. Another alternative would be to use interpolating functions to generate the same number of points on both curves. These points could then be interpolated on a point-by-point basis. Although these interpolations get the job done, they might not provide sufficientcontrol to a user who has specific ideas about how the intermediate curves should look. Animation languagesArtist-oriented animation languagesThe simple animation language are following examples are from ANIMA II: 1234set position&lt;name&gt;&lt;x&gt;&lt;y&gt;&lt;z&gt;at frame&lt;number&gt;set rotation&lt;name&gt;[X,Y,Z] to&lt;angle&gt;at frame&lt;number&gt;change position&lt;name&gt;to&lt;x&gt;&lt;y&gt;&lt;z&gt;from frame&lt;number&gt;to frame&lt;number&gt;change rotation&lt;name&gt;[X,Y,Z] by&lt;angle&gt;from frame&lt;number&gt;to frame&lt;number&gt; Full-featured programming languages for animationOne example of such a system is AutoDesk’s Maya. The scripting language is called MEL and there is an associated animation language whose scripts are referred to as expressions [36]. During the first frame, translation values for the objects are initialized. For each subsequent frame, two of the sphere’s translation values are set as a function of the frame number and the z-rotation of slab is set as a function of the sphere’s z-translation value. 12345678910if (frame == 1) { slab1.translateY=0; sphere1.translateX=-25; sphere1.translateY=15;}else { sphere1.translateX=25*sin(frame/100); sphere1.translateZ=25*cos(frame/100) - 25; slab1.rotateZ=10*sphere1.translateZ + 90;} Articulation variablesThen a script, or other type of animation system, when it needs a value of the variable in its computation, passes the time variable to the function, which returns its value for that time to the computation. This technique goes by a variety of names, such as track, channel, or articulation variable. Graphical languages Actor-based animation languagesActor-based languages are an object-oriented approach to animation in which an actor (encapsulated model [24]) is a graphical object with its associated data and procedures, including geometric description, display attributes, and motion control. Deforming objectsPicking and pullingDeforming an embedding spaceThree-dimensional shape interpolationMatching topologyStar-shaped polyhedraAxial slicesMap to sphereRecursive subdivisionMorphing (two-dimensional)Coordinate grid approachFeature-based morphingChapter summary","link":"/Graphics/Animation/Animation-C4-Interpolation-Based-Animation/"},{"title":"Animation-C14-Skinning","text":"Keywords: Skinning, LBS, DQS, python This is the ReadingNotes from video games105. Skinning Definition 关于Rigging和Skinning的definitionRigging: 创建一个控制器，通过控制器可以带来表情等的变化Skinning：Rigging的特例，一般是针对骨骼动画 Skinning DeformationLBSThe following figure describes basic idea of Linear Blend Skinning (LBS). Bind Pose Definition将骨骼和蒙皮绑定，计算这时候顶点在骨骼坐标系下的坐标表示，在图中用r来表示了 Bind Pose 和 T Pose对于T-pose来讲，一般认为所有关节点的局部坐标系和global coordinate system一样但Bind Pose可能不是T-Pose,是A-Pose，这时候的关节点的局部坐标系是有旋转的 一般情况下，我们约束一个顶点最多受到4个骨骼的影响。 Candy Wrapper ArtifactSince LBS is linear Blending Skinning, it has the artifacts called Candy Wrapper Artifact. —&gt;Why LBS has candy wrapper artifact?We all know that linear interpolate rotation matrix(which is orthogonal matrix) may get non-rotation matrix.—&gt;Can we use Quaternion slerp to solve candy wrapper artifact?No, from the figure we can see quaternion slerp is no better than matrix interpolate, the reason is $R_j$ is a rotation around global coordinate system origin point. The blending of $R_j$ can not fit well with $t_j$ Automated SkinningLatest research focuses on how to automatically calculate the weights given a binding model: [1]Learning Skeletal Articulations with Neural Blend Shapes_Siggraph_2021 [2]NeuroSkinning: Automatic Skinning Binding for Production Characters with Deep Graph Networks_Siggraph_2019 DQSThe following figure describes basic idea of Dual-Quaterninon Skinning (DQS). Dual Quaternion / QuaternionWhat’s the difference between Dual Quaternion and Quaternion? For Quaternion: The unit vector $\\pmb{\\hat{n}}$ defines an axis of rotation, and the scalar $\\theta$ is the amount of rotation about this axis.$$\\begin{aligned}\\pmb{q} &amp;= \\begin{bmatrix}w &amp; \\pmb{v} \\end{bmatrix}\\\\&amp;=\\begin{bmatrix}\\cos(\\theta/2) &amp; sin(\\theta/2)\\pmb{\\hat{n}} \\end{bmatrix}\\end{aligned}$$ The quaternions $\\pmb{q}$ and $-\\pmb{q}$ describe the same angular displacement. Any angular displacement in 3D has exactly two distinct representations in quaternion format, and they are negatives of each other.It’s not too difficult to see why this is true. If we add $360°$ to $\\theta$, it doesn’t change the angular displacement represented by $q$, but it negates all four components of $q$.$$\\begin{aligned}-\\pmb{q} &amp;= -\\begin{bmatrix}w &amp; \\pmb{v} \\end{bmatrix}\\\\&amp;=\\begin{bmatrix}-\\cos(\\theta/2) &amp; -sin(\\theta/2)\\pmb{\\hat{n}} \\end{bmatrix}\\end{aligned}$$ there are two “identity” quaternions that represent “no angular displacement.”$$\\begin{bmatrix}1 &amp; \\pmb{0} \\end{bmatrix}\\\\\\begin{bmatrix}-1 &amp; \\pmb{0} \\end{bmatrix}$$ Rotation quaternions have unit magnitude:$$\\begin{aligned}||q|| &amp;= \\sqrt{w^2 + ||\\pmb{v}||^2}\\\\&amp;= \\sqrt{\\cos^2(\\theta/2) + (\\sin(\\theta/2)||\\pmb{\\hat{n}}||)^2}\\\\&amp;= 1\\end{aligned}$$ The conjugate of a quaternion, denoted $\\pmb{q^\\ast}$, is obtained by negating the vector portion of the quaternion$$\\pmb{q^\\ast} = \\begin{bmatrix}w &amp; -\\pmb{v} \\end{bmatrix}$$ The inverse of a quaternion, denoted $\\pmb{q}^{−1}$, is defined as the conjugate of a quaternion divided by its magnitude:$$\\pmb{q}^{−1} = \\frac{\\pmb{q\\ast}}{||\\pmb{q}||}$$ quaternion dot product$$\\pmb{q_1q_2} = \\begin{bmatrix}w_1w_2 - \\pmb{v_1 \\cdot v_2} &amp; w_1\\pmb{v_2} + w_2\\pmb{v_1} + \\pmb{v_1} \\times \\pmb{v_2}\\end{bmatrix}$$ Budging Artifact To Correct LBSThis method is pretty intuitive, we just need to add a delta value to vertice $p$ at binding stage, the basic idea is as follows: RBF InterpolationThere are a lot of mothods to calculate interpolation among high dimension points, like Least Squares, Splines, Gaussian process, Radial Basis Function(径向基函数), new we introduce Radial Basis Function. SMPL ModelFirst we need to understand PCA method Facial Animation Morehttps://skinning.org/ Alec Jacobson, Zhigang Deng, Ladislav Kavan , and J. P. Lewis. Skinning: real time shape deformation. In ACM SIGGRAPH 2014 Courses (SIGGRAPH ‘14)","link":"/Graphics/Animation/Animation-C14-Skinning/"},{"title":"Animation-C8-Fluids","text":"Keywords: Inverse kinematics, Forward kinematics, Hierarchy This is the Chapter5 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012).","link":"/Graphics/Animation/Animation-C8-Fluids/"},{"title":"Calculus-C1-Functions","text":"Keywords: Orthogonal Projections, Gram-Schmidt Process, Least-Squares Problems, Linear Model This is the Chapter1 ReadingNotes from book Thomas Calculus 14th.","link":"/Math/Calculus/Calculus-C1-Functions/"},{"title":"Animation-C7-Physically-Based-Animation","text":"Keywords: Particle, Rigidbody, Cloth, Constraints, python This is the Chapter7 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). For our example of cloth, instead of specifying where wrinkles might go and the movement of individual vertices, the animator would merely be able to set parameters that indicate the type and thickness of the cloth material. Basic physics-a reviewSpring-damper pairSpring animation examplesFlexible objectsVirtual springsParticle systemsParticle generationParticle attributesParticle terminationParticle animationParticle renderingParticle system representationForces on particlesParticle life spanRigid body simulationBodies in free fallBodies in collisionDynamics of linked hierarchiesClothDirect modeling of foldsPhysically based modelingEnforcing soft and hard constraintsEnergy minimizationSpace-time constraintsChapter summary","link":"/Graphics/Animation/Animation-C7-Physically-Based-Animation/"},{"title":"Animation-C2-Technical-Background","text":"Keywords: Orientation, Quaternion, pipeline This is the Chapter2 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). Spaces and transformationsThe display pipelineHomogeneous coordinates and the transformation matrixConcatenating transformations: multiplying transformation matricesBasic transformationsRepresenting an arbitrary orientationExtracting transformations from a matrixDescription of transformations in the display pipelineError considerationsOrientation representationFixed-angle representationEuler angle representationAngle and axis representationQuaternion representation","link":"/Graphics/Animation/Animation-C2-Technical-Background/"},{"title":"Animation-C5-Kinematic-Linkages","text":"Keywords: Inverse kinematics, Forward kinematics, Hierarchy This is the Chapter5 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). This chapter is concerned with animating objects whose motion is relative to another object, especially when there is a sequence of objects where each object’s motion can easily be described relative to the previous one. Such an object sequence forms a motion hierarchy. Often the components of the hierarchy represent objects that are physically connected and are referred to by the term linked appendages or, more simply, as linkages. Hierarchical modelingHierarchical modeling is the enforcement of relative location constraints among objects organized in a treelike structure. A common type of hierarchical model used in graphics has objects that are connected end to end to form multibody jointed chains. Such hierarchies are useful for modeling animals and humans so that the joints of the limbs are manipulated to produce a figure with moving appendages. Such a figure is often referred to as articulated and the movement of an appendage by changing the configuration of a joint is referred to as articulation. The robotics literature discusses the modeling of manipulators, a sequence of objects connected in a chain by joints. The rigid objects forming the connections between the joints are called links, and the free end of the chain of alternating joints and links is called the end effector. The local coordinate system associated with each joint is referred to as the frame. The joints of Figure 5.3 allow motion in one direction and are said to have one degree of freedom (DOF). Structures in which more than one DOF are coincident are called complex joints. Complex joints include the planar joint and the ball-and-socket joint. Data structure for hierarchical modelingThe mapping between the hierarchy and tree structure relates a node of the tree to information about the object part (the link) and relates an arc of the tree (the joint) to the transformation to apply to all of the nodes below it in the hierarchy. 树的结点对应是人体骨骼，树的边对应是转换矩阵（关节点做origin）, T-Pose或者A-Pose状态下就是顶点在子骨骼空间的坐标–&gt;顶点在父骨骼空间坐标的转换矩阵，e.g.$$T_1 Link_1 = Link_0空间下的坐标$$ Relating a tree arc to a figure joint may seem counterintuitive, but it is convenient because a node of the tree can have several arcs emanating from it, just as an object part may have several joints attached to it. A simple exampleCoordinate Systems in Algebra &gt;&gt; In this example, there is assumed to be no transformation at any of the nodes; the data are defined in a position ready for articulation.（T-pose, A-pose） the data are defined in a position ready for articulation. $Link_0$, the root object, is transformed to its orientation and position in global space by $T_0$.$$T_0\\begin{bmatrix} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}是Link_0的坐标基矩阵\\begin{bmatrix} p &amp; q\\end{bmatrix}$$Because all of the other parts of the hierarchy will be defined relative to this part, this transformation affects the entire assemblage of parts and thus will transform the position and orientation of the entire structure. This transformation can be changed over time in order to animate the position and orientation of the rigid structure. $Link_1$ is defined relative to the untransformed root object by transformation $T_1$. Similarly, $Link_{1.1}$ is defined relative to the untransformed $Link_1$ by transformation $T_{1.1}$. These relationships can be represented in a tree structure by associating the links with nodes and the transformations with arcs. In the example shown in Figure 5.8, the articulation transformations are not yet included in the model. The vertices of a particular object can be transformed to their final positions by concatenating the transformations higher up the tree and applying the composite transformation matrix to the vertices.注意，是顶点，是点坐标 A vertex, $V_0$, of the root object, $Link_0$, is located in the world coordinate system, $V_0’$ by applying the rigid transformation that affects the entire structure; see Equation 5.1. (顶点$V_0$在$Link_0$骨骼空间下的坐标转换成了在世界坐标系standard coordinate下的坐标$V_0’$)$$V_0’ = T_0 V_0\\tag{5.1}$$A vertex of the $Link_1$ object is located in the world coordinate system by transforming it first to its location relative to $Link_0$ and then relocating it (conceptually along with $Link_0$) to world space by Equation 5.2.$$V_1’ = T_0T_1V_1\\tag{5.2}$$ To locate a vertex of $Link_1$ in world space, one must first transform it via the joint rotation matrix(关节旋转矩阵). Once that is complete, then the rest of the transformations up the hierarchy are applied (see Eq. 5.4).(Figure5.9,5.10)$$V_1’ = T_0T_1R_1(\\theta_1) V_1\\tag{5.4}$$A vertex of $Link_{1.1}$ is transformed similarly by compositing all of the transformations up the hierarchyto the root, as in Equation 5.5.$$V_{1.1}’ = T_0T_1R_1(\\theta_1) T_{1.1}R_{1.1}(\\theta_{1.1})V_{1.1}\\tag{5.5}$$For example Figure5.11,5.12, in a simplified human figure, the root hip area (see Figure 5.5) might branch into the torso and two legs. If prismatic joints are used, the strategy is the same; the only difference is that the rotation transformation of the joint (arc) is replaced by a translation. Local coordinate framesIn setting up complex hierarchies and in applying computationally involved procedures such as IK, it is convenient to be able to define points in the local coordinate system (frame) associated with a joint and to have a well-defined method for converting the coordinates of a point from one frame to another. A common use for this method is to convert points defined in the frame of a joint to the global coordinate system for display purposes. Forward kinematicsTraversal follows a depth-first pattern 深搜 from root to leaf node. The traversal then backtracks up回溯 the tree until an unexplored downward arc is encountered. The downward arc is then traversed, followed by backtracking up to find the next unexplored arc. This traversal continues until all nodes and arcs have been visited. Whenever an arc is followed down the tree hierarchy, its transformations are concatenated to the transformations of its parent node.Whenever an arc is traversed back up the tree to a node, the transformation of that node must be restored before traversal continues downward. each arc has associated with it the following: nodePtr: A pointer to a node that holds the data to be articulated by the arc. Lmatrix: A matrix that locates the following (child) node relative to the previous (parent) node. 骨骼空间转换矩阵 Amatrix: A matrix that articulates the node data; this is the matrix that is changed in order to animat (articulate) the linkage. 骨骼运动矩阵 arcPtr: A pointer to a sibling arc (another child of this arc’s parent node); this is NULL if there are no more siblings. Each node has associated with it the following: dataPtr: Data (possibly shared by other nodes) that represent the geometry of this segment of the figure. Tmatrix: A matrix to transform the node data into position to be articulated (e.g., put the point of rotation at the origin). ArcPtr: A pointer to a single child arc. 12345678910111213141516171819202122232425262728293031traverse(arcPtr,matrix){ //get transformations of arc and concatenate to current matrix matrix = matrix * arcPtr-&gt;Lmatrix // concatenate location matrix = matrix * arcPtr-&gt;Amatrix // concatenate articulation // process data at node nodePtr = arcPtr-&gt;nodePtr // get the node of the arc push(matrix) // save the matrix matrix = matrix * nodePtr-&gt;matrix // ready for articulation articulatedData = transformData(matrix,dataPtr) // articulate the data draw(articulatedData)// // and draw it matrix = pop() // restore matrix for node’s children // process children of node if (nodePtr-&gt;arcPtr!= NULL) { // if not a terminal node nextArcPtr = nodePtr-&gt;arcPtr // get first arc emanating from node while (nextArcPtr != NULL) { // while there’s an arc to process push(matrix) // save matrix at node traverse(nextArcPtr,matrix) // traverse arc matrix = pop() // restore matrix at node nextArcPtr = nextArcPtr-&gt;arcPtr // set next child of node } }}traverse(rootArcPtr,I) // ‘I’ is identity matrix In a simple animation, a user may determine a key position interactively or by specifying numeric values and then interpolate the joint values between key positions. Specifying all of the joint parameter values for key positions is called forward kinematics and is an easy way to animate the figure. A completely specified set of linkage parameters, which results in positioning the hierarchical figure, are called a pose. A pose is specified by a vector (the pose vector) consisting of one parameter for each DOF. Inverse kinematicsIn IK, the desired position and possibly orientation of the end effector are given by the user along with the initial pose vector. From this, the joint values required to attain that configuration are calculated, giving the final pose vector. If there are so many constraints on the configuration that no solution exists, the system is called overconstrained. If there are relatively few constraints on the system and there are many solutions to the problem posed, then it is underconstrained. The reachable workspace is that volume which the end effector can reach. The dexterous workspace is the volume that the end effector can reach in any orientation. Solving a simple system by analysisAssume for now (without loss of generality) that the base is at the origin. In a simple IK problem, the user gives the $(X, Y)$ coordinate of the desired position for the end effector. The joint angles, $\\theta_1$ and $\\theta_2$, can be determined by computing the distance from the base to the goal and using the law of cosines to compute the interior angles. Most linkages used in robotic applications are designed to be simple enough for this algebraic manipulation. However, for many cases that arise in computer animation, analytic solutions are not tractable. In such cases, iterative numeric solutions must be relied on. The JacobianAt each time step, a computation is performed that determines a way to change each joint angle in order to direct the current position and orientation of the end effector toward the desired configuration. There are several methods used to compute the change in joint angle, but most involve forming the matrix of partial derivatives called the Jacobian. Partial Calculus &gt;&gt; Given specific values for the input variables, $x_i$, each of the output variables, $y_i$, can be computed by its respective function.$$y_1 = f_1(x_1, x_2, x_3, \\cdots, x_6)\\\\\\cdots \\\\y_6 = f_6(x_1, x_2, x_3, \\cdots, x_6)\\tag{5.6}$$The differentials of $y_i$ can be written in terms of the differentials of $x_i$ using the chain rule. This generates Equation 5.7.$$dy_i = \\frac{\\partial f_i}{\\partial x_1} dx_1 + \\cdots + \\frac{\\partial f_i}{\\partial x_6} dx_6\\tag{5.7}$$ Equations 5.6 and 5.7 can be put in vector notation, producing Equations 5.8 and 5.9, respectively.$$d\\pmb{Y} = \\frac{\\partial \\pmb{F}}{\\partial \\pmb{X}} d \\pmb{X}\\tag{5.8}$$A matrix of partial derivatives, $\\frac{\\partial \\pmb{F}}{\\partial \\pmb{X}}$, is called the Jacobian and is a function of the current values of $x_i$. The Jacobian can be thought of as mapping the velocities of $\\pmb{X}$ to the velocities of $\\pmb{Y}$ (Eq. 5.10).$$\\dot{\\pmb{Y}} = \\pmb{J(X)}\\dot{\\pmb{X}}\\tag{5.10}$$ When one applies the Jacobian to a linked appendage, the input variables, $x_i$, become the joint values and the output variables, $y_i$, become the end effector position and orientation (in some suitable representation such as $x-y-z$ fixed angles).$$\\pmb{Y} = \\begin{bmatrix} p_x &amp; p_y &amp; p_z &amp; \\alpha_x &amp; \\alpha_y &amp; \\alpha_z\\end{bmatrix}^T\\tag{5.11}$$In this case, the Jacobian relates the velocities of the joint angles, $\\dot{\\theta}$, to the velocities of the end effector position and orientation, $\\pmb{\\dot{Y}}$ (Eq. 5.12).$$\\pmb{V} = \\pmb{\\dot{Y}} = \\pmb{J}(\\theta) \\dot{\\theta}\\tag{5.12}$$where, $\\pmb{V}$ is the vector of linear and rotational velocities and represents the desired change in the end effector.$$\\pmb{V} = \\begin{bmatrix} v_x &amp; v_y &amp; v_z &amp; \\omega_x &amp; \\omega_y &amp; \\omega_z\\end{bmatrix}^T\\tag{5.13}$$$\\pmb{J}$, the Jacobian, is a matrix that relates the two and is a function of the current pose$$\\pmb{J} = \\begin{bmatrix} \\frac{\\partial p_x}{\\partial \\theta_1} &amp; \\frac{\\partial p_x}{\\partial \\theta_2} &amp; \\cdots&amp; \\frac{\\partial p_x}{\\partial \\theta_n}\\\\ \\frac{\\partial p_y}{\\partial \\theta_1} &amp; \\frac{\\partial p_y}{\\partial \\theta_2} &amp; \\cdots &amp; \\frac{\\partial p_y}{\\partial \\theta_n}\\\\ \\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots\\\\ \\frac{\\partial \\alpha_z}{\\partial \\theta_1} &amp; \\frac{\\partial \\alpha_z}{\\partial \\theta_2} &amp; \\cdots &amp; \\frac{\\partial \\alpha_z}{\\partial \\theta_n}\\end{bmatrix}\\tag{5.15}$$ Each term of the Jacobian relates the change of a specific joint to a specific change in the end effector. For a revolute joint, the rotational change in the end effector, $\\omega$, is merely the velocity of the joint angle about the axis of revolution at the joint under consideration. 对于一个转动关节， 末端执行器的旋转变化，$\\omega$，仅仅是关节角绕转轴的速度。 For a prismatic joint, the end effector orientation is unaffected by the joint articulation. 对于移动关节，末端执行器方向不受关节运动的影响。 For a rotational joint, the linear change in the end effector is the cross-product of the axis of revolution and a vector from the joint to the end effector. The rotation at a rotational joint induces an instantaneous linear direction of travel at the end effector. 对于一个旋转关节，末端执行器的线性变化是旋转轴和一个从关节到末端执行器的向量的叉积。旋转关节上的旋转会在末端执行器上产生瞬时的线性运动方向。For a prismatic joint, the linear change is identical to the change at the joint (see Figure 5.16). 对于移动关节，线性变化等于关节处的变化。 In forming the Jacobian matrix, this information must be converted into some common coordinate system such as the global inertial (world) coordinate system or the end effector coordinate system. A simple exampleIn this example, the objective is to move the end effector, $E$, to the goal position, $G$. The orientation of the end effector is of no concern in this example. The axis of rotation of each joint is perpendicular to the figure, coming out of the paper. 关节的旋转轴朝向读者 The effect of an incremental rotation, $g_i$, of each joint can be determined by the cross-product of the joint axis and the vector from the joint to the end effector, $V_i$ (Figure 5.18), and form the columns of the Jacobian. A vector of the desired change in values is set equal to the Jacobian matrix (Eq. 5.17) multiplied by a vector of the unknown values, which are the changes to the joint angles. where desired change is,$$V = \\begin{bmatrix}(G-E)_x\\\\(G-E)_y\\\\(G-E)_z\\end{bmatrix}$$Jacobian matrix is,$$J =\\begin{bmatrix}((0,0,1) \\times E)_x &amp; ((0,0,1) \\times (E - P_1))_x &amp; ((0,0,1) \\times (E - P_2))_x\\\\((0,0,1) \\times E)_y &amp; ((0,0,1) \\times (E - P_1))_y &amp; ((0,0,1) \\times (E - P_2))_y\\\\((0,0,1) \\times E)_z &amp; ((0,0,1) \\times (E - P_1))_z &amp; ((0,0,1) \\times (E - P_2))_z\\end{bmatrix}\\tag{1.57}$$ 从几何意义上来求Jacobian矩阵，就是这种叉乘的做法 Numeric solutions to IKSolution using the inverse JacobianIf the inverse of the Jacobian ($J^{-1}$) does not exist, then the system is said to be singular for the given joint angles. (奇异 = 不可逆) A singularity occurs when a linear combination of the joint angle velocities cannot be formed to produce the desired end effector velocities. 大部分情况下，$J^{-1}$都是直接求不出来的 $$V = J\\dot{\\theta}\\tag{5.18}$$ $$J^{-1}V = \\dot{\\theta}\\tag{5.19}$$ Inverse Matrix in Algebra &gt;&gt; As shown in Figure5.19, a change in each joint angle would produce a vector perpendicular to the desired direction. Obviously, no linear combination of these vectors could produce the desired motion vector. Problems with singularities can be reduced if the manipulator is redundant—when there are more DOF than there are constraints to be satisfied. In this case, the Jacobian is not a square matrix and, potentially, there are an infinite number of solutions to the IK problem. 自由度大于约束的时候，可以减少奇点问题, 再这种情况下，Jacobian矩阵不是方阵，对于IK问题可能会存在很多个解 If the columns of $J$ are linearly independent(i.e., $J$ has full column rank, 此时$J$是一个高瘦矩阵，比如$9 \\times 3$), the $(J^TJ)^{-1}$ exists and instead, the pseudoinverse, $J^{\\dagger}$ an be used as in Equation 5.20. $$V = J\\dot{\\theta}\\\\J^TV = J^TJ\\dot{\\theta}\\\\(J^TJ)^{-1}J^TV = \\dot{\\theta}\\\\J^{\\dagger}V = \\dot{\\theta}\\tag{5.20}$$ understand pseudoinverse from least squares view: The pseudoinverse maps the desired velocities of the end effector to the required changes of the joint angles. If the rows of $J$ are linearly independent, 此时$J$是一个矮胖矩阵，比如$3 \\times 9$。$$J^{\\dagger} = J^T(JJ^T)^{-1}$$LU decomposition &gt;&gt; can be used to solve Equation.$$J^{\\dagger}V = \\dot{\\theta}\\\\J^T(JJ^T)^{-1}V = \\dot{\\theta}\\\\$$first, get $\\beta$ value,$$\\beta = (JJ^T)^{-1}V\\tag{5.22}$$then get $\\dot{\\theta}$ value$$(JJ^T)\\beta = V\\\\J^T\\beta = \\dot{\\theta}\\tag{5.23}$$ Simple Euler integration can be used at this point to update the joint angles. The Jacobian has changed at the next time step, so the computation must be performed again and another step taken. This process repeats until the end effector reaches the goal configuration within some acceptable (i.e., user-defined) tolerance. 通常会用Gaussian-Newton法对应Jacobian pseudoinverse方法 For example: Even the underconstrained case still has problems with singularities，也就是说即便使用伪逆的方法来求解，有时候计算机精度的问题，可能还是不可逆，解决这个问题可以加上damp. A proposed solution to such bad behavior is the damped least-squares approach [1] [2]. Referring to Equation 5.24, a user-supplied parameter is used to add in a term that reduces the sensitivity of the pseudoinverse.$$\\dot{\\theta} = J^T(JJ^T + \\lambda^2I)^{-1}V\\tag{5.24}$$ Adding more controlTo better control the kinematic model such as encouraging joint angle constraints, a control expression can be added to the pseudoinverse Jacobian solution.The form for the control expression is shown in Equation 5.25. $$\\dot{\\theta} = (J^{\\dagger}J - I)z\\tag{5.25}$$In Equation 5.26 it is shown that a change to pose parameters in the form of Equation 5.25 does not add anything to the velocities. $$V = J\\dot{\\theta}\\\\V = J(J^{\\dagger}J - I)z\\\\V = (JJ^{\\dagger}J - J)z\\\\V = 0z\\\\V = 0\\tag{5.26}$$ To bias the solution toward specific joint angles, such as the middle angle between joint limits, $z$ is defined as in Equation 5.27(因为关节角的转动给了一定的约束，所以要尽量的使solution偏向约束下的关节角度)$$z = \\alpha_i(\\theta_i - \\theta_{ci})^2\\tag{5.27}$$where, $\\theta_i$ are the current joint angles, $\\theta_{ci}$ are the desired joint angles, and $\\alpha_i$ are the joint gains. The desired angles and gains are input parameters. The gain indicates the relative importance of the associated desired angle; the higher the gain, the stiffer the joint 增益越高，关节越硬.If the gain for a particular joint is high, then the solution will be such that the joint angle quickly approaches the desired joint angle. The control expression is added to the conventional pseudoinverse of the Jacobian (Eq. 5.28).$$\\dot{\\theta} = J^{\\dagger}V + (J^{\\dagger}J - I)z\\tag{5.28}$$ Alternative JacobianInstead of trying to push the end effector toward the goal position, a formulation has been proposed that pulls the goal to the end effector [1]. Avoiding the inverse: using the transpose of the Jacobianthe vector of joint parameter changes is formed by multiplying the transpose of the Jacobian times the velocity vector and using a scaled version of this as the joint parameter change vector.$$\\dot{\\theta} = \\alpha J^TV$$ 这也是求解优化问题的梯度下降法 Procedurally determining the angles: cyclic coordinate descent CCDCyclic coordinate descent considers each joint one at a time, sequentially from the outermost inward [5]. At each joint, an angle is chosen that best gets the end effector to the goal position. CCD很简单，就是对于每个关节点都让这个关节点旋转一定角度，从而使end effector离目标点最近 [5] Welman C. Inverse Kinematics and Geometric Constraints for Articulated Figure Manipulation. M.S. Thesis. Simon Frasier University; 1993. Chapter summarynumeric approach is often needed. Trade-offs among the numeric approaches include ease of implementation, possible real-time behavior, and robustness in the face of singularities. An additional approach is targeted at linkages that are specifically human-like in their DOF and heuristically solves reaching problems by trying to model human-like reaching; this is discussed in Chapter 9. IK Survey:http://www.andreasaristidou.com/publications/papers/IK_survey.pdf","link":"/Graphics/Animation/Animation-C5-Kinematic-Linkages/"},{"title":"Animation-C9-Modeling-and-Animating-Human-Figures","text":"Keywords: Walk, Reaching and Grasping This is the Chapter9 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). Overview of virtual human representationRepresenting body geometryThe vast majority of human figures are modeled using a boundary representation constructed from either polygons (often triangles) or patches (usually nonuniform rational B-splines; NURBS). If the figure is constructed for real-time display in a game on a low-end PC or gaming console, usually it will be assembled from a relatively low number of triangular polygons, which, while giving a chunky appearance to the model, can be rendered quickly. If the figure will be used in an animation that will be rendered off-line by a high-end rendering package, it might be modeled with NURBS patch data, to obtain smooth curved contours. Polygonal representationsWhen multiple objects are used, they are generally arranged in a hierarchy of joints and rigid segments. Rotating a joint rotates all of that joint’s children (e.g., rotating a hip joint rotates all of the child’s leg segments and joints around the hip). If a single mesh is used, then rotating a joint must deform the vertices surrounding that joint, as well as rotate the vertices in the affected limb. Patch representationsWhile patches can easily provide much smoother surfaces than polygons in general, it is more challenging to add localized detail to a figure without adding a great deal of more global data. Hierarchical splines provide a partial solution to this problem [21]. Other representationsImplicit surfaces (Chapter 12.1) can be employed as sculpting material for building virtual humans. Frequently the term “metaballs” is used for spherical implicit surfaces. Geometry data acquisitionData representing a specific individual are often captured using a laser scanner or by performing image processing on video images.(三维重建) Geometry deformationBody parts are arranged in a hierarchy of joints and segments such that rotating a joint rotates all of the child segments and joints beneath it in the hierarchy [31]. While this method is quick, it yields suboptimal visual results at the joints, particularly if the body is textured, because the rigid parts merely overlap and occlude each other at the joint. When a bone rotates, the vertices move along with it [7]. Better results can be obtained, at the cost of additional computation, if the vertices surrounding a joint are weighted so that their position is affected by multiple bones [41]. While weighting the effects of bone rotations on vertices results in smoother skin around joints, severe problems can still occur with extreme joint bends.FFDs have been used in this case to simulate the skin-on-skin collision and the accompanying squashing and bulging that occur when a joint such as the elbow is fully bent [13]. Surface deformations that would be caused by changes in underlying material, such as muscle and fat in a real human, can be produced in a virtual human by a number of means. Methods range from those that simply allow an animator to specify muscular deformations [13] to those that require complex dynamics simulations of the various tissue layers and densities [49] [57]. Surface detailskin, wrinkles etc. Layered approach to human figure modelingA common approach to animating the human figure is to construct the figure in layers consisting of skeleton, muscles, and skin. The muscle layer implements a system of FFD lattices in which the surface geometry is embedded. The lattice is organized with two planes on each end that are responsible for maintaining continuity with adjoining lattices, and the interior planes are responsible for deforming the skin to simulate muscle bulging (Figure 9.1). RiggingRigging, as used here, refers to setting up interactive controls to facilitate animation of a character or other object. Reaching and graspingModeling the arm The shoulder jointThe handCoordinated movementReaching around obstaclesStrengthWalkingAn aspect of walking that complicates its analysis and generation is that it is dynamically stable but not statically stable. The mechanics of locomotionWalk cycleThe walk cycle can be broken down into various phases [11] based on the relation of the feet to their points of contact with the ground (see Figure 9.14). The stride is defined by the sequence of motions between two consecutive repetitions of a body configuration [32]. The left stance phase of a stride is initiated with the right foot on the ground and the left heel just starting to strike the ground. 右脚着地，左脚跟刚开始着地 The right swing phase is the period in which the right toe leaves the ground, the right leg swings forward, and the right heel strikes the ground. Notice that the right swing phase is a subinterval of the left stance phase. Run cycle Pelvic transportFigure 9.16 shows how the pelvis is supported by the stance leg at various points during the stance phase of the walk cycle. Figure 9.17 shows these positions superposed during a full stride and illustrates the abutting of two-dimensional circular arcs describing the basic path of the pelvis as it is transported by the legs. Pelvic rotationFigure 9.18 shows the position of the pelvis during various points in the walking cycle, as viewed from above. The pelvis rotates about a vertical axis centered at the stance leg, helping to lengthen the stride as the swing leg stretches out for its new foot placement. The top of the stance leg is rotating above the point of contact with the floor (Figure 9.18), so the path of the center of the pelvis resembles a sinusoidal curve (Figure 9.19). Pelvic listThe kinematics of the walkThe most basic approach to generating the walking motion is for the animator to specify a list of joint angle values for each DOF involved in the walk. There are various sources for empirical data describing the kinematics of various walks at various speeds.There are various sources for empirical data describing the kinematics of various walks at various speeds. Figures 9.23 through 9.27. When creating new walks, the animator can specify kinematic values such as pelvic movement, foot placement, and foot trajectories. Inverse kinematics can be used to determine the angles of the intermediate joints [12]. By constructing the time-space curves traced by the pelvis and each foot, the user can determine the position of each for a given frame of the animation. Each leg can then be positioned by considering the pelvis fixed and the leg a linked appendage whose desired end-effector position is the corresponding position on the foot trajectory curve (Figure 9.28). Using dynamics to help produce realistic motionDynamic simulation can be used to map specified actions and constraints to make the movement more accurate physically. Forward dynamic controlIn some cases, forward dynamic control instead of kinematic control can be effectively used. Kinematics still plays a role. CoveringsClothingReal-time application, such as computer games, still often use virtual humans that sport rigid body armor that merely rotates along with whatever limb it is attached to. High-end off-line animation systems are starting to offer advanced clothing simulation modules that attempt to calculate the effects of gravity as well as cloth–cloth and cloth–body collisions by using mass-spring networks or energy functions. HairThe most common, visually poor but computationally inexpensive technique has been to merely construct a rigid piece of geometry in the rough shape of the volume of hair and attach it to the top of the head, like a helmet, as in Figure 9.31. Texture maps with transparency information are sometimes used to improve the appearance, providing for a wispy boundary. FFDs can be used to animate the global shape of rigidly defined hair.","link":"/Graphics/Animation/Animation-C9-Modeling-and-Animating-Human-Figures/"},{"title":"Animation-C6-Motion-Capture","text":"Keywords: Camera calibration, motion Retargeting, python This is the Chapter6 ReadingNotes from book Computer Animation_ Algorithms and Techniques_ Parent-Elsevier (2012). Creating physically realistic motion using techniques involving key frames, forward kinematics, or inverse kinematics is a daunting task. In the previous chapters, techniques to move articulated linkages are covered, but they still require a large amount of animator talent in order to make an object move as in the real world. Motion capture technologiesElectromagnetic tracking, also simply called magnetic tracking, uses sensors placed at the joints that transmit their positions and orientations back to a central processor to record their movements. Optical markers, on the other hand, have a much larger range, and the performers only have to wear reflective markers on their clothes (see Figure 6.1, Color Plate 2). Processing the imagesThe objective of motion control is to reconstruct the three-dimensional motion of a physical object and apply it to a synthetic model. First, the two-dimensional images need to be processed so that the markers can be located, identified, and correlated in the multiple video streams. Second, the three-dimensional locations of the markers need to be reconstructed from their two-dimensional locations. Third, the three-dimensional marker locations must be constrained to a model of the physical system whose motion is being captured (e.g., a triangulated surface model of the performer). The difference between the position of the marker and that of the joint is one source of error in motion capture systems. Frame-to-frame coherence can be employed to track markers by making use of the position of a marker in a previous frame and knowing something about the limits of its velocity and acceleration. Unfortunately, one of the realities of optical motion capture systems is that periodically one or more of the markers are occluded. Marker swapping can happen even when markers are not occluded.How to solve this problem? Some simple heuristics can be used to track markers that drop from view for a few frames and that do not change their velocity much over that time. But these heuristics are not foolproof (and is, of course, why they are called heuristics). Sometimes these problems can be resolved when the threedimensional positions of markers are constructed, Data points that are radically inconsistent with typical values can be thrown out, and the rest can be filtered. At other times user intervention is required to resolve ambiguous cases. Camera calibrationCamera calibration is performed by recording a number of image space points whose world space locations are known. These pairs can be used to create an overdetermined set of linear equations that can be solved using a least-squares solution method. This is only the most basic computation needed to fully calibrate a camera with respect to its intrinsic properties and its environment. In the most general case, there are nonlinearities in the lens, focal length, camera tilt, and other parameters that need to be determined. Appendix B.11 Camera calibrationIn the capture of an image, a point in global space is projected onto the camera’s local coordinate system and then mapped to pixel coordinates. To establish the camera’s parameters, one uses several points whose coordinates are known in the global coordinate system and whose corresponding pixel coordinates are also known. By setting up a system of equations that relates these coordinates through the camera’s parameters, one can form a least-squares solution of the parameters [23]. This results in a series of five-tuples, $(x_i, y_i, z_i, c_i, r_i)$ consisting of the three-dimensional global coordinates and two-dimensional image coordinates for each point. The two-dimensional image coordinates are a mapping of the local two-dimensional image plane of the camera located a distance $f$ in front of the camera (Eq. B.171). 从摄像机空间下的坐标到屏幕坐标$$\\begin{cases}c_i - c_0 = s_uu_i\\\\r_i - r_0 = s_vv_i\\end{cases}\\tag{B.171}$$The image plane is located relative to the three-dimensional local coordinate system $(u, v, w)$ of the camera (Figure B.60). The imaging of a three-dimensional point is approximated using a pinhole camera model. The three-dimensional local coordinate system of the camera is related to the three dimensional global coordinate system by a rotation and translation (Eq. B.172); 摄像机坐标系通过旋转R和平移T变成世界坐标系，也就是说世界坐标系下的点坐标经过R和T可以变成摄像机坐标系的点坐标$$\\begin{bmatrix} u_i \\\\ v_i \\\\ f\\end{bmatrix}=R\\begin{bmatrix} x_i \\\\ y_i \\\\ z_i\\end{bmatrix} + T\\tag{B.172}$$$$R = \\begin{bmatrix} R_0 \\\\ R_1 \\\\ R_2\\end{bmatrix}=\\begin{bmatrix} r_{0,0} &amp; r_{0,1} &amp; r_{0,2}\\\\ r_{1,0} &amp; r_{1,1} &amp; r_{1,2}\\\\ r_{2,0} &amp; r_{2,1} &amp; r_{2,2}\\end{bmatrix}$$$$T = \\begin{bmatrix} t_0 \\\\ t_1 \\\\ t_2\\end{bmatrix}$$ the origin of the camera’s local coordinate system is assumed to be at the focal point of the camera. The three-dimensional coordinates are related to the two-dimensional coordinates by the transformation to be determined. Equation B.173 expresses the relationship between a pixel’s column and row number and the global coordinates of the point. 点在屏幕空间下的坐标和世界空间下的坐标关系 $$\\frac{u_i}{f} = \\frac{c_i - c_0}{s_uf} = \\frac{c_i - c_0}{f_u} = \\frac{R_0 \\cdot [x_iy_iz_i] + t_0}{R_2 \\cdot [x_iy_iz_i] + t_2}$$$$\\frac{v_i}{f} = \\frac{r_i - r_0}{s_vf} = \\frac{r_i - r_0}{f_v} = \\frac{R_1 \\cdot [x_iy_iz_i] + t_1}{R_2 \\cdot [x_iy_iz_i] + t_2}\\tag{B.173}$$ These equations are rearranged and set equal to zero in Equation B.174. They can be put in the form of a system of linear equations (Eq. B.175) so that the unknowns are isolated (Eq. B.176) by using substitutions common in camera calibration (Eqs. B.176 and B.177). 现在未知数全跑$W$上了 $$(c_i-c_0)(R_2 \\cdot [x_iy_iz_i] + t_2) - f_u(R_0 \\cdot [x_iy_iz_i] + t_0) = 0\\\\(r_i-r_0)(R_2 \\cdot [x_iy_iz_i] + t_2) - f_v(R_1 \\cdot [x_iy_iz_i] + t_1) = 0\\tag{B.174}$$ $$AW = 0 \\Longleftrightarrow\\begin{bmatrix} -x_1 &amp; -y_1 &amp; -z_1 &amp; 0 &amp; 0 &amp; 0 &amp; r_1x_1 &amp; r_1y_1 &amp; r_1z_1 &amp; -1 &amp; 0 &amp; r_1 \\\\ 0 &amp; 0 &amp; 0 &amp; -x_1 &amp; -y_1 &amp; -z_1 &amp; c_1x_1 &amp; c_1y_1 &amp; c_1z_1 &amp; 0 &amp; -1 &amp; c_1\\end{bmatrix}\\begin{bmatrix} w_0\\\\w_1\\\\w_2\\\\w_3\\\\ \\cdots \\\\w_{11}\\end{bmatrix}=0\\tag{B.175}$$ $$\\begin{aligned}&amp;W_0 = f_uR_0 + c_0R_2 = [w_0 w_1 w_2]^T\\\\&amp;W_3 = f_vR_1 + r_0R_2 = [w_3 w_4 w_5]^T\\\\&amp;W_6 = R_2 = [w_6 w_7 w_8]^T\\\\&amp;w_9 = f_ut_0 + c_0t_2\\\\&amp;w_{10} = f_vf_1 + r_0t_2\\\\&amp;w_{11} = t_2\\end{aligned}\\tag{B.176}$$ $$W = \\begin{bmatrix} w_0 &amp; w_1 &amp; w_2 &amp; w_3 &amp; w_4 &amp; w_5 &amp; \\cdots &amp; w_{11}\\end{bmatrix}^T\\tag{B.177}$$Temporarily dividing through by $t_2$ ensures that $t_2 \\neq 0.0$ and therefore that the global origin is in front of the camera. This step results in Equation B.178, where $A’$ is the first $11$ columns of $A$; $B’$ is the last columnof $A$; and $W’$ is the first $11$ rows of $W$. 不是很懂，但感觉是为了方便最小二乘计算 Typically, enough points are captured to ensure an overdetermined system. Then a least squares method, such as singular value decomposition, can be used to find the $W’$ that satisfies Equation B.179. 找到最优的 $W’$，使得 $A’W’$ 与 $-B’$ 距离最近$$A’W’ + B’ = 0\\tag{B.178}$$ $$\\underset{w}{min} ||A’W’ + B’||\\tag{B.179}$$ $W’$ is related to $W$ by Equation B.180, and the camera parameters can be recovered by undoing the substitutions made in Equation B.176 by Equation B.181.$$W = \\begin{bmatrix} W_0 \\\\ W_3 \\\\ W_6 \\\\ W_9 \\\\ W_{10} \\\\ W_{11}\\end{bmatrix}=\\frac{1}{||W_6’||} \\times\\begin{bmatrix} W_0’ \\\\ W_3’ \\\\ W_6’ \\\\ W_9’ \\\\ W_{10}’ \\\\ W_{11}’\\end{bmatrix}\\tag{B.180}$$ $$\\begin{aligned}&amp;c_0 = W_0^TW_6\\\\&amp;r_0 = W_1^TW_6\\\\&amp;f_u = - ||W_0 - c_0W_6||\\\\&amp;t_0 = (w_9 - c_0) / f_u\\\\&amp;t_1 = (w_{10} - r_0) / f_v\\\\&amp;t_2 = w_{11}\\\\&amp;R_0 = (W_0 - c_0W_6) / f_u\\\\&amp;R_1 = (W_3 - r_0W_6) / f_v\\\\&amp;R_2 = W_6\\end{aligned}\\tag{B.181}$$ Because of numerical imprecision, the rotation matrix recovered may not be orthonormal正交矩阵, so it is best to reconstruct the rotation matrix first (Eq. B.182), $$\\begin{aligned}&amp;R_0 = (W_0’ - c_0W_6’) / f_u\\\\&amp;R_1 = (W_3’ - r_0W_6’) / f_v\\\\&amp;R_2 = W_6’\\end{aligned}\\tag{B.182}$$massage it into orthonormality, and then use the new rotation matrix to generate the rest of the parameters (Eq. B.183).$$\\begin{aligned}&amp;c_0 = W_0^TR_2\\\\&amp;r_0 = W_1^TR_2\\\\&amp;f_u = - ||W_0 - c_0R_2||\\\\&amp;f_v = ||W_3 - r_0R_2||\\\\&amp;t_0 = (w_9 - c_0) / f_u\\\\&amp;t_1 = (w_{10} - r_0) / f_v\\\\&amp;t_2 = w_{11}\\\\\\end{aligned}\\tag{B.183}$$…感觉讲的不是很懂 Least Squares in Algebra &gt;&gt; SVD in Algebra &gt;&gt; Motion Capture Tutorial &gt;&gt; Three-dimensional position reconstructionThe greater the orthogonality of the two views, the better the chance for an accurate reconstruction. If the position and orientation of each camera are known with respect to the global coordinate system, along with the position of the image plane relative to the camera, then the images of the point to be reconstructed $(I_1, I_2)$ can be used to locate the point, $P$, in three-space (Figure 6.4). Using the location of a camera, the relative location of the image plane, and a given pixel location on the image plane, the user can compute the position of that pixel in world coordinates. Once that is known, a vector from the camera through the pixel can be constructed in world space for each camera (Eqs. 6.1 and 6.2). $$\\begin{aligned} &amp;C_1 + k_1(I_1 - C_1) = P\\\\ &amp;C_2 + k_2(I_2 - C_2) = P\\\\ &amp;C_1 + k_1(I_1 - C_1) = C_2 + k_2(I_2 - C_2)\\end{aligned}$$ Three equations and two unknows, so we can get $k_1, k_2$ easily. However, In practice, these two equations will not exactly intersect, although if the noise in the system is not excessive, they will come close. So, in practice, the points of closest encounter must be found on each line. This requires that a $P_1$ and a $P_2$ be found such that $P_1$ is on the line from Camera 1, $P_2$ is on the line from Camera 2, and $P_2P_1$ is perpendicular to each of the two lines (Eqs. 6.3 and 6.4).$$\\begin{aligned}&amp;(P_2 - P_1) \\cdot (I_1 - C_1) = 0\\\\&amp;(P_2 - P_1) \\cdot (I_2 - C_2) = 0\\end{aligned}\\tag{6.3}$$ Once the points $P_1$ and $P_2$ have been calculated, the midpoint of the chord between the two points can be used as the location of the reconstructed point. B.2.6 Closest point between two lines in three-spaceThe intersection of two lines in three-space often needs to be calculated. Because of numerical imprecision, the lines rarely, if ever, actually intersect in three-space. As a result, the computation that needs to be performed is to find the two points, one from each line, at which the lines are closest to each other. The points $P_1$ and $P_2$ at which the lines are closest form a line segment perpendicular to both lines (Figure B.13). They can be represented parametrically as points along the lines, and then the parametric interpolants can be solved for by satisfying the equations that state the requirement for perpendicularity (Eq. B.30) Multiple markersMultiple camerasTo reconstruct the three-dimensional position of a marker, the system must detect and identify the marker in at least two images. Fitting to the skeletonOnce the motion of the individual markers looks smooth and reasonable, the next step is to attach them to the underlying skeletal structure that is to be controlled by the digitized motion. One source of the problem is that the markers are located not at the joints of the performers, but outside the joints at the surface. This means that the point being digitized is displaced from the joint itself. One solution is to put markers on both sides of the joint. With two marker locations, the joint can be interpolated as the midpoint of the chord between the two markers. While effective for joints that lend themselves to this approach, the approach does not work for joints that are complex or more inaccessible (such as the hip, shoulder, and spine), and it doubles the number of markers that must be processed. A little geometry can be used to calculate the displacement of the joint from the marker. A plane formed by three markers can be used to calculate a normal to a plane of three consecutive joints, and this normal can be used to displace the joint location. Output from motion capture systems.bvh/.asf/.asm Manipulating motion capture dataThere are various ways to manipulate mocap data. Some are straightforward applications of standard signal processing techniques while others are active areas of research. These techniques hold the promise of enabling motion libraries to be constructed along with algorithms for editing the motions, mapping the motions to new characters, and combining the motions into novel motions and or longer motion sequences. Processing the signalsThe values of an individual parameter of the captured motion (e.g., a joint angle) can be thought of, and dealt with, as a time-varying signal. Motion signal processing considers how frequency components capture various qualities of the motion [1]. The lower frequencies of the signal are considered to represent the base activity (e.g., walking) while the higher frequencies are the idiosyncratic movements of the specific individual (e.g., walking with a limp). the signal is successively convolved with expanded versions of a filter kernel (e.g., 1/16, 1/4, 3/8, 1/4, 1/16). The frequency bands are generated by differencing the convolutions. Gains of each band are adjusted by the user and can be summed to reconstruct a motion signal.滤波器得到不同频率的信号 Motion wraping warps the signal in order to satisfy user-supplied key-frame-like constraints[9]. For example, The original signal is $y(t)$ and the key frames are given as a set of $(y_i, t_i)$ pairs. In addition, there is a set of time warp constraints $(t_j, t_j’)$. warping procedure: creates a time warp mapping $t = g(t’)$ by interpolating through the time warp constraints. creates a motion curve warping, $\\theta(t) = f(\\theta, t)$. the function is created by scaling and/or translating the original curve to satisfy the key-frame constraints $\\theta’(t) = a(t)\\theta(t) + b(t)$ whether to scale or translate can be user-specified. Once the functions $a(t)$ and $b(t)$ for the key frames are set, the functions $a(t)$ and $b(t)$ can be interpolated throughout the time span of interest. Combined, these functions establish a function $\\theta(t’)$ that satisfies the key-frame and time warp constraints by warping the original function. 其实就像一个$\\sin t$ 函数，被用户指定更改了频率，振幅，相位等一系列参数一样, like $3\\sin (2t + \\frac{\\pi}{4})$ Retargeting the motionWhat happens if the synthetic character doesn’t match the dimensions (e.g., limb length) of the captured subject? One solution is to map the motion onto the mismatched synthetic character and then modify it to satisfy important constraints. Important constraints include such things as avoiding foot penetration of the floor, avoiding self-penetration, not letting feet slide on the floor when walking, and so forth. A new motion is constructed, as close to the original motion as possible, while enforcing the constraints. Finding the new motion is formulated as a space–time, nonlinear constrained optimization problem. [2] Gleicher M. Retargeting Motion to New Characters. In: Cohen M, editor. Proceedings of SIGGRAPH 98,Computer Graphics, Annual Conference Series. Orlando, Fla.: Addison-Wesley; July 1998. p. 33–42. ISBN 0-89791-999-8. Combining motionsThe ability to assemble motion segments into longer actions makes motion capture a much more useful tool for the animator. More natural transitions between segments are possible by blending the end of one segment into the beginning of the next one. Such transitions may look awkward unless the portions to be blended are similar. Similarity can be defined as the sum of the differences over all DOFs over the time interval of the blend. Automatic techniques to identify the best subsegments for transitions is the subject of current research [4]. 有时候融合两个motion clip的时候，比如walk和run，会发现walk是从左到右，run从右到左，如果直接blend会滑步，这是因为没有运动对齐，此时需要facing frame坐标系来解决这个问题—A special coordinate system that moves horizontally with the character with one axis pointing to the “facing direction” of the character，一般情况下这个坐标系是根关节RootJoint在地上的投影，这个坐标系的z轴指向的是Character速度的方向或者朝向(默认绝对坐标系是y-up) [4] Kovar L, Gleicher M. Flexible Automatic Motion Blending with Registration Curves. In: Breen D, Lin M, editors. Symposium on Computer Animation. San Diego, Ca.: Eurographics Association; 2002. p. 214–24. Recent research has produced techniques such as motion graphs that identify good transitions between segments in a motion database [3] [5] [7]. When the system is confronted with a request for a motion task, the sequence of motions that satisfies the task is constructed from the segments in the motion capture database. motion graph 就是现在游戏开发中常说的状态机_2002，可以实现用户可交互的动画，但这种方法有个缺点，就是对用户的响应比较慢，需要播放完当前的motion clip后才进行下一个节点的运动。对这种方法的改进就是motion matching_2016，启发自motion fields_2010方法，此方法不是按照motion clip的切换这种视角来进行交互动画，而是每一帧的姿态都是独立的，根据用户输入，环境信息，从motion database中找到下一帧最适合的pose，然后进行平滑的transition。很明显motion matching的方法对用户输入响应及时，但是对内存和计算的需求很大，且在此方法中，两个pose之间的距离函数一定要设计好，或者feature要extract的不错，才能找到比较不错的nearest neighbor。 [3] Kovar L, Gleicher M, Pighin F. Motion Graphs, In: Proceedings of SIGGRAPH 2002, Computer Graphics, Annual Conference Series. San Antonio, Tex.; July 23–26,; 2002. p. 473–82.[5] Lee J, Chai J, Reitsma P, Hodgins J, Pollard N. Interactive Control of Avatars Animated with Human Motion Data, In: Computer Graphics, Proceedings of SIGGRAPH 2002, Annual Conference Series. San Antonio, Tex.; July 23–26, 2002. p. 491–500.[7] Reitsma P, Pollard N. Evaluating Motion Graphs for Character Navigation. In: Symposium on Computer Animation. Grenoble, France. p. 89–98. Chapter summaryCurrent research involves extracting motion capture data from markerless video. This has the potential to free the subject from instrumentation constraints and make motion capture even more useful. 深度学习—单目视频三维角色重建..","link":"/Graphics/Animation/Animation-C6-Motion-Capture/"},{"title":"Calculus-C10-Infinite-Sequences-and-Series","text":"Keywords: Taylor’s Theorem, Absolute Convergence This is the Chapter10 ReadingNotes from book Thomas Calculus 14th. Taylor and Maclaurin Series（泰勒和麦克劳林级数）Series Representations(级数表示)Taylor and Maclaurin Series DEFINITIONSLet $ƒ$ be a function with derivatives of all orders throughout some interval containing a as an interior point. Then the Taylor series generated by $ƒ$ at $x = a$ is$$\\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!}(x-a)^k = f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n + \\cdots$$The Maclaurin series of $ƒ$ is the Taylor series generated by $ƒ$ at $x = 0$, or$$\\sum_{k=0}^{\\infty} \\frac{f^{(k)}(0)}{k!}(x-0)^k = f(0) + f’(0)(x-0) + \\frac{f’’(0)}{2!}(x-0)^2 + \\cdots + \\frac{f^{(n)}(0)}{n!}(x-0)^n + \\cdots$$ Taylor Polynomials DEFINITIONLet $ƒ$ be a function with derivatives of order $k$ for $k = 1, 2, . . . , N$ in some interval containing $a$ as an interior point. Then for any integer $n$ from $0$ through $N$, the Taylor polynomial of order $n$ generated by $ƒ$ at $x = a$ is the polynomial$$P_n(x) = f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^{(k)}(a)}{k!}(x-a)^k + \\cdots + \\frac{f^{(n)}(a)}{n!}(x-a)^n$$ Just as the linearization of $ƒ$ at $x = a$ provides the best linear approximation of $ƒ$ in the neighborhood of $a$, the higher-order Taylor polynomials provide the “best” polynomial approximations of their respective degrees. Convergence of Taylor Series THEOREM 23—Taylor’s TheoremIf $ƒ$ and its first $n$ derivatives $ƒ’, ƒ’’, \\cdots, ƒ^{(n)}$ are continuous on the closed interval between $a$ and $b$, and $ƒ^{(n)}$ is differentiable on the open interval between $a$ and $b$, then there exists a number $c$ between $a$ and $b$ such that$$f(b) = f(a) + f’(a)(b-a) + \\frac{f’’(a)}{2!}(b-a)^2 + \\cdots + \\frac{f^{n}(a)}{n!}(b-a)^n + \\frac{f^{n+1}(c)}{(n+1)!}(b-a)^{n+1}$$ Taylor’s Theorem is a generalization of the Mean Value Theorem. Taylor’s FormulaIf $ƒ$ has derivatives of all orders in an open interval $I$ containing $a$, then for each positive integer $n$ and for each $x$ in $I$,$$f(x) = f(a) + f’(a)(x-a) + \\frac{f’’(a)}{2!}(x-a)^2 + \\cdots + \\frac{f^n(a)}{n!}(x-a)^n + R_n(x)\\tag{1}$$where$$R_n(x) = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}\\tag{2}$$for some $c$ between $a$ and $x$ Equation (1) is called Taylor’s formula. The function $R_n(x)$ is called the remainder of order $n$ or the error term for the approximation of $ƒ$ by $P_n(x)$ over $I$. If $R_n(x) \\rightarrow 0$ as $n \\rightarrow \\infty$ for all $x \\in I$, we say that Taylor series generated by $f$ at $x = a$ converges to $f$ on $I$, and we write,$$f(x) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(a)}{k!}(x-a)^k$$ Estimating the Remainder THEOREM 24—The Remainder Estimation TheoremIf there is a positive constant $M$ such that $|ƒ^{(n+1)}(t)| \\leq M$ for all $t$ between $x$ and $a$, inclusive, then the remainder term $R_n(x)$ in Taylor’s Theorem satisfies the inequality$$|R_n(x)| \\leq M\\frac{|x-a|^{n+1}}{(n+1)!}$$If this inequality holds for every $n$ and the other conditions of Taylor’s Theorem are satisfied by $ƒ$, then the series converges to $ƒ(x)$.","link":"/Math/Calculus/Calculus-C10-Infinite-Sequences-and-Series/"},{"title":"Calculus-C12-Vectors-and-the-Geometry-of-Space","text":"Keywords: Vector, Dot Product, Lines and Planes in Space This is the Chapter12 ReadingNotes from book Thomas Calculus 14th.","link":"/Math/Calculus/Calculus-C12-Vectors-and-the-Geometry-of-Space/"},{"title":"Calculus-C11-Parametric-Equations-and-Polar-Coordinates","text":"Keywords: Cycloids, Brachistochrones（最速降线）, Parametric Curves This is the Chapter11 ReadingNotes from book Thomas Calculus 14th. Parametrizations of Plane CurvesParametric Equations DefinitionIf $x$ and $y$ are given as functions$$x = f(t), y = g(t)$$over an interval $I$ of $t$-values, then the set of points $(x, y) = (ƒ(t), g(t))$ defined by these equations is a parametric curve. The equations are parametric equations for the curve. Cycloids（摆线）For example: A wheel of radius a rolls along a horizontal straight line. Find parametric equations for the path traced by a point $P$ on the wheel’s circumference. The path is called a cycloid. Solution: We take the line to be the $x$-axis, mark a point $P$ on the wheel, start the wheel with $P$ at the origin, and roll the wheel to the right. As parameter, we use the angle $t$ through which the wheel turns, measured in radians. Figure 11.9 shows the wheel a short while later when its base lies at units from the origin. The wheel’s center $C$ lies at $(at, a)$ and the coordinates of $P$ are $$x = at + a \\cos \\theta, y = a + a \\sin \\theta$$To express $\\theta$ in terms of $t$, we observe that $t + \\theta = \\frac{3\\pi}{2}$ in the figure, so that$$\\theta = \\frac{3\\pi}{2} - t$$This makes$$\\cos\\theta = -\\sin t, \\sin \\theta = -\\cos t$$The equations we seek are$$x = at - a \\sin t, y = a(1-\\cos t)$$ Brachistochrones（最速降线） and Tautochrones（等时曲线）two properties: Among all smooth curves joining these points, the cycloid is the curve along which a frictionless bead, subject only to the force of gravity, will slide from $O$ to $B$ the fastest. This makes the cycloid a brachistochrone (“brah-kiss-toe-krone”), or shortest-time curve for these points. even if you start the bead partway down the curve toward $B$, it will still take the bead the same amount of time to reach $B$. This makes the cycloid a tautochrone (“taw-toe-krone”),or same-time curve for $O$ and $B$. Are there any other brachistochrones joining $O$ and $B$, or is the cycloid the only one?We can formulate this as a mathematical question in the following way. At the start, the kinetic energy of the bead is zero, since its velocity (speed) is zero. The work done by gravity in moving the bead from $(0, 0)$ to any other point $(x, y)$ in the plane is $mgy$, and this must equal the change in kinetic energy. That is, $$mgy = \\frac{1}{2}mv^2 - \\frac{1}{2}m(0)^2$$Thus, the speed of the bead when it reaches $(x, y)$ has to be $v = \\sqrt{2gy}$. That is,$$\\frac{ds}{dT} = \\sqrt{2gy}, s: arc-length, T: time$$or$$dT = \\frac{ds}{\\sqrt{2gy}} = \\frac{\\sqrt{1+(dy/dx)^2}dx}{\\sqrt{2gy}}$$The time $T_ƒ$ it takes the bead to slide along a particular path $y = ƒ(x)$ from $O$ to $B(a\\pi, 2a)$ is$$T_f = \\int_{x=0}^{x=a\\pi} \\sqrt{\\frac{1+(dy/dx)^2}{2gy}}dx$$What curves $y = ƒ(x)$, if any, minimize the value of this integral? More about Calculus of Variations &gt;&gt; Calculus with Parametric CurvesTangents and AreasA parametrized curve $x = ƒ(t)$ and $y = g(t)$ is differentiable at $t$ if $ƒ$ and $g$ are differentiable at $t$.At a point on a differentiable parametrized curve where $y$ is also a differentiable function of $x$ follows the Chain Rule:$$\\frac{dy}{dt} = \\frac{dy}{dx} \\cdot \\frac{dx}{dt}$$ Parametric Formula for $dy/dx$If all three derivatives exist and $dx/dt \\neq 0$, then$$\\frac{dy}{dt} = \\frac{dy/dt}{dx/dt}\\tag{1}$$If parametric equations define $y$ as a twice-differentiable function of $x$, then$$\\frac{d^2y}{d^2x} = \\frac{d}{dx}y’ = \\frac{d’y/dt}{dx/dt}$$ Length of a Parametrically Defined CurveWe assume the functions $ƒ$ and $g$ are continuously differentiable, the derivatives $ƒ’(t)$ and $g’(t)$ are not simultaneously zero, which prevents the curve $C$ from having any corners or cusps. Such a curve is called a smooth curve. $$\\begin{aligned}L_k &amp;= \\sqrt{(\\Delta x_k)^2 + (\\Delta y_k)^2}\\\\&amp;= \\sqrt{[f(t_k) - f(t_{k-1})]^2 + [g(t_k) - g(t_{k-1})]^2}\\end{aligned}$$By the Mean Value Theorem there are numbers $t_k^\\ast$ and $t_k^{\\ast\\ast}$ in $[t_{k-1}, t_{k}]$ such that $$\\Delta x_k = f(t_k) - f(t_{k-1}) = f’(t_k^\\ast)\\Delta t_k$$$$\\Delta y_k = g(t_k) - g(t_{k-1}) = g’(t_k^{\\ast\\ast})\\Delta t_k$$ thus, $$\\begin{aligned}\\sum_{k=1}^{n} L_k &amp;= \\sum_{k=1}^{n}\\sqrt{[f’(t_k^\\ast)\\Delta t_k]^2 + [g’(t_k^{\\ast\\ast})\\Delta t_k]^2}\\\\&amp;= \\sum_{k=1}^{n}\\sqrt{[f’(t_k^\\ast)]^2 + [g’(t_k^{\\ast\\ast})]^2} \\Delta t_k\\end{aligned}$$ Although this last sum on the right is not exactly a Riemann sum (because $ƒ’$ and $g’$ are evaluated at different points), it can be shown that its limit, as the norm of the partition tends to zero and the number of segments $n \\rightarrow \\infty$, is the definite integral $$\\lim_{||P|| \\rightarrow 0} \\sum_{k=1}^{n}\\sqrt{[f’(t_k^\\ast)]^2 + [g’(t_k^{\\ast\\ast})]^2} \\Delta t_k =\\int_a^b \\sqrt{[f’(t)]^2 + [g’(t)]^2}dt$$ DefinitionIf a curve $C$ is defined parametrically by $x = ƒ(t)$ and $y = g(t)$, $a \\leq t \\leq b$, where $ƒ’$ and $g’$ are continuous and not simultaneously zero on $[a,b]$, and $C$ is traversed exactly once as $t$ increases from $t = a$ to $t = b$, then the length of $C$ is the definite integral$$L = \\int_a^b \\sqrt{[f’(t)]^2 + [g’(t)]^2} dt\\tag{3}$$ Length of a Curve $y = ƒ(x)$$$x = t, y = f(t), a \\leq t \\leq b$$thus,$$\\frac{dx}{dt} = 1, \\frac{dy}{dt} = f’(t)$$thus,$$L = \\int_a^b \\sqrt{1 + [f’(x)]^2} dx$$ The Arc Length Differential$$ds = \\sqrt{dx^2 + dy^2}$$ To be added…","link":"/Math/Calculus/Calculus-C11-Parametric-Equations-and-Polar-Coordinates/"},{"title":"Calculus-C15-Multiple-Integrals","text":"Keywords: Double Integration This is the Chapter15 ReadingNotes from book Thomas Calculus 14th. Double and Iterated Integrals over RectanglesDouble Integrals over General RegionsArea by Double IntegrationDouble Integrals in Polar FormTriple Integrals in Rectangular CoordinatesApplicationsTriple Integrals in Cylindrical and Spherical CoordinatesSubstitutions in Multiple Integrals","link":"/Math/Calculus/Calculus-C15-Multiple-Integrals/"},{"title":"Calculus-C13-Vector-Valued-Fuctions-and-Motion-in-Space","text":"Keywords: Vector Functions, Curves, Tangents, Normal Vectors This is the Chapter13 ReadingNotes from book Thomas Calculus 14th. Curves in Space and Their TangentsWe use the calculus of these functions to describe the paths and motions of objects moving in a plane or in space, so their velocities and accelerations are given by vectors. A curve in space can also be represented in vector form. The vector$$\\vec{r(t)} = \\vec{OP} = f(t)\\vec{i} + g(t)\\vec{j} + h(t)\\vec{k}\\tag{2}$$ Limits and Continuity DEFINITIONLet $\\vec{r(t)} = f(t)\\vec{i} + g(t)\\vec{j} + h(t)\\vec{k}$ be a vector function with domain $D$, and let $\\vec{L}$ be a vector. We say that $\\vec{r}$ has limit $\\vec{L}$ as $t$ approaches $t_0$ and write$$\\lim_{t\\rightarrow t_0} \\vec{r(t)} = \\vec{L}$$if, for every number $\\epsilon &gt; 0$, there exists a corresponding number $\\delta &gt; 0$ such that for all $t \\in D$$$|\\vec{r(t)} - \\vec{L}| &lt; \\epsilon, whenever, 0 &lt; |t - t_0| &lt; \\delta$$ DEFINITIONA vector function $\\vec{r(t)}$ is continuous at a point $t = t_0$ in its domain if $\\lim_{t \\rightarrow t_0} \\vec{r(t)} = \\vec{r(t_0)}$. The function is continuous if it is continuous at every point in its domain. Derivatives and Motion DEFINITIONThe vector function $\\vec{r(t)} = f(t)\\vec{i} + g(t)\\vec{j} + h(t)\\vec{k}$ has a derivative (is differentiable) at $t$ if $ƒ, g, h$ have derivatives at $t$. The derivative is the vector function$$\\begin{aligned}\\vec{r’(t)} &amp;= \\frac{d\\vec{r}}{dt} \\\\&amp;= \\lim_{\\Delta t \\rightarrow 0} \\frac{\\vec{r(t + \\Delta t)} - \\vec{r(t)}}{\\Delta t} \\\\&amp;= \\frac{df}{dt}\\vec{i} + \\frac{dg}{dt}\\vec{j} + \\frac{dh}{dt}\\vec{k}\\end{aligned}$$ DEFINITIONIf $\\vec{r}$ is the position vector of a particle moving along a smooth curve in space, then$$\\vec{v(t)} = \\frac{d\\vec{r}}{dt}$$is the particle’s velocity vector, tangent to the curve. At any time $t$, the direction of $\\vec{v}$ is the direction of motion, the magnitude of $\\vec{v}$ is the particle’s speed, and the derivative $\\vec{a} = d\\vec{v}/dt$, when it exists, is the particle’s acceleration vector. In summary, Velocity is the derivative of position: $\\vec{v} = \\frac{d\\vec{r}}{dt}$ Speed is the magnitude of velocity: $Speed = |\\vec{v}|$ Acceleration is the derivative of velocity: $\\vec{a} = \\frac{d\\vec{v}}{dt} = \\frac{d^2\\vec{r}}{dt^2}$ The unit vector $\\vec{v} / |\\vec{v}|$ is the direction of motion at time $t$. Differentiation Rules Vector Functions of Constant Length the position vector has a constant length equal to the radius of the sphere. The velocity vector $dr / dt$, tangent to the path of motion, is tangent to the sphere and hence perpendicular to $r$. If $r$ is a differentiable vector function of $t$ and the length of $r(t)$ is constant, then$$r \\cdot \\frac{dr}{dt} = 0\\tag{4}$$ Integrals of Vector Functions; Projectile MotionIntegrals of Vector Functions DEFINITIONThe indefinite integral of $\\vec{r}$ with respect to $t$ is the set of all antiderivatives of $\\vec{r}$, denoted by $\\int \\vec{r(t)} dt$. If $\\vec{R}$ is any antiderivative of $\\vec{r}$, then$$\\int \\vec{r(t)} dt = \\vec{R(t)} + C$$ DEFINITIONIf the components of $r(t) = ƒ(t)i + g(t)j + h(t)k$ are integrable over $[a,b]$, then so is $r$, and the definite integral of $r$ from $a$ to $b$ is$$\\int_a^b r(t) dt = \\left( \\int_a^b f(t) dt \\right) i + \\left( \\int_a^b g(t) dt \\right) j + \\left( \\int_a^b h(t) dt \\right) k$$ The Vector and Parametric Equations for Ideal Projectile Motion（抛射运动） $$\\vec{v_0} = (|\\vec{v_0}| \\cos \\alpha) \\vec{i} + (|\\vec{v_0}| \\sin \\alpha) \\vec{j}$$ use the simpler notation $v_0$ for the initial speed $\\vec{v_0}$,$$\\vec{v_0} = (v_0 \\cos \\alpha) \\vec{i} + (v_0 \\sin \\alpha) \\vec{j}\\tag{3}$$The projectile’s initial position is$$\\vec{r_0} = 0\\vec{i} + 0\\vec{j} + 0\\vec{k}\\tag{4}$$ Newton’s second law of motion says that the force acting on the projectile is equal to the projectile’s mass $m$ times its acceleration, or $m(d^2\\vec{r} / dt^2)$ if $\\vec{r}$ is the projectile’s position vector and $t$ is time. $$m\\frac{d^2\\vec{r}}{dt^2} = -mg\\vec{j}$$ Thus, Differential equation:$$\\frac{d^2\\vec{r}}{dt^2} = -g\\vec{j}$$Initial conditions:$$\\vec{r} = \\vec{r_0}, \\frac{d\\vec{r}}{dt} = \\vec{v_0}, when \\space t = 0$$The first integration gives:$$\\frac{d\\vec{r}}{dt} = -(gt)\\vec{j} + \\vec{v_0}$$A second integration gives:$$\\vec{r} = -\\frac{1}{2}gt^2\\vec{j} + \\vec{v_0}t + \\vec{r_0}$$Substituting the values of $\\vec{v_0}$ and $\\vec{r_0}$ from Equations (3) and (4) gives$$\\vec{r} = (v_0\\cos \\alpha) t \\vec{i} + \\left( (v_0\\sin \\alpha)t - \\frac{1}{2}gt^2\\right)\\vec{j}$$ Projectile Motion with Wind GustsArc Length in SpaceArc Length Along a Space Curve DEFINITIONThe length of a smooth curve $r(t) = x(t)i + y(t)j + z(t)k$, $a \\leq t \\leq b$, that is traced exactly once as $t$ increases from $t = a$ to $t = b$, is$$\\begin{aligned}L &amp;= \\int_a^b \\sqrt{(\\frac{dx}{dt})^2 + (\\frac{dy}{dt})^2 + (\\frac{dz}{dt})^2} dt \\\\&amp;= \\int_a^b |\\vec{v}| dt\\end{aligned}\\tag{1}$$ If we choose a base point $P(t_0)$ on a smooth curve $C$ parametrized by $t$, each value of $t$ determines a point $P(t) = (x(t), y(t), z(t))$ on $C$ and a “directed distance”$$\\begin{aligned}s(t) &amp;= \\int_{t_0}^t \\sqrt{[x’(\\tau)]^2 + [y’(\\tau)]^2 + [z’(\\tau)]^2} d\\tau\\\\&amp;= \\int_{t_0}^{t} |\\vec{v(\\tau)}| d\\tau\\end{aligned}\\tag{3}$$ Recall the similar definition form in Fundamental Theorem, Part 1 &gt;&gt;? We call $s$ an arc length parameter for the curve.We use the Greek letter $\\tau$ (“tau”) as the variable of integration in Equation (3) because the letter $t$ is already in use as the upper limit. Speed on a Smooth CurveSince the derivatives beneath the radical in Equation (3) are continuous (the curve is smooth), the Fundamental Theorem of Calculus tells us that $s$ is a differentiable function of $t$ with derivative$$\\frac{ds}{dt} = |\\vec{v(t)}|\\tag{4}$$ Notice that $ds/dt &gt; 0$ since, by definition, $|\\vec{v}|$ is never zero for a smooth curve. We see once again that $s$ is an increasing function of $t$. Unit Tangent Vector $$\\vec{T} = \\frac{\\vec{v}}{|\\vec{v}|}$$ The velocity vector is the change in the position vector $\\vec{r}$ with respect to time $t$, but how does the position vector change with respect to arc length? More precisely, what is the derivative $d\\vec{r}/ds$? Since $d\\vec{s}/dt &gt; 0$ for the curves we are considering, $s$ is one-to-one and has an inverse that gives $t$ as a differentiable function of $s$. The derivative of the inverse is$$\\frac{dt}{ds} = \\frac{1}{ds/dt} = \\frac{1}{|\\vec{v}|}$$This makes $\\vec{r}$ a differentiable function of $s$ whose derivative can be calculated with the Chain Rule to be$$\\frac{d\\vec{r}}{ds} = \\frac{d\\vec{r}}{dt} \\frac{dt}{ds} = \\vec{v}\\frac{1}{|\\vec{v}|} = \\frac{\\vec{v}}{|\\vec{v}|} = T$$ Curvature and Normal Vectors of a CurveCurvature of a Plane Curve The rate at which $T$ turns per unit of length along the curve is called the curvature (Figure 13.17). DEFINITIONIf $T$ is the unit vector of a smooth curve, the curvature function of the curve is$$\\kappa = |\\frac{dT}{ds}|$$ If $|dT/ds|$ is large, $T$ turns sharply as the particle passes through $P$, and the curvatureat $P$ is large. According to Chain Rule: $$\\kappa = |\\frac{dT}{ds}| = |\\frac{dT}{ds}\\frac{dt}{ds}| = \\frac{1}{|ds/dt|}|\\frac{dT}{dt}| = \\frac{1}{|\\vec{v}|}|\\frac{dT}{dt}|$$ Formula for Calculating CurvatureIf $\\vec{r(t)}$ is a smooth curve, then the curvature is the scalar function$$\\kappa = \\frac{1}{|\\vec{v}|}|\\frac{dT}{dt}|\\tag{}$$where T = v&gt; 0 v 0 is the unit tangent vector. Circle of Curvature for Plane Curves Curvature and Normal Vectors for Space Curves Tangential and Normal Components of AccelerationThe TNB FrameTangential and Normal Components of AccelerationTorsionFormulas for Computing Curvature and TorsionVelocity and Acceleration in Polar CoordinatesMotion in Polar and Cylindrical CoordinatesKepler’s First Law (Ellipse Law)Kepler’s Second Law (Equal Area Law)Kepler’s Third Law (Time–Distance Law)","link":"/Math/Calculus/Calculus-C13-Vector-Valued-Fuctions-and-Motion-in-Space/"},{"title":"Calculus-C2-Limits-and-Continuty","text":"Keywords: Continuity, The Intermediate Value Theorem This is the Chapter2 ReadingNotes from book Thomas Calculus 14th. ContinuityContinuity at a Point DEFINITIONSLet $c$ be a real number that is either an interior point or an endpoint of an interval in the domain of $ƒ$.The function $ƒ$ is continuous at c $if$$$\\lim_{x\\rightarrow c} f(x) = f(c)$$The function $ƒ$ is right-continuous at $c$ (or continuous from the right) if$$\\lim_{x\\rightarrow c^+} f(x) = f(c)$$The function $ƒ$ is** left-continuous** at c (or continuous from the left) if$$\\lim_{x\\rightarrow c^-} f(x) = f(c)$$ Continuous FunctionsInverse Functions and ContinuityContinuity of Compositions of FunctionsIntermediate Value Theorem for Continuous Functions THEOREM 11—The Intermediate Value Theorem for Continuous FunctionsIf $ƒ$ is a continuous function on a closed interval $[a,b]$, and if $y_0$ is any value between $ƒ(a)$ and $ƒ(b)$, then $y_0 = ƒ(c)$ for some $c$ in $[a,b]$.","link":"/Math/Calculus/Calculus-C2-Limits-and-Continuty/"},{"title":"Calculus-C17-Second-Order-Differential-Equations","text":"Keywords: Second-Order Linear Equations, Euler Equations, Nonhomogeneous Linear Equations This is the Chapter17 ReadingNotes from book Thomas Calculus 14th. First-Order Differential Equations &gt;&gt; Second-Order Linear EquationsAn equation of the form$$P(x)y’’(x) + Q(x)y’(x) + R(x)y(x) = G(x)\\tag{1}$$which is linear in $y$ and its derivatives, is called a second-order linear differential equation. If $G(x)$ is identically zero on $I$, the equation is said to be homogeneous; otherwise it is called nonhomogeneous. Thus, the form of a second-order linear homogeneous differential equation is$$P(x)y’’ + Q(x)y’ + R(x)y = 0\\tag{2}$$We assume that the functions $P, Q, R$, and $G$ are continuous throughout some open interval $I$. We also assume that $P(x)$ is never zero for any $x \\in I$. THEOREM 1—The Superposition PrincipleIf $y_1(x)$ and $y_2(x)$ are two solutions to the linear homogeneous equation (2), then for any constants $c_1$ and $c_2$, the function$$y(x) = c_1y_1(x) + c_2y_2(x)$$is also a solution to Equation (2). THEOREM 2If $P, Q$, and $R$ are continuous over the open interval $I$ and $P(x)$ is never zero on $I$, then the linear homogeneous equation (2) has two linearly independent solutions $y_1$ and $y_2$ on $I$. Moreover, if $y_1$ and $y_2$ are any two linearly independent solutions of Equation (2), then the general solution is given by$$y(x) = c_1y_1(x) + c_2y_2(x)$$where $c_1$ and $c_2$ are arbitrary constants. Constant-Coefficient Homogeneous EquationsSuppose we wish to solve the second-order homogeneous differential equation$$ay’’ + by’ + c = 0\\tag{3}$$where $a, b$, and $c$ are constants.If we substitute $y = e^{rx}$ into Equation (3), we obtain$$ar^2e^{rx} + bre^{rx} + ce^{rx} = x_0$$ $y = e^{rx}$ is a solution to Equation (3) if and only if $r$ is a solution to the auxiliary equation (or characteristic equation)$$ar^2 + br + c = 0\\tag{4}$$ the roots are$$r_1 = \\frac{-b + \\sqrt{b^2-4ac}}{2a}and r_2 = \\frac{-b - \\sqrt{b^2-4ac}}{2a}$$ Case1: $b^2-4ac &gt; 0$THEOREM 3If $r_1$ and $r_2$ are two real and unequal roots to the auxiliary equation(辅助方程) $ar^2 + br + c = 0$, then$$y = c_1 e^{r_1 x} + c_2 e^{r_2 x}$$is the general solution to $ay’’ + by’ + cy = 0$. Case2: $b^2-4ac = 0$THEOREM 4If $r$ is the only(repeated) root to the auxiliary equation(辅助方程) $ar^2 + br + c = 0$, then$$y = c_1 e^{r x} + c_2 x e^{rx}$$is the general solution to $ay’’ + by’ + cy = 0$. Case3: $b^2-4ac &lt; 0$THEOREM 5If $r_1 = \\alpha + i\\beta$ and $r_2 = \\alpha - i\\beta$ are two complex roots to the auxiliary equation $ar^2 + br + c = 0$, then$$y = e^{\\alpha x}(c_1\\cos\\beta x + c_2 \\sin \\beta x)$$is the general solution to $ay’’ + by’ + cy = 0$.$\\alpha = -b/2a, and ,\\beta = \\sqrt{4ac-b^2}/2a$ Initial Value and Boundary Value Problems THEOREM 6If $P, Q, R$, and $G$ are continuous throughout an open interval $I$, then there exists one and only one function $y(x)$ satisfying both the differential equation$$P(x)y’’(x) + Q(x)y’(x) + R(x)y(x) = G(x)$$on the interval $I$, and the initial conditions$$y(x_0) = y_0, and, y’(x_0) = y_1$$at the specified point $x \\in I$. Another approach to get a unique solution is boundary values,$$y(x_1) = y_1 ,and, y(x_2) = x_2$$The differential equation together with specified boundary values is called a boundary value problem. Nonhomogeneous Linear EquationsForm of the General SolutionSuppose we wish to solve the nonhomogeneous equation$$ay’’+by’+cy = G(x)\\tag{1}$$where $a, b$, and $c$ are constants and $G$ is continuous over some open interval $I$. Let $y_c = c_1y_1 + c_2y_2$ be the general solution to the associated complementary equation（互补方程） $$ay’’+by’+c = 0\\tag{2}$$ Now suppose we could somehow come up with a particular function $y_p$ that solves the nonhomogeneous equation (1). $$y = y_c + y_p\\tag{3}$$ THEOREM 7The general solution $y = y(x)$ to the nonhomogeneous differential equation (1) has the form$$y = y_c + y_p$$where the complementary solution $y_c$ is the general solution to the associated homogeneous equation (2) and $y_p$ is any particular solution to the nonhomogeneous equation (1). Recall what we have learned in Linear Algebra. The relationship between $Ax = 0$ and $Ax = b$? Homegenous and Nonhomegenous in Algebra &gt;&gt; The Method of Undetermined Coefficients(待定系数法) For example: Solve the nonhomogeneous equation $y’’-2y’-3y = 1-x^2$. Solution: auxiliary equation for the complementary equation $y’’-2y’-3y = 0$ is$$r^2 - 2r -3 = 0 \\rightarrow (r+1)(r-3) = 0$$the complementary solution$$y_c = c_1 e^{-x} + c_2 e^{3x}$$Now $G(x) = 1 - x^2$ is a polynomial of degree $2$. It would be reasonable to assume that a particular solution to the given nonhomogeneous equation is also a polynomial of degree $2$. $$y_p = Ax^2 + Bx + C$$substitute $y_p$ to the original equation, we get$$-3Ax^2 + (-4A-3B)x + (2A-2B-3C) = 1-x^2$$thus, we get$$\\begin{cases}-3A = -1\\\\-4A - 3B = 0\\\\2A-3B-3C = 1\\end{cases}$$so that,$$y = y_c + y_p = c_1 e^{-x} + c_2 e^{3x} + \\frac{1}{3}x^2 - \\frac{4}{9}x + \\frac{5}{27}$$ The Method of Variation of Parameters(参数变分法)Assume $c_1$ and $c_2$ are two functions $v_1$ and $v_2$.Then, First, the complementary solution $y = v_1y_1 + v_2y_2$ should satisfy Equation(1).Second,$$v_1’y_1 + v_2’y_2 = 0\\tag{4}$$ Thus, we have$$v_1(ay_1’’ + by_1’ + cy_1) + v_2(ay_2’’ + by_2’ + cy_2) + a(v_1’y_1’ + v_2’y_2’) = G(x)$$the first two terms are zero. so$$a(v_1’ y_1’ + v_2’y_2’) = G(x)\\tag{5}$$ Variation of Parameters ProcedureTo use the method of variation of parameters to find a particular solution(特解) to the nonhomogeneous equation$$ay’’ + by’ + cy = G(x)$$we can work directly with Equations (4) and (5). It is not necessary to rederive them. The steps are as follows. Solve the associated homogeneous equation$$ay’’+by’+c = 0$$to find the functions $y_1$ and $y_2$. Solve the equations$$v_1’y_1 + v_2’y_2 = 0,\\\\v_1’y_1’+v_2’y_2’ = \\frac{G(x)}{a}$$simultaneously for the derivative functions $v’_1$ and $v’_2$. (method of determinants (also known as Cramer’s Rule)) Integrate $v’_1$ and $v’_2$ to find the functions $v_1 = v_1(x)$ and $v_2 = v_2(x)$. Write down the particular solution to nonhomogeneous equation (1) as$$y_p = v_1y_1 + v_2y_2$$ For example: Find the general solution to the equation$$y’’+y = \\tan x$$ Solution: The solution of the homogeneous equation$$y’’ + y = 0$$ is given by $$y_c = c_1 \\cos x + c_2 \\sin x$$ Since $y_1(x) = \\cos x$ and $y_2(x) = \\sin x$, the conditions to be satisfied in Equations (4) and (5) are $$\\begin{cases} v_1’ \\cos x + v_2’ \\sin x = 0\\\\ -v_1’ \\sin x + v_2’ \\cos x = \\tan x, a = 1\\end{cases}$$Solution of this system gives$$v_1’ = \\frac{\\left | \\begin{matrix} 0 &amp; \\sin x\\\\ \\tan x &amp; \\cos x\\end{matrix}\\right |}{\\left | \\begin{matrix} \\cos x &amp; \\sin x\\\\ -\\sin x &amp; \\cos x\\end{matrix}\\right |}=\\frac{-\\sin^2 x}{\\cos x}$$$$v_2’ = \\frac{\\left | \\begin{matrix} \\cos x &amp; 0\\\\ -\\sin x &amp; \\tan x\\end{matrix}\\right |}{\\left | \\begin{matrix} \\cos x &amp; \\sin x\\\\ -\\sin x &amp; \\cos x\\end{matrix}\\right |}=\\sin x$$After integrating $v’_1$ and $v’_2$, we have$$v_1(x) = \\int \\frac{-\\sin^2 x}{\\cos x} dx=-\\ln |\\sec x + \\tan x| + \\sin x$$and$$v_2(x) = \\int \\sin x dx = -\\cos x$$Thus,$$y_p = (-\\cos x)\\ln|\\sec x + \\tan x|$$thus,$$y = c_1 \\cos x + c_2 \\sin x -\\cos x\\ln|\\sec x + \\tan x|$$ Cramer’s Rule &gt;&gt; ApplicationsVibrations(振动) See Figure 17.2, according Hooke’s Law, equilibrium requires that$$ks = mg\\tag{1}$$Suppose that the object is pulled down an additional amount $y_0$ beyond the equilibrium position and then released. Then the forces acting on the object are:the propulsion force due to gravity（由重力引起的推进力）,$$F_p = mg$$the restoring force of the spring’s tension(弹簧张力的恢复力),$$F_s = k(s+y)$$a frictional force assumed proportional to velocity(和速度成正比的摩擦力),$$F_r = \\delta \\frac{dy}{dt}$$ By Newton’s second law $F = ma$, $$m \\frac{d^2y}{d^2t} = mg - ks - ky - \\delta \\frac{dy}{dt}$$By Equation (1), $mg - ks = 0$, then $$m \\frac{d^2y}{d^2t} + \\delta \\frac{dy}{dt} + ky = 0\\tag{2}$$subject to the initial conditions $y(0) = y_0$ and $y’(0) = 0$. The motion is: motion predicted by Equation (2) will be oscillatory about the equilibrium position $y = 0$ and eventually damp to zero because of the retarding frictional force. Simple Harmonic Motion(简谐运动)Suppose no frictional force. $\\delta = 0$ and there is no damping. We substitude $\\omega = \\sqrt{k/m}$ to simplify our calculations. Then equation(2) becomes,$$y’’ + \\omega ^2 y = 0, with, y(0) = y_0, y’(0) = 0$$ The general solution is$$y = c_1 \\cos \\omega t + c_2 \\sin \\omega t\\tag{3}$$ The particular solution is$$y = y_0 \\cos \\omega t\\tag{4}$$ Equation (4) represents simple harmonic motion of amplitude $y_0$ and period $T = 2\\pi / \\omega$. The general solution given by Equation (3) can be combined into a single term by using the trigonometric identity（使用三角恒等式）$$\\sin(\\omega t + \\phi) = \\cos \\omega t \\sin \\phi + \\sin \\omega t \\cos \\phi$$so that$$c_1 = C\\sin \\phi, c_2 = C\\cos \\phi$$where$$C = \\sqrt {c_1^2 + c_2^2}, \\phi = \\tan^{-1} \\frac{c_1}{c_2}$$Then the general solution in Equation (3) can be written in the alternative form$$y = C \\sin (\\omega t + \\phi)\\tag{5}$$The angle $\\omega t + \\phi$ is called the phase angle(相位角), and $\\phi$ may be interpreted as its initial value.A graph of the simple harmonic motion represented by Equation (5) is given in Figure 17.5. Damped Motion(阻尼运动)Assume now that there is friction in the spring system, so $\\delta \\neq 0$. $$\\omega = \\sqrt{k/m}, 2b = \\delta / m$$then the differential equation (2) is$$y’’ + 2by’ + \\omega^2 y = 0\\tag{6}$$ Obviously the auxiliary equation of (6) is$$r^ 2 + 2br + \\omega^2 = 0$$the root $r = -b \\pm \\sqrt{b^2-\\omega^2}$ Case1. $b = \\omega$ The general solution is$$y = (c_1 + c_2t)e^{-\\omega t}$$This situation of motion is called critical damping and is not oscillatory(临界阻尼，不振荡). Case2. $b &gt; \\omega$ The general solution is$$y = c_1e^{(-b+\\sqrt{b^2-\\omega^2})t} + c_2e^{(-b-\\sqrt{b^2-\\omega^2})t}$$Here again the motion is not oscillatory and both $r_1$ and $r_2$ are negative. Thus $y$ approaches zero as time goes on. This motion is referred to as overdamping(过阻尼，不振荡). Case3. $b &lt; \\omega$ The general solution is$$y = e^{-bt}(c_1\\cos \\sqrt{\\omega^2-b^2}t + c_2\\sin \\sqrt{\\omega^2-b^2}t)$$ This situation, called underdamping(欠阻尼), represents damped oscillatory motion. It is analogous to simple harmonic motion of period $T = 2\\pi/ \\sqrt{\\omega^2 - b^2}$ except that the amplitude is not constant but damped by the factor $e^{-bt}$. Therefore, the motion tends to zero as $t$ increases, so the vibrations tend to die out as time goes on. Notice that the period $T = 2\\pi/ \\sqrt{\\omega^2 - b^2}$ is larger than the period $T_0 = 2\\pi/ \\omega$ in the friction-free system. Moreover, the larger the value of $b = \\delta / 2m$ in the exponential damping factor, the more quickly the vibrations tend to become unnoticeable. A curve illustrating underdamped motion is shown in Figure 17.6c. Electric Circuits(电路)to be added.. Summary Euler EquationsEuler equations have this form$$P(x) = ax^2, Q(x) = bx, R(x) = c$$where $a, b$, and $c$ are constants. The General Solution of Euler EquationsConsider the Euler equation$$ax^2 y’’ + bx y’ + cy = 0, x &gt; 0\\tag{1}$$ The essence is : change it into constant coefficients equation. Let $z = \\ln x$ and $y(x) = Y(z)$. $$\\begin{aligned} y’(x) &amp;= \\frac{d}{dx}Y(z)\\\\ &amp;= \\frac{d}{dz}Y(z)\\frac{dz}{dx}\\\\ &amp;= Y’(z)\\frac{1}{x}\\end{aligned}$$ $$\\begin{aligned} y’’(x) &amp;= \\frac{d}{dx}y’(x)\\\\ &amp;= \\frac{d}{dx}Y’(z)\\frac{1}{x}\\\\ &amp;=-\\frac{1}{x^2}Y’(z) + \\frac{1}{x}Y’’(z)\\frac{dz}{dx}\\\\ &amp;= -\\frac{1}{x^2}Y’(z) + \\frac{1}{x^2}Y’’(z)\\end{aligned}$$thus,$$ax^2 y’’ + bx y’ + cy = 0 = aY’’(z) + (b-a)Y’(z) + cY(z)$$ Now it’s changed to solve this equation:$$aY’’(z) + (b-a)Y’(z) + cY(z) = 0\\tag{2}$$we find the roots to the associated auxiliary equation$$ar^2 + (b-a)r + c = 0\\tag{3}$$ For example:Find the particular solution to $x^2y’’ - 3xy’ + 68y = 0$ that satisfies the initial conditions $y(1) = 0$ and $y’(1) = 1$. Solution: $$Y(z) = e^{2z}(c_1\\cos8z + c_2 \\sin 8z)$$$$y(x) = e^{2\\ln x}(c_1\\cos8 \\ln x + c_2 \\sin 8 \\ln x)$$ from the two conditions, we get$$c_1 = 0, c_2 = \\frac{1}{8}$$thus,$$y(x) = \\frac{1}{8}x^2 \\sin(8\\ln x), -\\frac{x^2}{8} \\leq y \\leq \\frac{x^2}{8}$$ Power-Series SolutionsMethod of SolutionThe power-series method for solving a second-order homogeneous differential equation consists of finding the coefficients of a power series(幂级数) $$y(x) = \\sum_{n=0}^{\\infty}c_nx^n = c_0 + c_1 x + c_2 x^2 + \\cdots\\tag{1}$$ which solves the equation. For example: Solve the equation $y’’ + y = 0$ by the power-series method. Solution: We assume the series solution takes the form of$$y = \\sum_{n=0}^{\\infty} c_n x^n$$and calculate the derivatives$$y’ = \\sum_{n=1}^{\\infty} nc_nx^{n-1}$$and$$y’’ = \\sum_{n=2}^{\\infty}n(n-1)c_nx^{n-2}$$Substitution of these forms into the second-order equation gives us$$\\sum_{n=2}^{\\infty}n(n-1)c_nx^{n-2} + \\sum_{n=0}^{\\infty}c_nx^n = 0$$Next, we equate the coefficients of each power of x to zero as summarized in the following table. Even indices: Here $n = 2k$, so the power is $x^{2k-2}$. we have $$2k(2k-1)c_{2k} + c_{2k-2} = 0$$or$$c_{2k} = -\\frac{1}{2k(2k-1)}c_{2k-2}$$From this recursive relation we find$$c_{2k} = [-\\frac{1}{2k(2k-1)}] \\cdots [-\\frac{1}{2}] c_0=\\frac{(-1)^k}{(2k)!}c_0$$ Odd indices: $$(2k+1)(2k)c_{2k+1} + c_{2k-1} = 0$$Thus,$$c_{2k+1} = \\frac{(-1)^k}{(2k+1)!}c_1$$ Thus, $$\\begin{aligned} y &amp;= \\sum_{n=0}^{\\infty} c_n x^n\\\\ &amp;= \\sum_{k=0}^{\\infty} c_{2k}x^{2k} + \\sum_{k=0}^{\\infty}c_{2k+1}x^{2k+1}\\\\ &amp;= c_0 \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} + c_1 \\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k+1)!}x^{2k+1}\\\\ &amp;=c_0 \\cos x + c_1 \\sin x\\end{aligned}$$","link":"/Math/Calculus/Calculus-C17-Second-Order-Differential-Equations/"},{"title":"Calculus-C14-Partial-Derivatives","text":"Keywords: Partial Derivatives, Directional Derivatives and Gradient Vectors, Lagrange Multipliers This is the Chapter14 ReadingNotes from book Thomas Calculus 14th. Functions of Several Variables DEFINITIONSSuppose $D$ is a set of $n$-tuples of real numbers $(x_1, x_2, \\cdots , x_n)$. A real-valued function $ƒ$ on $D$ is a rule that assigns a unique (single) real number$$w = f(x_1, x_2, \\cdots , x_n)$$to each element in $D$. The set $D$ is the function’s domain. The set of $w$-values taken on by $ƒ$ is the function’s range. The symbol $w$ is the dependent variable of $ƒ$, and $ƒ$ is said to be a function of the $n$ independent variables $x_1$ to $x_n$. We also call the $x_j$’s the function’s input variables and call $w$ the function’s output variable. Domains and RangesThe domain of a function is assumed to be the largest set for which the defining rule generates real numbers, unless the domain is otherwise specified explicitly. The range consists of the set of output values for the dependent variable. Functions of Two Variables DEFINITIONSA point $(x_0, y_0)$ in a region (set) $R$ in the $xy$-plane is an interior point of $R$ if it is the center of a disk of positive radius that lies entirely in R (Figure 14.2). A point $(x_0, y_0)$ is a boundary point of $R$ if every disk centered at $(x_0, y_0)$ contains points that lie outside of $R$ as well as points that lie in $R$. (The boundary point itself need not belong to $R$.)The interior points of a region, as a set, make up the interior of the region. The region’s boundary points make up its boundary. A region is open if it consists entirely of interior points. A region is closed if it contains all its boundary points (Figure 14.3). DEFINITIONSA region in the plane is bounded if it lies inside a disk of finite radius. A region is unbounded if it is not bounded. Graphs, Level Curves, and Contours of Functions of Two Variables DEFINITIONSThe set of points in the plane where a function $ƒ(x, y)$ has a constant value $ƒ(x, y) = c$ is called a level curve（等高线） of $ƒ$. The set of all points $(x, y, ƒ(x, y))$ in space, for $(x, y)$ in the domain of $ƒ$, is called the graph of $ƒ$. The graph of $ƒ$ is also called the surface $z = ƒ(x, y)$. Functions of Three Variables DEFINITIONThe set of points $(x, y, z)$ in space where a function of three independent variables has a constant value $ƒ(x, y, z) = c$ is called a level surface of $ƒ$. Since the graphs of functions of three variables consist of points $(x, y, z, ƒ(x, y, z))$ lying in a four-dimensional space, we cannot sketch them effectively in our three-dimensional frame of reference. We can see how the function behaves, however, by looking at its three-dimensional level surfaces. DEFINITIONSA point $(x_0, y_0, z_0)$ in a region $R$ in space is an interior point of $R$ if it is the center of a solid ball that lies entirely in $R$ (Figure 14.9a). A point $(x_0, y_0, z_0)$ is a boundary point of $R$ if every solid ball centered at $(x_0, y_0, z_0)$ contains points that lie outside of $R$ as well as points that lie inside R (Figure 14.9b). The interior of $R$ is the set of interior points of $R$. The boundary of $R$ is the set of boundary points of $R$.A region is open if it consists entirely of interior points. A region is closed if it contains its entire boundary. Computer GraphingFigure 14.11 shows computer-generated graphs of a number of functions of two variables together with their level curves. Limits and Continuity in Higher DimensionsLimits for Functions of Two Variables DefinitionWe say that a function ƒ(x, y) approaches the limit $L$ as $(x, y)$ approaches $(x_0 , y_0)$, and write$$\\lim_{(x,y)\\rightarrow (x_0,y_0)} f(x,y) = L$$if, for every number $\\epsilon &gt; 0$, there exists a corresponding number $\\delta &gt; 0$ such that for all $(x, y)$ in the domain of $ƒ$,$$|f(x,y) - L| &lt; \\epsilon, whenever, 0 &lt; \\sqrt{(x-x_0)^2 + (y-y_0)^2} &lt; \\delta$$ Continuity DefinitionA function ƒ(x, y) is continuous at the point $(x_0 , y_0)$ if ƒ is defined at $(x_0 , y_0)$, $\\lim_{(x, y)\\rightarrow (x_0, y_0)} ƒ(x, y)$ exists, $\\lim_{(x, y)\\rightarrow (x_0, y_0)} ƒ(x, y) = f(x_0,y_0)$.A function is continuous if it is continuous at every point of its domain. Continuity of CompositionsIf $ƒ$ is continuous at $(x_0 , y_0)$ and $g$ is a single-variable function continuous at $ƒ(x_0 , y_0)$, then the composition $h = g \\circ f$ defined by $h(x, y) = g(ƒ(x, y))$ is continuous at $(x_0, y_0)$. Partial DerivativesPartial Derivatives of a Function of Two Variables DEFINITIONThe partial derivative of $ƒ(x, y)$ with respect to $x$ at the point $(x_0 , y_0)$ is$$\\left. \\frac{\\partial f}{\\partial x} \\right|_{x_0, y_0} = \\lim_{h\\rightarrow 0} \\frac{f(x_0+h,y_0) - f(x_0,y_0)}{h}$$The partial derivative of $ƒ(x, y)$ with respect to $y$ at the point $(x_0 , y_0)$ is$$\\left. \\frac{\\partial f}{\\partial y} \\right|_{x_0, y_0} = \\lim_{h\\rightarrow 0} \\frac{f(x_0,y_0 + h) - f(x_0,y_0)}{h}$$ CalculationsFor example: The plane $x = 1$ intersects the paraboloid（抛物面） $z = x^2 + y^2$ in a parabola（抛物线）. Find the slope of the tangent to the parabola at $(1, 2, 5)$ (Figure 14.19). Solution: The parabola lies in a plane parallel to the $yz$-plane, and the slope is the value of the partial derivative $\\frac{\\partial z}{\\partial y}$ at $(1, 2)$: $$\\left. \\frac{\\partial z}{\\partial y} \\right|_{(1,2)} = \\left. 2y \\right|_{(1,2)} = 4$$ Functions of More Than Two VariablesPartial Derivatives and ContinuityA function $ƒ(x, y)$ can have partial derivatives with respect to both $x$ and $y$ at a point without the function being continuous there.（对于多变量（元）函数，偏导存在(可导)不一定连续） For example: Let$$f(x,y) =\\begin{cases}0, xy \\neq 0\\\\1, xy = 0\\end{cases}$$(Figure 14.21).(a) Find the limit of $ƒ$ as $(x, y)$ approaches $(0, 0)$ along the line $y = x$.(b) Prove that $ƒ$ is not continuous at the origin.(c) Show that both partial derivatives $\\partial ƒ /\\partial x$ and $\\partial ƒ /\\partial y$ exist at the origin. Solution: (a). Since $ƒ(x, y)$ is constantly zero along the line $y = x$ (except at the origin), we have $$\\left. \\lim_{(x,y) \\rightarrow (0,0)} f(x,y) \\right|_{y=x} = \\lim_{(x,y) \\rightarrow (0,0)} 0 = 0$$ (b). Since $ƒ(0, 0) = 1$, the limit in part (a) is not equal to $ƒ(0, 0)$, which proves that $ƒ$ is not continuous at $(0, 0)$. (c). To find $\\partial ƒ /\\partial x$ at $(0, 0)$, we hold $y$ fixed at $y = 0$. Then $ƒ(x, y) = 1$ for all $x$, and the graph of $ƒ$ is the line $L1$ in Figure 14.21. The slope of this line at any $x$ is $\\partial ƒ /\\partial x$. In particular, $\\partial ƒ /\\partial x = 0$ at $(0, 0)$. Similarly, $\\partial ƒ /\\partial y$ is the slope of line $L2$ at any $y$, so $\\partial ƒ /\\partial y = 0$ at $(0, 0)$. Second-Order Partial Derivatives$$\\frac{\\partial f}{\\partial x \\partial y}\\Longleftrightarrowf_{yx}$$Differentiate first with respect to $y$, then with respect to $x$. The Mixed Derivative Theorem THEOREM 2—The Mixed Derivative TheoremIf $ƒ(x, y)$ and its partial derivatives $f_x, f_y, f_{xy}, f_{yx}$ are defined throughout an open region containing a point $(a, b)$ and are all continuous at $(a, b)$, then$$f_{xy}(a,b) = f_{yx}(a,b)$$ Partial Derivatives of Still Higher OrderDifferentiability DEFINITIONA function $z = ƒ(x, y)$ is differentiable at $(x_0, y_0)$ if $ƒ_x(x_0 , y_0)$ and $ƒ_y(x_0, y_0)$ exist and $\\Delta z = ƒ(x_0 + \\Delta x, y_0 + \\Delta y) - ƒ(x_0, y_0)$ satisfies an equation of the form$$\\Delta z = f_x(x_0, y_0) \\Delta x + f_y(x_0,y_0)\\Delta y + \\epsilon_1 \\Delta x + \\epsilon_2 \\Delta y$$in which each of $\\epsilon_1, \\epsilon_2 \\rightarrow 0$ as both $\\Delta x, \\Delta y \\rightarrow 0$. We call $ƒ$ differentiable if it is differentiable at every point in its domain, and say that its graph is a smooth surface. THEOREM 4—Differentiability Implies Continuity（对于多变量（元）函数，可微一定连续）If a function $ƒ(x, y)$ is differentiable at $(x_0 , y_0)$, then $ƒ$ is continuous at $(x_0 , y_0)$. More about Derivative and Differentiation in Single Variable Function The Chain RuleFunctions of Two Variables Theorem 5—Chain Rule For Functions of One Independent Variable and Two Intermediate VariablesIf $w = ƒ(x, y)$ is differentiable and if $x = x(t), y = y(t)$ are differentiable functions of $t$, then the composition $w = ƒ(x(t), y(t))$ is a differentiable function of $t$ and$$\\frac{dw}{dt} = f_x(x(t), y(t)) x’(t) + f_y(x(t), y(t)) y’(t)$$or$$\\frac{dw}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt}$$ Functions of Three Variables Theorem 6—Chain Rule for Functions of One Independent Variable and Three Intermediate VariablesIf $w = ƒ(x, y, z)$ is differentiable and $x, y$, and $z$ are differentiable functions of $t$, then $w$ is a differentiable function of $t$ and$$\\frac{dw}{dt} = \\frac{\\partial w}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial w}{\\partial y} \\frac{dy}{dt} + + \\frac{\\partial w}{\\partial z} \\frac{dz}{dt}$$ Functions Defined on Surfaces THEOREM 7—Chain Rule for Two Independent Variables and Three Intermediate VariablesSuppose that $w = ƒ(x, y, z)$, $x = g(r, s), y = h(r, s)$, and $z = k(r, s)$. If all four functions are differentiable, then $w$ has partial derivatives with respect to $r$ and $s$, given by the formulas$$\\frac{\\partial w}{\\partial r} = \\frac{\\partial w}{\\partial x}\\frac{\\partial x}{\\partial r} + \\frac{\\partial w}{\\partial y}\\frac{\\partial y}{\\partial r} + \\frac{\\partial w}{\\partial z}\\frac{\\partial z}{\\partial r}$$$$\\frac{\\partial w}{\\partial s} = \\frac{\\partial w}{\\partial x}\\frac{\\partial x}{\\partial s} + \\frac{\\partial w}{\\partial y}\\frac{\\partial y}{\\partial s} + \\frac{\\partial w}{\\partial z}\\frac{\\partial z}{\\partial s}$$ Implicit Differentiation RevisitedSuppose that The function $F(x, y)$ is differentiable and The equation $F(x, y) = 0$ defines $y$ implicitly as a differentiable function of $x$, say $y = h(x)$. Since $w = F(x, y) = 0$, the derivative $dw/ dx$ must be zero. $$0 = \\frac{dw}{dx} = F_x\\frac{dx}{dx} + F_y\\frac{dy}{dx}= F_x \\cdot 1 + F_y \\cdot \\frac{dy}{dx}$$ If $F_y = \\frac{\\partial w}{\\partial y} \\neq 0$, then $$\\frac{dy}{dx} = -\\frac{F_x}{F_y}$$ Functions of Many VariablesDirectional Derivatives and Gradient VectorsDirectional Derivatives in the Plane Suppose that the function $ƒ(x, y)$ is defined throughout a region $R$ in the $xy$-plane, that $P_0(x_0, y_0)$ is a point in $R$, and that $\\vec{u} = u_1\\vec{i} + u_2\\vec{j}$ is a unit vector. Then the equations$$x = x_0 + s\\vec{u_1}, y = y_0 + s\\vec{u_2}$$parametrize the line through $P_0$ parallel to $\\vec{u}$. If the parameter $s$ measures arc length from $P_0$ in the direction of $\\vec{u}$, we find the rate of change of $ƒ$ at $P_0$ in the direction of $\\vec{u}$ by calculating $dƒ / ds$ at $P_0$. DEFINITIONThe derivative of $f$ at $P_0(x_0, y_0)$ in the direction of the unit vector $\\vec{u} = u_1\\vec{i} + u_2\\vec{j}$ is the number$$\\left( \\frac{df}{ds} \\right)_{\\vec{u}, P_0} = \\lim_{s \\rightarrow 0} \\frac{f(x_0 + su_1, y_0 + su_2) - f(x_0,y_0)}{s}\\tag{1}$$The directional derivative defined by Equation (1) is also denoted by$$D_{\\vec{u}} f(P_0)\\\\or\\\\\\left. D_{\\vec{u}} f \\right|_{P_0}$$ Interpretation of the Directional DerivativeThe equation $z = ƒ(x, y)$ represents a surface $S$ in space. If $z_0 = ƒ(x_0 , y_0)$, then the point $P(x_0 , y_0 , z_0)$ lies on $S$. The vertical plane that passes through $P$ and $P_0(x_0 , y_0)$ parallel to $\\vec{u}$ intersects $S$ in a curve $C$ (Figure 14.28). The rate of change of $ƒ$ in the direction of $\\vec{u}$ is the slope of the tangent to $C$ at $P$ in the right-handed system formed by the vectors $\\vec{u}$ and $\\vec{k}$. Calculation and GradientsBy Chain Rule: $$\\begin{aligned}\\left( \\frac{df}{ds}\\right)_{\\vec{u},P_0} &amp;= \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} \\frac{dx}{ds} + \\left. \\frac{\\partial f}{\\partial y} \\right|_{P_0} \\frac{dy}{ds}\\\\&amp;= \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} u_1 + \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} u_2 \\\\&amp;= \\underbrace{\\left[ \\left. \\frac{\\partial f}{\\partial x} \\right|_{P_0} \\vec{i} + \\left. \\frac{\\partial f}{\\partial y} \\right|_{P_0} \\vec{j} \\right]}_{Gradient-of-f-at-P_0} \\cdot \\underbrace{\\left[ u_1\\vec{i} + u_2\\vec{j} \\right]}_{Direction-of-\\vec{u}}\\end{aligned}\\tag{3}$$ DefinitionThe gradient vector (or gradient) of $ƒ(x, y)$ is the vector$$\\nabla f = \\frac{\\partial f}{\\partial x} \\vec{i} + \\frac{\\partial f}{\\partial y} \\vec{j}$$The value of the gradient vector obtained by evaluating the partial derivatives at a point $P_0(x_0, y_0)$ is written$$\\left. \\nabla f \\right|_{P_0}\\\\or\\\\\\nabla f(x_0,y_0)$$ theorem 9—The Directional Derivative Is a Dot ProductIf $ƒ(x, y)$ is differentiable in an open region containing $P_0(x_0, y_0)$, then$$\\left( \\frac{df}{ds} \\right)_{\\vec{u}, P_0} = \\left. \\nabla f \\right|_{P_0} \\cdot \\vec{u}\\tag{4}$$ For example: Find the derivative of $ƒ(x, y) = xe^y + cos (xy)$ at the point $(2, 0)$ in the direction of $\\vec{v} = 3\\vec{i} - 4\\vec{j}$.（directional derivative） Solution: Recall that the direction of a vector $\\vec{v}$ is the unit vector obtained by dividing $\\vec{v}$ by its length: $$\\vec{u} = \\frac{\\vec{v}}{|\\vec{v}|} = \\frac{3}{5}\\vec{i} - \\frac{4}{5}\\vec{j}$$ The partial derivatives of $f$ are everywhere continuous and at $(2, 0)$ are given by $$f_x(2,0) = \\left. (e^y - y\\sin(xy)) \\right|_{(2,0)} = 1\\\\f_y(2,0) = 2$$ The gradient of $ƒ$ at $(2, 0)$ is $$\\left. \\nabla f \\right|_{2,0} = f_x(2,0) \\vec{i} + f_y(2,0)\\vec{j} = \\vec{i} + 2\\vec{j}$$ The derivative of $f$ at $(2, 0)$ in the direction of $\\vec{v}$ is therefore $$D_{\\vec{u}} f |_{(2,0)} = \\nabla f_{(2,0)} \\cdot \\vec{u}= -1$$ Evaluating the dot product in the brief version of Equation (4) gives$$D_{\\vec{u}} f = \\nabla f \\cdot \\vec{u} = |\\nabla f| |\\vec{u}| \\cos \\theta = |\\nabla f| \\cos \\theta$$ Properties of the Directional Derivative $D_{\\vec{u}} f = \\nabla f \\cdot \\vec{u} = |\\nabla f| \\cos \\theta$ The function $ƒ$ increases most rapidly when $\\cos\\theta = 1$, which means that $\\theta = 0$ and $\\vec{u}$ is the direction of $\\nabla f$. That is, at each point $P$ in its domain, $ƒ$ increases most rapidly in the direction of the gradient vector $\\nabla f$ at $P$. The derivative in this direction is$$D_{\\vec{u}} f = |\\nabla f| \\cos(0) = |\\nabla f|$$ Similarly, $ƒ$ decreases most rapidly in the direction of $-\\nabla f$. The derivative in this direction is $D_{\\vec{u}} f = |\\nabla f| \\cos(\\pi) = -|\\nabla f|$. Any direction $\\vec{u}$ orthogonal to a gradient $\\nabla f \\neq 0$ is a direction of zero change in $ƒ$ because $\\theta$ then equals $\\pi / 2$ and$$D_{\\vec{u}} f = |\\nabla f| \\cos(\\pi/2) = 0$$ Gradients and Tangents to Level CurvesIf a differentiable function $ƒ(x, y)$ has a constant value $c$ along a smooth curve $r = g(t)i + h(t)j$, then$$\\frac{d}{dt}f(g(t), h(t)) = \\frac{d}{dt} c$$ $$\\frac{\\partial f}{\\partial x}\\frac{dg}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dh}{dt} = 0$$ $$\\underbrace{\\left( \\frac{\\partial f}{\\partial x} \\vec{i} + \\frac{\\partial f}{\\partial y} \\vec{j}\\right)}_{\\nabla f} \\cdot\\underbrace{\\left( \\frac{dg}{dt} \\vec{i} + \\frac{dh}{dt} \\vec{j}\\right)}_{\\frac{dr}{dt}} = 0\\tag{5}$$Equation (5) says that $\\nabla ƒ$ is normal to the tangent vector $dr / dt$, so it is normal to the curve. Tangent Line to a Level Curve$$f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0) = 0$$ Attention：gradient is perpendicular to the level curve, not the surface. Gradient in multi-variable function is like derivative in sigle-variable function. Functions of Three VariablesThe Chain Rule for PathsIf $r(t) = x(t) i + y(t) j + z(t)k$ is a smooth path $C$, and $w = ƒ(r(t))$ is a scalar function evaluated along $C$, then according to the Chain Rule,$$\\frac{dw}{dt} = \\frac{\\partial w}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial w}{\\partial y} \\frac{dy}{dt} + \\frac{\\partial w}{\\partial z} \\frac{dz}{dt}$$ The Derivative Along a Path$$\\frac{d}{dt}f(r(t)) = \\nabla f(r(t)) \\cdot r’(t)\\tag{7}$$What Equation (7) says is that the derivative of the composite function $ƒ(r(t))$ is the “derivative” (gradient) of the outside function $ƒ$ “times” (dot product) the derivative of the inside function $r$. Tangent Planes and DifferentialsRecall how the derivative defined the tangent line to the graph of a differentiable function at a point on the graph. The tangent line then provided for a linearization of the function at the point. More about Tangent Line in Singule Variable Function &gt;&gt; how the gradient defines the tangent plane to the level surface of a function $w = ƒ(x, y, z)$ at a point on the surface. The tangent plane then provides for a linearization of $ƒ$ at the point and defines the total differential of the function. Tangent Planes and Normal Lines DEFINITIONSThe tangent plane to the level surface $ƒ(x, y, z) = c$ of a differentiable function $ƒ$ at a point $P_0$ where the gradient is not zero is the plane through $P_0$ normal to $\\nabla f|_{P_0}$.The normal line of the surface at $P_0$ is the line through $P_0$ parallel to $\\nabla f |_{P_0}$. For example: Find the tangent plane and normal line of the level surface$$f(x,y,z) = x^2 + y^2 + z - 9 = 0$$at the point $P_0 (1, 2, 4)$. Solution: The tangent plane is the plane through $P_0$ perpendicular to the gradient of $ƒ$ at $P_0$. The gradient is$$\\nabla f |_{P_0} = (2xi + 2yj + k)|_{(1,2,4)} = 2i + j + k$$The tangent plane is therefore the plane$$2(x-1) + 4(y-2) + (z-4) = 0$$or$$2x + 4y + z = 14$$The line normal to the surface at $P_0$ is$$x = 1 + 2t, y = 2 + 4t, z = 4 + t$$ Plane Tangent to a Surface $z = ƒ(x, y)$ at $(x_0 , y_0 , ƒ(x_0 , y_0))$The plane tangent to the surface $z = ƒ(x, y)$ of a differentiable function $ƒ$ at the point $P_0(x_0 , y_0 , z_0) = (x_0 , y_0 , ƒ(x_0 , y_0))$ is$$f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0)-(z-z_0) = 0$$ For example: The surfaces $$f(x,y,z) = x^2 + y^2 -2 = 0$$and $$g(x,y,z) = x + z - 4 = 0$$ meet in an ellipse $E$ (Figure 14.35). Find parametric equations for the line tangent to $E$ at the point $P_0(1, 1, 3)$. Solution: The tangent line is orthogonal to both $\\nabla f$ and $\\nabla g$ at $P_0$, and therefore parallel to $v = \\nabla f \\times \\nabla g$. The components of $v$ and the coordinates of $P_0$ give us equations for the line. We have $$\\nabla f |_{(1,1,3)} = 2i + 2j$$ $$\\nabla g |_{(1,1,3)} = i + k$$ $$\\begin{aligned}v &amp;= (2i + 2j) \\times (i + k)\\\\&amp;=\\begin{vmatrix}i &amp; j &amp; k \\\\2 &amp; 2 &amp; 0\\\\1 &amp; 0 &amp; 1\\end{vmatrix}\\\\&amp;=2i - 2j - 2k\\end{aligned}$$ The tangent line to the ellipse of intersection is$$x = 1 + 2t, y = 1 - 2t, z = 3 - 2t$$ Estimating Change in a Specific Direction Estimating the Change in $ƒ$ in a Direction $u$To estimate the change in the value of a differentiable function $ƒ$ when we move a small distance $ds$ from a point $P_0$ in a particular direction $u$, use the formula$$df = \\underbrace{(\\nabla f |_{P_0} \\cdot u)}_{Directional-derivative} \\underbrace{ds}_{Distance-increment}$$ For example: Estimate how much the value of $$f(x,y,z) = y\\sin x + 2yz$$will change if the point $P(x, y, z)$ moves $0.1$ unit from $P_0(0, 1, 0)$ straight toward $P_1(2, 2, -2)$. Solution: We first find the derivative of $ƒ$ at $P_0$ in the direction of the vector $\\vec{P_0P_1} = 2i + j - 2k$. The direction of this vector is$$u = \\frac{\\vec{P_0P_1}}{|\\vec{P_0P_1}|} = \\frac{2}{3}i + \\frac{1}{3}j - \\frac{2}{3}k$$The gradient of $ƒ$ at $P_0$ is$$\\nabla f|_{(0,1,0)} = i + 2k$$Therefore,$$\\nabla f|_{P_0} \\cdot u = -\\frac{2}{3}$$The change $dƒ$ in $ƒ$ that results from moving $ds = 0.1$ unit away from $P_0$ in the direction of $u$ is approximately$$df = (\\nabla f|_{P_0} \\cdot u)(ds) \\approx -0.047unit.$$ How to Linearize a Function of Two Variables DEFINITIONSThe linearization of a function $ƒ(x, y)$ at a point $(x_0 , y_0)$ where $ƒ$ is differentiable is the function$$L(x,y) = f(x_0,y_0) + f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0)$$The approximation$$f(x,y) \\approx L(x,y)$$is the standard linear approximation of $ƒ$ at $(x_0 , y_0)$. Thus, the linearization of a function of two variables is a tangent-plane approximation in the same way that the linearization of a function of a single variable is a tangent-line approximation. The Error in the Standard Linear ApproximationIf $ƒ$ has continuous first and second partial derivatives throughout an open set containing a rectangle $R$ centered at $(x_0, y_0)$ and if $M$ is any upper bound for the values of $|f_{xx}|,|f_{yy}|$, and $|f_{xy}|$ on $R$, then the error $E(x, y)$ incurred in replacing $ƒ(x, y)$ on $R$ by its linearization$$L(x,y) = f(x_0, y_0) + f_x(x_0,y_0)(x-x_0) + f_y(x_0,y_0)(y-y_0)$$satisfies the inequality$$|E(x,y)| \\leq \\frac{1}{2}M(|x-x_0| + |y-y_0|)^2$$ Differentials DefinitionIf we move from $(x_0, y_0)$ to a point $(x_0 + dx, y_0 + dy)$ nearby, the resulting change$$df = f_x(x_0,y_0)dx + f_y(x_0,y_0)dy$$in the linearization of $ƒ$ is called the total differential of $ƒ$. Functions of More Than Two VariablesThe linearization of $ƒ(x, y, z)$ at a point $P_0(x_0, y_0, z_0)$ is$$L(x,y,z) = f(P_0) + f_x(P_0)(x-x_0) + f_y(P_0)(y-y_0) + f_z(P_0)(z-z_0)$$ Extreme Values and Saddle PointsSimilar Meanings in Single-Variable Functions Derivative Tests for Local Extreme Values DEFINITIONSLet $ƒ(x, y)$ be defined on a region $R$ containing the point $(a, b)$. Then $ƒ(a, b)$ is a local maximum value of $ƒ$ if $ƒ(a, b) \\geq ƒ(x, y)$ for all domain points $(x, y)$ in an open disk centered at $(a, b)$. $ƒ(a, b)$ is a local minimum value of $ƒ$ if $ƒ(a, b) \\leq ƒ(x, y)$ for all domain points $(x, y)$ in an open disk centered at $(a, b)$. theorem 10—First Derivative Test for Local Extreme ValuesIf $ƒ(x, y)$ has a local maximum or minimum value at an interior point $(a, b)$ of its domain and if the first partial derivatives exist there, then $ƒ_x(a, b) = 0$ and $ƒ_y(a, b) = 0$. DefinitionAn interior point of the domain of a function $ƒ(x, y)$ where both $ƒ_x$ and $ƒ_y$ are zero or where one or both of $ƒ_x$ and $ƒ_y$ do not exist is a critical point of $ƒ$. DefinitionA differentiable function $ƒ(x, y)$ has a saddle point at a critical point $(a, b)$ if in every open disk centered at $(a, b)$ there are domain points $(x, y)$ where $ƒ(x, y) &lt; ƒ(a, b)$ and domain points $(x, y)$ where $ƒ(x, y) &gt; ƒ(a, b)$. The corresponding point $(a, b, ƒ(a, b))$ on the surface $z = ƒ(x, y)$ is called a saddle point of the surface (Figure 14.45). theorem 11—Second Derivative Test for Local Extreme ValuesSuppose that $ƒ(x, y)$ and its first and second partial derivatives are continuous throughout a disk centered at $(a, b)$ and that $f_x(a, b) = f_y(a, b) = 0$. Theni) $f$ has a local maximum at $(a, b)$ if $f_{xx} &lt; 0$ and $f_{xx} f_{yy} - f_{xy}^2 &gt; 0$ at $(a, b)$.ii) $f$ has a local minimum at $(a, b)$ if $f_{xx} &gt; 0$ and $f_{xx} f_{yy} - f_{xy}^2 &gt; 0$ at $(a, b)$.iii) $f$ has a saddle point at $(a, b)$ if $f_{xx} f_{yy} - f_{xy}^2 &lt; 0$ at $(a, b)$.iv) the test is inconclusive at $(a, b)$ if $f_{xx} f_{yy} - f_{xy}^2 = 0$ at $(a, b)$. In this case, we must find some other way to determine the behavior of $f$ at $(a, b)$. The expression $f_{xx} f_{yy} - f_{xy}^2$ is called the discriminant(判别式) or Hessian of $f$. It is sometimes easier to remember it in determinant form,$$f_{xx} f_{yy} - f_{xy}^2 =\\begin{vmatrix}f_{xx} &amp; f_{xy} \\\\f_{xy} &amp; f_{yy}\\end{vmatrix}$$ Absolute Maxima and Minima on Closed Bounded RegionsWe organize the search for the absolute extrema of a continuous function $ƒ(x, y)$ on a closed and bounded region $R$ into three steps. List the interior points of $R$ where $ƒ$ may have local maxima and minima and evaluate $ƒ$ at these points. These are the critical points of $ƒ$. List the boundary points of $R$ where $ƒ$ has local maxima and minima and evaluate $ƒ$ at these points. We show how to do this in the next example. Look through the lists for the maximum and minimum values of $ƒ$. These will be the absolute maximum and minimum values of $ƒ$ on $R$. Lagrange Multipliers(拉格朗日乘子)Here we explore a powerful method for finding extreme values of constrained functions: the method of Lagrange multipliers. Constrained Maxima and MinimaFor example: Find the points on the hyperbolic cylinder(双曲柱面) $x^2 - z^2 - 1 = 0$ that are closest to the origin. Solution: Substitution. This problems means, we are seeking the points whose coordinates minimize the value of the function $f(x,y,z) = x^2 + y^2 + z^2$ subject to the constraint that $x^2 - z^2 - 1 = 0$. First, we try to regard $x$ and $y$ as independent variables, then $z^2 = x^2 - 1$. So, the problem becomes we look for the points in the $xy$-plane whose coordinates minimize $h$.$$f(x,y,z) = x^2 + y^2 + z^2 \\longmapsto \\\\h(x,y) = x^2 + y^2 + (x^2-1) \\\\= 2x^2 + y^2 -1$$ By First Derivative Test$$h_x = 4x = 0, h_y = 2y = 0$$The point should be $(0,0)$.But there are no points on the cylinder where both $x$ and $y$ are zero. Why? Although the domain of $h$ is the entire $xy$-plane, the domain from which we can select the first two coordinates of the points $(x, y, z)$ on the cylinder is restricted to the “shadow” of the cylinder on the $xy$-plane; it does not include the band between the lines $x = -1$ and $x = 1$ (Figure 14.53). If we eliminate the one variable by $$x^2 = z^2 + 1\\\\k(y,z) = 1 + y^2 + 2z^2$$we can get the right answer. Tangent plane and Normal line. At each point of contact, the cylinder and sphere have the same tangent plane and normal line. Therefore, if the sphere and cylinder are represented as the level surfaces obtained by setting $$f(x,y,z) = x^2 + y^2 + z^2 - a^2\\\\g(x,y,z) = x^2 -z^2 - 1$$equal to 0. Then the gradients $\\nabla ƒ$ and $\\nabla g$ will be parallel where the surfaces touch.$$\\nabla f = \\lambda \\nabla g$$which is$$2x i + 2y j + 2z k = \\lambda (2x i - 2zk)$$the solution point should satisfy$$\\begin{cases} 2x = 2\\lambda x\\\\ 2y = 0\\\\ 2z = -2\\lambda z\\end{cases}$$and$$x^2 - z^2 - 1 = 0$$thus,$$\\lambda = 1$$ The points on the cylinder closest to the origin are the points $(\\pm 1, 0, 0)$. The Method of Lagrange Multipliers THEOREM 12—The Orthogonal Gradient TheoremSuppose that $ƒ(x, y, z)$ is differentiable in a region whose interior contains a smooth curve$$C: r(t) = x(t) i + y(t) j + z(t) k$$If $P_0$ is a point on $C$ where $ƒ$ has a local maximum or minimum relative to its values on $C$, then $\\nabla f$ is orthogonal to $C$ at $P_0$. COROLLARYAt the points on a smooth curve $r(t) = x(t)i + y(t)j$ where a differentiable function $ƒ(x, y)$ takes on its local maxima and minima relative to its values on the curve, $\\nabla f \\cdot r’ = 0$. The Method of Lagrange MultipliersSuppose that $ƒ(x, y, z)$ and $g(x, y, z)$ are differentiable and $\\nabla g \\neq 0$ when $g(x, y, z) = 0$. To find the local maximum and minimum values of $ƒ$ subject to the constraint $g(x, y, z) = 0$ (if these exist), find the values of $x, y, z$, and $l$ that simultaneously satisfy the equations$$\\nabla f = \\lambda \\nabla g, and, g(x,y,z) = 0\\tag{1}$$For functions of two independent variables, the condition is similar, but without the variable $z$. For example: Find the greatest and smallest values that the function$$f(x,y) = xy$$takes on the ellipse (Figure 14.55)$$\\frac{x^2}{8} + \\frac{y^2}{2} = 1$$ Solution: the problem is abstracted as : find the extreme values of $f(x,y) = xy$ subject to the constraint $g(x,y) = \\frac{x^2}{8} + \\frac{y^2}{2} - 1 = 0$. $$\\nabla f = \\lambda \\nabla g, and, g(x,y) = 0$$ $$y i + x j = \\lambda (\\frac{1}{4}xi + yj)$$ we find $$\\begin{cases} y = \\frac{\\lambda}{4}x\\\\ x = \\lambda y\\end{cases}$$we get$$y = 0, or, \\lambda = \\pm 2$$ Case1: $y = 0$. Then $x = 0, y = 0$. But this point is not on the ellipse. Hence, $y \\neq 0$. Case2: $$\\lambda = \\pm 2, x = \\pm 2y$$$$\\frac{(\\pm 2y)^2}{8} + \\frac{y^2}{2} = 1, y = \\pm 1$$ So, The extreme values are $xy = 2$ and $xy = -2$. Lagrange Multipliers with Two Constraints$$\\nabla f = \\lambda \\nabla g_1 + \\mu \\nabla g_2, g_1(x,y,z)=0, g_2(x,y,z) = 0\\tag{2}$$ For example:The plane $x + y + z = 1$ cuts the cylinder $x^2 + y^2 = 1$ in an ellipse (Figure 14.59). Find the points on the ellipse that lie closest to and farthest from the origin. Solution: The problem is abstracted as: find the extreme values of $f(x,y,z) = x^2 + y^2 + z^2$ substract to the constraints $$g_1(x,y,z) = x^2 + y^2 - 1 = 0\\\\g_2(x,y,z) = x + y + z = 0$$ $$\\nabla f = \\lambda \\nabla g_1 + \\mu \\nabla g_2$$we get$$\\begin{cases} 2x = 2\\lambda x + \\mu\\\\ 2y = 2\\lambda y + \\mu\\\\ 2z = \\mu\\end{cases}\\longrightarrow\\begin{cases} (1-\\lambda)x = z\\\\ (1-\\lambda)y = z\\end{cases}\\tag{6}$$the result is $\\lambda = 1$ and $z = 0$, or $\\lambda \\neq 1$ and $x = y = \\frac{z}{(1-\\lambda)}$. Case1. $z = 0$, we get $(1,0,0)$ and $(0,1,0)$.Case2. $x = y$, we get $P_1(\\frac{\\sqrt 2}{2}, \\frac{\\sqrt 2}{2}, 1 - \\sqrt{2})$ and $P_2(-\\frac{\\sqrt 2}{2}, -\\frac{\\sqrt 2}{2}, 1 + \\sqrt{2})$ The points on the ellipse closest to the origin are $(1, 0, 0)$ and $(0, 1, 0)$. The point on the ellipse farthest from the origin is $P_2$. (See Figure 14.59.) Taylor’s Formula for Two VariablesDerivation of the Second Derivative Test The Error Formula for Linear ApproximationsTaylor’s Formula for Functions of Two VariablesTaylor’s Formula for Single-Variable Function &gt;&gt; Taylor’s Formula for $ƒ(x, y)$ at the Point $(a, b)$Suppose $ƒ(x, y)$ and its partial derivatives through order $n + 1$ are continuous throughout an open rectangular region $R$ centered at a point $(a, b)$. Then, throughout $R$,$$\\begin{aligned}f(a+h,b+k) &amp;= f(a,b) \\\\ &amp;+ \\left. (hf_x + kf_y)\\right|_{(a,b)}\\\\ &amp;+ \\left. \\frac{1}{2!}(h^2f_{xx} + 2hkf_{xy} + k^2f_{yy})\\right|_{(a,b)} \\\\ &amp;+ \\left. \\frac{1}{3!}(h^3f_{xxx} + 3h^2kf_{xxy} + 3hk^2f_{xyy} + k^3f_{yyy})\\right|_{(a,b)} \\\\ &amp;+ \\cdots \\\\ &amp;+ \\left. \\frac{1}{n!}(h\\frac{\\partial }{\\partial x} + k \\frac{\\partial}{\\partial y})^n f\\right|_{(a,b)} \\\\ &amp;+ \\left. \\frac{1}{(n+1)!}(h\\frac{\\partial}{\\partial x} + k \\frac{\\partial}{\\partial y})^{n+1} f\\right|_{(a+ch,b+ck)}\\end{aligned}\\tag{7}$$ Partial Derivatives with Constrained VariablesDecide Which Variables Are Dependent and Which Are Independent For example: Find $\\partial w/\\partial x$ if $w = x^2 + y^2 + z^2$ and $z = x^2 + y^2$. Solution: This is the formula for $\\partial w / \\partial x$ when $x$ and $y$ are the independent variables.$$\\frac{\\partial w}{\\partial x} = 2x + 4x^3 + 4xy^2$$This is the formula for $\\partial w / \\partial x$ when $x$ and $z$ are the independent variables.$$\\frac{\\partial w}{\\partial x} = 0$$ How to Find $\\partial w/ \\partial x$ When the Variables in $w = ƒ(x, y, z)$ Are Constrained by Another Equation Decide which variables are to be dependent and which are to be independent. (In practice, the decision is based on the physical or theoretical context of our work. In the exercises at the end of this section, we say which variables are which.) Eliminate the other dependent variable(s) in the expression for $w$. Differentiate as usual. Notation$\\partial w / \\partial x$ with $x$ and $y$ independent:$$(\\frac{\\partial w}{\\partial x})_y$$ $\\partial w / \\partial x$ with $x$ and $y$ and $t$ independent:$$(\\frac{\\partial w}{\\partial x})_{y,t}$$ Arrow Diagrams","link":"/Math/Calculus/Calculus-C14-Partial-Derivatives/"},{"title":"Calculus-C3-Derivatives","text":"Keywords: Derivative, Differentiation, ChainRule, Linearization and Differentials This is the Chapter3 ReadingNotes from book Thomas Calculus 14th. The Derivative as a Function DefinitionThe derivative of the function $f(x)$ with respect to the variable $x$ is the function $f’$ whose value at $x$ is$$\\begin{aligned}f’(x) &amp;= \\lim_{h\\rightarrow0}\\frac{f(x+h)-f(x)}{h}\\\\&amp;= \\lim_{z\\rightarrow x}\\frac{f(z)-f(x)}{z-x}\\end{aligned}$$ If $f’$ exists at a particular $x$, we say that $ƒ$ is differentiable （可微）(has a derivative（有导数）) at $x$. If $f’$ exists at every point in the domain of $f$, we call $f$ differentiable. The process of calculating a derivative is called differentiation.（求导的过程叫做微分） $$f’(x) = \\frac{d}{dx}f(x)$$ Differentiable Functions Are Continuous If $f$ has a derivative at $x = c$, then $ƒ$ is continuous at $x = c$. Differentiation RulesProducts and Quotients Derivative Product RuleIf $u$ and $v$ are differentiable at $x$, then so is their product $uv$, and$$\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}$$ The Chaine Rule If $ƒ(u)$ is differentiable at the point $u = g(x)$ and $g(x)$ is differentiable at $x$, then the composite function $(ƒ \\circ g) (x) = ƒ(g(x))$ is differentiable at x, and$$(f\\circ g)’(x) = f’(g(x)) \\cdot g’(x)$$In Leibniz’s notation, if $y = ƒ(u)$ and $u = g(x)$, then$$\\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx}$$ Implicit Differentiation$$\\begin{aligned}&amp;x^3 + y^3 -9xy = 0\\\\&amp;y^2-x=0\\\\&amp;x^2 + y^2 - 25 = 0\\end{aligned}$$These equations define an implicit relation between the variables $x$ and $y$, meaning that a value of $x$ determines one or more values of $y$, even though we do not have a simple formula for the $y$-values. Implicitly Defined Functions Differentiate both sides of the equation with respect to $x$, treating $y$ as a differentiable function of $x$. Collect the terms with $\\frac{dy}{dx}$ on one side of the equation and solve for $\\frac{dy}{dx}$. Derivatives of Higher OrderLenses, Tangent Lines, and Normal Lines For example: Show that the point $(2, 4)$ lies on the curve $x^3 + y^3 - 9xy = 0$. Then find the tangent and normal to the curve there (Figure 3.34). Solution: To find the slope of the curve at $(2, 4)$, we first use implicit differentiation to find a formula for $\\frac{dy}{dx}$: $$\\begin{aligned}&amp;x^3 + y^3 -9xy = 0\\\\&amp;\\frac{d}{dx}(x^3) - \\frac{d}{dx}(y^3) - \\frac{d}{dx}(9xy) = \\frac{d}{dx}(0)\\\\&amp;\\frac{dy}{dx} = \\frac{3y-x^2}{y^2-3x}\\end{aligned}$$We then evaluate the derivative at $(x, y) = (2, 4)$:$$\\left. \\frac{dy}{dx}\\right|_{(2,4)} = \\frac{4}{5}$$The tangent at $(2, 4)$ is the line through $(2, 4)$ with slope $\\frac{4}{5}$:$$y = 4 + \\frac{4}{5}(x-2)$$The normal line:$$y = 4 - \\frac{5}{4}(x-2)$$ Derivatives of Inverse Functions and LogarithmsDerivatives of Inverses of Differentiable Functions If $ƒ$ has an interval $I$ as domain and $ƒ’(x)$ exists and is never zero on $I$, then $ƒ^{-1}$ is differentiable at every point in its domain (the range of $ƒ$). The value of $(ƒ^{-1})’$ at a point $b$ in the domain of $ƒ^{-1}$ is the reciprocal of the value of $f’$ at the point $a = ƒ^{-1}(b)$: $$(f^{-1})’(b) = \\frac{1}{f’(f^{-1}(b))}$$or$$\\left. \\frac{df^{-1}}{dx}\\right|_{x=b} =$$ $$\\frac{1}{\\left. \\frac{df}{dx}\\right|_{x=f^{-1}(b)}}$$ Linearization and Differentials（线性化和微分)It is often useful to approximate complicated functions with simpler ones that give the accuracy we want for specific applications and at the same time are easier to work with than the original functions. The approximating functions discussed in this section are called linearizations, and they are based on tangent lines. Other approximating functions, such as polynomials, are discussed in Chapter 10. We introduce new variables $dx$ and $dy$, called differentials, and define them in a way that makes Leibniz’s notation for the derivative $\\frac{dy}{dx}$ a true ratio. Linearization DefinitionIf $ƒ$ is differentiable at $x = a$, then the approximating function$$L(x) = ƒ(a) + ƒ’(a)(x - a)$$is the linearization of $ƒ$ at $a$. The approximation$$f(x) \\approx L(x)$$of $ƒ$ by $L$ is the standard linear approximation of $ƒ$ at $a$. The point $x = a$ is the center of the approximation. Differentials DefinitionLet $y = ƒ(x)$ be a differentiable function. The differential $dx$ is an independent variable. The differential $dy$ is$$dy = f’(x)dx$$ Estimating with DifferentialsFor example: Use differentials to estimate $\\sin(\\pi/6+0.01)$. Solution: The dierential associated with $y = \\sin x$ is $$dy = \\cos x dx$$To estimate $\\sin(\\pi/6+0.01)$, we set $a = \\pi/6$ and $dx = 0.01$. Then$$\\begin{aligned}f(\\pi/6+0.01) &amp;= f(a + dx) \\\\&amp;\\approx f(a) + dy\\\\&amp;= \\sin\\frac{\\pi}{6} + (\\cos\\frac{\\pi}{6})(0.01)\\\\&amp;\\approx 0.5087\\end{aligned}$$For comparison, the true value of $\\sin(\\pi/6+0.01)$ to $6$ decimals is $0.508635$. The method in this example can be used in computer algorithms to give values of trigonometric functions. The algorithms store a large table of sine and cosine values between $0$ and $\\pi/4$. Values between these stored values are computed using differentials as in Example. Values outside of $[0,\\pi/4]$ are computed from values in this interval using trigonometric identities. Error in Differential ApproximationLet $ƒ(x)$ be differentiable at $x = a$ and suppose that $dx = \\Delta x$ is an increment of $x$. We have two ways to describe the change in $ƒ$ as $x$ changes from $a$ to $a + \\Delta x$: the true change:$$\\Delta f = f(a + \\Delta x) - f(a)$$the differential estimate:$$df = f’(a)\\Delta x$$ Change in $y = ƒ(x)$ near $x = a$If $y = ƒ(x)$ is differentiable at $x = a$ and $x$ changes from $a$ to $a + \\Delta x$, the change $\\Delta y$ in $ƒ$ is given by$$\\Delta y = f’(a)\\Delta x + \\epsilon \\Delta x$$in which $\\epsilon \\rightarrow 0$ as $\\Delta x \\rightarrow 0$. To be added…","link":"/Math/Calculus/Calculus-C3-Derivatives/"},{"title":"Calculus-C16-Integrals-and-Vector-Fields","text":"Keywords: Vector Fields and Line Integrals, Green’s Theorem, Stokes’ Theorem, Divergence， Vector Calculus This is the Chapter16 ReadingNotes from book Thomas Calculus 14th. Line Integrals of Scalar Functions(标量函数的曲线积分) Suppose that $ƒ(x, y, z)$ is a real-valued function we wish to integrate over the curve $C$ lying within the domain of $ƒ$ and parametrized by $r(t) = g(t)i + h(t)j + k(t)k$, $a \\leq t \\leq b$. The values of $ƒ$ along the curve are given by the composite function $ƒ(g(t), h(t), k(t))$. We are going to integrate this composition with respect to arc length from $t = a$ to $t = b$. To begin, we first partition the curve $C$ into a finite number $n$ of subarcs (Figure 16.1). The typical subarc has length $\\Delta s_k$. In each subarc we choose a point $(x_k, y_k, z_k)$ and form the sum $$S_n = \\sum_{k=1}^n f(x_k, y_k, z_k) \\Delta s_k$$ which is similar to a Riemann sum. DEFINITIONIf $ƒ$ is defined on a curve $C$ given parametrically by $r(t) = g(t)i + h(t)j + k(t)k$, $a \\leq t \\leq b$, then the line integral of $ƒ$ over $C$ is$$\\int_{C}f(x,y,z)ds = \\lim_{n-&gt;\\infty} \\sum_{k=1}^{n} f(x_k, y_k, z_k)\\Delta s_k\\tag{1}$$provided this limit exists. If the curve $C$ is smooth for $a \\leq t \\leq b$ (so $v = dr/$ is continuous and never 0) andthe function $ƒ$ is continuous on $C$, then the limit in Equation (1) can be shown to exist. We can then apply the Fundamental Theorem of Calculus &gt;&gt; to differentiate the arc length equation &gt;&gt;,$$s(t) = \\int_a^t |v(\\tau)| d\\tau$$to express $ds$ in Equation (1) as $ds = |v(t)|dt$ and evaluate the integral over $C$ as$$\\int_C f(x,y,z) ds = \\int_a^b f(g(t), h(t), k(t)) |v(t)| dt\\tag{2}$$ How to Evaluate a Line IntegralTo integrate a continuous function $ƒ(x, y, z)$ over a curve $C$:1.Find a smooth parametrization of $C$,$$r(t) = g(t)i + h(t)j + k(t)k, a \\leq t \\leq b$$2.Evaluate the integral as$$\\int_C f(x,y,z)ds = \\int_a^b f(g(t), h(t), k(t)) |v(t)| dt$$ For example: Integrate $ƒ(x, y, z) = x - 3y^2 + z$ over the line segment $C$ joining the origin to the point $(1, 1, 1)$ (Figure 16.2). Solution: Since any choice of parametrization will give the same answer, we choose the simplest parametrization we can think of: $$r(t) = ti + tj + tk, 0 \\leq t \\leq 1$$ The components have continuous first derivatives and $$|v(t)| = |i + j + k| = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}$$ is never 0, so the parametrization is smooth. The integral of ƒ over $C$ is $$\\int_C f(x,y,z)ds = \\int_0^1 f(t,t,t)\\sqrt{3}dt = 0$$ Additivity$$\\int_C f ds = \\int_{C_{1}} f ds + \\cdots + \\int_{C_{n}} f ds\\tag{3}$$ For example: Figure 16.3 shows another path from the origin to $(1, 1, 1)$, formed from two line segments $C_1 and C_2$. Integrate $ƒ(x, y, z) = x - 3y2 + z$ over $C_1 \\cup C_2$. Solution:$$C_1 : r(t) = ti + tj, 0 \\leq t \\leq 1; |v| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$$$$C_1 : r(t) = i + j + tk, 0 \\leq t \\leq 1; |v| = \\sqrt{0^2 + 0^2 + 1^2} = 1$$With these parametrizations we find that$$\\begin{aligned}\\int_{C_1 \\cup C_2} f(x,y,z) ds &amp;= \\int_{C_1} f(x,y,z) ds + \\int_{C_2} f(x,y,z) ds \\\\&amp;= \\int_0^1f(t,t,0) \\sqrt{2}dt + \\int_0^1f(1,1,t)1dt\\\\&amp;= -\\frac{\\sqrt{2}}{2} - \\frac{3}{2}\\end{aligned}$$ The value of the line integral along a path joining two points can change if you change the path between them. Mass and Moment CalculationsWe treat coil springs and wires as masses distributed along smooth curves in space. The distribution is described by a continuous density function $d(x, y, z)$ representing mass per unit length. Moment has different meanings in Math and Physics. For example: A slender metal arch, denser at the bottom than top, lies along the semicircle $y^2 + z^2 = 1, z \\geq 0$, in the $yz$-plane (Figure 16.5). Find the center of the arch’s mass if the density at the point $(x, y, z)$ on the arch is $d(x, y, z) = 2 - z$. Solution: We know that $\\bar{x} = 0$ and $\\bar{y} = 0$ because the arch lies in the $yz$-plane with its mass distributed symmetrically about the $z$-axis. To find $\\bar{z}$, we parametrize the circle as$$r(t) = (\\cos t)j + (\\sin t)k, 0 \\leq t \\leq \\pi$$For this parametrization,$$|v(t)| = \\sqrt{(\\frac{dx}{dt})^2 + (\\frac{dy}{dt})^2 + (\\frac{dz}{dt})^2} = 1$$so $ds = |v|dt = dt$. Thus,$$M = \\int_C \\delta ds = \\int_C (2-z) ds = \\int_0^\\pi (2-\\sin t) dt = 2\\pi-2$$$$M_{xy} = \\int_C z\\delta ds = \\frac{8-\\pi}{2}$$$$\\bar{z} = \\frac{M_{xy}}{M} \\approx 0.57$$ Line Integrals in the Plane $$\\int_C f ds = \\lim_{n-&gt;\\infty} \\sum_{k=1}^nf(x_k,y_k)\\Delta s_k$$ Vector Fields and Line Integrals: Work, Circulation, and FluxJust too lazy to add vector symbols while writing fomulas… Make sure you know which is a scalar function and which is a vector function.. Vector Fields(向量场) a vector field is a function that assigns a vector to each point in its domain. A vector field on a three-dimensional domain in space might have a formula like$$\\vec{F}(x,y,z) = M(x,y,z)i + N(x,y,z)j+P(x,y,z)k$$The vector field is continuous if the component functions $M, N$, and $P$ are continuous; it is differentiable if each of the component functions is differentiable. The formula for a field of two-dimensional vectors could look like $$\\vec{F}(x,y) = M(x,y)i + N(x,y)j$$ Recall what we leanred in Chapter13? The tangent vectors $T$ and normal vectors $N$ for a curve in space both form vector fields along the curve. Along a curve $r(t)$ they might have a component formula similar to the velocity field expression$$v(t) = f(t)i+g(t)j+h(t)k$$Vector Functions Tangents, Normals in Curve &gt;&gt; Gradient Fields(梯度场)More about Gradient Vectors and Several-Variables Functions &gt;&gt; The gradient vector of a differentiable scalar-valued function at a point gives the direction of greatest increase of the function. We define the gradient field of a differentiable function $ƒ(x, y, z)$ to be the field of gradient vectors $$\\nabla f = \\frac{\\partial f}{\\partial x} i + \\frac{\\partial f}{\\partial y} i + \\frac{\\partial f}{\\partial z} i$$ At each point $(x, y, z)$, the gradient field gives a vector pointing in the direction of greatest increase of $ƒ$, with magnitude being the value of the directional derivative in that direction.(注意，梯度是向量，方向导数是标量) In many physical applications, $ƒ$ represents a potential energy, and the gradient vector field indicates the corresponding force. In such situations, $ƒ$ is often taken to be negative, so that the force gives the direction of decreasing potential energy. For example: Suppose that a material is heated, that the resulting temperature $T$ at each point $(x, y, z)$ in a region of space is given by $$T = 100 - x^2 - y^2 - z^2,$$and that $\\vec{F}(x, y, z)$ is defined to be the gradient of $T$. Find the vector field $\\vec{F}$. Solution: The gradient field $F$ is the field $F = \\nabla T = -2x i - 2y j - 2z k$. Line Integrals of Vector Fields(向量场(函数)的曲线积分)The line integral of the vector field is the line integral of the scalar tangential component of $\\vec{F}$ along $C$. This tangential component is given by the dot product$$\\vec{F} \\cdot \\vec{T} = \\vec{F} \\cdot \\frac{d\\vec{r}}{ds}$$where,$$\\vec{T} = \\frac{d\\vec{r}}{ds} = \\frac{\\vec{v}}{|\\vec{v}|}, \\vec{v} = \\frac{d\\vec{r}}{dt}$$ DEFINITIONLet $\\vec{F}$ be a vector field with continuous components defined along a smooth curve $C$ parametrized by $\\vec{r}(t)$, $a \\leq t \\leq b$. Then the line integral of $\\vec{F}$ along $C$ is$$\\begin{aligned} \\int_C \\vec{F} \\cdot \\vec{T} ds &amp;= \\int_C(\\vec{F} \\cdot \\frac{d\\vec{r}}{ds}) ds\\\\ &amp;= \\int_C \\vec{F}\\cdot d\\vec{r}\\end{aligned}\\tag{1}$$ Evaluating the Line Integral of $\\vec{F} = M i + N j + Pk$ Along $C: \\vec{r}(t) = g(t)i + h(t) j + k(t)k$ Express the vector field $F$ along the parametrized curve $C$ as $F(r(t))$ by substituting the components $x = g(t), y = h(t), z = k(t)$ of r into the scalar components $M(x,y,z), N(x,y,z), P(x,y,z)$ of F. Find the derivative (velocity) vector $dr/dt$. Evaluate the line integral with respect to the parameter $t$, $a \\leq t \\leq b$, to obtain$$\\int_C \\vec{F} \\cdot d\\vec{r} = \\int_a^b F(r(t)) \\cdot \\frac{dr}{dt}dt$$ For example: Evaluate $\\int_C \\vec{F} \\cdot d\\vec{r}$, where $F(x, y, z) = z i + xy j - y^2 k$ along the curve $C$ given by $r(t) = t^2 i + t j + \\sqrt{t} k$, $0 \\leq t \\leq 1$ and shown in Figure 16.18. Solution: $$F(r(t)) = \\sqrt{t}i + t^3j-t^2k$$ $$\\frac{dr}{dt} = 2ti + j + \\frac{1}{2\\sqrt{t}}k$$ Thus, $$\\int_C F \\cdot dr = \\int_0^1 (2t^{3/2} + t^3 - \\frac{1}{2}t^{3/2})dt = \\frac{17}{20}$$ Line Integrals with Respect to $dx, dy$, or $dz$$$\\int_C M(x,y,z)dx = \\int_C \\vec{F} \\cdot d\\vec{r}, where, \\vec{F} = M(x,y,z)i.$$ $$\\int_C M(x,y,z)dx = \\int_a^b M(g(t),h(t),k(t))g’(t)dt\\tag{3}$$$$\\int_C N(x,y,z)dy = \\int_a^b N(g(t),h(t),k(t))h’(t)dt\\tag{4}$$$$\\int_C P(x,y,z)dz = \\int_a^b P(g(t),h(t),k(t))k’(t)dt\\tag{5}$$ Work Done by a Force over a Curve in Space We can approximate the work as$$W \\approx \\sum_{k=1}^n W_k \\approx \\sum_{k=1}^n F(x_k, y_k, z_k) \\cdot T(x_k, y_k, z_k) \\Delta s_k$$as $n\\rightarrow \\infty$ and $\\Delta s_k \\rightarrow 0$, these sums approach the line integral$$\\int_C F\\cdot T ds$$ DEFINITIONLet $C$ be a smooth curve parametrized by $r(t)$, $a \\leq t \\leq b$, and let $F$ be a continuous force field over a region containing $C$. Then the work done in moving an object from the point $A = r(a)$ to the point $B = r(b)$ along $C$ is$$W = \\int_C F \\cdot T ds = \\int_a^b F(r(t)) \\cdot \\frac{dr}{dt}dt\\tag{6}$$ For example: Find the work done by the force field $F = (y - x^2)i + (z - y^2)j + (x - z^2)k$ in moving an object along the curve $r(t) = t i + t^2j + t^3k$, $0 \\leq t \\leq 1$, from $(0, 0, 0)$ to $(1, 1, 1)$ (Figure 16.21). Solution: $$F = (t^3 - t^4) j + (t - t^6)k$$ $$\\frac{dr}{dt} = i + 2t j + 3t^2 k$$ $$work = \\int_a^b F \\cdot \\frac{dr}{dt} dt = \\frac{29}{60}$$ Flow Integrals and Circulation for Velocity Fields(速度场的流量积分和循环) DEFINITIONIf $r(t)$ parametrizes a smooth curve $C$ in the domain of a continuous velocity field $F$, the flow along the curve from $A = r(a)$ to $B = r(b)$ is$$Flow = \\int_C F \\cdot T ds\\tag{7}$$The integral is called a flow integral. If the curve starts and ends at the same point, so that $A = B$, the flow is called the circulation around the curve. Flux Across a Simple Closed Plane Curve(简单封闭平面曲线上的通量)When a curve starts and ends at the same point, it is a closed curve or loop. DEFINITIONIf $C$ is a smooth simple closed curve in the domain of a continuous vector field $F = M(x, y)i + N(x, y)j$ in the plane, and if $n$ is the outwardpointing unit normal vector on $C$, the flux of $F$ across $C$ is$$Flux-of-F-across-C = \\int_C F \\cdot n ds\\tag{8}$$ Notice the difference between flux and circulation. $$n = T \\times k = (\\frac{dx}{ds}i + \\frac{dy}{ds}j) \\times k = \\frac{dy}{ds}i-\\frac{dx}{ds}j$$$$F \\cdot n = M(x,y)\\frac{dy}{ds} - N(x,y)\\frac{dx}{ds}$$Hence,$$\\int_C F \\cdot n ds = \\int_C (M(x,y)\\frac{dy}{ds} - N(x,y)\\frac{dx}{ds}) ds = \\oint_C Mdy - Ndx$$We put a directed circle $\\oint$ on the last integral as a reminder that the integration around the closed curve $C$ is to be in the counterclockwise direction. Calculating Flux Across a Smooth Closed Plane Curve$$Flux-of-F = Mi + Nj \\space across \\space C =\\oint Mdy - Ndx\\tag{9}$$The integral can be evaluated from any smooth parametrization $x = g(t), y = h(t), a \\leq t \\leq b$, that traces $C$ counterclockwise exactly once. Path Independence, Conservative Fields, and Potential FunctionsPath Independence DEFINITIONSLet $F$ be a vector field defined on an open region $D$ in space, and suppose that for any two points $A$ and $B$ in $D$ the line integral $\\int_C F \\cdot dr$ along a path $C$ from $A$ to $B$ in $D$ is the same over all paths from $A$ to $B$. Then the integral $\\int_C F \\cdot dr$ is path independent in $D$ and the field $F$ is conservative on $D$.Sometimes we write $\\int_C$ as $\\int_A^B$ if it’s conservative. DEFINITIONIf $F$ is a vector field defined on $D$ and $F = \\nabla ƒ$ for some scalar function $ƒ$ on $D$, then $ƒ$ is called a potential function(势函数) for $F$. A gravitational potential is a scalar function whose gradient field is a gravitational field, an electric potential is a scalar function whose gradient field is an electric field, andso on. we can evaluate all the line integrals in the domain of $F$ over any path between $A$ and $B$ by$$\\int_A^B F \\cdot dr = \\int_A^B \\nabla f \\cdot dr = f(B) - f(A)\\tag{1}$$ If you think of $\\nabla f$ for functions of several variables as analogous to the derivative ƒ’ for functions of a single variable Similar to $\\int_a^b f’(x) dx = f(b) - f(a)$ right? Assumptions on Curves, Vector Fields, and Domains The domains $D$ we consider are connected. For an open region, this means that any two points in $D$ can be joined by a smooth curve that lies in the region. Some results require $D$ to be simply connected, which means that every loop in $D$ can be contracted to a point in $D$ without ever leaving $D$. Line Integrals in Conservative Fields THEOREM 1—Fundamental Theorem of Line IntegralsLet $C$ be a smooth curve joining the point $A$ to the point $B$ in the plane or in space and parametrized by $r(t)$. Let $ƒ$ be a differentiable function with a continuous vector $F = \\nablaƒ$ on a domain $D$ containing $C$. Then$$\\int_C F \\cdot dr = f(B) - f(A)$$ THEOREM 2—Conservative Fields are Gradient Fields$F = Mi + Nj + Pk$ be a vector field whose components are continuous throughout an open connected region $D$ in space. Then $F$ is conservative if and only if $F$ is a gradient field $\\nabla ƒ$ for a differentiable function $ƒ$. THEOREM 3—Loop Property of Conservative FieldsThe following statements are equivalent. $\\oint_C F \\cdot dr = 0$ around every loop (that is, closed curve $C$ ) in $D$. The field $F$ is conservative on $D$. Finding Potentials for Conservative Fields Component Test for Conservative FieldsLet $F = M(x, y, z)i + N(x, y, z)j + P(x, y, z)k$ be a field on an open simply connected domain whose component functions have continuous first partial derivatives. Then, $F$ is conservative if and only if$$\\frac{\\partial P}{\\partial y} = \\frac{\\partial N}{\\partial z},\\\\\\frac{\\partial M}{\\partial z} = \\frac{\\partial P}{\\partial x},\\\\\\frac{\\partial N}{\\partial x} = \\frac{\\partial M}{\\partial y},\\tag{2}$$ Exact Differential FormsGreen’s Theorem in the PlaneSpin Around an Axis: The k-Component of CurlDivergenceTwo Forms for Green’s TheoremUsing Green’s Theorem to Evaluate Line IntegralsProof of Green’s Theorem for Special RegionsSurfaces and AreaParametrizations of SurfacesSurface AreaImplicit SurfacesSurface IntegralsSurface IntegralsOrientation of aSurface Integrals of Vector FieldsComputing a Surface Integral for a Parametrized SurfaceComputing a Surface Integral for a Level SurfaceMoments and Masses of Thin ShellsStokes’ Theorem(斯托克斯定理)The Curl Vector FieldStokes’ TheoremPaddle Wheel Interpretation of $\\Delta \\times F$Proof Outline of Stokes’ Theorem for Polyhedral SurfacesStokes’ Theorem for Surfaces with HolesAn Important IdentityConservative Fields and Stokes’ TheoremThe Divergence Theorem and a Unified TheoryDivergence in Three DimensionsDivergence TheoremDivergence and the CurlProof of the Divergence Theorem for Special RegionsDivergence Theorem for Other RegionsGauss’s Law: One of the Four Great Laws of Electromagnetic TheoryContinuity Equation of HydrodynamicsUnifying the Integral Theorems","link":"/Math/Calculus/Calculus-C16-Integrals-and-Vector-Fields/"},{"title":"Calculus-C4-Applications-of-Derivatives","text":"Keywords: Applied Optimization, Concavity and Curve Sketching, Newton’s Method, The Mean Value Theorem This is the Chapter4 ReadingNotes from book Thomas Calculus 14th. Extreme Values of Functions on Closed Intervals DEFINITIONSLet $ƒ$ be a function with domain $D$. Then $ƒ$ has an absolute maximum value on $D$ at a point $c$ if$$f(x) \\leq f(c), for \\space x \\space in \\space D$$and an absolute minimum value on $D$ at $c$ if$$f(x) \\geq f(c), for \\space x \\space in \\space D$$Maximum and minimum values are called extreme values of the function $ƒ$. Absolute maxima or minima are also referred to as global maxima or minima. THEOREM 1—The Extreme Value TheoremIf $ƒ$ is continuous on a closed interval $[a,b]$, then $ƒ$ attains both an absolute maximum value $M$ and an absolute minimum value $m$ in $[a,b]$. That is, there are numbers $x_1$ and $x_2$ in $[a,b]$ with $ƒ(x_1) = m, ƒ(x_2) = M$, and $m \\leq ƒ(x) \\leq M$ for every other $x$ in $[a,b]$. Local (Relative) Extreme Values DEFINITIONSA function $ƒ$ has a local maximum value at a point $c$ within its domain $D$ if $ƒ(x) \\leq ƒ(c)$ for all $x \\in D$ lying in some open interval containing $c$.A function $ƒ$ has a local minimum value at a point $c$ within its domain $D$ if $ƒ(x) \\geq ƒ(c)$ for all $x \\in D$ lying in some open interval containing $c$. Finding Extrema THEOREM 2—The First Derivative Theorem for Local Extreme ValuesIf $ƒ$ has a local maximum or minimum value at an interior point $c$ of its domain, and if $ƒ’$ is defined at $c$, then$$f’(c) = 0$$ DEFINITIONAn interior point of the domain of a function $ƒ$ where $ƒ’$ is zero or undefined is a critical point（临界点） of $ƒ$. The Mean Value TheoremRolle’s Theorem（罗尔定理） THEOREM 3—Rolle’s TheoremSuppose that $y = ƒ(x)$ is continuous over the closed interval $[a,b]$ and di£erentiable at every point of its interior $(a, b)$. If $ƒ(a) = ƒ(b)$, then there is at least one number $c$ in $(a, b)$ at which $ƒ’(c) = 0$. The Mean Value Theorem（中值定理） THEOREM 4—The Mean Value TheoremSuppose $y = ƒ(x)$ is continuous over a closed interval $[a,b]$ and differentiable on the interval’s interior $(a, b)$. Then there is at least one point $c$ in $(a, b)$ at which$$\\frac{f(b) - f(a)}{b-a} = f’(c)$$ A Physical InterpretationWe can think of the number $\\frac{f(b) - f(a)}{b-a}$ as the average change in $ƒ$ over $[a,b]$ and $ƒ’(c)$ as an instantaneous change. Then the Mean Value Theorem says that the instantaneous change at some interior point is equal to the average change over the entire interval. Finding Velocity and Position from AccelerationMonotonic Functions and the First Derivative TestIncreasing Functions and Decreasing Functions COROLLARY 3Suppose that $ƒ$ is continuous on $[a,b]$ and di£erentiable on $(a, b)$.If $ƒ’(x) &gt; 0$ at each point x \\in (a, b), then $ƒ$ is increasing on $[a,b]$.If $ƒ’(x) &lt; 0$ at each point $x \\in (a, b)$, then $ƒ$ is decreasing on $[a,b]$. First Derivative Test for Local Extrema First Derivative Test for Local ExtremaSuppose that $c$ is a critical point of a continuous function $ƒ$, and that $ƒ$ is differentiable at every point in some interval containing $c$ except possibly at $c$ itself. Moving across this interval from left to right, if ƒ’ changes from negative to positive at $c$, then $ƒ$ has a local minimum at $c$; if $ƒ’$ changes from positive to negative at $c$, then $ƒ$ has a local maximum at $c$; if $ƒ’$ does not change sign at $c$ (that is, ƒ’ is positive on both sides of $c$ or negative on both sides), then $ƒ$ has no local extremum at $c$. Concavity and Curve SketchingConcavity DEFINITIONThe graph of a differentiable function $y = ƒ(x)$ is(a) concave up on an open interval $I$ if $ƒ’$ is increasing on $I$;(b) concave down on an open interval $I$ if $ƒ’$ is decreasing on $I$. A function whose graph is concave up is also often called convex. The Second Derivative Test for ConcavityLet $y = ƒ(x)$ be twice-di£erentiable on an interval $I$. If $ƒ’’&gt; 0$ on $I$, the graph of $ƒ$ over $I$ is concave up. If $ƒ’’ &lt; 0$ on $I$, the graph of $ƒ$ over $I$ is concave down. Points of Inflection DEFINITIONA point $(c, ƒ(c))$ where the graph of a function has a tangent line and where the concavity changes is a point of inflection（拐点）.At a point of inflection $(c, ƒ(c))$, either $ƒ’’(c) = 0$ or $ƒ’’(c)$ fails to exist. Second Derivative Test for Local Extrema THEOREM 5—Second Derivative Test for Local Extrema If $ƒ’(c) = 0$ and $ƒ’’(c) &lt; 0$, then $ƒ$ has a local maximum at $x = c$. If $ƒ’(c) = 0$ and ƒ’’(c) &gt; 0, then $ƒ$ has a local minimum at $x = c$. If $ƒ’(c) = 0$ and $ƒ’’(c) = 0$, then the test fails. The function $ƒ$ may have a local maximum, a local minimum, or neither. The following figure summarizes how the first derivative and second derivative affect the shape of a graph. Indeterminate Forms and L’Hôpital’s Rule（洛必达法则）Indeterminate Form 0/0 THEOREM 6—L’Hôpital’s RuleSuppose that $ƒ(a) = g(a) = 0$, that $ƒ$ and $g$ are differentiable on an open interval $I$ containing $a$, and that $g’(x) \\neq 0$ on $I$ if $x \\neq a$. Then$$\\lim_{x\\rightarrow a}\\frac{f(x)}{g(x)} = \\lim_{x\\rightarrow a}\\frac{f’(x)}{g’(x)}$$ Applied Optimization Solving Applied Optimization Problems Read the problem. Read the problem until you understand it. What is given? What is the unknown quantity to be optimized? Draw a picture. Label any part that may be important to the problem. Introduce variables. List every relation in the picture and in the problem as an equation or algebraic expression, and identify the unknown variable. Write an equation for the unknown quantity. If you can, express the unknown as a function of a single variable or in two equations in two unknowns. This may require considerable manipulation. Test the critical points and endpoints in the domain of the unknown. Use what you know about the shape of the function’s graph. Use the first and second derivatives to identify and classify the function’s critical points. Examples from Mathematics and PhysicsFor example: The speed of light depends on the medium through which it travels, and is generally slower in denser media. Fermat’s principle in optics states that light travels from one point to another along a path for which the time of travel is a minimum. Describe the path that a ray of light will follow in going from a point $A$ in a medium where the speed of light is $c_1$ to a point $B$ in a second medium where its speed is $c_2$. Solution: $$t_1 = \\frac{AP}{c_1} = \\frac{\\sqrt{a^2+x^2}}{c_1}\\\\t_2 = \\frac{PB}{c_2} = \\frac{\\sqrt{v^2 + (d-x)^2}}{c_2}\\\\t = t_1 + t_2$$ This equation expresses $t$ as a differentiable function of $x$ whose domain is $[0,d]$. We want to find the absolute minimum value of $t$ on this closed interval. We find the derivative $$\\frac{dt}{dx} = \\frac{x}{c_1\\sqrt{a^2+x^2}} - \\frac{d-x}{c_2\\sqrt{b^2+(d-x)^2}}$$ and observe that it is continuous. In terms of the angles $\\theta_1$ and $\\theta_2$ in Figure 4.43,$$\\frac{dt}{dx} = \\frac{\\sin\\theta_1}{c_1} - \\frac{\\sin\\theta_2}{c_2}$$ The function $t$ has a negative derivative at $x = 0$ and a positive derivative at $x = d$. Since $dt/dx$ is continuous over the interval [0,d], by the Intermediate Value Theorem for continuous functions, there is a point $x \\in [0,d]$, where $dt/dx = 0$ $$\\frac{\\sin\\theta_1}{c_1} = \\frac{\\sin\\theta_2}{c_2}$$ This equation is Snell’s Law or the Law of Refraction, Newton’s MethodProcedure for Newton’s MethodApplying Newton’s MethodThe goal of Newton’s method for estimating a solution of an equation $ƒ(x) = 0$ is to produce a sequence of approximations that approach the solution. Newton’s Method Guess a first approximation to a solution of the equation $ƒ(x) = 0$. A graph of $y = ƒ(x)$ may help. Use the first approximation to get a second, the second to get a third, and so on, using the formula$$x_{n+1} = x_n - \\frac{f(x_n)}{f’(x_n)}, if f’(x_n) \\neq 0.$$ Convergence of the ApproximationsIn practice, Newton’s method usually gives convergence with impressive speed, but this is not guaranteed. One way to test convergence is to begin by graphing the function to estimate a good starting value for $x_0$. You can test that you are getting closer to a zero of the function by checking that $|ƒ(x_n)|$ is approaching zero, and you can check that the approximations are converging by evaluating $|x_n - x_{n+1}|$. Newton’s method does not always converge. For instance, if$$f(x) =\\begin{cases}-\\sqrt{r-x}, x &lt; r\\\\\\sqrt{x-r}, x \\geq r\\end{cases}$$ there are other fails: To be added…","link":"/Math/Calculus/Calculus-C4-Applications-of-Derivatives/"},{"title":"Calculus-C7-Integrals-and-Transcendental-Functions","text":"Keywords: Separable Differential Equations This is the Chapter7 ReadingNotes from book Thomas Calculus 14th. Exponential Change and Separable Differential EquationsExponential ChangeSeparable Differential Equations","link":"/Math/Calculus/Calculus-C7-Integrals-and-Transcendental-Functions/"},{"title":"GameProgramming-CSVFile","text":"Keywords: Singleton, Resource Management","link":"/GameProgramming/System/GameProgramming-System-CSV/"},{"title":"Calculus-C6-Applications-of-Definite-Integrals","text":"Keywords: Arc Length, Work and Fluid Forces, Moments and Centers of Mass This is the Chapter6 ReadingNotes from book Thomas Calculus 14th. Arc LengthWe divide the curve into many pieces, and we approximate each piece by a straight-line segment.The total length of the curve is the limiting value of these approximations as the number of segments goes to infinity. Length of a Curve $y = ƒ(x)$ If we set $\\Delta x_k = x_k - x_{k-1}$ and $\\Delta y_k = y_k - y_{k-1}$, then a representative line segment in the path has length (see Figure 6.23) $$L_k = \\sqrt{(\\Delta x_k)^2 + (\\Delta y_k)^2}$$ so the length of the curve is approximated by the sum $$\\sum_{k=1}^n L_k = \\sum_{k=1}^n \\sqrt{(\\Delta x_k)^2 + (\\Delta y_k)^2}\\tag{1}$$ In order to evaluate this limit, we use the Mean Value Theorem, which tells us that there is a point $c_k$ , with $x_{k-1} &lt; c_k &lt; x_k$, such that$$\\Delta y_k = f’(c_k)\\Delta x_k$$Substituting this for $\\Delta y_k$, the sums in Equation (1) take the form$$\\sum_{k=1}^n L_k = \\sum_{k=1}^n \\sqrt{(\\Delta x_k)^2 + (f’(c_k)\\Delta x_k)^2}=\\Delta x_k \\sum_{k=1}^n \\sqrt{1 + [f’(c_k)]^2}\\tag{2}$$ This is a Riemann sum whose limit we can evaluate. Because $\\sqrt{1 + [f’(c_k)]^2}$ is continuous on $[a,b]$, the limit of the Riemann sum on the right-hand side of Equation (2) exists and has the value$$\\lim_{n\\rightarrow \\infty} \\sum_{k=1}^n L_k = \\lim_{n\\rightarrow \\infty} \\Delta x_k \\sum_{k=1}^n \\sqrt{1 + [f’(c_k)]^2} =\\int_a^b \\sqrt{1 + [f’(x)]^2} dx$$ DEFINITIONIf $ƒ’$ is continuous on $[a,b]$, then the length (arc length) of the curve $y = ƒ(x)$ from the point $A = (a, ƒ(a))$ to the point $B = (b, ƒ(b))$ is the value of the integral$$L = \\int_a^b \\sqrt{1 + [f’(x)]^2} dx = \\int_a^b \\sqrt{1 + [\\frac{dy}{dx}]^2} dx$$ Work and Fluid ForcesMoments and Centers of MassTo be added…","link":"/Math/Calculus/Calculus-C6-Applications-of-Definite-Integrals/"},{"title":"Calculus-C8-Techniques-of-Integration","text":"Keywords: Numerical Integration, Simpson’s Rule This is the Chapter8 ReadingNotes from book Thomas Calculus 14th. Integration by PartsProduct Rule in Integral FormIf $u$ and $v$ are differentiable functions of $x$, the Product Rule says that $$\\frac{d}{dx}\\left[ u(x)v(x) \\right] = u’(x)v(x) + u(x)’v(x)$$In terms of indefinite integrals, this equation becomes$$\\begin{aligned}\\int \\frac{d}{dx}\\left[ u(x)v(x)\\right]dx &amp;= \\int \\left[u’(x)v(x) + u(x)’v(x)\\right]dx \\\\&amp;= \\int u’(x)v(x) dx + \\int u(x)’v(x) dx \\\\\\Longrightarrow\\int u(x)’v(x) dx &amp;= \\int \\frac{d}{dx}\\left[ u(x)v(x)\\right]dx - \\int u’(x)v(x) dx\\end{aligned}$$ Integration by Parts Formula$$\\int u(x) v’(x) dx = u(x)v(x) - \\int v(x)u’(x)dx\\tag{1}$$ With $v’(x) dx = dv$ and $u’(x) dx = du$, the integration by parts formula becomes $$\\int udv = uv - \\int vdu$$ Evaluating Definite Integrals by PartsThe integration by parts formula in Equation (1) can be combined with Part 2 of the FundamentalTheorem in order to evaluate definite integrals by parts. Assuming that both $u’$ and $y’$ are continuous over the interval $[a,b]$, Part 2 of the Fundamental Theorem gives Integration by Parts Formula for Definite Integrals$$\\int_a^b u(x)v’(x) dx = \\left. u(x)v(x) \\right|_z^b - \\int_a^b v(x)u’(x)dx$$ Numerical IntegrationWhen we cannot find a workable antiderivative（不定积分）for a function $ƒ$ that we have to integrate, we can partition the interval of integration, replace $ƒ$ by a closely fitting polynomial on each subinterval, integrate the polynomials, and add the results to approximate the definite integral of $ƒ$. This procedure is an example of numerical integration. Trapezoidal Approximations（梯形近似） The Trapezoidal RuleTo approximate $\\int_a^b f(x) dx$, use$$T = \\frac{\\Delta x}{2}(y_0 + 2y_1 + 2y_2 + \\cdots + y_n)$$The y’s are the values of $ƒ$ at the partition points$$x_0 = a, x_1 = a + \\Delta x, x_2 = a + 2 \\Delta x, \\cdots, x_{n-1} = a + (n-1)\\Delta x, x_n = b,$$where, $\\Delta x = (b-a)/n$ Simpson’s Rule: Approximations Using Parabolas(抛物线近似)Another rule for approximating the definite integral of a continuous function results from using parabolas instead of the straight-line segments that produced trapezoids. Simpson’s RuleTo approximate $\\int_a^b f(x) dx$, use$$S = \\frac{\\Delta x}{3}(y_0 + 4y_1 + 4y_2 + \\cdots + 2y_{n-2} + 4y_{n-1} + y_n)$$The y’s are the values of $ƒ$ at the partition points$$x_0 = a, x_1 = a + \\Delta x, x_2 = a + 2 \\Delta x, \\cdots, x_{n-1} = a + (n-1)\\Delta x, x_n = b,$$where, $\\Delta x = (b-a)/n$ Error AnalysisTo be added…","link":"/Math/Calculus/Calculus-C8-Techniques-of-Integration/"},{"title":"Calculus-C5-Integrals","text":"Keywords: Riemann Sums, Mean Value Theorem This is the Chapter5 ReadingNotes from book Thomas Calculus 14th. Sigma Notation and Limits of Finite SumsRiemann Sums The set of all of these points,$$P = \\lbrace x_0, x_1, \\cdots, x_n \\rbrace$$is called a partition $P$ of $[a,b]$.$$S_p = \\sum_{k=1}^n f(c_k)\\Delta x_k$$The sum $S_P$ is called a Riemann sum for $ƒ$ on the interval $[a,b]$. we could choose $n$ subintervals all having equal width $\\Delta x = (b-a) / n$ to partition $[a,b]$, and then choose the point $c_k$ to be the right-hand endpoint of each subinterval when forming the Riemann sum. This choice leads to the Riemann sum formula$$S_n = \\sum_{k=1}^n f(a + k\\frac{(b-a)}{n}) \\cdot (\\frac{b-a}{n})$$ We define the norm（范数） of a partition $P$, written $||P||$, to be the largest of all the subinterval widths. If $||P||$ is a small number, then all of the subintervals in the partition $P$ have a small width. 范数(norm)是数学中的一种基本概念。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。 The Definite IntegralDefinition of the Definite Integral DEFINITIONLet $ƒ(x)$ be a function defined on a closed interval [a,b]. We say that a number $J$ is the definite integral of $ƒ$ over $[a,b]$ and that $J$ is the limit of the Riemann sums $\\sum_{k=1}^n f(c_k)\\Delta x_k$ if the following condition is satisfied:Given any number $\\epsilon &lt; 0$ there is a corresponding number $\\delta &gt; 0$ such that for every partition $P = \\lbrace x_0, x_1, \\cdots, x_n\\rbrace$ of $[a,b]$ with $||P|| &lt; \\delta$ and any choice of $c_k$ in $[x_{k-1},x_k]$, we have$$|\\sum_{k=1}^{n}f(c_k)\\Delta x_k - J| &lt; \\epsilon$$where, $J = \\lim_{||P||-&gt;0} \\sum_{k=1}^{n}f(c_k)\\Delta x_k$ A Formula for the Riemann Sum with Equal-Width Subintervals$$\\int_a^b f(x) dx = \\lim_{n-&gt;\\infty} \\sum_{k=1}^n f(a + k \\frac{b-a}{n})(\\frac{b-a}{n})\\tag{1}$$ Integrable and Nonintegrable Functions THEOREM 1—Integrability of Continuous FunctionsIf a function $ƒ$ is continuous over the interval $[a,b]$, or if $ƒ$ has at most finitely many jump discontinuities there, then the definite integral $\\int_a^b f(x)dx$ exists and $ƒ$ is integrable over $[a,b]$. Properties of Definite Integrals Area Under the Graph of a Nonnegative FunctionTheorem 1 guarantees that all of these Riemann sums converge to a single definite integral as the norm of the partitions approaches zero and the number of subintervals goes to infinity. DEFINITIONIf $y = ƒ(x)$ is nonnegative and integrable over a closed interval $[a, b]$, then the area under the curve $y = ƒ(x)$ over $[a,b]$ is the integral of $ƒ$ from $a$ to $b$,$$A = \\int_a^b f(x)dx$$ Average Value of a Continuous Function Revisited DEFINITIONIf ƒ is integrable on $[a,b]$, then its average value on $[a,b]$, which is also called its mean, is$$av(f) = \\frac{1}{b-a}\\int_a^b f(x)dx$$ The Fundamental Theorem of CalculusFundamental Theorem of Calculus, which is the central theorem of integral calculus. It connects integration and differentiation, enabling us to compute integrals by using an antiderivative of the integrand function rather than by taking limits of Riemann sums. Mean Value Theorem for Definite Integrals(定积分的中值定理) THEOREM 3—The Mean Value Theorem for Definite IntegralsIf $ƒ$ is continuous on $[a,b]$, then at some point $c$ in [a,b],$$f(c) = \\frac{1}{b-a}\\int_a^b f(x) dx$$ Fundamental Theorem, Part 1If $ƒ(t)$ is an integrable function over a finite interval $I$, then the integral from any fixed number $a \\in I$ to another number $x \\in I$ defines a new function $F$ whose value at $x$ is$$F(x) = \\int_a^xf(t)dt\\tag{1}$$ It is reasonable to expect that $F’(x)$, which is the limit of this difference quotient as $h\\rightarrow 0$, equals ƒ(x), so that $$F’(x) = \\lim_{h\\rightarrow 0} \\frac{F(x+h) - F(x)}{h} = f(x)$$ THEOREM 4—The Fundamental Theorem of Calculus, Part 1If $ƒ$ is continuous on [a,b], then $F(x) = \\int_a^x ƒ(t) dt$ is continuous on $[a,b]$ and differentiable on $(a,b)$ and its derivative is $ƒ(x)$:$$F’(x) = \\frac{d}{dx}\\int_a^xf(t)dt = f(x)\\tag{2}$$ Fundamental Theorem, Part 2 (The Evaluation Theorem) THEOREM 4 (Continued)—The Fundamental Theorem of Calculus, Part 2If $ƒ$ is continuous over [a,b], and $F$ is any antiderivative of $ƒ$ on [a,b], then$$\\int_a^b f(x)dx = F(b) - F(a)$$ The Integral of a Rate THEOREM 5—The Net Change TheoremThe net change in a differentiable function $F(x)$ over an interval $a \\leq x \\leq b$ is the integral of its rate of change:$$F(b) - F(a) = \\int_a^bF’(x)dx\\tag{6}$$ The Relationship Between Integration and DifferentiationThe conclusions of the Fundamental Theorem tell us several things. Equation (2) can be rewritten as $$\\frac{d}{dx}\\int_a^xf(t)dt = f(x)$$ which says that if you first integrate the function $ƒ$ and then differentiate the result, you get the function $ƒ$ back again. Likewise, replacing $b$ by $x$ and $x$ by $t$ in Equation (6) gives$$\\int_a^x F’(t)dt = F(x)-F(a)$$so that if you first differentiate the function $F$ and then integrate the result, you get the function $F$ back (adjusted by an integration constant). In a sense, the processes of integration and differentiation are “inverses” of each other. Indefinite Integrals and the Substitution MethodThe connection between antiderivatives and the definite integral stated in the Fundamental Theorem now explains this notation: $$\\begin{aligned}\\int_a^b f(x)dx &amp;= F(b) - F(a) \\\\&amp;= \\left[F(b) + C\\right] - \\left[F(a) + C\\right] \\\\&amp;= \\left[F(x) + C\\right]_a^b \\\\&amp;= \\left[\\int f(x) dx\\right]_a^b\\end{aligned}$$ Substitution: Running the Chain Rule BackwardsIf $u$ is a differentiable function of $x$, then$$du = \\frac{du}{dx} dx$$ THEOREM 6—The Substitution RuleIf $u = g(x)$ is a differentiable function whose range is an interval $I$, and $ƒ$ is continuous on $I$, then$$\\int f(g(x)) \\cdot g’(x)dx = \\int f(u) du$$ Definite Integral Substitutions and the Area Between CurvesThe Substitution Formula THEOREM 7—Substitution in Definite IntegralsIf $g’$ is continuous on the interval $[a,b]$ and $ƒ$ is continuous on the range of $g(x) = u$, then$$\\int_a^b f(g(x))g’(x)dx = \\int_{g(a)}^{g(b)} f(u) du$$ Definite Integrals of Symmetric Functions THEOREM 8 Let $ƒ$ be continuous on the symmetric interval $[-a,a]$.a. If $f$ is even, then $\\int_{-a}^a f(x) dx = 2\\int_0^a f(x)dx$b. If $f$ is odd, then, $\\int_{-a}^a f(x) dx = 0$ Areas Between Curves DEFINITIONIf $ƒ$ and $g$ are continuous with $ƒ(x) \\geq g(x)$ throughout $[a,b]$, then the area of the region between the curves $y = f (x)$ and $y = g(x)$ from $a$ to $b$ is the integral of $(ƒ - g)$ from $a$ to $b$:$$A = \\int_a^b \\left[ f(x) - g(x) \\right] dx$$ To be added…","link":"/Math/Calculus/Calculus-C5-Integrals/"},{"title":"GameProgramming-MVCFramework","text":"Keywords: MVC Framework, TCP Communication","link":"/GameProgramming/System/GameProgramming-System-MVC/"},{"title":"GameProgramming-LadderSeason-Interface","text":"Keywords: Auto Scrolling, Parametric Animation, Sort by Filter The new ladder season is coming and sometimes the ladder system interface should be updated. Mission Auto ScrollingReward PopWindowLadder Settlement AnimationLarge Rank Change AnimationNormal Rank Change AnimationLadderboard SortConceptsBattle Royal Rank: A video game genre that blends elements of survival games with last-man-standing gameplay. Players search for equipment while eliminating competitors in a shrinking safe zone. Usually there are many more players involved than in other kinds of multi-player games.(from Wiki) Clash Squad Rank: always a 4v4 game mode. In this game mode, the goal is to win four out of seven rounds by eliminating other players. At the start of each round, players can stay only within their starting area and purchase weapons and items in the shop. The Balance is given out each round based on what players did in the previous round (with the exception of the first round, where everyone gets the same amount).(from esports)","link":"/GameProgramming/System/GameProgramming-System-LadderSeason/"},{"title":"Geometry-ConvexHull","text":"Keywords: 2D ConvexHull, 3D ConvexHull, Algorithms, Half Plane Intersection, C++ More about Convex Concept&gt;&gt; Andrew AlgorithmHalf Plane IntersectionIncremental Method ConvexHull","link":"/Graphics/Geometry/Geometry-ConvexHull/"},{"title":"Calculus-C9-First-Order-Differential-Equations","text":"Keywords: Euler’s Method, Slope Fields, Autonomous Equations This is the Chapter9 ReadingNotes from book Thomas Calculus 14th. Solutions, Slope Fields, and Euler’s MethodMany differential equations cannot be solved by obtaining an explicit formula for the solution. However, we can often find numerical approximations to solutions. General First-Order Differential Equations and SolutionsA first-order differential equation is an equation$$\\frac{dy}{dx} = f(x,y)\\tag{1}$$in which $ƒ(x, y)$ is a function of two variables defined on a region in the $xy$-plane. In a typical situation $y$ represents an unknown function of $x$, and $ƒ(x, y)$ is a known function. Such as:$$y’ = x + y, y’ = y / x, y’ = 3xy$$ A solution of Equation (1) is a differentiable function y = y(x) defined on an interval $I$ of $x$-values (perhaps infinite) such that$$\\frac{d}{dx}y(x) = f(x,y(x))$$ For example: Show that the function $y = (x+1)-\\frac{1}{3}e^x$ is a solution to the first-order initial value problem（一阶初值问题） $\\frac{dy}{dx} = y-x, y(0) = \\frac{2}{3}$ Solution: The equation$$\\frac{dy}{dx} = y-x$$is a first-order differential equation with $ƒ(x, y) = y - x$. On the left side of the equation:$$\\frac{dy}{dx} = \\frac{d}{dx} (x+1-\\frac{1}{3}e^x) = 1 - \\frac{1}{3}e^x$$ On the Right side of the equation:$$y-x = 1 - \\frac{1}{3}e^x$$ The function satisfies the initial condition because$$y(0) = [(x+1)-\\frac{1}{3}e^x]_{x=0} = \\frac{2}{3}$$The graph of the function is shown in Figure 9.1. Slope Fields: Viewing Solution CurvesEach time we specify an initial condition $y(x_0) = y_0$ for the solution of a differential equation $y’ = ƒ(x, y)$, the solution curve (graph of the solution) is required to pass through the point $(x_0, y_0)$ and to have slope $ƒ(x_0, y_0)$ there. We can picture these slopes graphically by drawing short line segments of slope $ƒ(x, y)$ at selected points $(x, y)$ in the region of the $xy$ plane that constitutes the domain of $ƒ$. Each segment has the same slope as the solution curve through $(x, y)$ and so is tangent to the curve there. The resulting picture is called a slope field (or direction field) and gives a visualization of the general shape of the solution curves. Figure 9.2a shows a slope field, with a particular solution sketched into it in Figure 9.2b. We see how these line segments indicate the direction the solution curve takes at each point it passes through. Slope fields are useful because they display the overall behavior of the family of solution curves for a given differential equation. Euler’s MethodThe basis of Euler’s method is to patch together a string of linearizations to approximate the curve over a longer stretch.（本质上还是数值分析, 迭代逼近） Given a differential equation $dy/dx = ƒ(x, y)$ and an initial condition $y(x_0) = y_0$, we can approximate the solution $y = y(x)$ by its linearization $$L(x) = y(x_0) + y’(x_0)(x-x_0)$$ or $$L(x) = y_0 + f(x_0, y_0)(x-x_0)$$ For example: Use Euler’s method to solve $y’ = 1 + y , y(0) = 1$, on the interval $0 \\leq x \\leq 1$, starting at $x_0 = 0$ and taking $dx = 0.1$, Compare the approximations with the values of the exact solution $y = 2e^x - 1$. Solution: First:$$\\begin{aligned}y_1 &amp;= y_0 + f(x_0,y_0)dx \\\\&amp;= y_0 + (1+y_0)dx\\\\&amp;= 1 + (1+1)(0.1)\\\\&amp;= 1.2\\end{aligned}$$ Second:$$\\begin{aligned}y_2 &amp;= y_1 + f(x_1,y_1)dx \\\\&amp;= y_1 + (1+y_1)dx\\\\&amp;= 1.2 + (1+1.2)(0.1)\\\\&amp;= 1.42\\end{aligned}$$ First-Order Linear EquationsA first-order linear differential equation is one that can be written in the form$$\\frac{dy}{dx} + P(x)y = Q(x)$$where $P$ and $Q$ are continuous functions of $x$. Equation (1) is the linear equation’s standard form Solving Linear Equations$v(x)$ is choosen to make $v\\frac{dy}{dx} + Pvy = \\frac{d}{dx}(v \\cdot y)$: $$\\begin{aligned}&amp;\\frac{dy}{dx} + P(x)y = Q(x)\\\\&amp;v(x)\\frac{dy}{dx} + P(x)v(x)y = v(x)Q(x)\\\\&amp;\\frac{d}{dx}(v(x) \\cdot y) = v(x)Q(x)\\\\&amp;v(x) \\cdot y = \\int v(x)Q(x)dx\\\\&amp;y = \\frac{1}{v(x)} \\int v(x)Q(x)dx\\end{aligned}$$ about $v(x)$: $$\\begin{aligned}&amp;\\frac{d}{dx}(vy) = v\\frac{dy}{dx} + yPv\\\\&amp;v\\frac{dy}{dx} + y\\frac{dv}{dx} = v\\frac{dy}{dx} + yPv\\\\&amp;y\\frac{dv}{dx} = yPv\\\\&amp;\\frac{dv}{dx} = Pv\\\\&amp;\\frac{dv}{v} = Pdx\\\\&amp;\\int\\frac{dv}{v} = \\int Pdx\\\\&amp;\\ln v = \\int Pdx\\\\&amp;e^{\\ln v} = e^{\\int Pdx}\\\\&amp;v = e^{\\int Pdx}\\end{aligned}$$ ApplicationsMotion with Resistance Proportional to VelocityInaccuracy of the Exponential Population Growth ModelWe model the population growth with the Law of Exponential Change:$$\\frac{dP}{dt} = kP, P(0) = P_0$$where $P$ is the population at time $t$, $k &gt; 0$ is a constant growth rate, and $P_0$ is the size of the population at time $t = 0$. The solution is $P = P_0 e^{kt}$, the initial value is$$\\frac{dP}{dt} = 0.017P, P(0) = 4454$$ Orthogonal TrajectoriesGraphical Solutions of Autonomous Equations(自控(自治)微分方程)A differential equation for which $dy / dx$ is a function of $y$ only is called an autonomous differential equation. Logistic Population GrowthWithout considering the carrying capacity of environment, the population growth can be modeled as$$\\frac{dP}{dt} = kP, k-constant\\tag{5}$$ Considering the environment factors, it should be $$k = r(M-P), r-constant$$where, $M$ is the maximum population it can carry. thus,$$\\frac{dP}{dt} = r(M-P)P = rMP-rP^2\\tag{6}$$ The model given by Equation (6) is referred to as logistic growth(对数增长). $$\\begin{aligned}\\frac{d^2P}{d^2t} &amp;= \\frac{d}{dt}(rMP-rP^2)\\\\&amp;= rM\\frac{dP}{dt} - 2rP\\frac{dP}{dt}\\\\&amp;= r(M-2P)\\frac{dP}{dt}\\end{aligned}$$ Systems of Equations and Phase PlanesTo be added…","link":"/Math/Calculus/Calculus-C9-First-Order-Differential-Equations/"},{"title":"Geometry-Curves","text":"Keywords: Parametic Polynomial Curves, Bezier Curves, Spline Curves, Subdivision, C++ This is the Chapter13 ReadingNotes from book 3D Math Primer for Graphics and Game Development 2nd Edition. Parametic Polynomial CurvesParametric CurvesThe word parametric in the phrase “parametric polynomial curve” means that the curve can be described by a function of an independent paramete, which is often assigned the symbol $t$. This curve function is of the form $p(t)$, taking a scalar input (the parameter $t$) and returning the point on the curve corresponding to that parameter value as a vector output. The function $p(t)$ traces out the shape of the curve as $t$ varies. implicit: $x^2 + y^2 = 1$, explicit: $y = \\sqrt{1-x^2}$ parametric:$$\\vec{p(t)} = (x(t), y(t))\\\\\\begin{cases}x(t) = \\cos(2\\pi t)\\\\y(t) = sin(2\\pi t)\\end{cases}$$ Polynomial Curves（多项式曲线）The number $n$ is called the degree of the polynomial.$$\\vec{p(t)} = \\vec{c_0} + \\vec{c_1}t + \\vec{c_2}t^2 + \\cdots + \\vec{c_{n-1}}t^{n-1} + \\vec{c_n}t^n$$ Cubic Curve in Monomial Form$$\\vec{p(t)} = \\vec{c_0} + \\vec{c_1}t + \\vec{c_2}t^2 + \\vec{c_3}t^3\\tag{13.2}$$ Matrix Notation2D cubic curve in expanded monomial form:$$\\begin{cases}x(t) = c_{1,0} + c_{1,1}t + c_{1,2}t^2 + c_{1,3}t^3\\\\y(t) = c_{2,0} + c_{2,1}t + c_{2,2}t^2 + c_{2,3}t^3\\end{cases}$$thus,$$\\begin{aligned}\\vec{p(t)} = C\\vec{t} &amp;=\\begin{bmatrix}c_{1,0} &amp; c_{1,1} &amp; c_{1,2} &amp; c_{1,3}\\\\ c_{2,0} &amp; c_{2,1} &amp; c_{2,2} &amp; c_{2,3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\&amp;=\\begin{bmatrix}\\vec{c_0} &amp;\\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{c_3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\end{aligned}$$ Two Trivial Types of CurvesConsider a ray from the point $p_0$ to the point $p_1$. If we let $\\vec{d}$ be the delta vector $\\vec{p_1} − \\vec{p_0}$, then the ray is expressed parametrically as $$\\vec{p(t)} = \\vec{p_0} + \\vec{d}t\\tag{13.3}$$ let $\\vec{c_0} = \\vec{p_0}$, $\\vec{c1} = \\vec{d}$,this linear curve is a polynomial curve of degree 1. Endpoints in Monomial FormSuppose, $0 \\leq t \\leq 1$, start and end points are $\\vec{p(0)}$ and $\\vec{p(1)}$. $c_0$ specifies the start point; The endpoint is the sum of the coefficients.$$\\begin{aligned}&amp;\\vec{p_0} = \\vec{c_0}\\\\&amp;\\vec{p_1} = \\vec{c_0} + \\vec{c_1} + \\vec{c_2} + \\vec{c_3}\\end{aligned}$$ Velocities and Tangentsthe velocity function $v(t)$ is the first derivative of the position function $p(t)$ because velocity measures the rate of change in position over time. Likewise, the acceleration function $a(t)$ is the derivative of the velocity function $v(t)$ because acceleration measures the rate of change of velocity over time. Velocity and Acceleration of Cubic Monomial Curve$$\\vec{p(t)} = \\vec{c_0} + \\vec{c_1}t + \\vec{c_2}t^2 + \\vec{c_3}t^3\\tag{13.4}$$$$\\vec{v(t)} = \\dot{\\vec{p(t)}} = \\vec{c_1} + 2\\vec{c_2}t + 3\\vec{c_3}t^2\\tag{13.5}$$$$\\vec{a(t)} = \\ddot{\\vec{p(t)}} = 2\\vec{c_2} + 6\\vec{c_3}t\\tag{13.6}$$ Two curves that define the same “shape,” but not the same “path”: $$p(t) = p_0 + dt$$$$Let \\space s(t) = t^2 \\mapsto p(s(t)) = p_0 + dt^2$$ The tangent at a point is the direction the curve is moving at that point, the line that just touches the curve. The tangent is basically the normalized velocity of the curve. $$\\vec{t(t)} = \\widehat{\\vec{v(t)}} = \\frac{\\vec{v(t)}}{||\\vec{v(t)}||}$$ The second derivative is related to curvature, which is sometimes denoted $\\kappa$, $$\\vec{\\kappa(t)} = \\frac{||\\vec{v(t)} \\times \\vec{a(t)}||}{||\\vec{v(t)}||^3}$$ Polynomial Interpolationlinear interpolation: Given two “endpoint” values, create a function that transitions at a constant rate (spatially, in a straight line) from one to the other (is simply first-degree polynomial interpolation). Polynomial interpolation: Given a series of control points, our goal is to construct a polynomial that interpolates them. A polynomial of degree $n$ can be made to interpolate $n + 1$ control points. We use the word “knot” to refer to control points that are interpolated, invoking the metaphor of a rope with knots in it. Aitken’s Algorithm(Geometry View)recursive algorithms, divide and conquer. To solve a difficult problem, we first divide it into two (or more) easier problems, solve the easier problems independently, and then combine the results to get the solution to the harder problem. We split this curve into two “easier” curves: (1) one that interpolates only the first $n − 1$ points, disregarding the last point; and (2) another that interpolates the last $n − 1$ points without worrying about the first point. Then, we blend these two curves together. We let $y_i^1(t)$ denote the linear curve between $y_i$ and $y_{i+1}$, the notation $y_i^2(t)$ denote the quadratic curve between $y_i$ and $y_{i+2}$, and so on Linear interpolation between two control points:$$y_1^1(t) = \\frac{(t_2 - t)y_1 + (t - t_1)y_2}{t_2 - t_1}$$$$y_2^1(t) = \\frac{(t_3 - t)y_2 + (t - t_2)y_3}{t_3 - t_2}$$Linear interpolation of lines yields a quadratic curve $$y_1^2(t) = \\frac{(t_3 - t)[y_1^1(t)] + (t - t_1)[y_2^1(t)]}{t_3 - t_1}$$ Lagrange Basis Polynomials(Math View)Each control point gives us one equation, and each coefficient gives us one unknown. This system of equations can be put into an $n \\times n$ matrix, which can be solved by standard techniques such as Gaussian elimination or $LU$ decomposition. we could create a polynomial for each knot $t_i$ such that the polynomial evaluates to unity at that knot, but for all the other knots it evaluates to zero. $$\\zeta_1(t_1) = 1, \\zeta_2(t_1) = 1, \\zeta_3(t_1) = 1, \\zeta_4(t_1) = 1\\\\\\cdots \\\\\\zeta_1(t_4) = 0, \\zeta_2(t_4) = 0, \\zeta_3(t_4) = 0, \\zeta_4(t_4) = 1\\\\$$ Interpolating polynomial in Lagrange basis form$$p(t) = \\sum_{i=1}^n y_i\\zeta_i(t) =y_1\\zeta_1(t) + y_2\\zeta_2(t) + \\cdots + y_3\\zeta_3(t)\\tag{13.7}$$ Lagrange Basis Polynomial$$\\zeta_i(t) = \\prod_{1 \\leq j \\leq n, j \\neq i} \\frac{t - t_i}{t_i - t_j}$$ Thus, we could calculate: $$\\zeta_1(t) = -(\\frac{9}{2})t^3 + 9t^2 - (\\frac{11}{2})t + 1$$ we scale each basis polynomial by the corresponding coordinate value, adding the scaled basis vectors together yields the interpolating polynomial $P$. $$\\begin{aligned}P(t) &amp;= y_1\\zeta_1(t) + y_2\\zeta_2(t) + \\cdots + y_3\\zeta_3(t)\\\\&amp;= 18t^3 − 27t^2 + 10^t + 2.\\end{aligned}$$ We can think of basis polynomials as functions yielding barycentric coordinates (blending weights). view1: the control points are the building blocks and the basis polynomials provide the scale factors, although we prefer to be more specific and call these scale factors barycentric coordinates. view2: any arbitrary vector can be described as a linear combination of the basis vectors. In our case, the space being spanned by the basis is not a geometric 3D space, but the vector space of all possible polynomials of a certain degree, and the scale values for each curve are the known values of the polynomial at the knots. More about Curve Fitting in PatternRecognition&gt;&gt; Hermite CurvesPolynomial interpolation doesn’t work as well as we would like, because of the tendency to oscillate and overshoot, so let’s try a different approach. Instead of specifying the interior positions to interpolate, let’s control the shape of the curve through the tangents at the endpoints. System of equations for Hermite conditions (cubic curve) $$\\begin{aligned}&amp;p(0) = p_0 \\Longrightarrow c_0 = p_0\\\\&amp;v(0) = v_0 \\Longrightarrow c_1 = v_0\\\\&amp;v(1) = v_1 \\Longrightarrow c_1 + 2c_2 + 3c_3 = v_1\\\\&amp;p(1) = p_1 \\Longrightarrow c_0 + c_1 + c_2 + c_3 = p_1\\end{aligned}$$Converting Hermite form to monomial form:$$\\begin{aligned}&amp;c_0 = p_0\\\\&amp;c_1 = v_0\\\\&amp;c_2 = -3p_0 - 2v_0 - v_1 +3p_1\\\\&amp;c_3 = 2p_0 +v_0 +v_1 -2p_1\\\\\\end{aligned}$$Thus,$$\\begin{aligned}\\vec{p(t)} &amp;= C\\vec{t}\\\\&amp;= \\begin{bmatrix}\\vec{c_0} &amp;\\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{c_3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\&amp;= PH\\vec{t}\\\\&amp;=\\begin{bmatrix}\\vec{p_0} &amp;\\vec{v_0} &amp; \\vec{v_1} &amp; \\vec{p_1}\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; -3 &amp; 2\\\\0 &amp; 1 &amp; -2 &amp; 1\\\\0 &amp; 0 &amp; -1 &amp; 1\\\\0 &amp; 0 &amp; 3 &amp; -2\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\end{aligned}$$$C = PH$, in which case, multiplication by $H$ can be considered a conversion from the Hermite basis to the monomial basis,（this book right multiply） We convert a curve from Hermite form to monomial form by using Equations (13.13)–(13.16), and from monomial form to Hermite form with Equations (13.9)–(13.12). $$\\vec{p(t)} = \\begin{bmatrix}\\vec{p_0} &amp;\\vec{v_0} &amp; \\vec{v_1} &amp; \\vec{p_1}\\end{bmatrix}\\begin{bmatrix}1-3t^2+2t^3 \\\\ t-2t^2+t^3 \\\\ -t^2+t^3 \\\\ 3t^3-2t^3\\end{bmatrix}$$ The curve $H_3(t)$ deserves special attention. It is also is known as the smoothstep function and is truly a gem that every game programmer should know. To remove the rigid, robotic feeling from any linear interpolation (especially camera transitions), simply compute the normalized interpolation fraction $t$ as usual (in the range $0 ≤ t ≤ 1$), and then replace $t with $3t^2 − 2t^3$. The reason for this is that the smoothstep function eliminates the sudden jump in velocity at the endpoints: $H_3’(0) = H_3’(1) = 0$. Bezier CurvesImportantly, B´ezier curves approximate rather than interpolate. The de Casteljau AlgorithmThe de Casteljau algorithm defines a method for constructing B´ezier curves through repeated linear interpolation. De Casteljau Recurrence Relation$$b_i^0(t) = b_i\\\\b_i^n(t) = (1-t)[b_i^{n-1}(t)] + t[b_{i+1}^{n-1}(t)]$$ The cubic equation for a specific point on the curve $p(t)$ is written in matrix notation as $$\\begin{aligned}\\vec{p(t)} &amp;= C\\vec{t}\\\\&amp;= \\begin{bmatrix}\\vec{c_0} &amp;\\vec{c_1} &amp; \\vec{c_2} &amp; \\vec{c_3}\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\\\&amp;= BM\\vec{t}\\\\&amp;=\\begin{bmatrix}\\vec{b_0} &amp;\\vec{b_1} &amp; \\vec{b_2} &amp; \\vec{b_3}\\end{bmatrix}\\begin{bmatrix}1 &amp; -3 &amp; 3 &amp; 1\\\\0 &amp; 3 &amp; -6 &amp; 3\\\\0 &amp; 0 &amp; -3 &amp; -3\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\begin{bmatrix}1 \\\\ t \\\\ t^2 \\\\ t^3\\end{bmatrix}\\end{aligned}$$ The Bernstein BasisWe can also write the polynomial in B´ezier form by collecting the terms on the control points rather than the powers of $t$. When written this way, each control point has a coefficient that represents the barycentric weight as a function of $t$ that the control point contributes to the curve. Now that we get: $$\\begin{aligned}b_0^1(t) &amp;= (1-t)b_0 + tb_1\\\\b_0^2(t) &amp;= (1-t)^2b_0 + 2t(1-t)b_1 + t^2b_2\\\\b_0^3(t) &amp;= (1-t)^3b_0 + 3t(1-t)^2b_1 + 3t^2(1-t)b_2 + t^3b_3\\\\\\cdots\\end{aligned}$$ Thus, we can see the pattern: $$\\begin{aligned}b_0^n(t) &amp;= \\sum_{i=0}^n[B_i^n(t)]b_i\\\\B_i^n(t) &amp;= \\begin{pmatrix}n \\\\ i\\end{pmatrix}t^i(1-t)^{n-i}, 0 \\leq i \\leq n\\\\\\begin{pmatrix}n \\\\ i\\end{pmatrix} &amp;= \\frac{n!}{k!(n-k)!}\\end{aligned}$$ The properties of the Bernstein polynomials tell us a lot about how B´ezier curves behave. Sum to one. The Bernstein polynomials sum to unity for all values of $t$, which is nice because if they didn’t, then they wouldn’t define proper barycentric coordinates. Convex hull property.The range of the Bernstein polynomials is $0 \\cdots 1$ for the entire length of the curve, $0 ≤ t ≤ 1$. The curve is bounded to stay within the convex hull of the control points. Endpoints interpolated. Global support.The practical result is that when any one control point is moved, the entire curve is affected. This is not a desirable property for curve design. One local maximum.Each Bernstein polynomial $B_i^n(t)$, which serves as the blend weight for the control point $b_i$, has one maximum at the auspicious time $t = i/n$. B´ezier Derivatives and Their Relationship to the Hermite FormThe $nth$ derivative at either endpoint is completely determined by the nearest $n + 1$ control points. The second derivative (acceleration) at the end of the curve is determined by the closest three control points. SubdivisionWhy need subdivision? Curve refinement. Approximation techniques. Subdividing Curves in Monomial Formthe problem of subdivision can easily be viewed as a simple problem of reparameterization.$$t = F(s)\\\\q(s) = p(t) = p(F(s))$$$$\\begin{cases}F(0) = a\\\\F(1) = b\\end{cases}\\longmapstot = F(s) = a + s(b-a), s \\in [0,1]$$ Subdividing Curves in B´ezier FormAs it turns out, each round of de Casteljau interpolation produces one of our B´ezier control points.Thus，To extract the left half of a curve, $0 ≤ t ≤ b$, we perform de Casteljau subdivision as if we were trying to locate the endpoint at $t = b$. The first control point from each round of interpolation gives us another control point for our subdivided curve. SplineRules of the Gamethe function $q(s)$ refers to the entire spline, and the parameter $s$ (without subscript) is a global parameter. As $s$ varies from $0$ to $n$, the function $q(s)$ traces out the entire spline. In general, we can define $p(t)$ in terms of $q(s)$ by creating a function that maps a time value $t$ to a parameter value $s$. If you’re a computer programmer, you can think of $p(t)$ as the public interface, and $q(s)$ as an internal implementation detail. Map the time value $t$ into a value of $s$ by evaluating the time-toparameter function $s(t)$. Extract the integer portion of $s$ as $i$, and the fractional portion as $s_i$. Evaluate the curve segment $q_i(s_i)$. KnotsFor the curve to be continuous, clearly the ending point of one segment must be coincident with the starting point of the next segment. These shared control points that are interpolated by the spline are called the knots of the spline. In animation contexts, the knots are sometimes called keys. In computer animation, a key can be any position, orientation, or other piece of data whose value at a particular time is specified by a human animator (or any other source). The role of the apprentice to “fill in the missing frames” is played by the animation program, using interpolation methods such as the ones being discussed in this chapter. Hermite and B´ezier SplinesWhen we were focused on a single segment, we denoted the positions by $p_0$ and $p_1$, and the velocities(tangents) by $v_0$ and $v_1$. the knot $k_i$, which is the starting position of the segment $q_i(0)$, also serves as the ending position of the previous segment at $q_{i−1}(1)$. For velocities, the notation $v^{out}_i$ refers to the outgoing velocity at knot $i$ and defines the starting velocity for the segment $q_i$. (Photoshop calls the knots the “anchor points” and refers to the interior B´ezier control points that are not interpolated as “control points.”) When we were dealing with only a single B´ezier segment, we referred to the $ith$ control point on that segment as $b_i$. Here we use the notation $f_i$ to refer to the control point “in front” of the ith knot, and $a_i$ for the control point “after” it. Converting between B´ezier and Hermite forms:$$\\begin{aligned}&amp;v_i^{in} = 3(k_i - f_i), f_i = k_i - v_i^{in}/3\\\\&amp;v_i^{out} = 3(a_i - k_i), a_i = k_i + v_i^{out}/3\\end{aligned}$$ ContinuityParametric ContinuityA curve is said to have $C^n$ continuity if its first $n$ derivatives are continuous. $C^1$ continuity condition for Hermite splines:$$v_i^{in} = v_i^{out}$$ $C^1$ continuity condition for cubic B´ezier splines $$k_i - f_i = a_i - k_i$$ $C^2$ continuity condition for cubic B´ezier splines$$f_i + (f_i - a_{i-1}) = a_i + (a_i - f_{i+1})$$ Geometric Continuity$G^1$: the tangents are parallel at the knot. if the tangents are parallel, then the discontinuity is purely a change in speed, not a change in direction. We say that a curve is $G^2$ continuous if its curvature changes continuously. Automatic Tagent ControlWe want to get the smooth curve only by several knots, without the need for the user to specify any additional criteria(like the $C^1$ continuity). CatmullRom Splinesthe formular of Catmull-Rom Spline$$\\begin{aligned}v_i^{in} &amp;= v_i^{out} = \\frac{k_{i+1} - k_{i-1}}{2}\\\\&amp;=\\frac{(k_{i+1} - k_i) + (k_i - k_{i-1})}{2}\\end{aligned}$$ TCB SplinesEndpoint Conditions","link":"/Graphics/Geometry/Geometry-Curves/"},{"title":"Geometry-Transformation","text":"Keywords: Coordinate Space, Transformation, Matrix, Quaternions, Polar Coordinate System This is the partly ReadingNotes from book 3D Math Primer for Graphics and Game Development 2nd Edition. Attetion: left-hand rule, row vectors, right multiply. Deduce the Projection and Rotation MatrixPerspective Projection Matrix from the picture above, the camera is at origin $(0,0,0)$, the projection plane is $z = d$ (also, the Focus distance is $d$), by similar triangles, we can see that $$\\frac{p_y’}{d} = \\frac{p_y}{z}\\Rightarrow p_y’ = \\frac{dp_y}{z}, also \\space p_x’ = \\frac{dp_x}{z}$$ so $$p = \\begin{bmatrix} x &amp; y &amp; z \\end{bmatrix}\\longmapsto\\begin{aligned}p’ &amp;= \\begin{bmatrix} \\frac{dx}{z} &amp; \\frac{dy}{z} &amp; \\frac{dz}{z} \\end{bmatrix}\\\\&amp;= \\frac{\\begin{bmatrix} x &amp; y &amp; z \\end{bmatrix}}{z/d}\\end{aligned}$$ So we need a $4 \\times 4$ matrix that multiplies a homogeneous vector $[x, y, z, 1]$ to produce $[x, y, z, z/d]$. The matrix that does this is $$\\begin{bmatrix}x &amp; y &amp; z &amp; 1\\end{bmatrix}\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 1/d\\\\0 &amp; 0 &amp; 0 &amp; 0\\end{bmatrix}=\\begin{bmatrix}x &amp; y &amp; z &amp; z/d\\end{bmatrix}$$ Multiplication by this matrix doesn’t actually perform the perspective transform, it just computes the proper denominator into $w(w = z/d)$. Remember that the perspective division actually occurs when we convert from $4D$ to $3D$ by dividing by $w$. Orthogonal Projection MatrixRotation Matrix3D Rotation about Cardinal Axes matrix to rotate about the $x-axis$:$$R_x(\\theta) =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\0 &amp; \\cos\\theta &amp; \\sin \\theta\\\\0 &amp; -\\sin \\theta &amp; \\cos \\theta\\end{bmatrix}$$ matrix to rotate about the $y-axis$:$$R_y(\\theta) =\\begin{bmatrix}\\cos\\theta &amp; 0 &amp; -\\sin\\theta \\\\0 &amp; 1 &amp; 0\\\\\\sin\\theta &amp; 0 &amp; \\cos\\theta\\end{bmatrix}$$ matrix to rotate about the $z-axis$:$$R_z(\\theta) =\\begin{bmatrix}\\cos\\theta &amp; \\sin\\theta &amp; 0\\\\-\\sin\\theta &amp; \\cos\\theta &amp; 0\\\\0 &amp; 1 &amp; 0\\end{bmatrix}$$ 3D Rotation about Arbitrary AxisLet’s derive a matrix to rotate about $\\widehat{\\vec{n}}$ by the angle $\\theta$. that means: $$\\vec{v}’ = \\vec{v}R(\\widehat{\\vec{n}},\\theta)$$ The basic idea is to solve the problem in the plane perpendicular to $\\widehat{\\vec{n}}$. we separate $\\vec{v}$ into two vectors, $\\vec{v_{\\parallel}}$ and $\\vec{v_{\\perp}}$, which are parallel and perpendicular to $\\vec{v}$, respectively, such that $\\vec{v} = \\vec{v_{\\parallel}} + \\vec{v_{\\perp}}$. By rotating each of these components individually, we can rotate the vector as a whole. In other words, $\\vec{v’} = \\vec{v_{\\parallel}’} + \\vec{v_{\\perp}’}$. Since $\\vec{v_{\\parallel}}$ is parallel to $\\widehat{\\vec{n}}$, it will not be affected by the rotation about $\\widehat{\\vec{n}}$. In other words, $\\vec{v_{\\parallel}’} = \\vec{v_{\\parallel}}$. So all we need to do is compute $\\vec{v_{\\perp}’}$, and then we have $\\vec{v’} = \\vec{v_{\\parallel}’} + \\vec{v_{\\perp}’}$. To compute $\\vec{v_{\\perp}’}$ , we construct the vectors $\\vec{v_{\\parallel}}$ and $\\vec{v_{\\perp}}$ and an intermediate vector $\\vec{w}$, The vector $\\vec{w}$ is mutually perpendicular to $\\vec{v_{\\parallel}}$ and $\\vec{v_{\\perp}}$ and has the same length as $\\vec{v_{\\perp}}$: $$\\begin{aligned}\\vec{v_{\\parallel}} &amp;= (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}\\\\\\vec{v_{\\perp}} &amp;= \\vec{v} - \\vec{v_{\\parallel}} \\\\&amp;= \\vec{v} - (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}} \\\\\\vec{w} &amp;= \\widehat{\\vec{n}} \\times \\vec{v_{\\perp}} \\\\&amp;= \\widehat{\\vec{n}} \\times (\\vec{v} - \\vec{v_{\\parallel}})\\\\&amp;= \\widehat{\\vec{n}} \\times \\vec{v} - \\widehat{\\vec{n}} \\times \\vec{v_{\\parallel}}\\\\&amp;= \\widehat{\\vec{n}} \\times \\vec{v}\\end{aligned}$$ How do these vectors help us compute $\\vec{v_{\\perp}’}$? Notice that $\\vec{w}$ and $\\vec{v_{\\perp}}$ form a 2D coordinate space, with $\\vec{v_{\\perp}}$ as the $“x-axis”$ and $\\vec{w}$ as the $“y-axis”$. (Note that the two vectors don’t necessarily have unit length.) $\\vec{v_{\\perp}’}$ is the result of rotating $\\vec{v’}$ in this plane by the angle $θ$. Remember the endpoints of a unit ray rotated by an angle $\\theta$ are $\\cos\\theta$ and $\\sin\\theta$? $$\\begin{aligned}\\vec{p’} &amp;= \\cos \\theta \\vec{e_1} + \\sin \\theta \\vec{e_2} \\\\&amp;= \\cos\\theta \\begin{bmatrix} 1 &amp; 0\\end{bmatrix} + \\sin\\theta \\begin{bmatrix} 0 &amp; 1\\end{bmatrix}\\\\&amp;= \\begin{bmatrix} \\cos\\theta &amp; 0\\end{bmatrix} + \\begin{bmatrix} 0 &amp; \\sin\\theta\\end{bmatrix} \\\\&amp;= \\begin{bmatrix} \\cos\\theta &amp; \\sin\\theta \\end{bmatrix}\\end{aligned}$$The only difference here is that our ray is not a unit ray, and we are using $\\vec{v_{\\perp}}$ and $\\vec{w}$ as our basis vectors. Thus, $\\vec{v_{\\perp}’}$ can be computed as $$\\begin{aligned}\\vec{v_{\\perp}’} &amp;= \\cos\\theta \\vec{v_{\\perp}} + \\sin\\theta \\vec{w}\\\\&amp;= \\cos\\theta (\\vec{v} - (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}) + \\sin\\theta (\\widehat{\\vec{n}} \\times \\vec{v})\\end{aligned}$$ Thus, $$\\begin{aligned}\\vec{v’} &amp;= \\vec{v_{\\parallel}’} + \\vec{v_{\\perp}’} \\\\&amp;= \\cos\\theta (\\vec{v} - (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}) + \\sin\\theta (\\widehat{\\vec{n}} \\times \\vec{v}) + (\\vec{v} \\cdot \\widehat{\\vec{n}})\\widehat{\\vec{n}}\\end{aligned}\\tag{5.1}$$ the remaining arithmetic is essentially a notational change that expresses Equation (5.1) as a matrix multiplication. Now that we have expressed $\\vec{v’}$ in terms of $\\vec{v}, \\widehat{\\vec{n}}$ and $θ$. we can compute what the basis vectors are after transformation and construct our matrix.$$\\vec{e_1} = \\begin{bmatrix} 1 &amp; 0 &amp; 0\\end{bmatrix}\\longmapsto\\vec{e_1’} =\\begin{bmatrix}n_x^2(1-\\cos\\theta) + \\cos\\theta\\\\n_x n_y(1-\\cos\\theta) + n_z \\sin\\theta\\\\n_x n_z(1-\\cos\\theta) - n_y\\sin\\theta\\end{bmatrix}^T$$$$\\vec{e_2} = \\begin{bmatrix} 0 &amp; 1 &amp; 0\\end{bmatrix}\\longmapsto\\vec{e_2’} =\\begin{bmatrix}n_x n_y(1-\\cos\\theta) - n_z \\sin\\theta\\\\n_y^2(1-\\cos\\theta) + \\cos\\theta\\\\n_y n_z(1-\\cos\\theta) + n_x\\sin\\theta\\end{bmatrix}^T$$$$\\vec{e_3} = \\begin{bmatrix} 0 &amp; 0 &amp; 1\\end{bmatrix}\\longmapsto\\vec{e_3’} =\\begin{bmatrix}n_x n_z(1-\\cos\\theta) + n_y\\sin\\theta\\\\n_y n_z(1-\\cos\\theta) - n_x \\sin\\theta\\\\n_z^2(1-\\cos\\theta) +\\cos\\theta\\end{bmatrix}^T$$Thus $$\\begin{aligned}R(\\widehat{\\vec{n}},\\theta) &amp;=\\begin{bmatrix}R(\\vec{e_1}) &amp; R(\\vec{e_2}) &amp; R(\\vec{e_3’})\\end{bmatrix}^T\\\\&amp;=\\begin{bmatrix}\\vec{e_1’} &amp; \\vec{e_2’} &amp; \\vec{e_3’}\\end{bmatrix}^T\\\\&amp;=\\begin{bmatrix}n_x^2(1-\\cos\\theta) + \\cos\\theta &amp; n_x n_y(1-\\cos\\theta) + n_z \\sin\\theta &amp; n_x n_z(1-\\cos\\theta) - n_y\\sin\\theta\\\\n_x n_y(1-\\cos\\theta) - n_z \\sin\\theta &amp; n_y^2(1-\\cos\\theta) + \\cos\\theta &amp; n_y n_z(1-\\cos\\theta) + n_x\\sin\\theta\\\\n_x n_z(1-\\cos\\theta) + n_y\\sin\\theta &amp; n_y n_z(1-\\cos\\theta) - n_x \\sin\\theta &amp; n_z^2(1-\\cos\\theta) +\\cos\\theta\\end{bmatrix}\\end{aligned}$$ How to understand Basis in Linear Algebra &gt;&gt; How to understand Basis and Coordinate Systems in Linear Algebra &gt;&gt; How to understand Basis and Coordinate Systems and Coordinate Mapping in Linear Algebra &gt;&gt; QuaternionDeduce Quaternion we can say that if we multiply a complex number by $i$, we can rotate the complex number through the complex plane at 90° increments. $$\\begin{aligned}p &amp;= 2 + i\\\\q &amp;= pi\\\\&amp;= -1+2i\\\\r &amp;= qi\\\\&amp;= -2-i\\\\s &amp;= ri\\\\&amp;= 1-2i\\\\t &amp;= si\\\\&amp;= 2 + i\\end{aligned}$$ Rotate a point through the 2D complex plane as$$q = \\cos\\theta + i \\sin\\theta$$ Then,$$\\begin{aligned}p &amp;= a + bi\\\\pq &amp;= (a+bi)(\\cos\\theta + i \\sin\\theta)\\\\ &amp;= a\\cos\\theta - b\\sin\\theta + (a\\sin\\theta + b\\cos\\theta)i\\end{aligned}$$Written in matrix form:$$\\begin{bmatrix}a’ &amp; -b’\\\\b’ &amp; a’\\end{bmatrix}=\\begin{bmatrix}\\cos\\theta &amp; -\\sin\\theta\\\\\\sin\\theta &amp; \\cos\\theta\\end{bmatrix}\\begin{bmatrix}a &amp; -b\\\\b &amp; a\\end{bmatrix}$$Which is the method to rotate an arbitrary point in the complex plane counter-clockwise about the origin. The general form to express quaternions is$$\\begin{aligned}\\pmb{q} &amp;= s + xi + yj + zk, s,x,y,z\\in R\\\\&amp;= [s,\\vec{v}]\\end{aligned}$$ Let express a quaternion that can be used to rotate a point in 3D-space as such:$$\\pmb{q} = [\\cos\\theta, \\sin\\theta\\vec{v}]$$ let $\\pmb{p}$ as a Pure quaternion in the form, $\\pmb{q}$ is a unit-norm quaternion:$$\\pmb{p} = [0,\\vec{p}], \\pmb{q} = [s, \\lambda \\hat{\\vec{v}}]$$Then,$$\\begin{aligned}\\pmb{p’} &amp;= \\pmb{qp}\\\\&amp;= [s, \\lambda \\hat{\\vec{v}}][0,\\vec{p}]\\\\&amp;= [-\\lambda \\hat{\\vec{v}} \\cdot \\vec{p}, s\\vec{p}+ \\lambda \\hat{\\vec{v}} \\times \\vec{p}]\\end{aligned}$$ First, think special case $\\vec{p} \\perp \\hat{\\vec{v}}$, so the result becomes Pure quaternion:$$\\pmb{p’} = [0, s\\vec{p}+ \\lambda \\hat{\\vec{v}} \\times \\vec{p}]$$ In this case, to rotate $\\vec{p}$ about $\\hat{\\vec{v}}$ we just substitute $s = \\cos\\theta$ and $\\lambda = \\sin\\theta$.$$\\pmb{p’} = [0, \\cos\\theta \\vec{p}+ \\sin \\theta \\hat{\\vec{v}} \\times \\vec{p}]$$ For example: let’s rotate a vector $\\vec{p}$ 45° about the $z-axis$. our quaternion $\\pmb{q}$ is: $$\\begin{aligned}\\pmb{q} &amp;= [\\cos\\theta, \\sin\\theta \\vec{k}]\\\\&amp;= [\\frac{\\sqrt 2}{2}, \\frac{\\sqrt 2}{2} \\vec{k}]\\end{aligned}$$ let $\\vec{p} \\perp \\vec{k}$, so $$\\pmb{p} = [0, 2\\vec{i}]$$Thus, $$\\begin{aligned}\\pmb{p’} &amp;= \\pmb{qp}\\\\&amp;= [\\frac{\\sqrt 2}{2}, \\frac{\\sqrt 2}{2} \\vec{k}][0, 2\\vec{i}]\\\\&amp;= [0, \\frac{\\sqrt 2}{2} \\vec{i} + \\frac{\\sqrt 2}{2} \\vec{j}]\\end{aligned}$$$$|\\pmb{p’}| = 2$$ For example let’s consider a quaternion that is not orthogonal to $\\vec{p}$, but still 45°. $$\\begin{aligned}&amp;\\hat{\\vec{v}} = \\frac{\\sqrt 2}{2}\\vec{i} + \\frac{\\sqrt 2}{2}\\vec{k}\\\\&amp;\\vec{p} = 2\\vec{i}\\\\&amp;\\pmb{q} = [\\cos\\theta, \\sin\\theta \\hat{\\vec{v}}]\\\\&amp;\\pmb{p} = [0,\\vec{p}]\\end{aligned}$$ Thus,$$\\begin{aligned}\\pmb{p’} &amp;= \\pmb{qp}\\\\&amp;= [\\cos\\theta, \\sin\\theta \\hat{\\vec{v}}][0,\\vec{p}]\\\\&amp;= [-1, \\sqrt 2 \\vec{i} + \\vec{j}]\\end{aligned}$$But, the norm has changed, that’s not we want.$$|\\pmb{p’}| = \\sqrt 3$$ Hamilton recognized (but didn’t publish) that if we post-multiply the result of $\\pmb{qp}$ by the inverse of $\\pmb{q}$ then the result is a pure quaternion and the norm of the vector component is maintained.so,$$\\begin{aligned}\\pmb{q} &amp;= [\\cos\\theta, \\sin\\theta \\hat{\\vec{v}}]\\\\&amp;= [\\cos\\theta, \\sin\\theta (\\frac{\\sqrt 2}{2}\\vec{i} + \\frac{\\sqrt 2}{2}\\vec{k})]\\\\\\pmb{q}^{-1} &amp;= [\\cos\\theta, -\\sin\\theta \\hat{\\vec{v}}]\\\\&amp;= [\\cos\\theta, -\\sin\\theta (\\frac{\\sqrt 2}{2}\\vec{i} + \\frac{\\sqrt 2}{2}\\vec{k})]\\\\\\end{aligned}$$ Thus,$$\\begin{aligned}\\pmb{qp} &amp;= [-1, \\sqrt 2 \\vec{i} + \\vec{j}]\\\\\\pmb{qpq^{-1}} &amp;= [0, \\vec{i} + \\sqrt 2 \\vec{j} + \\vec{k}]\\end{aligned}$$ Which is a pure quaternion and the norm of the result is:$$|\\pmb{p’}| = 2$$ but the vector has been rotated 90° rather than 45° which is twice as much as desired! So in order to correctly rotate a vector $\\vec{p}$ by an angle $\\theta$ about an arbitrary axis $\\widehat{\\vec{v}}$ , we must consider the half-angle and construct the following quaternion: $$\\pmb{q} = [\\cos \\frac{1}{2} \\theta, \\sin \\frac{1}{2} \\theta \\widehat{\\vec{v}}]$$ Quaternion Interpolationslerp: Spherical Linear interpolation. The slerp operation is useful because it allows us to smoothly interpolate between two orientations. Deduce SlerpFirst, recall how we interpolation between scalar, the simple linear interpolation is:$$\\begin{aligned}&amp;\\Delta a = a_1 - a_0\\\\&amp;lerp(a_0, a_1, t) = a_0 + t\\Delta a, t \\in [0,1]\\end{aligned}$$Now we use the same basic idea in quaterninon: Compute the difference between the two values. $$\\Delta q = q_1 q_0^{-1}$$ Take a fraction of this difference. $$(\\Delta q)^t$$ The meaning of quaternion exponentiation is similar to that of real numbers.Recall that for any scalar $a$, besides zero, $a^0 = 1$ and $a^1 = a$. As the exponent $t$ varies from $0$ to $1$ the value of at varies from $1$ to $a$.A similar statement holds for quaternion exponentiation: as $t$ varies from $0$ to $1$ the quaternion exponentiation $\\pmb{q}^t$ varies from $[1, \\pmb{0}]$ to $\\pmb{q}$. Take the original value and adjust it by this fraction of the difference. $$(\\Delta q)^tq_0$$ Thus, the equation for slerp is given by: $$slerp(q_0, q_1, t) = (q_1 q_0^{-1})^tq_0, t \\in [0,1]$$ Hard to understand, right? Why there’s no adding operation? Don’t worry, next we give you the adding formula. Imagine two 2D vectors $v_0$ and $v_1$, both of unit length. We wish to compute the value of $v_t$, which is the result of smoothly interpolating around the arc by a fraction $t$ of the distance from $v_0$ to $v_1$. If we let $\\omega$ be the angle intercepted by the arc from $v_0$ to $v_1$, then $v_t$ is the result of rotating $v_0$ around this arc by an angle of $t\\omega$. Let,$$v_t = k_0v_0 + k_1v_1$$ 相似三角形， we find, $$\\sin \\omega = \\frac{\\sin t\\omega}{k_1} \\longrightarrowk_1 = \\frac{\\sin t\\omega}{\\sin \\omega}$$ similar, $$k_0 = \\frac{\\sin (1-t)\\omega}{\\sin \\omega}$$ thus, $$slerp(q_0, q_1, t) = \\frac{\\sin (1-t)\\omega}{\\sin \\omega} q_0 + \\frac{\\sin t\\omega}{\\sin \\omega} q_1$$ We just need a way to compute $\\omega$, the “angle” between the two quaternions. Appendix$$OpenGL-perspective-matrix:\\begin{bmatrix} \\frac{1}{aspect _ ratio \\cdot tan\\frac{\\theta}{2}} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{zFar+zNear}{zNear-zFar}&amp; \\frac{2 \\cdot zNear \\cdot zFar}{zNear-zFar} \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\\\end{bmatrix} $$ $$OpenGL-perspective-matrix:\\begin{bmatrix} \\frac{2|n|}{r-l} &amp; 0 &amp; \\frac{r+l}{r-l} &amp; 0 \\\\ 0 &amp; \\frac{2|n|}{t-b} &amp; \\frac{t+b}{t-b} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{|n|+|f|}{|n|-|f|} &amp; \\frac{2|f||n|}{|n|-|f|} \\\\ 0 &amp; 0 &amp; -1 &amp; 0 \\\\\\end{bmatrix} $$ $$OpenGL-orthographic-matrix:\\begin{bmatrix} \\frac{1}{aspect _ ratio*tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{2}{zNear-zFar} &amp; \\frac{zNear+zFar}{zNear-zFar} \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\\\\\end{bmatrix} $$ so,for the OpenGL conventions, we can tell whether a projection matrix is perspective or orthographic based on the bottom row.$$OpenGL-Perspective\\begin{bmatrix} 0 &amp; 0 &amp; -1 &amp; 0 \\end{bmatrix}$$$$penGL-Orthographic\\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}$$ $$Dx-orthographic-matrix:\\begin{bmatrix} \\frac{2}{w} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{2}{h} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{1}{zF-zN} &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{zn}{zN-zF} &amp; 1 \\\\\\end{bmatrix}$$ $$Dx-perspective-matrix:\\begin{bmatrix} \\frac{1}{aspect _ ratio*tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; \\frac{1}{tan \\frac{\\theta}{2}} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; \\frac{zF}{zF-zN} &amp; 1 \\\\ 0 &amp; 0 &amp; \\frac{zN \\cdot zF}{zN-zF} &amp; 1 \\\\\\end{bmatrix}$$ OpenGL use column vectors, Projection_Matrix * View_Matrix * Model_Matrix * VectorDirectx use row vectors, Vector * Model_Matrx * View_Matrx * Projection_Matrix https://www.qiujiawei.com/understanding-quaternions/","link":"/Graphics/Geometry/Geometry-Transformation/"},{"title":"GameProgramming-Relationship-Graph","text":"Keywords: Relationship graph, Random distribution, LineRenderer Knuth-Durstenfeld ShuffleGiven an integer array nums, design an algorithm to randomly shuffle the array. All permutations of the array should be equally likely as a result of the shuffling. 123456789vector&lt;int&gt; shuffle() { int len = nums.size(); for(int i = len - 1; i &gt;= 0; i--) { int j = rand() % (i + 1); //rand() % M : [0,M) swap(nums[i],nums[j]); } return nums;} If we need to distribute the players at random positions, we can shuffle the position array and assign them to the players in sequence. EllipseThough we know the formula of ellipse $\\frac{x^2}{a^2} + \\frac{y^2}{b^2} = 1$, it’s not efficient to compute the data strictly by it. Sometimes we view an ellipse as a squashed circle, which means $x$ stay the same, but $y$ compress by a factor. For example, there are several elliptical concentric tracks, we want to sample $N$ points randomly along each orbitals (for $N$ players). Method: Set a maxValue $M$, denotes the maximum number of points can be sampled from one orbital, $N \\leq M$. 123456789101112131415161718192021222324for(int i = 1; i &lt;= orbitalNums; i++){ int maxPointNum = M; //M can be set by designer //uniformly sample M points in the orbital for(int j = 0; j &lt; maxPointNum; j++) { float x = Math.Cos(2pi / M * j); float y = Math.Sin(2pi / M * j) * compressCoefficient; //compress y value //the radius of the concentric circle is gradually increased Vector2 point = Vector2(x,y) * circleRadius * i; //add a perturbation(disturbance) avoiding rigid distribution float disturbance = circleRadius * UnityEngine.Random.Range(-offset, offset); orbit.Add(point.x + disturbance, point.y + disturbance); } //shuffle orbit array shuffle(orbit); //n players' positions &lt;-- first n of the m points assignPos();} Connect Nodes ClockwiselySometimes we want to connect several nodes by specific rule. For example, we want to connect those players on the first three orbits clockwisely, connect those players on the second four orbits clockwisely…In general, a closer orbit indicates a closer relationship to me. Method: sort the players according to the degree of intimacy, decide which orbit and which position they belong to. group those players by groupId, hash: &lt;groupId, List&lt;accountId&gt;()&gt; create a LinkedList clockwisely, linklist records the previous and next neighbor accountId for this node(player) Render the line while iterate linklist, thus we can see the palyers distribute on the orbits and connected clockwisely by group. 12345678//Vector B should be in the clickwies direction of Afloat a = Vector2.Angle(Vector2.up, A);float b = Vector2.Angle(Vector2.up, B);a = A.x &gt;= 0 ? a : 360 - a;b = B.x &gt;= 0 ? b : 360 - b;//B is in the clockwise direction of Aif(b &gt; a){A-&gt;next = B}else{B-&gt;next = A} Also, the line effect is realized by LineRenderer, inner class in Unity. Play Animations In OrderSometimes animation can be broken down into successive steps, for example, 1. the two lines disappear 2. two nodes move toward each other 3. new lines generate. Step1,2,3 must be done in sequence, but in animation1, the two lines disappear animation can run at the same time. The normal code can write like this: 1234567891011121314151617181920212223242526272829303132333435363738394041424344void updatedata(){ //data change causes the animation to occur StartCoroutine(PlayXAnimation);}IEnumerator PlayXAnimation(){ //Step1. yield return PlayStep1Animation(); //Step2. yield return PlayStep2Animation(); //Step3. yield return PlayStep3Animation();}//Step1.IEnumerator PlayStep1Animation(){ StartCoroutine(Play1.1); StartCoroutine(Play1.2); //suppose 3f is the animation length yield return new WaitForSeconds(3f);}//Step2.IEnumerator PlayStep2Animation(){ ... yield return new WaitForSeconds(3f);}//Step3.IEnumerator PlayStep3Animation(){ ... yield return new WaitForSeconds(3f);}//Step1.1IEnumerator Play1.1(){ AnimationClip clip = getclip(\"clipname\"); if(clip) { //Animation ani; ani.play(\"clipname\"); }} The essence of Coroutine is an Iterator. Click and LongPressSometimes we need to distinguish the click action and longpress action on one object. We can think of it in terms of state machines. For example, once we press our fingure on this object, $t \\leq 0.2s$, it’s click action, $0.2s &lt; t \\leq 0.7s$, it’s longpress action. once the press time $t &gt; 0.7s$, we think the action is invalid, it’s reset to idle state. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879enum State{ Idle = 0, Click = 1, Holding = 2, LongHolding = 3}State st = State.Idle; //no click and pressfloat t = 0f;bool pressed = false;//UIEventTriggervoid init(){ EventDelegate onPress = new EventDelegate(onPressobBtn); EventDelegate onRelease = new EventDelegate(onReleaseobBtn); UIEventTrigger trigger = ob.gameObject.GetComponent&lt;UIEventTrigger&gt;(); if(trigger != null) { trigger.onPress.Add(onPress); trigger.onRelease.Add(onRelease); }}void onPressobBtn(){ pressed = true;}void onReleaseobBtn(){ pressed = false;}//update functionvoid Update(){ switch (st) { case State.Idle: if(pressed) { st = State.Click; t = 0; } break; case State.Click: if(!pressed) { st = State.Idle; //click effect: show click UI, cause click effects are often displayed at the moment of release return; } t += Time.deltaTime; if(t &gt; 0.2) { st = State.Holding; //holding effect: show holding UI } break; case State.Holding: if(!pressed) { st = State.Idle; } //holding effect: cause holding effects are often displayed with holding time. like fillAmount if(t &gt; 0.7) { st = State.LongHolding; //hide holding UI } break; case State.LongHolding: if(!pressed) { st = State.Idle; } break; }}","link":"/GameProgramming/System/GameProgramming-System-RelationshipGraph/"},{"title":"NumericalAnalysis-C12-Eigenvalues-and-Singular-Values","text":"Keywords: Power Iteration Method, QR Algorithm, SVD(Singular Value Decomposition), Mathlab This is the Chapter12 ReadingNotes from book Numercial Analysis(Timothy Sauer). More about EigenValues and EigenVectors in Algebra &gt;&gt; Power Iteration MethodsThere is no direct method for computing eigenvalues. The situation is analogous to rootfinding, in that all feasible methods depend on some type of iteration. Power IterationThe motivation behind Power Iteration is that multiplication by a matrix tends to move vectors toward the dominant eigenvector direction. DEFINITION 12.1Let $A$ be an $m × m$ matrix. A dominant eigenvalue of $A$ is an eigenvalue $\\lambda$ whose magnitude is greater than all other eigenvalues of $A$. If it exists, an eigenvector associated to $\\lambda$ is called a dominant eigenvector. To keep the numbers from getting out of hand, it is necessary to normalize the vector at each step. Given an approximate eigenvector, the Rayleigh quotient is the best approximate eigenvalue. (least squares, take $\\lambda$ as unknows, $x$ as coefficient matrix)$$Ax = \\lambda x \\\\x\\lambda = Ax\\\\x^Tx\\lambda = x^TAx\\\\\\lambda = \\frac{x^TAx}{x^Tx}\\tag{12.2}$$ 123456789101112%powerit.m% Program 12.1 Power Iteration% Computes dominant eigenvector of square matrix% Input: matrix A, initial (nonzero) vector x, number of steps k% Output: dominant eigenvalue lam, eigenvector ufunction [lam,u]=powerit(A,x,k)for j=1:k u=x/norm(x); % normalize vector x=A*u; % power step lam=u'*x; % Rayleigh quotientendu=x/norm(x); Convergence of Power Iteration THEOREM 12.2Let $A$ be an $m \\times m$ matrix with real eigenvalues $\\lambda_1, \\cdots, \\lambda_m$ satisfying $|\\lambda_1| &gt; |\\lambda_2| \\geq |\\lambda_3| \\geq \\cdots \\geq |\\lambda_m|$. Assume that the eigenvectors of $A$ span $R^m$. For almost every initial vector, Power Iteration converges linearly to an eigenvector associated to $\\lambda_1$ with convergence rate constant $S = |\\lambda_2 / \\lambda_1|$. Inverse Power Iteration LEMMA 12.3Let the eigenvalues of the $m \\times m$ matrix A be denoted by $\\lambda_1, \\cdots, \\lambda_m$ .(a) The eigenvalues of the inverse matrix $A^{−1}$ are $\\lambda_1^{-1}, \\cdots, \\lambda_m^{-1}$, assuming that the inverse exists. The eigenvectors are the same as those of $A$.(b) The eigenvalues of the shifted matrix $A − sI$ are $\\lambda_1 − s, \\lambda_2 − s, \\cdots,\\lambda_m − s$ and the eigenvectors are the same as those of $A$. According to Lemma 12.3, the largest magnitude eigenvalue of the matrix $A^{−1}$ is the reciprocal of the smallest magnitude eigenvalue of $A$. $$x_{k+1} = A^{-1}x_k \\Leftrightarrow Ax_{k+1} = x_{k}\\tag{12.4}$$which is then solved for $x_{k+1}$ by Gaussian elimination. Suppose $A$ has an eigenvalue 10.05, near 10, then $A-10I$ has an eigenvalue $\\lambda = 0.05$. If it is the smallest magnitude eigenvalue of $A-10I$, then the inverse Power Iteration $x_{k+1} = (A-10I)^{-1}x_k$ will locate it. That is, the Inverse Power Iteration will converge to the reciprocal $1/(.05) = 20$, after which we invert to $.05$ and add the shift back to get $10.05$. This trick will locate the eigenvalue that is smallest after the shift—which is another wayof saying the eigenvalue nearest to the shift. Look at the figure again. 12345678910111213%invpowerit.m% Program 12.2 Inverse Power Iteration% Computes eigenvalue of square matrix nearest to input s% Input: matrix A, (nonzero) vector x, shift s, steps k% Output: dominant eigenvalue lam, eigenvector of inv(A-sI)function [lam,u]=invpowerit(A,x,s,k)As=A-s*eye(size(A));for j=1:k u=x/norm(x); % normalize vector x=As\\u; % power step lam=u'*x; % Rayleigh Quotientendlam=1/lam+s; u=x/norm(x); Rayleigh Quotient IterationIf at any step along the way an approximate eigenvalue were known, it could be used as the shift $s$, to speed convergence. 123456789101112%rqi.m% Program 12.3 Rayleigh Quotient Iteration% Input: matrix A, initial (nonzero) vector x, number of steps k% Output: eigenvalue lam and eigenvector ufunction [lam,u]=rqi(A,x,k)for j=1:k u=x/norm(x); % normalize lam=u'*A*u; % Rayleigh quotient x=(A-lam*eye(size(A)))\\u; % inverse power iterationendu=x/norm(x);lam=u'*A*u; % Rayleigh quotient QR AlgorithmsThe goal of this section is to develop methods for finding all eigenvalues at once. More about Symmetric Matrix EigenVector and SVD in Algebra&gt;&gt; Simultaneous iteration（同时迭代） Real Schur form and the QR algorithmUpper Hessenberg formSingular Value DecompositionFinding the SVD in general LEMMA 12.10Let $A$ be an $m \\times n$ matrix. The eigenvalues of $A^TA$ are nonnegative. THEOREM 12.11Let $A$ be an $m \\times n$ matrix. Then there exist two orthonormal bases ${v_1, \\cdots, v_n}$ of $R^n$, and ${u_1, \\cdots ,u_m}$ of $R^m$, and real numbers $s_1 \\geq \\cdots \\geq s_n \\geq 0$ such that $Av_i = s_iu_i$ for $1 \\leq i \\leq min\\lbrace m,n\\rbrace$. The columns of $V = [v_1| . . . |v_n]$, the right singular vectors, are the set of orthonormal eigenvectors of $A^TA$; and the columns of $U = [u_1| \\cdots |u_m]$, the left singular vectors, are the set of orthonormal eigenvectors of $AA^T$. $$A = \\begin{bmatrix} 3 &amp; 0\\\\ 0 &amp; \\frac{1}{2}\\end{bmatrix}\\tag{12.15}$$$$Av_1 = s_1 u_1\\\\\\cdots \\\\Av_n = s_n u_n\\tag{12.16}$$ Special case: symmetric matricesFinding the SVD of a symmetric $m \\times m$ matrix is simply a matter of finding the eigenvalues and eigenvectors. Application of SVDProperties of the SVDDimension reductionCompressionCalculating the SVD","link":"/Math/Numerical-Analysis/NumericalAnalysis-C12-Eigenvalues-and-Singular-Values/"},{"title":"NumericalAnalysis-C0-Fundamentals","text":"Keywords: Polynomial, Binary, Floating, Matlab This is the Chapter0 ReadingNotes from book Numerical Analysis by Timothy. EVALUATING A POLYNOMIALWhat is the best way to evaluate $$P(x) = 2x^4 + 3x^3 -3x^2 + 5x - 1$$ nested multiplication or Horner’s method:$$P(x) =−1 + x ∗ (5 + x ∗ (−3 + x ∗ (3 + x ∗ 2)))\\tag{0.2}$$evaluates the polynomial in $4$ multiplications and $4$ additions. A general degree $d$ polynomial can be evaluated in $d$ multiplications and $d$ additions. While the standard form for a polynomial $c_1 + c_2x + c_3x^2 + c_4x^3 + c_5x^4$ can be written in nested form as$$c_1 + x(c_2 + x(c_3 + x(c_4 + x(c_5))))\\tag{0.4}$$ In particular, interpolation calculations in Chapter 3 will require the form$$c_1 + (x − r_1)(c_2 + (x − r_2)(c_3 + (x − r_3)(c_4 + (x − r_4)(c_5))))\\tag{0.5}$$where we call $r_1,r_2,r_3$, and $r_4$ the base points. 123456789101112131415161718192021%nest.m%Program 0.1 Nested multiplication%Evaluates polynomial from nested form using Horner’s Method%Input: degree d of polynomial,% array of d+1 coefficients c (constant term first),% x-coordinate x at which to evaluate, and% array of d base points b, if needed%Output: value y of polynomial at xfunction y = nest(d,c,x,b)if nargin&lt;4, b=zeros(d,1); endy=c(d+1);for i=d:-1:1y = y.*(x-b(i))+c(i);end&gt;&gt; nest(4,[-1 5 -3 3 2],1/2)ans = 1.2500 BINARY NUMBERS$$4 = (100.)_2, \\frac{3}{4} = (0.11)_2$$ Decimal to binary$$(53.7)_{10} = (53)_2 + (0.7)_2 = (?)_2$$ Integer partConvert decimal integers to binary by dividing by $2$ successively and recording the remainders.$$53 \\div 2 = 26 … 1\\\\26 \\div 2 = 13 … 0\\\\13 \\div 2 = 6 … 1\\\\6 \\div 2 = 3 … 0\\\\3 \\div 2 = 1 … 1\\\\1 \\div 2 = 0 … 1\\\\$$$$(53)_10 = (110101.)_2.$$ Fractional partConvert $(0.7)_10$ to binary by reversing the preceding steps. Multiply by $2$ successively and record the integer parts, moving away from the decimal point to theright. $$.7 × 2 = .4 + 1\\\\.4 × 2 = .8 + 0\\\\.8 × 2 = .6 + 1\\\\.6 × 2 = .2 + 1\\\\.2 × 2 = .4 + 0\\\\.4 × 2 = .8 + 0\\\\\\cdots$$$$(0.7)_10 = (.1011001100110. . .)_2 = (.1\\overline{0110})_2,$$ Thus,$$(53.7)_{10} = (53)_2 + (0.7)_2 = (110101.1\\overline{0110})2.$$ Binary to decimalInteger part$$(10101)_2 = 1 \\cdot 2^4 + 0 \\cdot 2^3 + 1 \\cdot 2^2 + 0 \\cdot 2^1 + 1 \\cdot 2^0= (21)_{10}$$ Fractional part$$(.1011)_2 = \\frac{1}{2} + \\frac{1}{8} + \\frac{1}{16}= (\\frac{11}{16})_{10}$$ What if the fractional part is not a finite base 2 expansion? For example: $$x = (0.\\overline{1011})_2$$ Solution: Multiply $x$ by $2^4$, which shifts $4$ places to the left in binary.$$2^4x = (1011.\\overline{1011})_2$$Thus,$$(2^4-1)x = (1011)_2 = (11)_{10}$$So that$$x = \\frac{11}{15}$$ What if the fractional part does not immediately repeat? For example: $$x = (0.10\\overline{101})_2$$ Solution: Multiplying by $2^2$ shifts to $y = 2^2x = 10.\\overline{101}$. The fractional part of $y$, call it $z = .\\overline{101}$, is calculated as before: $$2^3z = 101.\\overline{101}\\\\z = 000.\\overline{101}$$Therefore,$$7z = 5$$$$y = 2 + \\frac{5}{7}$$$$x = 2^{-2}y = \\frac{19}{28}$$ FLOATING POINT REPRESENTATION OF REAL NUMBERSSimple algorithms, such as Gaussian elimination or methods for solving differential equations, can magnify microscopic errors to macroscopic size. Floating point formatsThe IEEE standard consists of a set of binary representations of real numbers. A floating point number consists of three parts: the sign (+ or −)（符号位）, a mantissa（尾数）, which contains the string of significant bits, and an exponent（指数）. The three parts are stored together in a single computer word(字). word, Byte, bit1Byte = 8bitcomputer word: 16,32,64 bits The form of a normalized IEEE floating point number is$$\\pm 1.bbb \\cdots b \\times 2^p\\tag{0.6}$$where each of theN $b$’s is $0$ or $1$, and $p$ is an $M$-bit binary number representing the exponent. The double precision number $1$ is$$+1. \\underbrace{0000000000000000000000000000000000000000000000000000}_{52} × 2^0$$where we have 52 bits of the mantissa. The next floating point number greater than $1$ is$$+1. \\underbrace{0000000000000000000000000000000000000000000000000001}_{52} × 2^0$$which is $1 + 2^{−52}$. DEFINITION 0.1The number machine epsilon, denoted $\\epsilon_{match}$, is the distance between $1$ and the smallest floating point number greater than $1$.For the IEEE double precision floating point standard,$$\\epsilon_{match} = 2^{-52}$$ How do we fit the infinite binary number representing 9.4 in a finite number of bits? IEEE Rounding to Nearest RuleFor double precision, if the 53rd bit to the right of the binary point is 0, then round down (truncate after the 52nd bit). If the 53rd bit is 1, then round up (add 1 to the 52 bit), unless all known bits to the right of the 1 are 0’s, in which case 1 is added to bit 52 if and only if bit 52 is 1. $$9.4 = +1. \\underbrace{0010110011001100110011001100110011001100110011001100}_{52} 110 \\cdots × 2^3$$ $$fl(9.4) = +1. \\underbrace{0010110011001100110011001100110011001100110011001101}_{52} × 2^3$$ DEFINITION 0.2Denote the IEEE double precision floating point number associated to $x$, using the Rounding to Nearest Rule, by $fl(x)$. Obviously, for 9.4, we discarding the infinite tail$$\\begin{aligned}.\\overline{1100} \\times 2^{-52} \\times 2^3&amp;=.\\overline{0110} \\times 2^{-51} \\times 2^3 \\\\&amp;= .4 \\times 2^{-48}\\end{aligned}$$from the right end of the number. Then, we adding $2^{-52} \\times 2^3 = 2^{-49}$ in the rounding step. Therefore,$$\\begin{aligned}fl(9.4) &amp;= 9.4 + 2^{-49} - .4 \\times 2^{-48} \\\\&amp;= 9.4 + 0.2 \\times 2^{-49}\\end{aligned}\\tag{0.8}$$ We call $0.2 \\times 2^{-49}$ the rounding error. DEFINITION 0.3Let $x_c$ be a computed version of the exact quantity $x$. Then$$absolute-error = |x_c - x|$$and$$relative-error = \\frac{|x_c - x|}{x}$$ Relative rounding errorIn the IEEE machine arithmetic model, the relative rounding error of $fl(x)$ is no more than one-half machine epsilon:$$\\frac{|fl(x) - x|}{x} \\leq \\frac{1}{2} \\epsilon_{match}$$ Machine representationEach double precision floating point number is assigned an $8$-byte word, or $64$ bits, to store its three parts. Each such word has the form$$se_1e_2 . . . e_{11}b_1b_2 . . . b_{52}\\tag{0.10}$$11 bits representing the exponent and 52 bits following the decimal point, representing the mantissa. $11$ bits can represent $2^{11} = 2048$ states $\\longrightarrow [0,2047]$. In left-justifying, we find that the exponent can be negative, like $0.4 = 1.10\\overline{0110} \\times 2^{-2}$. For brevity, we don’t want to consider negative in computer exponents. So we introduce a exponent bias. In double precision, the exponent bias is $2^{10}-1 = 1023$. So that, for left-justifying double exponent $[-1022, 1023]$, the computer exponent covers $[1, 2046]$. For example: $$1 = +1. \\underbrace{0000000000000000000000000000000000000000000000000000}_{52} × 2^0,$$ Adding the exponent bias to $0$, we get $1023$, so the double precision machine number form is$$\\boxed{0}\\boxed{01111111111}\\boxed{0000000000000000000000000000000000000000000000000000}$$the hex format is$$3FF0000000000000$$ What about 0 and 2017 ? &gt;&gt;&gt;&gt; $0$ and $2047$ for special purposes. $2047$, is used to represent $\\infty$ if the mantissa bit string is all zeros and NaN, which stands for Not a Number,otherwise. the first twelve bits of Inf and -Inf are $\\boxed{0111} \\boxed{1111} \\boxed{1111}$ and $\\boxed{1111} \\boxed{1111} \\boxed{1111}$ , respectively, and the remaining 52 bits (the mantissa) are zero. The machine number NaN also begins$\\boxed{1111} \\boxed{1111} \\boxed{1111}$ but has a nonzero mantissa. The special exponent 0, also denotes a departure from the standard floating point form. In this case the machine number is interpreted as the non-normalized floating point number $$\\pm 0.\\boxed{b_1b_2\\cdots b_{52}} \\times 2^{-1022}\\tag{0.11}$$ These non-normalized numbers are called subnormal floating point numbers. They extend the range of very small numbers by a few more orders of magnitude. Therefore, $2^{−52} × 2^{−1022} = 2^{−1074}$ is the smallest nonzero representable number in double precision. Its machine word is$$\\boxed{0} \\boxed{00000000000} \\boxed{0000000000000000000000000000000000000000000000000001}$$The subnormal numbers include the most important number $0$. Addition of floating point numbersFor example: Calculate $9.4-9-0.4$ in Computer. Solution: 9.4 is stored as$$9.4 + 0.2 \\times 2^{-49}$$thus,$$9.4 - 9 = 0.4 + 0.2 \\times 2^{-49}$$Since, 0.4 is stored as$$fl(0.4) = 0.4 + 0.1 \\times 2^{-52}$$so, the result is $$0.2 \\times 2^{-49} - 0.1 \\times 2^{-52} = 3 \\times 2^{-53}$$ 123456789101112131415161718&gt;&gt; format long&gt;&gt; x = 9.4x = 9.400000000000000&gt;&gt; y = x-9y = 0.400000000000000&gt;&gt; z = y-0.4z = 3.330669073875470e-16 LOSS OF SIGNIFICANCE（丢失有效数字）For example: We assume that we are using a three-decimal-digit computer. Now Calculate $\\sqrt{9.01} - 3$. Solution: Checking on a hand calculator, we see that the correct answer is approximately $0.0016662 = 1.6662 × 10^{−3}$. But in Computer, $\\sqrt{9.01} \\approx 3.0016662$, we store it as $3.00$. Subtracting $3.00$, we get a final answer of $0.00$. No significant digits in our answer are correct. What is causing the loss of significance is the fact that we are explicitly subtracting nearly equal numbers, We can avoid this problem by using algebra to rewrite the expression:$$\\sqrt{9.01} - 3 = \\frac{(\\sqrt{9.01} - 3) (\\sqrt{9.01} + 3)}{\\sqrt{9.01} + 3}\\approx 1.67 \\times 10^{-3}$$ The lesson is that it is important to find ways to avoid subtracting nearly equal numbers in calculations, if possible. Often, specific identities can be used, as with trigonometric expressions. For example: Compare the two expressions:$$E_1 = \\frac{1-\\cos x}{\\sin ^2 x}, E_2 = \\frac{1}{1+\\cos x}$$ $E_2$ is better than $E_1$.The quadratic formula is often subject to loss of significanc REVIEWOF CALCULUSIntermediate Value Theorem &gt;&gt; Mean Value Theorem &gt;&gt; Taylor’s Theorem &gt;&gt;","link":"/Math/Numerical-Analysis/NumericalAnalysis-C0-Fundamentals/"},{"title":"NumericalAnalysis-C10-Trigonometric-Interpolation-and-the-FFT","text":"Keywords: Fourier Transform, Trigonometric Interpolation, FFT, Mathlab The Fourier TransformComplex arithmeticDiscrete Fourier TransformThe Fast Fourier TransformTrigonometric InterpolationThe DFT Interpolation TheoremEfficient evaluation of trigonometric functionsThe FFT and Signal ProcessingOrthogonality and interpolationLeast squares fitting with trigonometric functionsSound, noise, and filteringReality Check 10: The Wiener Filter","link":"/Math/Numerical-Analysis/NumericalAnalysis-C10-Trigonometric-Interpolation-and-the-FFT/"},{"title":"NumericalAnalysis-C11-Compression","text":"Keywords: DCT, Image compression, Quantization, Mathlab The Discrete Cosine TransformOne-dimensional DCTThe DCT and least squares approximationTwo-Dimensional DCT and Image CompressionTwo-dimensional DCTImage compressionQuantization（量化）Huffman CodingInformation theory and codingHuffman coding for the JPEG formatModified DCT and Audio CompressionModified Discrete Cosine TransformBit quantizationReality Check 11: A Simple Audio Codec","link":"/Math/Numerical-Analysis/NumericalAnalysis-C11-Compression/"},{"title":"NumericalAnalysis-C13-Optimization","text":"Keywords: Unconstrained Optimization, Mathlab Unconstrained Optimization without DerivativesGolden Section SearchSuccessive parabolic interpolationNelder–Mead searchUnconstrained Optimization with DerivativesNewton’s MethodSteepest DescentConjugate Gradient SearchReality Check 13: Molecular Conformation and Numerical Optimization","link":"/Math/Numerical-Analysis/NumericalAnalysis-C13-Optimization/"},{"title":"NumericalAnalysis-C1-Solving-Equations","text":"Keywords: Bisection Method, Fixed-point Method, Newton’s Method, Brent’s Method, Kinematics, Matlab This is the Chapter1 ReadingNotes from book Numerical Analysis by Timothy. THE BISECTION METHODBracketing a root（二分法） DEFINITION 1.1The function $f(x)$ has a root at $x = r$ if $f(r) = 0$. This fact is summarized in the following corollary of the Intermediate Value Theorem: THEOREM 1.2Let $f$ be a continuous function on $[a,b]$, satisfying $f(a)f(b) &lt; 0$. Then $f$ has a root between $a$ and $b$, that is, there exists a number $r$ satisfying $a &lt; r &lt; b$ and $f (r) = 0$. The algorithm can be written in the following Matlab code: 1234567891011121314151617181920212223242526272829303132%bisect.m%Program 1.1 Bisection Method%Computes approximate solution of f(x)=0%Input: function handle f; a,b such that f(a)*f(b)&lt;0,% and tolerance tol%Output: Approximate solution xcfunction xc=bisect(f,a,b,tol)if sign(f(a))*sign(f(b)) &gt;= 0error(\"f(a)f(b)&lt;0 not satisfied!\") %ceases executionendfa=f(a);fb=f(b);while (b-a)/2&gt;tolc=(a+b)/2;fc=f(c);if fc == 0 %c is a solution, done breakendif sign(fc)*sign(fa)&lt;0 %a and c make the new intervalb=c;fb=fc;else %c and b make the new intervala=c;fa=fc;endendxc=(a+b)/2; %new midpoint is best estimate&gt;&gt; f=@(x) x^3+x-1;&gt;&gt; xc=bisect (f,0,1,0.00005)xc = 0.682342529296875 How accurate and how fast?If $[a,b]$ is the starting interval, then after $n$ bisection steps, the interval $[a_n,b_n]$ has length $(b − a)/2^n$. Choosing the midpoint $x_c = (a_n + b_n)/2$ gives a best estimate of the solution $r$, which is within half the interval length of the true solution.$$\\begin{aligned}Solution-error &amp;= |x_c - r| \\\\&amp;= |\\frac{a_n + b_n}{2} - r| \\\\&amp;&lt; \\frac{b_n - a_n}{2} \\\\&amp;= \\frac{b-a}{2^{n+1}}\\end{aligned}\\tag{1.1}$$and$$Function-evaluations = n + 2\\tag{1.2}$$ DEFINITION 1.3A solution is correct within $p$ decimal places if the error is less than $0.5 × 10^{−p}$ (在小数点后p位内正确). For example: Use the Bisection Method to find a root of $f(x) = cosx − x$ in the interval $[0,1]$ to within six correct places. Solution: $$\\frac{b-a}{2^{n+1}} &lt; 0.5 \\times 10^{-6}$$$$n &gt; 19.9$$Therefore, $n = 20$ steps will be needed. FIXED-POINT ITERATION (FPI)Fixed points of a function DEFINITION 1.4The real number $r$ is a fixed point of the function $g$ if $g(r) = r$. The number $r = 0.7390851332$ is an approximate fixed point for the function $g(x) = cosx$. The function $g(x) = x^3$ has three fixed points, $r =−1, 0$, and $1$. According to (Continuous Limits) Let $f$ be a continuous function in a neighborhood of $x_0$, and assume $\\lim_{n\\rightarrow \\infty}x_n = x_0$. Then$$\\lim_{n \\rightarrow \\infty} f(x_n) = f(\\lim_{n\\rightarrow \\infty} x_n) = f(x_0)$$ we can get$$g(r) = g(\\lim_{i\\rightarrow \\infty}x_i) = \\lim_{i\\rightarrow \\infty} g(x_i) = r$$ The Fixed-Point Iteration algorithm applied to a function g is easily written in Matlab code: 12345678910111213141516171819%fpi.m%Program 1.2 Fixed-Point Iteration%Computes approximate solution of g(x)=x%Input: function handle g, starting guess x0,% number of iteration steps k%Output: Approximate solution xcfunction xc=fpi(g, x0, k)x(1)=x0;for i=1:kx(i+1)=g(x(i));endxc=x(k+1);&gt;&gt; g=@(x) cos(x)&gt;&gt; xc=fpi(g,0,10)xc = 0.731404042422510 Can every equation $f(x) = 0$ be turned into a fixed-point problem $g(x) = x$? Yes. For example: $$x^3 + x - 1 = 0\\tag{1.4}$$ Three Solutions： Initial value $x_0 = 0.5$ 1.$$x = 1-x^3\\longrightarrow g(x) = 1 - x^3\\tag{1.5}$$Instead of converging, the iteration tends to alternate between the numbers $0$ and $1$. 2. $$x = \\sqrt[3]{1-x}\\longrightarrow g(x) = \\sqrt[3]{1-x}\\tag{1.6}$$ This time FPI is successful. 3. $$3x^3 + x = 1 + 2x^3\\\\x = \\frac{1+2x^3}{1+3x^2}\\longrightarrow g(x) = \\frac{1+2x^3}{1+3x^2}\\tag{1.7}$$This time FPI is successful, but in a much more striking way. Geometry of Fixed-Point IterationThis geometric illustration of a Fixed-Point Iteration is called a cobweb diagram（蛛网图）. Linear convergence of Fixed-Point Iteration Figure1.4 shows Fixed-Point Iteration for two linear functions$$g_1(x) = -\\frac{3}{2}x + \\frac{5}{2}\\\\g_2(x) = -\\frac{1}{2}x + \\frac{3}{2}$$From geometric view, For $g_1(x)$, Because the slope of $g_1(x)$ at the fixed point is greater than one, the vertical segments（看箭头段）, the ones that represent the change from $x_n$ to $x_{n+1}$, are increasing in length as FPI proceeds. As a result, the iteration “spirals out’’ from the fixed point $x = 1$, even if the initial guess $x_0$ was quite near. For $g_2(x)$, it “spirals in”. From equation view, $$g_1(x) = -\\frac{3}{2}(x-1) + 1$$$$g_1(x) - 1 = -\\frac{3}{2}(x-1)$$$$x_{i+1} - 1 = -\\frac{3}{2}(x_i-1)\\tag{1.8}$$ If we view $e_i = |r − x_i|$ as the error at step $i$ (meaning the distance from the best guess at step $n$ to the fixed point), we see from (1.8) that $e_{i+1} = \\frac{3e_i}{2}$, implying that errors increase at each step by a factor of approximately $3/2$. This is divergence. $g_2(x)$ is convergence. DEFINITION 1.5Let $e_i$ denote the error at step $i$ of an iterative method. If$$\\lim_{i\\rightarrow \\infty} \\frac{e_{i + 1}}{e_i} = S &lt; 1$$the method is said to obey linear convergence with rate $S$. THEOREM 1.6Assume that $g$ is continuously differentiable, that $g(r) = r$, and that $S = |g’(r)| &lt; 1$. Then Fixed-Point Iteration converges linearly with rate $S$ to the fixed point $r$ for initial guesses sufficiently close to $r$. DEFINITION 1.7An iterative method is called locally convergent to $r$ if the method converges to $r$ for initial guesses sufficiently close to $r$. For example: Calculate $\\sqrt{2}$ by using FPI. Solution: Suppose we want to find the first $10$ digits of $\\sqrt 2$. Start with the initial guess $x_0 = 1$. Obviously this guess is too low, we want to find a fomula to change 1 to higher number to approximate $\\sqrt{2}$. Obviously $\\frac{2}{1}$ seems too high. In fact, any initial guess $0 &lt; x_0 &lt; 2$, together with $\\frac{2}{x_0}$, form a bracket for $\\sqrt{2}$. (像一对括号一样，不断逼近$\\sqrt{2}$) We guess $$x_1 = \\frac{1 + \\frac{2}{1}}{2} = \\frac{3}{2}$$$$x_2 = \\frac{\\frac{3}{2} + \\frac{4}{3}}{2} = 1.4\\overline{16}$$$$x_3 = \\frac{\\frac{17}{12} + \\frac{24}{17}}{2} \\approx 1.414215686$$ The FPI we are executing is$$x_{i+1} = \\frac{x_i + \\frac{2}{x_i}}{2}$$Note that $\\sqrt{2}$ is a fixed point of the iteration.$$g’(\\sqrt{2}) = 0$$ Stopping criteriaan absolute error stopping criterion$$|x_{i+1} - x_{i}| &lt; TOL\\tag{1.16}$$the relative error stopping criterion$$\\frac{|x_{i+1} - x_{i}|}{|x_{i+1}|} &lt; TOL\\tag{1.17}$$A hybrid absolute/relative stopping criterion such as$$\\frac{|x_{i+1} - x_{i}|}{max(|x_{i+1}|, \\theta)} &lt; TOL\\tag{1.18}$$ LIMITS OF ACCURACYWorking in double precision means that we store and operate on numbers that are kept to 52-bit accuracy, about 16 decimal digits. Forward and backward errorFor example: Use the Bisection Method to find the root of $f (x) = x^3 − 2x^2 + \\frac{4}{3}x − \\frac{8}{27}$ to within six correct significant digits（6个正确的有效位）. Solution: $20$ bisection steps should be sufficient for six correct places. In fact, it is easy to check without a computer that $r = 2/3 = 0.666666666. . .$ is a root: How many of these digits can the Bisection Method obtain? From the figure above, we can see that Bisection Method stops after 16 steps, because the computer get $f(0.6666641) = 0$, satisfying the stop condition. From Figure1.7, we can see that the computer thinks there are many floating point numbers within $10^{−5}$ of the correct root $r = 2/3$ that are evaluated to machine zero, and therefore have an equal right to be called the root! This is not the method fault, but the computer! (Computer is not precise enough!) If the computer arithmetic is showing the function to be zero at a nonroot, there is no way the method can recover. DEFINITION 1.8Assume that $f$ is a function and that $r$ is a root, meaning that it satisfies $f(r) = 0$. Assume that $x_a$ is an approximation to $r$. For the root-finding problem, the backward error of the approximation $x_a$ is $|0 - f(x_a)|$ and the forward error is $|r − x_a|$. Backward error is on the left or input (problem data) side. It is the amount we would need to change the problem (the function $f$ ) to make the equation balance with the output approximation $x_a$, which is $|0-f(x_a)|$.Forward error is the error on the right or output (problem solution) side. It is the amount we would need to change the approximate solution to make it correct, which is $|r - x_a|$. The difficulty with Example 1.7 is that, according to Figure 1.7, the backward error is near $\\epsilon_{mach} \\approx 2.2 \\times 10^{−16}$, while forward error is approximately $10^{−5}$. Double precision numbers cannot be computed reliably below a relative error of machine epsilon($2^{-52}$ for double precision). Since the backward error cannot be decreased further with reliability, neither can the forward error. （总结来讲，到了第16步，计算的数值已经超出double的精度了，所以不会再迭代精确了…） DEFINITION 1.9Assume that $r$ is a root of the differentiable function $f$; that is, assume that $f(r) = 0$. Then if $0 = f(r) = f’(r) = f’’(r) = \\cdots = f^(m−1)(r)$, but $f^{(m)}(r) \\neq 0$, we say that $f$ has a root of multiplicity $m$ at $r$. We say that $f$ has a multiple root at $r$ if the multiplicity is greater than one. The root is called simple if the multiplicity is one. For example, $f(x) = x^2$ has a multiplicity two, or double, root at $r = 0$, because $f(0) = 0,f’(0) = 2(0) = 0$, but $f’’(0) = 2 \\neq 0$. Back to Figure1.7, Because the graph of the function is relatively flat near a multiple root, a great disparity exists between backward and forward errors for nearby approximate solutions. The backward error, measured in the vertical direction, is often much smaller than the forward error, measured in the horizontal direction. TheWilkinson polynomialThe Wilkinson polynomial is$$W(x) = (x − 1)(x − 2) · · · (x − 20)\\tag{1.19}$$which, when multiplied out, is$$\\begin{aligned}W(x) = &amp;x^{20} − 210x^{19} + 20615x^{18} − 1256850x^{17} + 53327946x^{16} − 1672280820x^{15}\\\\&amp;+ 40171771630x^{14} − 756111184500x^{13} + 11310276995381x^{12}\\\\&amp;− 135585182899530x^{11} + 1307535010540395x^{10} − 10142299865511450x^{9}\\\\&amp;+ 63030812099294896x^{8} − 311333643161390640x^{7}\\\\&amp;+ 1206647803780373360x^{6} − 3599979517947607200x^{5}\\\\&amp;+ 8037811822645051776x^{4}− 12870931245150988800x^{3}\\\\&amp;+ 13803759753640704000x^{2} − 8752948036761600000x\\\\&amp;+ 2432902008176640000\\end{aligned}\\tag{1.20}$$The roots are the integers from $1$ to $20$. However, when $W(x)$ is defined according to its unfactored（没有因式分解的形式） form (1.20), its evaluation suffers from cancellation of nearly equal, large numbers. Sensitivity of root-findingSmall floating point errors in the equation can translate into large errors in the root. To understand what causes this magnification of error, we will establish a formula predicting how far a root moves when the equation is changed. Assume that the problem is to find a root $r$ of $f(x) = 0$, but that a small change $\\epsilon g(x)$ is made to the input, where $\\epsilon$ is small. Let $\\Delta r$ be the corresponding change in the root, so that$$f(r+\\Delta r) + \\epsilon g(r + \\Delta r) = 0$$ Expanding $f$ and $g$ in degree-one Taylor polynomials &gt;&gt; implies that $$f(r) + (\\Delta r) f’(r) + \\epsilon g(r) + \\epsilon(\\Delta r)g’(r) + O((\\Delta r)^2) = 0$$For small $\\Delta r$, the $O((\\Delta r)^2)$ terms can be neglected to get$$(\\Delta r) (f’(r) + \\epsilon g’(r)) \\approx -f(r)-\\epsilon g(r) = -\\epsilon g(r)$$or$$\\Delta r \\approx \\frac{-\\epsilon g(r)}{f’(r)+\\epsilon g’(r)} \\approx -\\epsilon \\frac{g(r)}{f’(r)}$$assuming that $\\epsilon$ is small compared with $f’(r)$, and in particular, $f’(r) \\neq 0$. Sensitivity Formula for RootsAssume that $r$ is a root of f(x) and $r + \\Delta r$ is a root of $f(x) + \\epsilon g(x)$. Then,$$\\Delta r \\approx -\\epsilon \\frac{g(r)}{f’(r)}$$if $\\epsilon \\ll f’(r)$ A problem with high condition number is called ill-conditioned, and a problem with a condition number near $1$ is called well-conditioned. NEWTON’S METHOD Quadratic convergence of Newton’s Method DEFINITION 1.10Let $e_i$ denote the error after step $i$ of an iterative method. The iteration is quadratically convergent（成平方收敛） if$$M = \\lim_{i\\rightarrow \\infty} \\frac{e_{i+1}}{e_i^2} &lt; \\infty$$ THEOREM 1.11Let $f$ be twice continuously differentiable and $f(r) = 0$. If $f’(r) \\neq 0$, then Newton’s Method is locally and quadratically convergent to $r$. The error $e_i$ at step $i$ satisfies$$\\lim_{i \\rightarrow \\infty} \\frac{e_{i+1}}{e_i^2} = M$$where$$M = \\frac{f’’(r)}{2f’(r)}$$ Linear convergence of Newton’s Method THEOREM 1.12Assume that the $(m + 1)$-times continuously differentiable function $f$ on $[a,b]$ has a multiplicity $m$ root at $r$. Then Newton’s Method is locally convergent to $r$, and the error $e_i$ at step $i$ satisfies$$\\lim_{i \\rightarrow \\infty} \\frac{e_{i+1}}{e_i} = S\\tag{1.29}$$where,$$S = (m-1)/m,$$ If the multiplicity of a root is known in advance, convergence of Newton’s Method can be improved with a small modification. THEOREM 1.13If f is $(m + 1)$-times continuously differentiable on $[a,b]$, which contains a root $r$ of multiplicity $m&gt;1$, then Modified Newton’s Method$$x_{i+1} = x_i - \\frac{mf(x_i)}{f’(x_i)}\\tag{1.32}$$converges locally and quadratically to $r$. For example: Find the multiplicity of the root $r = 0$ of $f(x) = \\sin x + x^2 \\cos x − x^2 − x$, and estimate the number of steps of Newton’s Method required to converge within six correct places (use $x_0 = 1$). Solution: $$f(x) = \\sin x + x^2 \\cos x - x^2 - x\\\\f’(x) = \\cos x + 2x \\cos x - x^2 \\sin x - 2x - 1\\\\f’’(x) = -\\sin x + 2 \\cos x - 4x\\sin x - x^2 \\cos x - 2\\\\$$and that each evaluates to $0$ at $r = 0$. The third derivative,$$f’’’(x) = -\\cos x - 6\\sin x - 6x \\cos x + x^2 \\sin x\\tag{1.30}$$satisfies $f(0)=−1$, so the root $r = 0$ is a triple root, meaning that the multiplicity is $m = 3$. By Theorem 1.12, Newton should converge linearly with $e_{i+1} \\approx \\frac{2e_i}{3}$. So that $$(\\frac{2}{3})^n &lt; 0.5 \\times 10^{-6}\\\\n &gt; 35.78$$ Approximately 36 steps will be needed. The first 20 steps are shown in the table. But if we apply Modified Newton’s Method to achieve quadratic convergence. After five steps, convergence to the root $r = 0$ has taken place to about eight digits of accuracy: ROOT-FINDING WITHOUT DERIVATIVESBrent’s Method, a hybrid method which combines the best features of iterative and bracketing methods. Secant Method and variantsThe Secant Method is similar to the Newton’s Method, but replaces the derivative by a difference quotient. An approximation for the derivative at the current guess $x_i$ is the difference quotient$$\\frac{f(x_i) - f(x_{i-1})}{x_i - x_{i-1}}$$ Unlike Fixed-Point Iteration and Newton’s Method, two starting guesses are needed to begin the Secant Method. the approximate error relationship of Secant Method is $$e_{i+1} \\approx |\\frac{f’’(r)}{2f’(r)}| e_ie_{i-1}$$ and it implies that $$e_{i+1} \\approx |\\frac{f’’(r)}{2f’(r)}|^{\\alpha - 1}e_i^\\alpha$$where $\\alpha = (1 + \\sqrt{5}) / 2 \\approx 1.62$ The convergence of the Secant Method to simple roots is called superlinear, meaning that it lies between linearly and quadratically convergent methods. Brent’s MethodReality Check 1: Kinematics of the Stewart platform","link":"/Math/Numerical-Analysis/NumericalAnalysis-C1-Solving-Equations/"},{"title":"NumericalAnalysis-C3-Interpolation","text":"Keywords: Lagrange interpolation, Chebyshev Interpolation, Cubic Splines, Bézier Curves, Mathlab This is the Chapter3 ReadingNotes from book Numerical Analysis by Timothy. First Understanding about Interpolation and Curves&gt;&gt; Data and Interpolating Functions DEFINITION 3.1The function $y = P(x)$ interpolates the data points $(x_1,y_1), \\cdots , (x_n,y_n)$ if $P(x_i) = y_i$ for each $1 \\leq i \\leq n$. No matter how many points are given, there is some polynomial $y = P(x)$ that runs through all the points. Lagrange interpolation THEOREM 3.2Main Theorem of Polynomial Interpolation. Let $(x_1,y_1), . . . , (x_n,y_n)$ be $n$ points in the plane with distinct $x_i$. Then there exists one and only one polynomial $P$ of degree $n − 1$ or less that satisfies $P(x_i) = y_i$ for $i = 1,…,n$. Newton’s divided differences（牛顿分差法） DEFINITION 3.3Denote by $f[x_1 . . . x_n]$ the coefficient of the $x^{n−1}$ term in the (unique) polynomial that interpolates $(x_1,f(x_1)), . . . , (x_n,f(x_n))$. For example: Find an interpolating polynomial for the data points $(0,1), (2,2)$, and $(3,4)$ in Figure 3.1. Solution: By Lagrange’s formula: $$P_2(x) = \\frac{1}{2}x^2 - \\frac{1}{2}x + 1$$ Check that $P_2(0) = 1,P_2(2) = 2$, and $P_2(3) = 4$. Thus, by definition 3.3$$f[0 \\space 2 \\space3] = \\frac{1}{2}$$ Using this definition, the following somewhat remarkable alternative formula for the interpolating polynomial holds, called the Newton’s divided difference formula $$\\begin{aligned}P(x) = f[x_1] &amp;+ f[x_1 x_2](x - x_1) \\\\&amp;+ f[x_1 x_2 x_3](x - x_1)(x - x_2)\\\\&amp;+ \\cdots \\\\&amp;+ f[x_1 x_2 \\cdots x_n](x - x_1)(x-x_2)\\cdots(x - x_{n-1})\\end{aligned}\\tag{3.2}$$and it’s easy to calculate$$\\begin{aligned}&amp;f[x_k] = f(x_k)\\\\&amp;f[x_k x_{k+1}] = \\frac{f[x_{k+1}]-f[x_k]}{x_{k+1} - x_{k}}\\\\&amp;f[x_k x_{k+1} x_{k+2}] = \\frac{f[x_{k+1} x_{k+2}] - f[x_k x_{k+1}]}{x_{k+2} - x_{k}}\\\\&amp;f[x_k x_{k+1} x_{k+2} x_{k+3}] = \\frac{f[x_{k+1} x_{k+2} x_{k+3}] - f[x_k x_{k+1} x_{k+2}]}{x_{k+3} - x_{k}}\\\\\\end{aligned}\\tag{3.3}$$ For three points the table has the form(convenient to understand) The divided difference approach has a “real-time updating’’ property that the Lagrange form lacks: The Lagrange polynomial must be restarted from the beginning when a new point is added; none of the previous calculation can be used; in divided difference form, we keep the earlier work and add one new term to the polynomial. How many degree d polynomials pass through n points?Code for interpolation12345678910111213141516171819%newtdd.m%Program 3.1 Newton Divided Difference Interpolation Method%Computes coefficients of interpolating polynomial%Input: x and y are vectors containing the x and y coordinates% of the n data points%Output: coefficients c of interpolating polynomial in nested form%Use with nest.m to evaluate interpolating polynomialfunction c=newtdd(x,y,n)for j=1:n v(j,1)=y(j); % Fill in y column of Newton triangleendfor i=2:n % For column i, for j=1:n+1-i % fill in column from top to bottom v(j,i)=(v(j+1,i-1)-v(j,i-1))/(x(j+i-1)-x(j)); endendfor i=1:n c(i)=v(1,i); % Read along top of triangleend % for output coefficients Note that nest.m is in Chapter0, so you can add this file to search path, the program will run successfully. 1234567891011121314151617181920212223%clickinterp.m%Program 3.2. Polynomial Interpolation Program%Click in MATLAB figure window to locate data point.% Continue, to add more points.% Press return to terminate program.function clickinterpxl=-3;xr=3;yb=-3;yt=3;plot([xl xr],[0 0],'k',[0 0],[yb yt],'k');grid on;xlist=[];ylist=[];k=0; % initialize counter kwhile(0==0) [xnew,ynew] = ginput(1); % get mouse click if length (xnew) &lt;1 break % if return pressed, terminate end k=k+1; % k counts clicks xlist(k)=xnew; ylist(k)=ynew; % add new point to the list c=newtdd(xlist,ylist,k); % get interpolation coeffs x=xl:.01:xr; % define x coordinates of curve y=nest(k-1,c,x,xlist); % get y coordinates of curve plot(xlist,ylist,'o',x,y,[xl xr],[0,0],'k',[0 0],[yb yt],'k'); axis([xl xr yb yt]);grid on;end Representing functions by approximating polynomialsFor example: Interpolate the function $f(x) = \\sin x$ at $4$ equally spaced points on $[0,\\pi/2]$. Solution: The interval $[0,\\pi/2]$ is a so-called fundamental domain for sine, meaning that an input from any other interval can be referred back to it. The degree 3 interpolating polynomial is therefore $$P_3(x) = 0 + x(0.9549 + (x − \\pi/6)(−0.2443 + (x − \\pi/3)(−0.1139)))$$ 123456789101112131415161718192021222324%sin1.m%Program 3.3 Building a sin calculator key, attempt #1%Approximates sin curve with degree 3 polynomial% (Caution: do not use to build bridges,% at least until we have discussed accuracy.)%Input: x%Output: approximation for sin(x)function y=sin1(x)%First calculate the interpolating polynomial and% store coefficientsb=pi*(0:3)/6;yb=sin(b); % b holds base pointsc=newtdd(b,yb,4);%For each input x, move x to the fundamental domain and evaluate% the interpolating polynomials=1; % Correct the sign of sinx1=mod(x,2*pi);if x1&gt;pi x1 = 2*pi-x1; s = -1;endif x1 &gt; pi/2 x1 = pi-x1;endy = s*nest(3,c,x1,b); Interpolation ErrorInterpolation error formula THEOREM 3.4Assume that $P(x)$ is the (degree $n − 1$ or less) interpolating polynomial fitting the $n$ points $(x_1,y_1), . . . , (x_n,y_n)$. The interpolation error is$$f(x) - P(x) = \\frac{(x-x_1)(x-x_2)\\cdots(x-x_n)}{n!}f^{(n)}(c)\\tag{3.6}$$where $c$ lies between the smallest and largest of the numbers $x,x_1, . . . , x_n$. For example:Still $f(x) = \\sin x, x \\in [0, \\pi/2]$. Cal the 4 points interpolation error. By (3.6), we get,$$\\sin x - P(x) = \\frac{(x-0)(x-\\frac{\\pi}{6})(x-\\frac{\\pi}{3})(x-\\frac{\\pi}{2})}{4!} f’’’’(c),0 &lt; c &lt; \\pi/2$$ The fourth derivative $f’’’’(c) = \\sin c$. At worst, $|\\sin c|$ is no more than 1, so we can be assured of an upper bound on interpolation error:$$|\\sin x - P(x)| \\leq \\frac{(x-0)(x-\\frac{\\pi}{6})(x-\\frac{\\pi}{3})(x-\\frac{\\pi}{2})}{24} |1|$$ Note that the interpolation error will tend to be smaller close to the center of the interpolation interval. Proof of Newton form and error formulato be added… Runge phenomenon（龙格现象）For example: Interpolate $f(x) = 1/(1 + 12x^2)$ at evenly spaced points in $[−1,1]$. Runge phenomenon: polynomial wiggle near the ends of the interpolation interval. The cure for this problem is intuitive: Move some of the interpolation points toward the outside of the interval, where the function producing the data can be better fit. Chebyshev InterpolationIt turns out that the choice of base point spacing can have a significant effect on the interpolation error. Chebyshev interpolation refers to a particular optimal way of spacing the points. Chebyshev’s theoremThe interpolation error is$$\\frac{(x-x_1)(x-x_2)\\cdots(x-x_n)}{n!}f^{(n)}(c)$$Let’s fix the interval to be $[−1,1]$ for now. The numerator of the interpolation error formula is itself a degree $n$ polynomial in $x$ and has some maximum value on $[−1,1]$. $$(x-x_1)(x-x_2)\\cdots(x-x_n)\\tag{3.9}$$ We need to minimum it. So this is called the minimax problem of interpolation. THEOREM 3.6The choice of real numbers $−1 \\leq x_1, . . . , x_n \\leq 1$ that makes the value of$$\\underbrace{max}_{-1 \\leq x \\leq 1} |(x-x_1)(x-x_2)\\cdots(x-x_n)|$$as small as possible is$$x_i = \\cos\\frac{(2i-1)\\pi}{2n}, for, i = 1, \\cdots, n$$and the minimum value is $\\frac{1}{2^{n-1}}$. In fact, the minimum is achieved by$$(x-x_1)(x-x_2)\\cdots(x-x_n) = \\frac{1}{2^{n-1}}T_n(x)$$where $T_n(x)$ denotes the degree $n$ Chebyshev polynomial（$n$阶切比雪夫多项式）. We conclude from the theorem that interpolation error can be minimized if the $n$ interpolation base points in $[−1,1]$ are chosen to be the roots of the degree $n$ Chebyshev polynomial $T_n(x)$. We will call the interpolating polynomial that uses the Chebyshev roots as base points the Chebyshev interpolating polynomial（$n-1$阶切比雪夫插值多项式）. Chebyshev polynomialsThe $n$th Chebyshev polynomial is$$T_n(x) = \\cos(n \\arccos x)$$Set $y = \\arccos x$, so that $\\cos y = x$. $$T_{n+1}(x) = 2xT_n(x) - T_{n-1}(x)\\tag{3.13}$$is called the recursion relation for the Chebyshev polynomials. Several facts followfrom (3.13): FACT 1The $T_n$’s are polynomials. We showed this explicitly for $T_0,T_1, and T_2$. Since $T_3$ is a polynomial combination of $T_1$ and $T_2$, $T_3$ is also a polynomial. The same argument goes for all $T_n$. The first few Chebyshev polynomials (see Figure 3.9) are$$T_0(x) = 1\\\\T_1(x) = x\\\\T_2(x) = 2x^2-1\\\\T_3(x) = 4x^3 - 3x\\\\$$ FACT 2$deg(T_n) = n$, and the leading coefficient is $2n−1$. This is clear for $n = 1$ and $2$, and the recursion relation extends the fact to all $n$. FACT 3$T_n(1) = 1$ and $T_n(−1) = (−1)^n$. FACT 4The maximum absolute value of $T_n(x)$ for $−1 \\leq x \\leq 1$ is $1$. This follows immediately from the fact that $T_n(x) = \\cos y$ for some $y$. FACT 5All zeros of $T_n(x)$ are located between $−1$ and $1$. See Figure 3.10. In fact, the zeros are the solution of $0 = cos(n \\arccos x)$. Since $cosy = 0$ if and only if $y = odd-integer \\cdot (\\pi/2)$, we find that$$n\\arccos x = odd \\cdot \\pi/2\\\\x = \\cos \\frac{odd \\cdot \\pi}{2n}$$ Change of interval Cubic SplinesProperties of splinesA cubic spline $S(x)$ through the data points $(x_1,y_1), . . . , (x_n,y_n)$ is a set of cubic polynomials $$S_1(x) = y_1 + b_1(x-x_1) + c_1(x-x_1)^2 + d_1(x - x_1)^3, on [x_1,x_2]\\\\S_2(x) = y_2 + b_2(x-x_2) + c_2(x-x_2)^2 + d_2(x - x_2)^3, on [x_2,x_3]\\\\\\cdots\\\\S_{n-1}(x) = y_{n-1} + b_{n-1}(x-x_{n-1}) + c_{n-1}(x-x_{n-1})^2 + d_{n-1}(x - x_{n-1})^3, on [x_{n-1},x_n]\\tag{3.17}$$ with the following properties: Property 1$$S_i(x_i) = y_i\\\\S_i(x_{i+1}) = y_{i+1}, i = 1, \\cdots, n-1$$ Property 2$$S’_{i-1}(x_i) = S’_i(x_i), i = 2, \\cdots, n-1, slope$$ Property 3$$S’’_{i-1}(x_i) = S’’_i(x_i), i = 2, \\cdots, n-1, curvature$$ Property 4a Natural spline.$$S_1’’(x_1) = 0, S’’_{n-1}(x_n) = 0$$ Constructing a spline from a set of data points means finding the coefficients $b_i,c_i,d_i$ that make Properties 1–3 hold. THEOREM 3.7Let $n \\neq 2$. For a set of data points $(x_1,y_1), . . . , (x_n,y_n)$ with distinct $x_i$, there is a unique natural cubic spline fitting the points. Endpoint conditionsBézier Curves Reality Check 3: Fonts from Bézier curves","link":"/Math/Numerical-Analysis/NumericalAnalysis-C3-Interpolation/"},{"title":"NumericalAnalysis-C6-Ordinary-Differential-Equations","text":"Keywords: Euler’s Method, First-order linear equations, Systems of Ordinary Differential Equations, Implicit Methods and Stiff Equations, Mathlab More about First-Order-Differential-Equations in Calculus &gt;&gt; More about Second-Order-Differential-Equations in Calculus &gt;&gt; Initial Value ProblemsA wide majority of interesting equations have no closed-form solution, which leaves approximations as the only recourse. It will be helpful to think of a differential equation as a field of slopes, When an initial condition is specified on a slope field, one out of the infinite family of solutions can be identified. Euler’s Method For example: draw the slope field of this initial value problem, and apply Euler’s Method to this initial value problem with initial condition $y_0 = 1$$$\\begin{cases} y’ = ty + t^3\\\\ y(0) = y_0\\\\ t \\in [0,1]\\end{cases}\\tag{6.5}$$ Solution:The right-hand side of the differential equation is $f(t,y) = ty + t^3$. Therefore, Euler’s Method will be the iteration$$w_0 = 1\\\\w_{i+1} = w_i + h(t_iw_i + t_i^3)\\tag{6.8}$$ 1234567891011121314%eulerr.m%Program 6.1 Euler’s Method for Solving Initial Value Problems%Use with ydot.m to evaluate rhs of differential equation% Input: interval inter, initial value y0, number of steps n% Output: time steps t, solution y% Example usage: euler([0 1],1,10);function [t,y]=eulerr(inter,y0,n)t(1)=inter(1); y(1)=y0;h=(inter(2)-inter(1))/n;for i=1:n t(i+1)=t(i)+h; y(i+1)=eulerstep(t(i),y(i),h);endplot(t,y) 123456%eulerstep.mfunction y=eulerstep(t,y,h)%one step of Euler’s Method%Input: current time t, current value y, stepsize h%Output: approximate solution value at time t+hy=y+h*ydot(t,y); 1234%ydot.mfunction z=ydot(t,y)%right-hand side of differential equationz=t*y+t^3; 12345&gt;&gt; eulerr([0,1],1,10)ans = 0 0.1000 0.2000 0.3000 0.4000 0.5000 0.6000 0.7000 0.8000 0.9000 1.0000 Existence, uniqueness, and continuity for solutions DEFINITION 6.1A function $f(t,y)$ is Lipschitz continuous(李普希兹连续) in the variable $y$ on the rectangle $S = [a,b] \\times [\\alpha,\\beta]$ if there exists a constant $L$ (called the Lipschitz constant) satisfying$$|f(t,y_1) - f(t,y_2)| \\leq L|y_1 - y_2|$$for each $(t, y_1), (t ,y_2)$ in $S$.A function that is Lipschitz continuous in $y$ is continuous in $y$, but not necessarily differentiable. For example: Find the Lipschitz constant for the right-hand side$$f(t,y) = ty + t^3$$of (6.5). Solution: The function $f(t,y) = ty + t^3$ is Lipschitz continuous in the variable $y$ on the set $0 \\leq t \\leq 1, -\\infty &lt; y &lt; +\\infty$. Check that$$|f(t,y_1) - f(t,y_2)| = |ty_1 - ty_2| \\leq |t||y_1-y_2| \\leq |y_1 - y_2|\\tag{6.10}$$on the set. The Lipschitz constant is $L = 1$. If the function $f$ is continuously differentiable in the variable $y$, the maximum absolute value of the partial derivative $\\frac{\\partial f}{\\partial y}$ is a Lipschitz constant. According to the Mean Value Theorem &gt;&gt;, for each fixed $t$, there is a $c$ between $y_1$ and $y_2$ such that$$\\frac{f(t,y_1) - f(t,y_2)}{y_1 - y_2} = \\frac{\\partial f}{\\partial y}(t,c)$$Therefore, $L$ can be taken to be the maximum of$$|\\frac{\\partial f}{\\partial y}(t,c)|$$on the set. The Lipschitz continuity hypothesis guarantees the existence and uniqueness of solutions of initial value problems. THEOREM 6.2Assume that $f(t,y)$ is Lipschitz continuous in the variable $y$ on the set $S = [a,b] \\times [\\alpha,\\beta]$ and that $\\alpha &lt; y_a &lt; \\beta$. Then there exists $c$ between $a$ and $b$ such that the initial value problem$$\\begin{cases}y’ = f(t,y)\\\\y(a) = y_a\\\\t \\in [a,c]\\end{cases}\\tag{6.11}$$has exactly one solution $y(t)$.Moreover, if $f$ is Lipschitz on $[a,b] \\times [-\\infty,+\\infty]$, then there exists exactly one solution on $[a,b]$. THEOREM 6.3Assume that $f(t,y)$ is Lipschitz in the variable $y$ on the set $S = [a,b] \\times [\\alpha,\\beta]$. If $Y(t)$ and $Z(t)$ are solutions in $S$ of the differential equation$$y’ = f(t,y)$$with initial conditions $Y(a)$ and $Z(a)$ respectively, then$$|Y(t) - Z(t)| \\leq e^{L(t-a)} |Y(a)-Z(a)|\\tag{6.13}$$ ==hard to understand…== First-order linear equations$$\\begin{cases} y’ = g(t)y+h(t)\\\\ y(a) = y_a\\\\ t \\in [a,b]\\end{cases}$$ the explicit solution: $$y(t) = e^{\\int g(t) dt} \\int e^{-\\int g(t) dt} h(t) dt\\tag{6.16}$$ Analysis of IVP SolversA careful investigation of error in Euler’s Method will illustrate the issues for IVP solvers in general. Local and global truncation errorThe explicit Trapezoid MethodTaylor MethodsSystems of Ordinary Differential EquationsHigher order equationsComputer simulation: the pendulumComputer simulation: orbital mechanicsRunge–Kutta Methods and ApplicationsThe Runge–Kutta familyComputer simulation: the Hodgkin–Huxley neuronComputer simulation: the Lorenz equationsReality Check 6: The Tacoma Narrows BridgeVariable Step-Size MethodsEmbedded Runge–Kutta pairsOrder 4/5 methodsImplicit Methods and Stiff EquationsMultistep MethodsGenerating multistep methodsExplicit multistep methodsImplicit multistep methods","link":"/Math/Numerical-Analysis/NumericalAnalysis-C6-Ordinary-Differential-Equations/"},{"title":"NumericalAnalysis-C2-Systems-of-Equations","text":"Keywords: Gaussian Elimination, LU Factorization, Jacobi Matrix, Jacobi Iterative Method, Symmetric positve-definite matrice, Nonlinear Systems, Mathlab This is the Chapter2 ReadingNotes from book Numerical Analysis by Timothy. Gaussian EliminationConsider the system$$\\begin{cases}x + y = 3\\\\3x - 4y = 2\\end{cases}\\tag{2.1}$$ Naive Gaussian EliminationThe advantage of the tableau form is that the variables are hidden during elimination.（其实就是增广矩阵, 矩阵行变换，最简阶梯式求解的过程就叫消元）$$\\begin{bmatrix}1 &amp; 1 &amp; 3\\\\3 &amp; -4 &amp; 1\\end{bmatrix}\\longrightarrow\\begin{bmatrix}1 &amp; 1 &amp; 3\\\\0 &amp; -7 &amp; -7\\end{bmatrix}$$the corresponding equation is:$$\\begin{cases}x + y = 3\\\\-7y = -7\\end{cases}$$we solve y, x in order, this part is called back substitution, or backsolving. Operation countsThe general form of the tableau for $n$ equations in $n$ unknowns is$$\\begin{bmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2\\\\\\cdots &amp; \\cdots &amp; \\cdots &amp; \\cdots &amp; | &amp; \\cdots\\\\a_{n1} &amp; a_{n2} &amp; \\cdots &amp; a_{nn} &amp; | &amp; b_n\\\\\\end{bmatrix}$$ The elimination process can be written as: 12345for j = 1 : n-1 for i = j+1 : n eliminate entry a(i,j) endend This number and the other numbers $a_{ii}$ that are eventually divisors（除数） in Gaussian elimination are called pivots. More specific of elimination process: 1234567891011121314//columnfor j = 1 : n-1 if abs(a(j,j))&lt;eps; error(’zero pivot encountered’); end //row for i = j+1 : n mult = a(i,j)/a(j,j); //elements in this row for k = j+1:n a(i,k) = a(i,k) - mult * a(j,k); end //diagnal element b(i) = b(i) - mult*b(j); endend For the operation, we can see that: $$\\begin{bmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\a_{21} &amp; a_{22} &amp; \\cdots &amp; a_{2n} &amp; | &amp; b_2\\\\\\end{bmatrix}\\longrightarrow\\begin{bmatrix}a_{11} &amp; a_{12} &amp; \\cdots &amp; a_{1n} &amp; | &amp; b_1\\\\0 &amp; a_{22} - \\frac{a_{21}}{a_{11}}a_{12} &amp; \\cdots &amp; a_{2n} - \\frac{a_{21}}{a_{11}}a_{1n} &amp; | &amp; b_2 - \\frac{a_{21}}{a_{11}}b_1\\\\\\end{bmatrix}$$Accounting for the operations, this requires one division (to find the multiplier $\\frac{a_{21}}{a_{11}}$), plus $n$ multiplications and $n$ additions, which is $2n+1$. The total operations in the matrix is$$\\begin{bmatrix}0\\\\2n+1 &amp; 0\\\\2n+1 &amp; 2(n-1)+1 &amp; 0\\\\2n+1 &amp; 2(n-1)+1 &amp; 2(n-2)+1\\\\\\cdots &amp; \\cdots &amp; \\cdots \\\\2n+1 &amp; 2(n-1)+1 &amp; 2(n-2)+1 &amp; \\cdots &amp; 2(3)+1 &amp; 0\\\\2n+1 &amp; 2(n-1)+1 &amp; 2(n-2)+1 &amp; \\cdots &amp; 2(3)+1 &amp; 2(2) + 1 &amp; 0\\\\\\end{bmatrix}$$We total up the operation as$$\\begin{aligned}\\sum_{j=1}^{n-1}\\sum_{i=1}^{j} 2(j+1) + 1 &amp;= \\frac{2}{3}n^3 + \\frac{1}{2}n^2 -\\frac{7}{6}n\\end{aligned}$$The operation count shows that direct solution of $n$ equations in $n$ unknowns by Gaussian elimination is an $O(n^3)$ process. The back-substitution step is: 123456for i = n : -1 : 1 for j = i+1 : n b(i) = b(i) - a(i,j)*x(j); end x(i) = b(i)/a(i,i);end Counting operations yield$$1 + 3 + 5 + \\cdots + (2n-1) = n^2$$ The computer can carry out $(5000)^2$ operations in $0.1$ seconds, or $(5000)^2(10) =2.5 × 10^8$ operations/second. The $LU$ FactorizationMatrix form of Gaussian elimination DEFINITION 2.2An $m \\times n$ matrix $L$ is lower triangular if its entries satisfy $l_{ij} = 0$ for $i &lt; j$. An $m \\times n$ matrix $U$ is upper triangular if its entries satisfy $u_{ij} = 0$ for $i &gt; j$. For example: Fine the $LU$ factorization of$$A = \\begin{bmatrix}1 &amp; 2 &amp; -1\\\\2 &amp; 1 &amp; -2\\\\-3 &amp; 1 &amp; 1\\end{bmatrix}\\tag{2.13}$$ Solution: $$A = \\begin{bmatrix}1 &amp; 2 &amp; -1\\\\2 &amp; 1 &amp; -2\\\\-3 &amp; 1 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space 2 \\times row \\space 1 }\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -3 &amp; 0\\\\-3 &amp; 1 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space 3 \\times row \\space 1 }\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -3 &amp; 0\\\\0 &amp; 7 &amp; -2\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space -\\frac{7}{3} \\times row \\space 2 }\\begin{bmatrix}1 &amp; 2 &amp; -1\\\\0 &amp; -3 &amp; 0\\\\0 &amp; 0 &amp; -2\\end{bmatrix} = U$$ The lower triangular $L$ matrix is formed, as in the previous example, by putting $1$’s on the main diagonal and the multipliers in the lower triangle—in the specific places they were used for elimination. That is,$$L =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\2 &amp; 1 &amp; 0\\\\-3 &amp; -\\frac{7}{3} &amp; 1\\end{bmatrix}\\tag{2.14}$$ Back substitution with the LU factorization $Ax = b$(a) Solve $Lc = b$ for $c$.(b) Solve $Ux = c$ for $x$. Complexity of the LU factorizationNow, suppose that we need to solve a number of different problems with the same $A$ and different $b$. That is, we are presented with the set of problems$$Ax = b_1, \\\\Ax = b_2, \\\\…\\\\Ax = b_k\\\\$$with naive Gaussian Elimination, the complexity is$$\\frac{2kn^3}{2} + kn^2$$ but with $LU$, the complesity is $$\\underbrace{\\frac{2n^3}{3}}_{LU \\space Factorization} + \\underbrace{2kn^2}_{back \\space substitutions}$$ The $LU$ approach allows efficient handling of all present and future problems that involve the same coefficient matrix $A$. Source of ErrorError magnification and condition number DEFINITION 2.3The infinity norm, or maximum norm, of the vector $x = (x_1, \\cdots , x_n)$ is $||x||_\\infty = max|x_i|$, $i = 1,\\cdots,n$, that is, the maximum of the absolute values of the components of $x$. DEFINITION 2.4Let $x_a$ be an approximate solution of the linear system $Ax = b$. The residual is the vector $r = b − Ax_a$. The backward error is the norm of the residual $||b − Ax_a||_\\infty$, and the forward error is $||x − x_a||_\\infty$. For example: Find the forward and backward errors for the approximate solution $[−1,3.0001]$ of the system$$\\begin{cases}x_1 + x_2 = 2\\\\1.0001x_1 + x_2 = 2.0001.\\end{cases}\\tag{2.17}$$ Solution: The correct solution is$$[x_1, x_2] = [1,1]$$ The backward error is the infinity norm of the vector$$b - Ax_a =\\begin{bmatrix}-0.0001\\\\0.0001\\end{bmatrix}$$which is $0.0001$. The forward error is the infinity norm of the difference $$x - x_a =\\begin{bmatrix}2\\\\−2.0001\\end{bmatrix}$$which is $2.0001$. Figure 2.2 helps to clarify how there can be a small backward error and large forward error at the same time. Even though the “approximate root’’ $(−1,3.0001)$ is relatively far from the exact root $(1,1)$, it nearly lies on both lines. This is possible because the two lines are almost parallel. If the lines are far from parallel, the forward and backward errors will be closer in magnitude. Denote the residual by $r = b - Ax_a$. The relative backward error of system $Ax = b$is defined to be$$\\frac{||r||_\\infty}{||b||_\\infty}$$ and the relative forward error is $$\\frac{||x - x_a||_\\infty}{||x||_\\infty}$$ The error magnification factor for $Ax = b$ is the ratio of the two, or$$error-magnification-factor = \\frac{relative-forward-error}{relative-backward-error} = \\frac{\\frac{||x - x_a||_\\infty}{||x||_\\infty}}{\\frac{||r||_\\infty}{||b||_\\infty}}\\tag{2.18}$$ For system(2.17), the error magnification factor is$$\\frac{\\frac{2.0001}{1}}{\\frac{0.0001}{2.0001}} \\approx 40004.0001$$ DEFINITION 2.5The condition number of a square matrix $A$, $cond(A)$, is the maximum possible error magnification factor for solving $Ax = b$, over all right-hand sides $b$.Surprisingly, there is a compact formula for the condition number of a square matrix. Analogous to the norm of a vector, define the matrix norm of an $n \\times n$ matrix $A$ as$$||A||_\\infty = maximum-absolute-row-sum\\tag{2.19}$$ THEOREM 2.6The condition number of the $n \\times n$ matrix $A$ is$$cond(A) = ||A|| \\cdot ||A^{-1}||$$ Thus, The norm of$$A =\\begin{bmatrix}1 &amp; 1\\\\1.0001 &amp; 1\\end{bmatrix}$$is $||A|| = 2.0001$ The condition number of $A$ is $$cond(A) = 40004.0001.$$ This is exactly the error magnification we found in Example above, which evidently achieves the worst case, defining the condition number. The error magnification factor for any other $b$ in this system will be less than or equal to $40004.0001$. If $cond(A) ≈ 10^k$, we should prepare to lose $k$ digits of accuracy in computing $x$. In the example above, since $cond(A) \\approx 40004.0001$. so in double precision we should expect about $16 − 4 = 12$ correct digits in the solution $x$. We use Matlab for a computation: 12345&gt;&gt; A = [1 1;1.0001 1]; b=[2;2.0001];&gt;&gt; xa = A\\bxa =1.000000000002220.99999999999778 Compared with the correct solution $x = [1,1]$, the computed solution has about $11$ correct digits, close to the prediction from the condition number. vector norm $||x||$, which satisfies three properties:(i) $||x|| \\neq 0$ with equality if and only if $x = [0, . . . ,0]$(ii) for each scalar $\\alpha$ and vector $x$, $||\\alpha x|| = |\\alpha| \\cdot ||x||$(iii) for vectors $x,y$, $||x + y|| \\leq ||x|| + ||y||$. matrix norm $||A||\\infty$ which satisfies three properties:(i) $||A|| \\geq 0$ with equality if and only if $A = 0$(ii) for each scalar $\\alpha$ and matrix $A$, $||\\alpha A|| = |\\alpha| \\cdot ||A||$(iii) for matrices $A,B$, $||A + B|| \\leq ||A|| + ||B||$. the vector 1-norm of the vector $x$ is $||x||_1 = |x_1| + |x_2| + \\cdots + |x_n|$. The matrix 1-norm of the $n \\times n$ matrix $A$ is $||A||_1 = maximum-absolute-column-sum$, which is the maximum of the 1-norms of the column vectors. SwampingFor example: $$\\begin{cases}10^{-20}x_1 + x_2 = 1\\\\x_1 + 2x_2 = 4\\end{cases}$$We will solve the system three times: once with complete accuracy, second where we mimic a computer following IEEE double precision arithmetic, and once more where we exchange the order of the equations first. Solution: Exact solution. $$\\begin{bmatrix}10^{-20} &amp; 1 &amp; | &amp; 1\\\\1 &amp; 2 &amp; | &amp; 4\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space 10^{20} \\times row \\space 1 }\\begin{bmatrix}10^{-20} &amp; 1 &amp; | &amp; 1\\\\0 &amp; 2 - 10^{20} &amp; | &amp; 4 - 10^{20}\\end{bmatrix}$$The bottom equation is$$(2 - 10^{20}) x_2 = 4 - 10^{20}\\longrightarrowx_2 = \\frac{4 - 10^{20}}{2 - 10^{20}}$$and the top equation yields$$x_1 = \\frac{-2 \\times 10^{20}}{2 - 10^{20}}$$The exact solution is$$[x_1, x_2] \\approx [2,1]$$ IEEE double precision. $2 - 10^{20}$ is the same as $-10^{20}$, due to rounding. $4 - 10^{20}$ is stored as $-10^{20}$ Thus,$$[x_1, x_2] = [0,1]$$ IEEE double precision, after row exchange.$$\\begin{bmatrix}1 &amp; 2 &amp; | &amp; 4 \\\\10^{-20} &amp; 1 &amp; | &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space 10^{-20} \\times row \\space 1 }\\begin{bmatrix}1 &amp; 2 &amp; | &amp; 4\\\\0 &amp; 1 - 2 \\times 10^{-20} &amp; | &amp; 1 - 4 \\times 10^{-20}\\\\\\end{bmatrix}$$ $1 - 2 \\times 10^{-20}$ is stored as $1$, $1 - 4 \\times 10^{-20}$ is stored as $1$. $$[x_1, x_2] = [2,1]$$ The effect of subtracting $10^20$ times the top equation from the bottom equation was to overpower, or “swamp”, the bottom equation. The $PA = LU$ FactorizationPartial pivotingThe partial pivoting protocol consists of comparing numbers before carrying out each elimination step. (一种交换矩阵行的方法，用于避免因主对角线元素过小而导致的数值不稳定问题) For example: Apply Gaussian elimination with partial pivoting to solve the system$$\\begin{cases}x_1 - x_2 + 3x_3 = -3\\\\-x_1 - 2x_3 = 1\\\\2x_1 + 2x_2 + 4x_3 = 0\\end{cases}$$ Solution: This example is written in tableau form as$$\\begin{bmatrix}1 &amp; -1 &amp; 3 &amp; | &amp; -3\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\\\2 &amp; 2 &amp; 4 &amp; | &amp; 0\\end{bmatrix}$$Under partial pivoting we compare $|a_{11}| = 1$ with $|a_{21}| = 1$ and $|a_{31}| = 2$, choose $a_{31}$ for the new pivot. $$\\begin{bmatrix}1 &amp; -1 &amp; 3 &amp; | &amp; -3\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\\\2 &amp; 2 &amp; 4 &amp; | &amp; 0\\end{bmatrix}\\xrightarrow[exchange]{row \\space 1 &lt;-&gt;row \\space 3}\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\1 &amp; -1 &amp; 3 &amp; | &amp; -3\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space -\\frac{1}{2} \\times row \\space 1}\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; | &amp; 1\\\\-1 &amp; 0 &amp; -2 &amp; | &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space \\frac{1}{2} \\times row \\space 1}\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; | &amp; 1\\\\0 &amp; -2 &amp; 1 &amp; | &amp; -3\\end{bmatrix}$$ Before eliminating column $2$ we must compare the current $|a_{22}|$ with the current $|a_{32}|$.Because the latter is larger, we again switch rows: $$\\begin{bmatrix}2 &amp; 2 &amp; 4 &amp; | &amp; 0\\\\0 &amp; -2 &amp; 1 &amp; | &amp; -3\\\\0 &amp; 1 &amp; 0 &amp; | &amp; 1\\end{bmatrix}\\longrightarrow x = [1,1,-1]$$ Permutation matrices（置换矩阵） DEFINITION 2.7A permutation matrix is an $n \\times n$ matrix consisting of all zeros, except for a single $1$ in every row and column.（主元法） THEOREM 2.8Fundamental Theorem of Permutation Matrices. Let $P$ be the $n \\times n$ permutation matrix formed by a particular set of row exchanges applied to the identity matrix. Then, for any $n \\times n$ matrix $A$, $PA$ is the matrix obtained by applying exactly the same set of row exchanges to $A$. $PA = LU$ factorizationAs its name implies, the $PA=LU$ factorization is simply the $LU$ factorization of a row-exchanged version of $A$. For example: Find the $PA=LU$ factorization of the matrix, and solve $Ax = b$, where$$A =\\begin{bmatrix}2 &amp; 1 &amp; -5\\\\4 &amp; 4 &amp; -4\\\\1 &amp; 3 &amp; 1\\end{bmatrix},b =\\begin{bmatrix}5\\\\0\\\\6\\end{bmatrix}$$ Solution: $$\\begin{bmatrix}2 &amp; 1 &amp; -5\\\\4 &amp; 4 &amp; -4\\\\1 &amp; 3 &amp; 1\\end{bmatrix}\\xrightarrow[P = \\begin{bmatrix} 0 &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\end{bmatrix}]{exchange \\space row \\space 1&lt;-&gt;row \\space 2}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\2 &amp; 1 &amp; -5\\\\1 &amp; 3 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 2]{substract \\space \\frac{1}{2} \\times row \\space 1}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{2}} &amp; -1 &amp; 7\\\\1 &amp; 3 &amp; 1\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space \\frac{1}{4} \\times row \\space 1}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{2}} &amp; -1 &amp; 7\\\\\\boxed{\\frac{1}{4}} &amp; 2 &amp; 2\\end{bmatrix}$$ $$\\xrightarrow[P = \\begin{bmatrix} 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 0\\end{bmatrix}]{exchange \\space row \\space 2&lt;-&gt;row \\space 3}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{4}} &amp; 2 &amp; 2\\\\\\boxed{\\frac{1}{2}} &amp; -1 &amp; 7\\end{bmatrix}\\xrightarrow[from \\space row \\space 3]{substract \\space -\\frac{1}{2} \\times row \\space 2}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\\\boxed{\\frac{1}{4}} &amp; 2 &amp; 2\\\\\\boxed{\\frac{1}{2}} &amp; \\boxed{-\\frac{1}{2}} &amp; 8\\end{bmatrix}$$ So the $PA = LU$ factorization is :$$\\begin{bmatrix}0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\\\1 &amp; 0 &amp; 0\\end{bmatrix}\\begin{bmatrix}2 &amp; 1 &amp; -5\\\\4 &amp; 4 &amp; -4\\\\1 &amp; 3 &amp; 1\\end{bmatrix} =\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\\\boxed{\\frac{1}{4}} &amp; 1 &amp; 0\\\\\\boxed{\\frac{1}{2}} &amp; \\boxed{-\\frac{1}{2}} &amp; 1\\end{bmatrix}\\begin{bmatrix}4 &amp; 4 &amp; -4\\\\0 &amp; 2 &amp; 2\\\\0 &amp; 0 &amp; 8\\end{bmatrix}$$ This solving solution proess is:$$PAx = Pb \\Leftrightarrow LUx = Pb.$$ $Lc = Pb$$$c = \\begin{bmatrix} 0\\\\ 6 \\\\ 8\\end{bmatrix}$$ $Ux = c$$$x = [-1,2,1]$$（其实$x$是个列向量…） 12345678910111213141516171819202122&gt;&gt; A=[2 1 5; 4 4 -4; 1 3 1];&gt;&gt; [L,U,P]=lu(A)L = 1.0000 0 0 0.2500 1.0000 0 0.5000 -0.5000 1.0000U = 4 4 -4 0 2 2 0 0 8P = 0 1 0 0 0 1 1 0 0 Reality Check 2: The Euler–Bernoulli Beamto be added… Iterative MethodsGaussian elimination is called a direct method for solving systems of linear equations.So-called iterative methods also can be applied to solving systems of linear equations. Similar to Fixed-Point Iteration, the methods begin with an initial guess and refine the guess at each step, converging to the solution vector. Jacobi MethodThe Jacobi Method is a form of Fixed-point Iteration &gt;&gt; for a system of equations. DEFINITION 2.9The $n \\times n$ matrix $A = (a_{ij})$ is strictly diagonally dominant（严格对角占优） if, for each $1 \\leq i \\leq n$, $|a_{ii}| &gt; \\sum_{j \\neq i}|a_{ij} |$. In other words, each main diagonal entry dominates its row in the sense that it is greater in magnitude than the sum of magnitudes of the remainder of the entries in its row. THEOREM 2.10If the $n \\times n$ matrix $A$ is strictly diagonally dominant, then (1) $A$ is a nonsingular matrix（invertible）, and (2) for every vector $b$ and every starting guess, the Jacobi Method applied to $Ax = b $ converges to the (unique) solution. Note that strict diagonal dominance is only a sufficient condition. The Jacobi Method may still converge in its absence. (严格的对角占优只是充分条件，没有这个条件Jacobi Method 也可能收敛) Let $D$ denote the main diagonal of $A$, $L$ denote the lower triangle of $A$ (entries below the main diagonal), and $U$ denote the upper triangle (entries above the main diagonal). Then $A = L + D + U$, and the equation to be solved is $Lx + Dx + Ux = b$. $$Ax = b\\\\(D + L + U)x = b\\\\Dx = b - Lx - Ux\\\\x = D^{-1}(b - (L+u)x)\\tag{2.39}$$ Gauss–Seidel Method and SORGauss–Seidel often converges faster than Jacobi if the method is convergent. The method called Successive Over-Relaxation (SOR) takes the Gauss–Seidel direction toward the solution and “overshoots” to try to speed convergence. Let $\\omega$ be a real number, and define each component of the new guess $x_{k+1}$ as a weighted average of $\\omega$ times the Gauss–Seidel formula and $1 - \\omega$ times the current guess $x_k$. The number $\\omega$ is called the relaxation parameter, and $\\omega &gt; 1 $ is referred to as over-relaxation. For example:Apply the Jacobi Method, Gauss-Seidel Method and SOR Method to the system$$\\begin{cases}3u + v = 5\\\\u + 2v = 5\\end{cases}$$ Solution: We use the initial guess $(u_0, v_0) = (0,0)$.$$\\begin{bmatrix}3 &amp; 1\\\\1 &amp; 2\\end{bmatrix}\\begin{bmatrix}u \\\\ v\\end{bmatrix}=\\begin{bmatrix}5\\\\ 5\\end{bmatrix}$$ Jacobi Method$$\\begin{aligned}\\begin{bmatrix}u_{k+1} \\\\ v_{k+1}\\end{bmatrix}&amp;= D^{-1}(b - (L+U)x_k)\\\\&amp;=\\begin{bmatrix}\\frac{1}{3} &amp; 0\\\\0 &amp; \\frac{1}{2}\\end{bmatrix}(\\begin{bmatrix} 5 \\\\ 5\\end{bmatrix} - \\begin{bmatrix} 0 &amp; 1\\\\ 1 &amp; 0\\end{bmatrix}\\begin{bmatrix}u_{k} \\\\ v_{k}\\end{bmatrix})\\\\&amp;= \\begin{bmatrix}(5-v_k)/3\\\\(5-u_k)/2\\end{bmatrix}\\end{aligned}$$ Gauss-Seidel Method $$\\begin{bmatrix}u_{k+1} \\\\ v_{k+1}\\end{bmatrix}=\\begin{bmatrix}(5-v_k)/3\\\\(5-u_{k+1})/2\\end{bmatrix}$$ SOR Method $$u_{k+1} = (1-\\omega)u_k + \\omega \\frac{5-v_k}{3}\\\\v_{k+1} = (1-\\omega)v_k + \\omega \\frac{5-u_{k+1}}{2}$$ For example: Compare Jacobi, Gauss–Seidel, and SOR on the system of six equations in six unknowns:$$\\begin{bmatrix}3 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; \\frac{1}{2}\\\\−1 &amp; 3 &amp; −1 &amp; 0 &amp; \\frac{1}{2} &amp; 0\\\\0 &amp; −1 &amp; 3 &amp; −1 &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; -1 &amp; 3 &amp; -1 &amp; 0\\\\0 &amp; \\frac{1}{2} &amp; 0 &amp; −1 &amp; 3 &amp; −1\\\\\\frac{1}{2} &amp; 0 &amp; 0 &amp; 0 &amp; −1 &amp; 3\\\\\\end{bmatrix}\\begin{bmatrix}u_1 \\\\ u_2 \\\\ u_3 \\\\ u_4 \\\\ u_5 \\\\ u_6\\end{bmatrix}=\\begin{bmatrix}\\frac{5}{2} \\\\ \\frac{3}{2} \\\\ 1 \\\\ 1 \\\\ \\frac{3}{2} \\\\ \\frac{5}{2}\\end{bmatrix}$$ Solution: The solution is $x = [1,1,1,1,1,1]$. The approximate solution vectors $x_6$, after running six steps of each of the three methods, are shown in the following table: SOR appears to be superior for this problem. Convergence of iterative methodsto be added… Sparse matrix computationsA coefficient matrix is called sparse if many of the matrix entries are known to be zero Often, of the $n^2$ eligible entries in a sparse matrix, only $O(n)$ of them are nonzero. There are two reasons to use iterative methods while Gaussian elimination provide the user a finite number of steps that terminate the solution. For real-time applications, suppose that a solution to $Ax = b$ is known, after which $A$ and/or $b$ change by a small amount. In this case, we can iterative from the origin solution, which brings fast convergence. It’s called polish technique. Gaussian elimination for an $n \\times n$ matrix costs on the order of $n^3$ operations.A single step of Jacobi’s Method, for example, requires about $n^2$ multiplications (one for each matrix entry) and about the same number of additions. For sparse matrix, Gaussian elimination often brings fill-in, where the coefficient matrix changes from sparse to full due to the necessary row operations. Iterative method can avoid this problem. For example: Use the Jacobi Method to solve the 100,000-equation version of the Example above. Let $n$ be an even integer, and consider the $n \\times n$ matrix $A$ with $3$ on the main diagonal, $−1$ on the super-and subdiagonal, and $1/2$ in the $(i,n + 1 − i)$ position for all $i = 1,…,n$, except for $i = n/2$ and $n/2 + 1$. For $n = 12$, Define the vector $b = (2.5,1.5, . . . ,1.5,1.0,1.0,1.5, . . . ,1.5,2.5)$, where there are $n − 4$repetitions of $1.5$ and $2$ repetitions of $1.0$. Solution: Since fewer than $4n$ of the potential entries are nonzero, we may call the matrix sparse. In size, treating the coefficient matrix $A$ as a full matrix means storing $n^2 = 10^{10}$ entries, we need $10^{10} * 8Byte \\approx 80GB$ to store this matrix in computer. In time, $n^3 \\approx 10^{15}$, if the machine runs $10^9$ cycles per second, and upper bound on the number of floating point operations per second is around $10^{8}$. Then you need $\\frac{10^{15}}{10^{8}} = 10^7$ seconds, and there are $3 \\times 10^{7}$ seconds in a year, which means you need $\\frac{1}{3}$ year to solve this problem… On the other hand, one step of an iterative method will require approximately $2 \\times 4n = 800,000$ operations. We could do $100$ steps of Jacobi iteration and still finish with fewer than $10^8$ operations, which should take roughly a second or less on a modern PC. 1234567891011%sparsesetup.m% Program 2.1 Sparse matrix setup% Input: n = size of system% Outputs: sparse matrix a, r.h.s. bfunction [a,b] = sparsesetup(n)e = ones(n,1); n2=n/2;a = spdiags([-e 3*e -e],-1:1,n,n); % Entries of ac=spdiags([e/2],0,n,n);c=fliplr(c);a=a+c;a(n2+1,n2) = -1; a(n2,n2+1) = -1; % Fix up 2 entriesb=zeros(n,1); % Entries of r.h.s. bb(1)=2.5;b(n)=2.5;b(2:n-1)=1.5;b(n2:n2+1)=1; 12345678910111213%jacobi.m% Program 2.2 Jacobi Method% Inputs: full or sparse matrix a, r.h.s. b,% number of Jacobi iterations, k% Output: solution xfunction x = jacobi(a,b,k)n=length(b); % find nd=diag(a); % extract diagonal of ar=a-diag(d); % r is the remainderx=zeros(n,1); % initialize vector xfor j=1:k % loop for Jacobi iterationx = (b-r*x)./d;end % End of Jacobi iteration loop 123456789101112131415161718192021222324252627282930313233343536373839404142434445&gt;&gt; [a,b] = sparsesetup(6)a = (1,1) 3.0000 (2,1) -1.0000 (6,1) 0.5000 (1,2) -1.0000 (2,2) 3.0000 (3,2) -1.0000 (5,2) 0.5000 (2,3) -1.0000 (3,3) 3.0000 (4,3) -1.0000 (3,4) -1.0000 (4,4) 3.0000 (5,4) -1.0000 (2,5) 0.5000 (4,5) -1.0000 (5,5) 3.0000 (6,5) -1.0000 (1,6) 0.5000 (5,6) -1.0000 (6,6) 3.0000b = 2.5000 1.5000 1.0000 1.0000 1.5000 2.5000&gt;&gt; jacobi(a,b,40)ans = 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 Methods for symmetric positive-definite matricesSymmetric positive-definite matricesMore about Symmetric Matrix &gt;&gt; DEFINITION 2.12The $n \\times n$ matrixAis symmetric if $A^T = A$. The matrixAis positive-definite if $x^TAx &gt; 0$ for all vectors $x \\neq 0$. Note that a symmetric positive-definite matrix must be nonsingular（invertible）, since it is impossible for a nonzero vector $x$ to satisfy $Ax = 0$. Property 1If the $n \\times n$ matrix $A$ is symmetric, then $A$ is positive-definite if and only if all of its eigenvalues are positive. Property 2If A is $n \\times n$ symmetric positive-definite and $X$ is an $n \\times m$ matrix of full rank &gt;&gt; with $n \\geq m$, then $X^TAX$ is $m × m$ symmetric positive-definite. DEFINITION 2.13A principal submatrix of a square matrix $A$ is a square submatrix whose diagonal entries are diagonal entries of $A$. Property 3Any principal submatrix of a symmetric positive-definite matrix is symmetric positive definite.(对称正定矩阵的任何主子矩阵都是对称正定矩阵) Extra DefinitionA principal submatrix of a square matrix $A$ is the matrix obtained by deleting any $k$ rows and the corresponding $k$ columns.The determinant of a principal submatrix is called the principal minor of $A$. Extra DefinitionThe leading principal submatrix of order $k$ of an $n \\times n$ matrix is obtained by deleting the last $n - k$ rows and column of the matrix.The determinant of a leading principal submatrix is called the leading principal minor of $A$. Principal minors can be used in definitess tests Cholesky factorization THEOREM 2.14(Cholesky Factorization Theorem) If $A$ is a symmetric positive-definite $n \\times n$ matrix, then there exists an upper triangular $n \\times n$ matrix $R$ such that $A = R^TR$. For example: Find the Cholesky factorization of$$\\begin{bmatrix}4 &amp; -2 &amp; 2\\\\-2 &amp; 2 &amp; -4\\\\2 &amp; -4 &amp; 11\\end{bmatrix}$$ Solution: The top row of $R$ is $R_{11} = \\sqrt{a_{11}} = 2$, followed by $u^T = \\frac{1}{R_{11}}A_{1,2:3} = [-2,2]/R_{11} = [-1,1]$:$$R =\\left[\\begin{array}{c:cc}2 &amp; -1 &amp; 1\\\\\\hdashline\\\\\\end{array}\\right]$$So, $uu^T = \\begin{bmatrix}-1 &amp; 1\\end{bmatrix} \\begin{bmatrix}-1\\\\1\\end{bmatrix}$. The lower principle $2 \\times 2$ submatrix $A_{2:3,2:3}$ of $A$ is$$\\left[\\begin{array}{c:cc}&amp; &amp; &amp;\\\\\\hdashline&amp; 2 &amp; -4 \\\\&amp; -4 &amp; 11\\end{array}\\right]-\\left[\\begin{array}{c:cc}&amp; &amp; &amp;\\\\\\hdashline&amp; 1 &amp; -1 \\\\&amp; -1 &amp; 1\\end{array}\\right]=\\left[\\begin{array}{c:cc}&amp; &amp; &amp;\\\\\\hdashline&amp; 1 &amp; -3 \\\\&amp; -3 &amp; 10\\end{array}\\right]$$Now we repeat the same steps on the $2 \\times 2$ submatrix to find $R_{22} = 1$ and $R_{23} = −3/1=−3$:$$R = \\left[\\begin{array}{c:cc}2 &amp; -1 &amp; 1\\\\\\hdashline&amp; 1 &amp; -3\\\\\\end{array}\\right]$$The lower $1 \\times 1$ principal submatrix of $A$ is $10 − (−3)(−3) = 1$, so $R_{33} = \\sqrt 1$. The Cholesky factor of $A$ is$$R = \\left[\\begin{array}{ccc}2 &amp; -1 &amp; 1\\\\0 &amp; 1 &amp; -3\\\\0 &amp; 0 &amp; 1\\end{array}\\right]$$ Conjugate Gradient Method（共轭梯度法） DEFINITION 2.15Let $A$ be a symmetric positive-definite $n \\times n$ matrix. For two $n$-vectors $v$ and $w$, define the $A$-inner product$$(v,w)_A = v^TAw$$The vectors $v$ and $w$ are $A$-conjugate if $(v,w)_A = 0$. The vector $x_k$ is the approximate solution at step $k$. The vector $r_k$ represents the residual of the approximate solution $x_k$. The vector $d_k$ represents the new search direction used to update the approximation $x_k$ to the improved version $x_{k+1}$. THEOREM 2.16Let $A$ be a symmetric positive-definite $n \\times n$ matrix and let $b \\neq 0$ be a vector. In the Conjugate Gradient Method, assume that $r_k \\neq 0$ for $k &lt; n$ (if $r_k = 0$ the equation is solved). Then for each $1 \\leq k \\leq n$,(a) The following three subspaces of $R^n$ are equal: $&lt;x_1, . . . , x_k&gt; = &lt;r_0, . . . , r_{k−1}&gt; = &lt;d_0, . . . , d_{k−1}&gt;$(b) the residuals $r_k$ are pairwise orthogonal: $r^T_k r_j = 0$ for $j &lt; k$,(c) the directions $d_k$ are pairwise $A$-conjugate: $d^T_k A d_j = 0$ for $j &lt; k$. Preconditioning Nonlinear Systems of EquationsThis section describes Newton’s Method and variants for the solution of systems of nonlinear equations. Multivariate Newton’s MethodOne-variable Newton’s Method is$$f(x) = 0\\\\x_{k+1} = x_k - \\frac{f(x)}{f’(x_k)}$$ Let $$f_1(u,v,w) = 0\\\\f_2(u,v,w) = 0\\\\f_3(u,v,w) = 0\\\\\\tag{2.49}$$be three nonlinear equations in three unknowns $u,v,w$. Define the vector-valued function $F(u,v,w) = (f_1, f_2,f_3)$. The analogue of the derivative $f’$ in the one-variable case is the Jacobian matrix defined by $$DF(x) =\\begin{bmatrix} \\frac{\\partial f_1}{\\partial u} &amp; \\frac{\\partial f_1}{\\partial v} &amp; \\frac{\\partial f_1}{\\partial w}\\\\ \\frac{\\partial f_2}{\\partial u} &amp; \\frac{\\partial f_2}{\\partial v} &amp; \\frac{\\partial f_2}{\\partial w}\\\\ \\frac{\\partial f_3}{\\partial u} &amp; \\frac{\\partial f_3}{\\partial v} &amp; \\frac{\\partial f_3}{\\partial w}\\end{bmatrix}$$ The Taylor expansion for vector-valued functions around $x_0$ is $$F(x) = F(x_0) + DF(x_0) \\cdot (x - x_0) + O(x - x_0)^2$$ Newton’s Method is based on a linear approximation, ignoring the $O(x^2)$ terms. Since computing inverses is computationally burdensome, we use a trick to avoid it. Set $x_{k+1} = x_k − s$, where s is the solution of $DF(x_k)s = F(x_k)$. Now, only Gaussian elimination ($n^3/3$ multiplications) is needed to carry out a step, instead of computing an inverse (about three times as many). $$\\begin{cases}DF(x_k)s = -F(x_k)\\\\x_{k+1} = x_k + s\\end{cases}\\tag{2.51}$$ For example: Use Newton’s Method with starting guess $(1,2)$ to find a solution of the system$$\\begin{cases}v - u^3 = 0\\\\u^2 + v^2 - 1= 0\\end{cases}$$ Solution: The Jacobian matrix is$$DF(u,v) =\\begin{bmatrix}-3u^2 &amp; 1\\\\2u &amp; 2v\\end{bmatrix}$$Using starting point $x_0 = (1,2)$, on the first step we must solve the matrix equation (2.51): $$\\begin{bmatrix} -3 &amp; 1\\\\ 2 &amp; 4\\end{bmatrix}\\begin{bmatrix} s_1\\\\s_2\\end{bmatrix}=-\\begin{bmatrix} 1\\\\4\\end{bmatrix}$$The solution is s = $(0,−1)$, so the first iteration produces $x_1 = x_0 + s = (1,1)$. The secondstep requires solving $$\\begin{bmatrix} -3 &amp; 1\\\\ 2 &amp; 2\\end{bmatrix}\\begin{bmatrix} s_1\\\\s_2\\end{bmatrix}=-\\begin{bmatrix} 0\\\\1\\end{bmatrix}$$ … Broyden’s MethodNewton’s Method is a good choice if the Jacobian can be calculated. If not, the best alternative is Broyden’s Method. Suppose $A_i$ is the best approximation available at step $i$ to the Jacobian matrix, and that it has been used to create$$x_{i+1} = x_i - A_i^{-1}F(x_i)\\tag{2.52}$$To update $A_i$ to $A_{i+1}$ for the next step, we would like to respect the derivative aspect of the Jacobian $DF$（从导数的定义出发）, and satisfy$$A_{i+1}\\delta_{i+1} = \\Delta_{i+1}\\tag{2.53}$$where, $\\delta_{i+1} = x_{i+1} - x_i$ and $\\Delta_{i+1} = F(x_{i+1}) - F(x_{i})$. = = Don’t understand.. = = On the other hand, for the orthogonal complement(正交补) &gt;&gt; of $\\delta_{i+1}$, we have no new information. Therefore, we ask that$$A_{i+1}w = A_i w\\tag{2.54}$$for every $w$ satisfying (why?)$$\\delta_{i+1}^T w = 0$$One checks that a matrix that satisfies both (2.53) and (2.54) is$$A_{i+1} = A_i + \\frac{(\\Delta_{i+1} - A_i \\delta_i)\\delta_{i+1}^T}{\\delta_{i+1}^T\\delta_{i+1}}\\tag{2.55}$$Broyden’s Method uses the Newton’s Method step (2.52) to advance the current guess, while updating the approximate Jacobian by (2.55).","link":"/Math/Numerical-Analysis/NumericalAnalysis-C2-Systems-of-Equations/"},{"title":"NumericalAnalysis-C7-Boundary-Value-Problems","text":"Keywords: Finite elements and the Galerkin Method, Finite Difference Methods, Mathlab Shooting MethodSolutions of boundary value problemsShooting Method implementationReality Check 7: Buckling of a Circular RingFinite Difference MethodsLinear boundary value problemsNonlinear boundary value problemsCollocation and the Finite Element MethodCollocationFinite elements and the Galerkin Method","link":"/Math/Numerical-Analysis/NumericalAnalysis-C7-Boundary-Value-Problems/"},{"title":"NumericalAnalysis-C4-Least-Squares","text":"Keywords: Least Squares, QR Factorization, Levenberg–Marquardt Method, Gauss–Newton Method, Mathlab Least Squares and the Normal EquationsWhy Least Squares? For Chapter2, get the solution of $Ax = b$, what if there’s no Solution? When the equations are inconsistent, which is likely if the number of equations exceeds the number of unknowns, the answer is to find the next best thing: the least squares approximation. For Chapter3, find the polynomials to fit data points. However, if the data points are numerous, or the data points are collected only within some margin of error, fitting a high-degree polynomial exactly is rarely the best approach. In such cases, it is more reasonable to fit a simpler model that may only approximate the data points. More about Least Squares in Algebra &gt;&gt; Inconsistent systems of equationsA system of equations with no solution is called inconsistent. There are at least three ways to express the size of the residual. The Euclidean length of a vector, $$||r||_2 = \\sqrt{r_1^2 + r_2^2 + \\cdots + r_m^2}\\tag{4.7}$$is a norm, called the 2-norm. The squared error,$$SE = r_1^2 + r_2^2 + \\cdots + r_m^2$$and the root mean squared error (the root of the mean of the squared error)$$RMSE = \\sqrt{SE/m} = \\frac{||r||_2}{\\sqrt m}\\tag{4.8}$$ Fitting models to dataFitting data by least squares STEP 1. Choose a model. STEP 2. Force the model to fit the data. STEP 3. Solve the normal equations. Usually, the reason for using least squares is to replace noisy data with a plausible underlying model.The model is then often used for signal prediction or classification purposes. We have presented the normal equations as the most straightforward approach to solving the least squares problem, and it is fine for small problems.However, the condition number $cond(A^TA)$ is approximately the square of the original $cond(A)$, which will greatly increase the possibility that the problem is ill-conditioned. More sophisticated methods allow computing the least squares solution directly from $A$ without forming $A^TA$. These methods are based on the QR-factorization, introduced in Section 4.3, and the singular value decomposition of Chapter 12. Conditioning of least squaresFor example: Let $x_1 = 2.0,x_2 = 2.2,x_3 = 2.4, . . . , x_{11} = 4.0$ be equally spaced points in $[2,4]$, and set $y_i = 1 + x_i + x^2_i + x^3_i + x^4_i + x^5_i + x^6_i + x^7_i$ for $1 \\leq i \\leq 11$. Use the normal equations to find the least squares polynomial $P(x) = c_1 + c_2x + · · · + c_8x^7$ fitting the $(x_i,y_i)$. Solution:A degree $7$ polynomial is being fit to $11$ data points lying on the degree $7$ polynomial $P(x) = 1 + x + x^2 + x^3 + x^4 + x^5 + x^6 + x^7$. $$\\begin{bmatrix}1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^7\\\\1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^7\\\\&amp; &amp; \\cdots &amp;\\\\1 &amp; x_{11} &amp; x_{11}^2 &amp; \\cdots &amp; x_{11}^7\\end{bmatrix}\\begin{bmatrix}c_1\\\\c_2\\\\ \\cdots \\\\c_8\\end{bmatrix}=\\begin{bmatrix}y_1\\\\y_2\\\\ \\cdots \\\\y_{11}\\end{bmatrix}$$The coefficient matrix $A$ is a Van der Monde matrix. 12345678910111213141516171819202122232425&gt;&gt; x = (2+(0:10)/5)';&gt;&gt; y = 1+x+x.^2+x.^3+x.^4+x.^5+x.^6+x.^7;&gt;&gt; A = [x.^0 x x.^2 x.^3 x.^4 x.^5 x.^6 x.^7];&gt;&gt; c = (A'*A)\\(A'*y)警告: 矩阵接近奇异值，或者缩放错误。结果可能不准确。RCOND = 2.075619e-20。 c = 5.1141 -9.1641 11.6534 -5.1416 3.1036 0.5718 1.0480 0.9977&gt;&gt; cond(A'*A)ans = 6.4586e+18 The value in my experiment is not the same with the book, so far I can’t find the reason… Obviously, The condition number of $A^TA$ is too large to deal with in double precision arithmetic, and the normal equations are ill-conditioned, even though the originalleast squares problem is moderately conditioned. A Survey of ModelsPeriodic dataPeriodic data calls for periodic models.Outside air temperatures, for example, obey cycles on numerous timescales, including daily and yearly cycles governed by the rotation of the earth and the revolution of the earth around the sun. Data linearizationExponential growth of a population is implied when its rate of change is proportional to its size. The exponential model $$y = c_1e^{c_2t}\\tag{4.10}$$cannot be directly fit by least squares $Ax = b$ because $c_2$ does not appear linearly in the model equation. Two methods to solve this problem. 1. Linearization thie nonlinear problem. 2. use nonlinear techniques, like Gauss-Newton Method, which will introduce next. $$\\ln y = \\ln(c_1e^{c_2t}) = \\ln c_1 + \\ln^{e^{c_2t}} = \\ln c_1 + c_2 t\\tag{4.11}$$The $c_2$ coefficient is now linear in the model, but $c_1$ no longer is. By renaming $k = \\ln c_1$, we can write $$\\ln y = k + c_2 t\\tag{4.12}$$Now both coefficients $k$ and $c_2$ are linear in the model. The original problem and the linearization problem, these are two different minimizations and have different solutions, meaning that they generally result in different values of the coefficients $c_1,c_2$. When you choose the techniques, choose the more natural choice. For example: Use model linearization to find the best least squares exponential fit $y = c_1e^{c_2t}$ to the following world automobile supply data: Solution: $$k_1 \\approx 3.9896, c_2 \\approx 0.06152, c_1 = e^{3.9896} \\approx 54.03$$ the RMSE of the log-linearized model in log space is $\\approx 0.0357$, while RMSE of the original exponential model is $\\approx 9.56$. QR FactorizationGram–Schmidt orthogonalization and least squaresThe result of Gram–Schmidt orthogonalization can be written in matrix form as, let A be $m \\times n$ matrix.$$(A_1|\\cdots|A_n) = (q_1|\\cdots|q_n)\\begin{bmatrix}r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n}\\\\&amp; r_{22} &amp; \\cdots &amp; r_{2n}\\\\&amp; &amp; \\ddots &amp; \\vdots\\\\&amp; &amp; &amp; r_{nn}\\end{bmatrix}\\tag{4.26}$$where, $r_{jj} = ||y_j||2$ and $r{ij} = q_i^TA_j$. Like$$y_1 = A_1, q_1 = \\frac{y_1}{||y_1||_2}\\tag{4.23}$$ $$y_2 = A_2 - q_1(q_1^TA_2), q_2 = \\frac{y_2}{||y_2||_2}\\tag{4.24}$$ We call this $A = QR$ the reduced QR factorization. This matrix equation is the full QR factorization:$$(A_1|\\cdots|A_n) = (q_1|\\cdots|q_m)\\begin{bmatrix}r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n}\\\\&amp; r_{22} &amp; \\cdots &amp; r_{2n}\\\\&amp; &amp; \\ddots &amp; \\vdots\\\\&amp; &amp; &amp; r_{nn}\\\\0 &amp; \\cdots &amp; \\cdots &amp; 0\\\\\\vdots &amp; &amp; &amp; \\vdots\\\\0 &amp; \\cdots &amp; \\cdots &amp; 0\\\\\\end{bmatrix}\\tag{4.27}$$ DEFINITION 4.1A square matrix $Q$ is orthogonal if $Q^T = Q^{−1}$. The key property of an orthogonal matrix is that it preserves the Euclidean norm of a vector. LEMMA 4.2If $Q$ is an orthogonal $m \\times m$ matrix and $x$ is an $m$-dimensional vector, then $||Qx||_2 = ||x||_2$.Proof.$$||Qx||_2^2 = (Qx)^T(Qx) = x^TQ^TQx = x^Tx = ||x||_2^2$$ Doing calculations with orthogonal matrices is preferable because (1) they are easy to invert by definition,and (2) by Lemma4.2,they do not magnify errors. There are three ways to use $QR$ factorization. Sovle $Ax = b$. The $QR$ factorization of an $m \\times m$ matrix by the Gram–Schmidt method requires approximately $m^3$ multiplication/divisions, three times more than the $LU$ factorization, plus about the same number of additions. Calculate eigenvalues, which will be introduced in Chapter12. Solve Least squares. Let A be an $m \\times n$ matrix, $m &gt; n$. To minimize $||Ax - b||_2$, rewrite as $||QRx - b||_2 = ||Rx - Q^Tb||_2$ by lemma 4.2. The vector inside the Euclidean norm is$$\\begin{bmatrix}e_1\\\\ \\vdots \\\\ e_n \\\\ \\hdashline \\\\ e_{n+1} \\\\ \\vdots\\end{bmatrix}=\\begin{bmatrix}r_{11} &amp; r_{12} &amp; \\cdots &amp; r_{1n}\\\\&amp; r_{22} &amp; \\cdots &amp; r_{2n}\\\\&amp; &amp; \\ddots &amp; \\vdots\\\\&amp; &amp; &amp; r_{nn}\\\\0 &amp; \\cdots &amp; \\cdots &amp; 0\\\\\\vdots &amp; &amp; &amp; \\vdots\\\\0 &amp; \\cdots &amp; \\cdots &amp; 0\\\\\\end{bmatrix}\\begin{bmatrix} x_1\\\\ \\vdots \\\\ x_n\\end{bmatrix}-\\begin{bmatrix} d_1 \\\\ \\vdots \\\\ d_n \\\\ \\hdashline \\\\ d_{n+1} \\\\ \\vdots \\\\ d_m\\end{bmatrix}\\tag{4.28}$$ the least squares solution is minimized by using the $x$ from back-solving the upper part, andthe least squares error is $||e||2^2 = d{n+1}^2 + \\cdots + d^2_m$. For example: Use the full $QR$ factorization to solve the least squares problem$$\\begin{bmatrix} 1 &amp; -4\\\\ 2 &amp; 3\\\\ 2 &amp; 2\\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix} -3\\\\ 15\\\\ 9\\end{bmatrix}$$ Solution: $$Rx = Q^Tb$$$$\\begin{bmatrix} 3 &amp; 2\\\\ 0 &amp; 5\\\\ \\hdashline 0 &amp; 0\\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix} 15\\\\ 9 \\\\ \\hdashline 3\\end{bmatrix}$$The least squares error will be $||e||_2 = ||(0,0,3)||_2 = 3$. Equating the upper parts yields$$\\begin{bmatrix} 3 &amp; 2\\\\ 0 &amp; 5\\end{bmatrix}\\begin{bmatrix} x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix} 15\\\\ 9\\end{bmatrix}$$whose solution is$$\\bar{x_1} = 3.8, \\bar{x_2} = 1.8$$ 1234567891011121314151617&gt;&gt; x = (2+(0:10)/5)';&gt;&gt; y = 1+x+x.^2+x.^3+x.^4+x.^5+x.^6+x.^7;&gt;&gt; A = [x.^0 x x.^2 x.^3 x.^4 x.^5 x.^6 x.^7];&gt;&gt; [Q,R] = qr(A)&gt;&gt; b = Q'*y&gt;&gt; c=R(1:8,1:8)\\b(1:8)c = 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 1.0000 Modified Gram–Schmidt orthogonalizationA slight modification to Gram–Schmidt turns out to enhance its accuracy in machine calculations. Householder reflectors LEMMA 4.3Assume that $x$ and $w$ are vectors of the same Euclidean length, $||x||_2 = ||w||_2$. Then $w − x$ and $w + x$ are perpendicular. THEOREM 4.4 Householder reflectors. Let $x$ and $w$ be vectors with $||x||_2 = ||w||_2$ and define $v = w − x$. Then $H = I − 2vv^T / v^Tv$ is a symmetric orthogonal matrix and $Hx = w$. 4.4 Generalized Minimum Residual (GMRES) MethodKrylov methods Preconditioned GMRES Nonlinear Least SquaresGauss–Newton Method Models with nonlinear parametersThe Levenberg–Marquardt Method（LM Algorithm） Reality Check 4: GPS,Conditioning, and Nonlinear Least Squaresto be added..","link":"/Math/Numerical-Analysis/NumericalAnalysis-C4-Least-Squares/"},{"title":"Paper-Keyframe-Control-of-Music-driven-3D-Dance-Generation","text":"Keywords: Probability Distribution Model, Variational Inference, Normalizing Flow, 2022 Keyframe Control of Music-driven 3D Dance Generation_IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS_2022 • Z. Yang, S.-Y. Chen and L. Gao are with the Beijing Key Laboratory of Mobile Computing and Pervasive Device, Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China, and also with the University of Chinese Academy of Sciences, Beijing 100190, China.E-mail: yangzhipeng19s@ict.ac.cn, chenshuyu@ict.ac.cn, gaolin@ict.ac.cn • Y.-H. Wen and Y.-J. Liu are with the CS Dept, BNRist, Tsinghua University,Beijing 100190, China.Email: wenyh1616@tsinghua.edu.cn, liuyongjin@tsinghua.edu.cn • X. Liu and Y. Gao are with Tomorrow Advancing Life Education Group,Beijing 100190, China.Email: liuxiao15@tal.com, gaoyuan23@tal.com • H. F is with the School of Creative Media, City University of Hong Kong.Email: hongbofu@cityu.edu.hk Video:https://www.youtube.com/watch?v=eJ8mFzzZ0mk WorkFlow ProblemMost existing deep learning methods mainly rely on music for dance generation and lack sufficient control over generated dance motions. To address this issue, we introduce the idea of keyframe interpolation for music-driven dance generation and present a novel transition generation technique for choreography. Inspiration keyframe-based control, which allows artists to depict their ideas via a sparse set of keyframes, has been widely adopted for creating 2D and 3D animations. Inspired by this, we build a probabilistic model for dance generation conditioned on the input music and key poses. the probabilistic model cannot generate transition dance motions between keyframes robustly, because the distance between a target pose and its corresponding generated pose is not clear and each generated dance motion is not aware of the number of left frames to its target keyframe due to the varying intervals between key poses. To solve this problem, we add a key pose loss term that describes the distance between the input key poses and their corresponding generated poses to our objective function for modeling the distribution of dance motions. introduce a time embedding at each timestep as an additional condition to achieve a robust transition generation of varying lengths. MethodsFirst Understand Variational Inference &gt;&gt; Second Understand Variational Inference with Normalizing Flow &gt;&gt; Losses$$\\mathcal{L}_{flows} = \\log q(z_K | x_{t-\\tau : t}, s_t)= - \\sum_{d=1}^{D} (\\frac{1}{2}\\epsilon^2_d + \\frac{1}{2}\\log(2\\pi) + \\sum_{k=0}^K \\log \\delta_{k,d})$$ calculate the key pose loss for the generated sequence with an $L_2$ norm, where Forward Kinematics (FK) is performed to calculate the global joint positions of the human motion.$$\\mathcal{L}_{key-pose} = ||FK(x_\\gamma’) - FK(e_\\gamma)||_2$$ use a contact-based loss constrain the distance between the generated feet contacts $c_t’$ of $x_t’$ and ground-truth contacts $c_t$ of $x_t$,$$\\mathcal{L}_{feet} = \\frac{1}{\\gamma - \\tau} \\sum_{t = \\tau}^{\\gamma} ||c_t’ - c_t||_1$$ $$\\mathcal{L}_{total} = \\omega_1 \\mathcal{L}_{flows} + \\omega_2\\mathcal{L}_{key-pose} + \\omega_3 \\mathcal{L}_{feet}$$ Time Embeddinguse positional encodings, which shift smoothly and uniquely to represent the location of each frame$$E_{t,2l} = \\sin(\\frac{nt}{basis^{2l/D}})$$$$E_{t,2l+1} = \\cos(\\frac{nt}{basis^{2l/D}})$$ where $nt$ is the time step between the start frame and the target frame. $nt$ can evolve forward and backward in time. it is better to use $nt$ evolving backward, which represents the number of frames left to reach the target frame. $D$ denotes the dimension of the input motion, and $l \\in [0, \\cdots, D/2]$. Comparision our method is more realistic, generate more movements. our method outperforms the other methods in beat alignment. our method generates more variable dance motions given the same music clip. out method generates diverse results given different music inputs while other methods generate similar motions. Limitation our method cannot generate dance styles not covered in the training dataset. This might be improved by expanding the datasets with additional styles, like Classical, Jazz, etc. the generated poses in key frames might still have small differences compared to the given key poses. The gaps particularly exist in rotation angles of end effectors (feet and hands). This issue might addressed by a further refinement in the feet and hand details. Future Research developing an interactive dance composing system, where users can easily input desired music pieces and target poses to drive automatic dance generation. the target poses can be chosen from a database or reconstructed from user-specified human dancing photos by a reconstruction function integrated in the system. synthesizing dance motions conditioned on human emotion is another interesting research direction.","link":"/Paper/Motion-Generation/Paper-Keyframe-Control-of-Music-driven-3D-Dance-Generation/"},{"title":"NumericalAnalysis-C5-Numerical-Differentiation-and-Integration","text":"Keywords: Numerical Differentiation, Simpson’s Rule, Gaussian Quadrature, Mathlab Numerical Differentiation（数值微分）More about Differentiation and Derivative in Calculus &gt;&gt; Finite difference formulasFrom Taylor’s Theorem &gt;&gt;, we know if $f$ is twice continuously differentiable, then$$f(x+h) = f(x) + hf’(x) + \\frac{h^2}{2}f’’(c)\\tag{5.2}$$ we use the equation (5.3) as the approximation$$f’(x) \\approx \\frac{f(x+h)-f(x)}{h}\\tag{5.4}$$and treating the last term in (5.3) as error. In general, if the error is $O(h^n)$, we call the formula an order $n$ approximation. Rounding errorExtrapolationSymbolic differentiation and integrationNewton–Cotes Formulas for Numerical Integration (数值积分)Trapezoid RuleSimpson’s RuleComposite Newton–Cotes formulasOpen Newton–Cotes MethodsRomberg IntegrationAdaptive QuadratureGaussian QuadratureReality Check 5: Motion Control in Computer-Aided Modeling","link":"/Math/Numerical-Analysis/NumericalAnalysis-C5-Numerical-Differentiation-and-Integration/"},{"title":"Paper-A-Method-for-Animating-Childrens-Drawings-of-the-Human-Figure","text":"Keywords: Children’s Drawing, Existing Models and Techniques, Mask R-CNN, A Method for Animating Children’s Drawings of the Human Figure_ACM Transactions on Graphics_2023 HARRISON JESSE SMITH , Meta AI Research, USAQINGYUAN ZHENG , Tencent America, USAYIFEI LI , MIT CSAIL, USASOMYA JAIN , Meta AI Research, USAJESSICA K. HODGINS , Carnegie Mellon University, USA Authors’ addresses:H. J. Smith and S. Jain, Meta AI, 1 Hacker Way, Menlo Park, CA 94025 USA; emails: {hjessmith, somyaj}@gmail.com;Q. Zheng, Tencent America, 2747 Park Blvd, Palo Alto, CA 94306 USA; email: qyzzheng@global.tencent.com;Y. Li, MIT CSAIL, 32 Vassar St, Cambridge, MA 02139 USA; email: liyifei@csail.mit.edu;J. K. Hodgins, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213 USA; email: jkh@cmu.edu. https://sketch.metademolab.com/ WorkFlowProblemWho among us has not wished, either as a child or as an adult, to see such figures come to life and move around on the page? Sadly, while it is relatively fast to produce a single drawing, creating the sequence of images necessary for animation is a much more tedious endeavor, requiring discipline, skill, patience, and sometimes complicated software. As a result, most of these figures remain static upon the page. InnovationInspired by the importance and appeal of the drawn human figure, we design and build a system to automatically animate it given an in-the-wild photograph of a child’s drawing. While our system leverages existing models and techniques, most are not directly applicable to the task due to the many differences between photographic images and simple pen-and- paper representations. To summarize, our contributions are as follows: We explore the problem of automatic sketch-to-animation for children’s drawings of human figures and present a framework that achieves this effect.We also present a set of experiments determining the amount of training data necessary to achieve high levels of success and a perceptual study validating the usefulness of our motion retargeting technique. To encourage additional research in the domain of amateur drawings, we present a first-of-its-kind dataset of 178,166 user-submitted amateur drawings, along with user accepted bounding box, segmentation mask, and joint location annotations. Our work builds on existing methods from several fields but is, to our knowledge, the first work focused specifically on fully automatic animation of children’s drawings of human figures. Methods In Machine Learning, in-the-wild means 数据源的多样性比较高，包含的场景多，可以用来间接证明模型的泛化能力比较强 Figure DetectionWe make use of a state-of-the-art object detection model, Mask R-CNN [He et al. 2017], with a ResNet-50+FPN backbone. Therefore, we fine-tune the model.The model’s backbone weights are frozen and attached to a head, which predicts a single class, human figure. The weights of the head are then optimized using cross-entropy loss and stochastic gradient descent with an initial learning rate of 0.02, momentum of 0.9, weight decay of 1e-4, and minibatches of size 8. Training was conducted using OpenMMLab Detection Toolbox [Chen et al. 2019]; all other hyperparameters were kept at the default values provided by the toolbox. Figure Segmentation traditional contour detection algorithms / 形态学 轮廓 边界跟踪 算法 First, we resize the bounding box-cropped image to a width of 400 pixels while preserving the aspect ratio. Next, we convert the image to grayscale and perform adaptive thresholding, where the threshold value is a Gaussian-weighted sum of the neighborhood pixel values minus a constant C [Gonzalez and Woods 2008].Here, we use a distance of 8 pixels to define the neighborhood and a value of 115 for $C$. 高斯核模糊：平滑最终提取的轮廓, Gaussian kernel blur: Smooths the final extracted contour To remove noise and connect foreground pixels, we next perform morphological closing, followed by dilating(扩张), using 3×3 rectangular kernels. 扩张可以防止提取到的轮廓过于贴合从而丢失完整性, Dilation prevents the extracted contours from fitting too well and losing completeness We then flood fill from the edges of the image, ensuring that any closed groups of foreground pixels are solid and do not contain holes. (BFS) Finally, we calculate the area of each distinct foreground polygon and retain only the one with the largest area. OpenCV has Mature algorithms However, it will fail when body parts are drawn separated, limbs are drawn touching at points other than the joints, the figure is not fully contained by the bounding box, or the outline of the figure is not completely connected. Pose Estimation We assume the presence of the 17 keypoints used by MS-COCO [Lin et al. 2014](see Figure 8 ) and use a pose estimation model to predict their locations. We therefore train a custom pose estimation model utilizing a ResNet-50 backbone, pretrained on ImageNet, and a top-down heat map keypoint head that predicts an individual heatmap for each joint location. The cropped human figure bounding box is resized to 192×256 and fed into the model, and the highest-valuedpixel in each heatmap is taken as the predicted joint location. Mean squared error is used for joint loss, and optimization is performed using Adaptive Momentum Estimation with learning rate of 5e-4 and minibatches of size 512. Training was conducted using the OpenMMLab Pose Toolbox [Contributors 2020]; all other hyperparameters were kept at the default values provided by this toolbox. Animation From the segmentation mask, we use Delaunay triangulation to generate a 2D mesh. Using the joint locations, we construct a character skeleton. We average the position of the two hips to obtain a root joint and average the position of the two shoulders to obtain the chest joint. We connect these joints to create the skeletal rig as shown in Figure 8 (b). Finally, we assign each mesh triangle to one of nine different body part groups (left upper leg, left lower leg, right up- per leg, right lower leg, left upper arm, left lower arm, right upper arm, right lower arm, and trunk) by finding the closest bone to each triangle’s centroid. We animate the character rig by translating the joints and using as-rigid-as-possible (ARAP) shape manipulation[Igarashi et al. 2005] to repose the character mesh. To make the process simple for the user, we drive the character rig using a library of preselected motion clips obtained from human performers. We retarget the motion in the following manner: Fine-Tune Related Work2D Image to Animation Hornung et al. present a method to animate a 2D character in a photograph, given user-annotated joint locations [Hornung et al. 2007].Alexander Hornung, Ellen Dekkers, and Leif Kobbelt. 2007. Character animation from 2D pictures and 3D motion data. ACM Trans. Graph. 26, 1 (Jan. 2007), 1–es. DOI: https://doi.org/10.1145/1189762.1189763 Pan and Zhang demonstrate a method to animate 2D characters with user-annotated joint locations via a variable-length needle model [Pan and Zhang 2011].Junjun Pan and Jian J. Zhang. 2011. Sketch-based Skeleton-driven 2D Animation and Motion Capture . Springer Berlin, 164–181. DOI: https://doi.org/10.1007/978- 3- 642- 22639-7 _ 17 Jain et al. present an integrated approach to generate 3D proxies for animation given joint locations, segmentation masks, and per-part bounding boxes [Jain et al. 2012].Eakta Jain, Yaser Sheikh, Moshe Mahler, and Jessica Hodgins. 2012. Three- dimensional proxies for hand-drawn characters. ACM Trans. Graph. 31, 1 (Feb. 2012). Levi and Gotsman provide a method to create an artic- ulated 3D object from a set of annotated 2D images and an initial 3D skeletal pose [Levi and Gotsman 2013].Zohar Levi and Craig Gotsman. 2013. ArtiSketch: A system for articulated sketch modeling. Comput. Graph. Forum 32, 2pt2 (2013), 235–244. DOI: https://doi.org/ 10.1111/cgf.12043 Live Sketch [Su et al. 2018] tracks control points from a video and applies their motion to user-specified control points upon a character.Qingkun Su, Xue Bai, Hongbo Fu, Chiew-Lan Tai, and Jue Wang. 2018. Live sketch: Video-driven dynamic deformation of static drawings. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI’18) . Association for Computing Machinery, New York, NY, 1–12. DOI: https://doi.org/10.1145/ 3173574.3174236 Other approaches allow the user to specify character motions through a puppeteer interface, using RGB or RGB-D cameras [Barnes et al. 2008 ; Held et al. 2012].Robert Held, Ankit Gupta, Brian Curless, and Maneesh Agrawala. 2012. 3D puppetry: A Kinect-based interface for 3D animation. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST’12) . Associa- tion for Computing Machinery, New York, NY, 423–434. DOI: https://doi.org/10. 1145/2380116.2380170 ToonCap [Fan et al. 2018] focuses on an inverse problem, capturing poses of a known cartoon character, given a previous image of the character annotated with layers, joints, and handles.Xinyi Fan, Amit H. Bermano, Vladimir G. Kim, Jovan Popović, and Szymon Rusinkiewicz. 2018. ToonCap: A layered deformable model for capturing poses from cartoon characters. In Proceedings of the Joint Symposium on Computational Aesthetics and Sketch-Based Interfaces and Modeling and Non-photorealistic Animation and Rendering (Expressive’18) . Association for Computing Machinery, New York, NY. DOI: https://doi.org/10.1145/3229147.3229149 ToonSynth [Dvorožnák et al. 2018 ] and Neural Puppet [Poursaeed et al. 2020] both present methods to synthesize animations of hand-drawn characters given a small set of drawings of the character in specified poses.Marek Dvorožnák, Wilmot Li, Vladimir G. Kim, and Daniel Sýkora. 2018. ToonSynth: Example-based synthesis of hand-colored cartoon animations. ACM Trans. Graph. 37, 4 (July 2018). DOI: https://doi.org/10.1145/3197517.3201326 Omid Poursaeed, Vladimir Kim, Eli Shechtman, Jun Saito, and Serge Belongie. 2020. Neural puppet: Generative layered cartoon characters. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision . 3346–3356. Hinz et al. train a network to generate new animation frames of a single character given 8–15 training images with user-specified keypoint annotations [Hinz et al. 2022].Tobias Hinz, Matthew Fisher, Oliver Wang, Eli Shechtman, and Stefan Wermter. 2022. CharacterGAN: Few-shot keypoint character animation and reposing. In Pro- ceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision . 1988–1997. Monster Mash [Dvorožňák et al. 2020] presents an intuitive framework for sketch-based modeling and animation.Marek Dvorožňák, Daniel Sýkora, Cassidy Curtis, Brian Curless, Olga Sorkine- Hornung, and David Salesin. 2020. Monster mash: A single-view approach to ca- sual 3D modeling and animation. ACM Trans. Graph. 39, 6 (Nov. 2020). DOI: https: //doi.org/10.1145/3414685.3417805 . 2.5D Cartoon Models [Rivers et al. 2010] presents a novel method of con- structing 3D-like characters from a small number of 2D represen- tations.Alec Rivers, Takeo Igarashi, and Frédo Durand. 2010. 2.5D cartoon models. ACM Trans. Graph. 29, 4 (July 2010). DOI: https://doi.org/10.1145/1778765.1778796 Some animation methods are specifically tailored toward par- ticular forms, such as faces [Averbuch-Elor et al. 2017], coloring- book characters [Magnenat et al. 2015], or characters with human- like proportions. One notable work that is focused on the human form is Photo Wake Up [Weng et al. 2019]. Detection, Segmentation, and Pose Estimation on Non-photorealistic ImagesSome researchers are addressing this problem by devel- oping methods and releasing datasets focused on the domain of anime characters [Chen and Zwicker 2022; Khungurn and Chou 2016]Shuhong Chen and Matthias Zwicker. 2022. Transfer learning for pose estimation of illustrated characters. In Proceedings of the IEEE/CVF Winter Conference on Appli- cations of Computer Vision . professional sketches [Brodt and Bessmeltsev 2022]Kirill Brodt and Mikhail Bessmeltsev. 2022. Sketch2Pose: Estimating a 3D character pose from a bitmap sketch. ACM Trans. Graph. 41, 4 (July 2022). DOI: https://doi. org/10.1145/3528223.3530106 mouse doodles [Ha and Eck 2017].","link":"/Paper/Sketch/Paper-A-Method-for-Animating-Childrens-Drawings-of-the-Human-Figure/"},{"title":"Paper-Learned-Motion-Matching","text":"Keywords: Neural Network, Generative Models, Memory, 2020 Learned Motion Matching_SIGGRAPH_2020 DANIEL HOLDEN, Ubisoft La Forge, Ubisoft, Canada OUSSAMA KANOUN, Ubisoft La Forge, Ubisoft, Canada MAKSYM PEREPICHKA, Concordia University, Canada TIBERIU POPA, Concordia University, Canada Code: https://github.com/orangeduck/Motion-Matching WorkFlow ProblemThe primary limitation of Motion Matching is that the memory usage (and run time performance to some degree) scale linearly with the amount of data that is provided and the number of features to be matched. InspirationRecent methods have shown that neural-network-based models can be effectively applied to generate realistic motion in a number of difficult cases including navigation over rough terrain [Holden et al. 2017], quadruped motion [Zhang et al. 2018], and interactions with the environment or other characters [Lee et al. 2018; Starke et al. 2019]. Yet, such models are often difficut to control, have unpredictable behaviour, long training times, and can produce animation of lower quality than that of the original training set. MethodsBasic Motion Matching a feature vector $\\pmb{x}$, which describes the features we wish to match at each frame.$$\\pmb{x} = \\lbrace \\pmb{t^t} \\pmb{t^d} \\pmb{f^t} \\pmb{\\dot{f}^t} \\pmb{\\dot{h}^t}\\rbrace \\in R^{27}$$ $\\pmb{t^t} \\in R^6$, 2D future trajectory positions projected on the ground, 20, 40, and 60 frames in the future (at 60Hz) local to the character. $\\pmb{t^d} \\in R^6$, the future trajectory facing directions 20, 40, and 60 frames in the future local to the character. $\\pmb{f^t} \\in R^6$, the two foot joint positions local to the character. $\\pmb{\\dot{f}^t} \\in R^6$, the two foot joint velocities local to the character. $\\pmb{\\dot{h}^t} \\in R^3$, the hip joint velocity local to the character. 12345678910111213141516171819202122232425262728293031323334void database_build_matching_features( database&amp; db, const float feature_weight_foot_position, const float feature_weight_foot_velocity, const float feature_weight_hip_velocity, const float feature_weight_trajectory_positions, const float feature_weight_trajectory_directions){ int nfeatures = 3 + // Left Foot Position 3 + // Right Foot Position 3 + // Left Foot Velocity 3 + // Right Foot Velocity 3 + // Hip Velocity 6 + // Trajectory Positions 2D 6 ; // Trajectory Directions 2D db.features.resize(db.nframes(), nfeatures); db.features_offset.resize(nfeatures); db.features_scale.resize(nfeatures); int offset = 0; compute_bone_position_feature(db, offset, Bone_LeftFoot, feature_weight_foot_position); compute_bone_position_feature(db, offset, Bone_RightFoot, feature_weight_foot_position); compute_bone_velocity_feature(db, offset, Bone_LeftFoot, feature_weight_foot_velocity); compute_bone_velocity_feature(db, offset, Bone_RightFoot, feature_weight_foot_velocity); compute_bone_velocity_feature(db, offset, Bone_Hips, feature_weight_hip_velocity); compute_trajectory_position_feature(db, offset, feature_weight_trajectory_positions); compute_trajectory_direction_feature(db, offset, feature_weight_trajectory_directions); assert(offset == nfeatures); database_build_bounds(db);} Normalizing the features: scale each feature (e.g. left foot position) by its standard deviation. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263//Z-score normalization: (x - mu) / stdvoid normalize_feature( slice2d&lt;float&gt; features, slice1d&lt;float&gt; features_offset, slice1d&lt;float&gt; features_scale, const int offset, const int size, const float weight = 1.0f){ // First compute what is essentially the mean // value for each feature dimension for (int j = 0; j &lt; size; j++) { features_offset(offset + j) = 0.0f; } for (int i = 0; i &lt; features.rows; i++) { for (int j = 0; j &lt; size; j++) { features_offset(offset + j) += features(i, offset + j) / features.rows; } } // Now compute the variance of each feature dimension array1d&lt;float&gt; vars(size); vars.zero(); for (int i = 0; i &lt; features.rows; i++) { for (int j = 0; j &lt; size; j++) { vars(j) += squaref(features(i, offset + j) - features_offset(offset + j)) / features.rows; } } // We compute the overall std of the feature as the average // std across all dimensions float std = 0.0f; for (int j = 0; j &lt; size; j++) { std += sqrtf(vars(j)) / size; } // Features with no variation can have zero std which is // almost always a bug. assert(std &gt; 0.0); // The scale of a feature is just the std divided by the weight for (int j = 0; j &lt; size; j++) { features_scale(offset + j) = std / weight; } // Using the offset and scale we can then normalize the features for (int i = 0; i &lt; features.rows; i++) { for (int j = 0; j &lt; size; j++) { features(i, offset + j) = (features(i, offset + j) - features_offset(offset + j)) / features_scale(offset + j); } }} a pose vector $\\pmb{y}$, which contains all the pose information for a single frame of animation. $$\\pmb{y} = \\lbrace \\pmb{y^t} \\pmb{y^r} \\pmb{\\dot{y}^t} \\pmb{\\dot{y}^r} \\pmb{\\dot{r}^t} \\pmb{\\dot{r}^r} \\pmb{o^{\\ast}}\\rbrace$$ $\\pmb{y^t} \\pmb{y^r}$ are the joint local translations and rotations, local to the character forward facing direction. $\\pmb{\\dot{y}^t} \\pmb{\\dot{y}^r}$ are the joint local translational and rotational velocities, local to the character forward facing direction. $\\pmb{\\dot{r}^t} \\pmb{\\dot{r}^r}$ are the character root translational and rotational velocity, local to the character forward facing direction. $\\pmb{o^{\\ast}}$ are all the other additional task specific outputs, such as foot contact information, the position or trajectory of other characters in the scene, or the future positions of some joints of the character. Motion Database: $\\pmb{X} = [\\pmb{x_0}, \\pmb{x_1}, \\cdots, \\pmb{x_{n-1}}]$ Animation Database: $\\pmb{Y} = [\\pmb{y_0}, \\pmb{y_1}, \\cdots, \\pmb{y_{n-1}}]$ at runtime, every $N$ frames, or when the user input changes significantly, a query vector $\\pmb{\\hat{x}}$ is constructed which contains the desired feature vector. To control the features corresponding to the future trajectory position and direction we use a spring-damper based system which generates a desired velocity and direction based on the current state of the joystick. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** convert the gamepad stick direction into a desired target velocity in the world space, * allowing for separate speeds for forward, backward, and sideways movement*/vec3 desired_velocity_update( const vec3 gamepadstick_left, const float camera_azimuth, const quat simulation_rotation, const float fwrd_speed, const float side_speed, const float back_speed){ // Find stick position in world space by rotating using camera azimuth vec3 global_stick_direction = quat_mul_vec3( quat_from_angle_axis(camera_azimuth, vec3(0, 1, 0)), gamepadstick_left); // Find stick position local to current facing direction vec3 local_stick_direction = quat_inv_mul_vec3( simulation_rotation, global_stick_direction); // Scale stick by forward, sideways and backwards speeds vec3 local_desired_velocity = local_stick_direction.z &gt; 0.0 ? vec3(side_speed, 0.0f, fwrd_speed) * local_stick_direction : vec3(side_speed, 0.0f, back_speed) * local_stick_direction; // Re-orientate into the world space return quat_mul_vec3(simulation_rotation, local_desired_velocity);}quat desired_rotation_update( const quat desired_rotation, const vec3 gamepadstick_left, const vec3 gamepadstick_right, const float camera_azimuth, const bool desired_strafe, const vec3 desired_velocity){ quat desired_rotation_curr = desired_rotation; // If strafe is active then desired direction is coming from right // stick as long as that stick is being used, otherwise we assume // forward facing if (desired_strafe) { vec3 desired_direction = quat_mul_vec3(quat_from_angle_axis(camera_azimuth, vec3(0, 1, 0)), vec3(0, 0, -1)); if (length(gamepadstick_right) &gt; 0.01f) { desired_direction = quat_mul_vec3(quat_from_angle_axis(camera_azimuth, vec3(0, 1, 0)), normalize(gamepadstick_right)); } return quat_from_angle_axis(atan2f(desired_direction.x, desired_direction.z), vec3(0, 1, 0)); } // If strafe is not active the desired direction comes from the left // stick as long as that stick is being used else if (length(gamepadstick_left) &gt; 0.01f) { vec3 desired_direction = normalize(desired_velocity); return quat_from_angle_axis(atan2f(desired_direction.x, desired_direction.z), vec3(0, 1, 0)); } // Otherwise desired direction remains the same else { return desired_rotation_curr; }} find the entry in the Matching Database which minimizes the squared euclidean distance to the query vector.$$k^{\\ast} = \\underbrace{argmin}_{k} ||\\pmb{\\hat{x}} - \\pmb{x_k}||^2$$ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133// Motion Matching search function essentially consists// of comparing every feature vector in the database, // against the query feature vector, first checking the // query distance to the axis aligned bounding boxes used // for the acceleration structure.void motion_matching_search( int&amp; __restrict__ best_index, float&amp; __restrict__ best_cost, const slice1d&lt;int&gt; range_starts, const slice1d&lt;int&gt; range_stops, const slice2d&lt;float&gt; features, const slice1d&lt;float&gt; features_offset, const slice1d&lt;float&gt; features_scale, const slice2d&lt;float&gt; bound_sm_min, const slice2d&lt;float&gt; bound_sm_max, const slice2d&lt;float&gt; bound_lr_min, const slice2d&lt;float&gt; bound_lr_max, const slice1d&lt;float&gt; query_normalized, const float transition_cost, const int ignore_range_end, const int ignore_surrounding){ int nfeatures = query_normalized.size; int nranges = range_starts.size; int curr_index = best_index; // Find cost for current frame if (best_index != -1) { best_cost = 0.0; for (int i = 0; i &lt; nfeatures; i++) { best_cost += squaref(query_normalized(i) - features(best_index, i)); } } float curr_cost = 0.0f; // Search rest of database for (int r = 0; r &lt; nranges; r++) { // Exclude end of ranges from search int i = range_starts(r); int range_end = range_stops(r) - ignore_range_end; while (i &lt; range_end) { // Find index of current and next large box int i_lr = i / BOUND_LR_SIZE; int i_lr_next = (i_lr + 1) * BOUND_LR_SIZE; // Find distance to box curr_cost = transition_cost; for (int j = 0; j &lt; nfeatures; j++) { curr_cost += squaref(query_normalized(j) - clampf(query_normalized(j), bound_lr_min(i_lr, j), bound_lr_max(i_lr, j))); if (curr_cost &gt;= best_cost) { break; } } // If distance is greater than current best jump to next box if (curr_cost &gt;= best_cost) { i = i_lr_next; continue; } // Check against small box while (i &lt; i_lr_next &amp;&amp; i &lt; range_end) { // Find index of current and next small box int i_sm = i / BOUND_SM_SIZE; int i_sm_next = (i_sm + 1) * BOUND_SM_SIZE; // Find distance to box curr_cost = transition_cost; for (int j = 0; j &lt; nfeatures; j++) { curr_cost += squaref(query_normalized(j) - clampf(query_normalized(j), bound_sm_min(i_sm, j), bound_sm_max(i_sm, j))); if (curr_cost &gt;= best_cost) { break; } } // If distance is greater than current best jump to next box if (curr_cost &gt;= best_cost) { i = i_sm_next; continue; } // Search inside small box while (i &lt; i_sm_next &amp;&amp; i &lt; range_end) { // Skip surrounding frames if (curr_index != - 1 &amp;&amp; abs(i - curr_index) &lt; ignore_surrounding) { i++; continue; } // Check against each frame inside small box curr_cost = transition_cost; for (int j = 0; j &lt; nfeatures; j++) { curr_cost += squaref(query_normalized(j) - features(i, j)); if (curr_cost &gt;= best_cost) { break; } } // If cost is lower than current best then update best if (curr_cost &lt; best_cost) { best_index = i; best_cost = curr_cost; } i++; } } } }} Once found, if the current frame $i$ does not match the nearest frame, $i$ , $k^{\\ast}$, then animation playback continues from this point $i := k^{\\ast}$, and a transition is inserted using inertialization blending [Bollo 2016, 2017]. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576void inertialize_pose_transition( slice1d&lt;vec3&gt; bone_offset_positions, slice1d&lt;vec3&gt; bone_offset_velocities, slice1d&lt;quat&gt; bone_offset_rotations, slice1d&lt;vec3&gt; bone_offset_angular_velocities, vec3&amp; transition_src_position, quat&amp; transition_src_rotation, vec3&amp; transition_dst_position, quat&amp; transition_dst_rotation, const vec3 root_position, const vec3 root_velocity, const quat root_rotation, const vec3 root_angular_velocity, const slice1d&lt;vec3&gt; bone_src_positions, const slice1d&lt;vec3&gt; bone_src_velocities, const slice1d&lt;quat&gt; bone_src_rotations, const slice1d&lt;vec3&gt; bone_src_angular_velocities, const slice1d&lt;vec3&gt; bone_dst_positions, const slice1d&lt;vec3&gt; bone_dst_velocities, const slice1d&lt;quat&gt; bone_dst_rotations, const slice1d&lt;vec3&gt; bone_dst_angular_velocities){ // First we record the root position and rotation // in the animation data for the source and destination // animation transition_dst_position = root_position; transition_dst_rotation = root_rotation; transition_src_position = bone_dst_positions(0); transition_src_rotation = bone_dst_rotations(0); // We then find the velocities so we can transition the // root inertiaizers vec3 world_space_dst_velocity = quat_mul_vec3(transition_dst_rotation, quat_inv_mul_vec3(transition_src_rotation, bone_dst_velocities(0))); vec3 world_space_dst_angular_velocity = quat_mul_vec3(transition_dst_rotation, quat_inv_mul_vec3(transition_src_rotation, bone_dst_angular_velocities(0))); // Transition inertializers recording the offsets for // the root joint inertialize_transition( bone_offset_positions(0), bone_offset_velocities(0), root_position, root_velocity, root_position, world_space_dst_velocity); inertialize_transition( bone_offset_rotations(0), bone_offset_angular_velocities(0), root_rotation, root_angular_velocity, root_rotation, world_space_dst_angular_velocity); // Transition all the inertializers for each other bone for (int i = 1; i &lt; bone_offset_positions.size; i++) { inertialize_transition( bone_offset_positions(i), bone_offset_velocities(i), bone_src_positions(i), bone_src_velocities(i), bone_dst_positions(i), bone_dst_velocities(i)); inertialize_transition( bone_offset_rotations(i), bone_offset_angular_velocities(i), bone_src_rotations(i), bone_src_angular_velocities(i), bone_dst_rotations(i), bone_dst_angular_velocities(i)); }} Learned Motion Matchingthe Motion Matching algorithm consists of three key stages: Projection: where a nearest neighbor search is used to find the feature vector in the Matching Database which best matches the query vector. Stepping: where we advance forward the index in the Matching Database. Decompression: where we look up the associated pose in the Animation Database which corresponds to our current index in the Matching Database. Decompressor Stepper Projector","link":"/Paper/Motion-Control/Paper-Learned-Motion-Matching/"},{"title":"NumericalAnalysis-C8-Partial-Differential-Equations","text":"Keywords: Newton solver, Backward Difference Method, Finite Element Method, Nonlinear partial differential equations, Mathlab Parabolic EquationsForward Difference MethodStability analysis of Forward Difference MethodBackward Difference MethodCrank–Nicolson MethodHyperbolic EquationsThe wave equationThe CFL conditionElliptic EquationsFinite Difference Method for elliptic equationsReality Check 8: Heat distribution on a cooling finFinite Element Method for elliptic equationsNonlinear partial differential equationsImplicit Newton solverNonlinear equations in two space dimensions","link":"/Math/Numerical-Analysis/NumericalAnalysis-C8-Partial-Differential-Equations/"},{"title":"Paper-MAPConNet-SelfSupervised-3D-Pose-Transfer-with-Mesh-and-Point-ConstrastiveLearning","text":"Keywords: Contrastive Learning, SMPL, SMAL, 3D-CoreNet, 2023 MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning_ICCV_2023 Jiaze Sun1* Zhixiang Chen2 Tae-Kyun Kim1,3 1 Imperial College London 2 University of Sheffield 3 Korea Advanced Institute of Science and Technology https://github.com/justin941208/MAPConNet ChallengeOne of the main challenges of 3D pose transfer is that current methods still put certain requirements on their training data making it difficult to collect and expensive to annotate. Innovation To prevent the network from exploiting shortcuts in unsupervised learning that circumvent the desired objective, we introduce disentangled latent pose and identity representations. To strengthen the disentanglement and guide the learning process more effectively, we propose mesh-level contrastive learning to force the model’s intermediate output to have matching latent identity and pose representations with the respective inputs. To further improve the quality of the model’s intermediate output as well as the correspondence module, we also propose point-level contrastive learning, which enforces similarity between representations of corresponding points and dissimilarity between non-corresponding ones. Workflow3D-CoreNetIt has two modules: correspondence and refinement. The training of 3D-CoreNet is supervised, given the model output $\\hat{\\pmb{x}}^{B1}$ and the ground truth output $\\pmb{x}^{B1}$, it minimises the reconstruction loss $$L_{rec}(\\hat{\\pmb{x}}^{B1}; \\pmb{x}^{B1}) = \\frac{1}{3N_{id}} ||\\hat{\\pmb{x}}^{B1} - \\pmb{x}^{B1}||_F^2\\tag{1}$$ $$L_{edge}(\\hat{\\pmb{x}}^{B1}; \\pmb{x}^{B2}) = \\frac{1}{\\xi} \\sum_{(j,k) \\in \\xi} |\\frac{||\\hat{\\pmb{x_j}}^{B1} - \\hat{\\pmb{x_k}}^{B1}||_2}{||\\pmb{x_j}^{B2} - \\pmb{x_k}^{B2}||_2} - 1|\\tag{2}$$ where $\\xi$ is the set of all index pairs representing vertices that are connected by an edge, and $\\hat{\\pmb{x_j}}^{B1}, \\pmb{x_j}^{B2}$ are the coordinates of the $j$-th (similarly, $k$-th) vertices of $\\hat{\\pmb{x}}^{B1}$ and $\\pmb{x}^{B2}$ respectively. Finally, the overall supervised loss is given by$$L_s = \\lambda_{rec} L_{rec}(\\hat{\\pmb{x}}^{B1}; \\pmb{x}^{B1}) + \\lambda_{edge}L_{edge}(\\hat{\\pmb{x}}^{B1}; \\pmb{x}^{B2})\\tag{3}$$ Correspondence ModuleThe correspondence module produces an intermediate “warped” output $\\pmb{w}^{B1} \\in R^{N_{id} \\times 3}$ inheriting the pose from $\\pmb{x}^{A1}$ but the vertex order of $\\pmb{x}^{B2}$, $$\\pmb{w}^{B1} = \\pmb{T}\\pmb{x}^{A1}$$ where$$\\pmb{T} \\in R_{+}^{N_{id} \\times N_{pose}}$$ is an optimal transport (OT) matrix learned based on the latent features of both inputs. Refinement ModuleThe refinement module then uses the features of the identity input $\\pmb{x}^{B2}$ as style condition for the warped output $\\pmb{w}^{B1}$, refining it through elastic instance normalisation and producing the final output $\\hat{\\pmb{x}}^{B1}$. Unsupervised Pose Transferwe present our unsupervised pipeline with the self- and cross-consistency losses by [47]. (a) when both inputs share a common identity, and (b) when the pose input in (a) is replaced by a different identity with the same pose. Task (a) is called “cross-consistency” and is readily available from the dataset. The pose input in task (b) is unavailable but can be generated by the network itself, i.e. “self-consistency”. The overall unsupervised loss is given by $$L_{us} = L_{cc} + L{sc}\\tag{6}$$ Cross-Consistencythe network should reconstruct $\\pmb{x}^{A1}$, through $\\hat{\\pmb{x}}^{A1} = G(\\pmb{x}^{A1}, \\pmb{x}^{A2})$ (Figure 3, left). This is enforced via $$L_{cc} = \\lambda_{rec} L_{rec}(\\hat{\\pmb{x}}^{A1}; \\pmb{x}^{A1}) + \\lambda_{edge}(\\hat{\\pmb{x}}^{A1}; \\pmb{x}^{A2})\\tag{4}$$ Self-ConsistencyAs mentioned previously, the model should reconstruct the same output as that in CC when its pose input is replaced by a different identity with the same pose – which is usually not available from the training data. In the first pass, given pose input $\\pmb{x}^{A1}$ and identity input $\\pmb{x}^{B3}$, the network generates a proxy $\\hat{\\pmb{x}}^{B1} = G(\\pmb{x}^{A1}, \\pmb{x}^{B3})$. In the second pass, reconstruct the initial pose input $\\widetilde{\\pmb{x}}^{A1} = G(SG(\\hat{\\pmb{x}}^{B1}), \\pmb{x}^{A2})$. where $SG$ stops the gradient from passing through. The purpose of $SG$ is to prevent the model from exploiting shortcuts such as using the input $\\pmb{x}^{A1}$ from the first pass to directly reconstruct the output in the second pass. The SC loss is $$L_{sc} = \\lambda_{rec} L_{rec}(\\widetilde{\\pmb{x}}^{A1}; \\pmb{x}^{A1}) + \\lambda_{edge}(\\widetilde{\\pmb{x}}^{A1}; \\pmb{x}^{A2})\\tag{5}$$ Latent disentanglement of pose and identitywe disentangle the latents into pose and identity channels. This allows us to impose direct constraints on the meaning of these channels to improve the accuracy of the model output. Mesh Contrastive LearningAs we cannot compare pose and identity directly in the mesh space, we take a self supervised approach by feeding meshes through the feature extractor $F$ and imposing the triplet loss [32] on the latent representations (see Figure 4). Specifically, given an anchor latent $\\pmb{a}$, a positive latent $\\pmb{p}$, and a negative latent $\\pmb{n}$ which are all in $R^{N \\times D}$, our mesh triplet loss is given by $$l(\\pmb{a,p,n}) = (m + \\frac{1}{N} \\sum_{j=1}^N d(\\pmb{a_j, p_j, n_j}))^{+}\\tag{7}$$where$$(\\cdot)^{+} = max(0, \\cdot)$$$m$ is the margin,$$d(\\pmb{a_j, p_j, n_j}) = ||a_j, p_j||_2 - ||a_j, n_j||_2\\tag{8}$$ Contrastive learning in CCBy intuition, the pose representation of $\\pmb{x}^{A1}$ should be closer to that of $\\pmb{w}^{A1}$ than $\\pmb{x}^{A2}$, the identity representation of $\\pmb{x}^{A1}$ should be closer to that of $\\pmb{x}^{A2}$ than $\\pmb{w}^{A1}$ as $\\pmb{w}^{A1}$ should generally avoid inheriting the identity representation from the pose input. Hence, $$L_{mesh}^{cc} = l(F_{pose}(\\pmb{x}^{A1}), F_{pose}(\\pmb{w}^{A1}), F_{pose}(\\pmb{x}^{A2})) + l(F_{id}(\\pmb{x}^{A1}), F_{id}(\\pmb{x}^{A2}), F_{id}(\\pmb{w}^{A1}))\\tag{9}$$ Contrastive learning in SCHowever, unlike CC, the vertex orders of $\\pmb{u}$ and $\\pmb{v}$ in SC are not aligned. As a result, equations 7 and 8 cannot be applied directly as they only compare distances between aligned vertices. We again take a self-supervised approach to “reorder” the feature of $\\pmb{v}$ by utilising the OT matrix $\\pmb{T}$. As the cost matrix for OT is based on the pairwise similarities between point features of $\\pmb{u}$ and $\\pmb{v}$, most entries in $\\pmb{T}$ are made close to zero except for a small portion which are more likely to be corresponding points. Therefore, we use the following binary version of $\\pmb{T}$ to select vertices from $\\pmb{v}$$$\\pmb{B_{jk}} = I \\lbrace \\pmb{T_{jk}} = \\underbrace{max}_{l} \\pmb{T_{jl}} \\rbrace\\tag{10}$$ In other words, each row of $\\pmb{B} \\in \\lbrace 0, 1 \\rbrace ^{N_{id} \\times N_{pose}}$ is a binary vector marking the location of the maximum value in the corresponding row of $\\pmb{T}$. We further constrain the rows of $\\pmb{B}$ to be one-hot in case of multiple maximum entries. Now, the triplet loss for SC with the “reordered” pose feature is given by$$L_{mesh}^{sc} = l(F_{pose}(\\pmb{w}), \\pmb{B}F_{pose}(\\pmb{v}), F_{pose}(\\pmb{u})) + l(F_{id}(\\pmb{w}), F_{id}(\\pmb{u}), \\pmb{B}F_{id}(\\pmb{v}))\\tag{11}$$ Point Contrastive LearningIntuitively, the final output would be more accurate if the warped output more closely resembles the ground truth. However, as later experiments will demonstrate, both 3DCoreNet and $L_{mesh}$ have a shrinking effect on the warped output, particularly on the head and lower limbs. We propose to address this problem by enforcing similarity between corresponding points and dissimilarity between noncorresponding points across different meshes. Specifically, given identity input $\\pmb{u}$ and warped output $\\pmb{w}$, we propose the following triplet loss for point-level contrastive learning $$L_{point} = \\frac{1}{N_{id}} \\sum_{j=1}^{N_{id}} (m + d(F(\\pmb{w})_j, F(\\pmb{u})_j, F(\\pmb{u})_k))^{+}\\tag{12}$$ Algorithm Reference[47] Keyang Zhou, Bharat Lal Bhatnagar, and Gerard PonsMoll. Unsupervised shape and pose disentanglement for 3D meshes. In The European Conference on Computer Vision (ECCV), August 2020 [33] Chaoyue Song, Jiacheng Wei, Ruibo Li, Fayao Liu, and Guosheng Lin. 3D pose transfer with correspondence learning and mesh refinement. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. [20] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM Transaction on Graphics, 34(6):248:1–248:16, Oct. 2015. [40] Jiashun Wang, Chao Wen, Yanwei Fu, Haitao Lin, Tianyun Zou, Xiangyang Xue, and Yinda Zhang. Neural pose transfer by spatially adaptive instance normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.","link":"/Paper/Human-Pose/Paper-MAPConNet-SelfSupervised-3D-Pose-Transfer-with-Mesh-and-Point-ConstrastiveLearning/"},{"title":"Paper-Live-Sketch-Video-driven-Dynamic-Deformation-of-Static-Drawings","text":"Keywords: Sketch Animation, ARAP, K-Shortest Path, Feature Tracking, ShapeContext Live Sketch: Video-driven Dynamic Deformation of Static Drawings_CHI_2018 Qingkun Su 1, Xue Bai 3, Hongbo Fu 2, Chiew-Lan Tai 1, Jue Wang 31 Department of Computer Science and Engineering, HKUST2 School of Creative Media, City University of Hong Kong3 Megvii Inc. US WorkFlowProblemCreating sketch animations using traditional tools requires special artistic skills, and is tedious even for trained professionals. To lower the barrier for creating sketch animations, we propose a new system, Live Sketch, which allows novice users to interactively bring static drawings to life by applying deformation-based animation effects that are extracted from video examples. Innovation motion extraction from video. We propose a new semiautomatic video tracking approach, which is able to track complex non-rigid object motion and is robust against occlusion遮挡 and ambiguity混淆. video-to-sketch alignment. and many-to-one motion driven sketch animation. A new stroke-preserving mesh deformation method for animating sketches, with constraints for better preserving the shape of user-specified strokes. MethodsControl Point ExtractionWe have tried to use image features such as SIFT or SURF to find good points to track, and also explored determining the points based on the initial object shape. In the end we conclude that although these automatic approaches do work well in some cases, they cannot handle a wide variety ofexamples well. The control points are mainly rely on user specification Motion Extraction We first adopt a dynamic-programming-based trajectory optimization framework [2, 11] to track individual points. KLT point tracker [53]. these methods may fail for some points that are hard to track, e.g., points specified in textureless regions, resulting in drifting. When tracking drifting occurs for one point, the user only needs to modify the tracking position of the failed point in a particular frame, which is incorporated as a hard constraint to globally refine the whole trajectory of the failed point. Therefore, the user can quickly refine the tracking results by only editing a few frames. This interactive tracking method is quite efficient and is able to address drifting due to occlusion. Specifically, the tracking algorithm optimizes the following energy for each control point trajectory: $$E_{tr}(x) = \\sum_t (\\lambda_d d(x_t, x_{t+1}) + \\lambda_u u(p_t, p_{t+1}) + \\underset{k}{min}(a(p_t,c_k)))\\tag{1}$$ where $x_t$ represents the position of a control point $x$ in frame $t$, and $p_t$ denotes the feature descriptor (SIFT in our implementation) of the image patch (we use $9 \\times 9$ patches) centered at $x_t$. The velocity term:$$d(\\cdot) = ||x_t, x_{t+1}||_2^2$$ update penalty term:$$u(\\cdot) = ||p_t, p_{t+1}||_2^2$$ these two terms measure the respective motion intensity and appearance variation between two consecutive frames, to achieve a smooth motion and appearance transition. the third term:$$a(\\cdot) = ||p_t,q_k||_2^2$$is a measure of the appearance deviation from the user specified control point locations ${c_k}$, where $q_k$ is the patch feature descriptor of $c_k$. The user may manually specify two or more control point positions at different frames. We measure the appearance energy of points at other frames $\\underset{k}{min}(a(p_t,c_k))$, so that they should match at least one of these user-specified control points. We minimize the energy function in Eq. 1, by finding the shortest path of a graph (Fig. 4).The weight of each edge is computed by $$w_e(p_t, p_{t+1}) = \\lambda_d d(x_t, x_{t+1}) + \\lambda_u u(p_t, p_{t+1})$$ To handle the occlusion problem, we also fully connect the points of each frame with those of its next m frames (dot lines in Fig. 4, we set m = 10).The weight of the extra edge between two respective nodes $p_t$ and $p_t’$ of frame $t$ and $t’, (t + 1 &lt; t’ \\leq t + m)$ is$$w_e(p_t, p_{t’}) = \\lambda_o + \\lambda_d d(x_t, x_{t’}) + (t’ - t)\\lambda_u u(p_t, p_{t’})$$ 本质上就是给出edge weight，图论中的起点到终点的最短路径, Dijkstra似乎就可以呀 Another problem is drifting by ambiguity. To solve this problem, we propose a new energy minimization method to jointly optimize the tracking trajectories of multiple points (Fig 5). The main idea is to first generate a set oftrajectories for each control point as candidate trajectories (Fig. 5(top)) and then employ a global optimization approach to select the best one for each point by jointly considering multiple points together. we apply the method proposed in [16] to compute a set of candidate tracking trajectories.(k-shortest path) We jointly optimize over multiple tracking points to find the best trajectory for each point. This can be formulated as the following energy minimization problem: $$arg min \\sum_i E_{tr}(x^i) + \\beta \\sum_{i \\neq j} E_o(x^i, x^j)\\tag{2}$$$E_o(x^i, x^j)$ is defined as the normalized duration of the overlapping portion of the two trajectories. Intuitively, this term prevents two similar control points to collapse into a single one, in which case the overlapping portion of the two trajectories will be large. We use a greedy algorithm to minimize this energy. We first initialize the solution by choosing the trajectory that has the minimal tracking energy for each point (according to Eq. 1). Starting from the point that has the largest total energy according to Eq. 2, we assign to it another candidate trajectory that best minimizes this energy. We repeat this process until the total energy cannot be further reduced. Control Point TransferThe contour of the sketch can be extracted using the Active Contour models [26]. The contour of the video object can be extracted by automatic GrabCut segmentation [37] with the bounding box enclosing all the tracking points as input.Otherwise, the user can quickly draw the correct outline using a pen tool (e.g., the example in Fig. 6(bottom)). Then we apply the Shape Context method [7] to build correspondence between the two contours. Next, for a control point inside the video object, we represent its location using the barycentric coordinate of a set of evenly sampled contour points, and compute its location using the same barycentric coordinate on the sketch image. Motion TransferTraditional controlled mesh deformation methods, such as the as-rigid-as-possible (ARAP) mesh deformation [24, 25], are designed to keep only the rigidity of the mesh triangles (seeFig.8) Our method adds new mesh triangles derived from the input strokes to the original mesh. Original mesh, shown in gray in Figure8.$$\\mathcal{M_0} = (\\mathcal{V_0, J_0})$$ uniformly sample points from each user-specified input stroke, and construct a sketch triangle set, purple in Figure8.$$\\mathcal{M_s} = (\\mathcal{V_s, J_s})$$ construct a link triangle set $\\mathcal{J_l}$ by connecting each vertex in $\\mathcal{V_s}$ with the three vertices of the triangle in $\\mathcal{V_0}$ on which it falls. Give the augumented mesh structure,$$\\mathcal{M} = (\\mathcal{V_0 \\cup V_s, J_0 \\cup J_s \\cup J_l})$$we formulate the stroke-preserving ARAP deformation as the following energy minimization problem:$$\\underset{\\mathcal{M}}{min} E_0 + \\gamma_l E_{link} + \\gamma_s E_{sketch}\\tag{3}$$where $E_0$, $E_{link}$ and $E_{sketch}$ are the deformation energy terms of the three mesh triangles set $\\mathcal{J_0}$, $\\mathcal{J_l}$ and $\\mathcal{J_s}$, respectively. Each energy term is defined as:$$E(\\mathcal{J}) = \\sum_{t \\in \\mathcal{J}} \\sum_{v_i,v_j \\in t} || \\overrightarrow{v_i’v_j’} - H \\overrightarrow{v_iv_j}||^2\\tag{4}$$which is based on [24]. It can also be formulated by other ARAP optimization methods, like [25].H is a rigid and scale irrelevant ARAP transformation matrix, which is achieved by a two-step optimization algorithm. Minimizing $E_{sketch}$ keeps the shape of the strokes while minimizing $E_{link}$ transfers the deformation of the original mesh to the strokes. Multiple-Layer AnimationAs discussed earlier, single layer, mesh-deformation-based animation cannot handle topology change, which is common for dynamic objects (see Fig. 9a). To handle such cases, our system allows the user to create animations with multiple layers. To avoid detaching different layers from each other during animation, our system then automatically detects the intersections of these layers, and adds one joint point at the center of each intersection region (Fig. 9c, blue circles) so that these layers are connected. Classic Reference[53]. Alper Yilmaz, Omar Javed, and Mubarak Shah. 2006. Object tracking: A survey. Acm computing surveys (CSUR) 38, 4 (2006), 13. [2]. B. Amberg and T. Vetter. 2011. GraphTrack: Fast and globally optimal tracking in videos. In CVPR 2011. 1209–1216. [11]. A. Buchanan and A. Fitzgibbon. 2006. Interactive Feature Tracking using K-D Trees and Dynamic Programming. In CVPR’06, Vol. 1. 626–633. [16]. Theodoros Chondrogiannis, Panagiotis Bouros, Johann Gamper, and Ulf Leser. 2015. Alternative routing: k-shortest paths with limited overlap. In SIGSPATIAL GIS ’15. ACM, 68. [26]. Michael Kass, Andrew Witkin, and Demetri Terzopoulos. 1988. Snakes: Active contour models. IJCV 1, 4 (1988), 321–331. [37]. Carsten Rother, Vladimir Kolmogorov, and Andrew Blake. 2004. “GrabCut”: Interactive Foreground Extraction Using Iterated Graph Cuts. ACM Trans. Graph. 23, 3 (Aug. 2004), 309–314 [7]. Serge Belongie, Jitendra Malik, and Jan Puzicha. 2000. Shape context: A new descriptor for shape matching and object recognition. In Nips, Vol. 2. [24]. Takeo Igarashi, Tomer Moscovich, and John F. Hughes. 2005. As-rigid-as-possible Shape Manipulation. In ACM Trans. Graph., Vol. 24. 1134–1141. [25]. Alec Jacobson, Ilya Baran, Jovan Popovic, and Olga Sorkine. 2011. Bounded Biharmonic Weights for Real-time Deformation. In SIGGRAPH ’11. ACM, Article 78, 8 pages.","link":"/Paper/Sketch/Paper-Live-Sketch-Video-driven-Dynamic-Deformation-of-Static-Drawings/"},{"title":"Paper-Normalized-Human-Pose-Features-for-Human-Action-Video-Alignment","text":"Keywords: Metric Learning, deterministic forward kinematics (FK) layer, Procrustes alignment Normalized Human Pose Features for Human Action Video Alignment_ICCV_2021 Jingyuan Liu 1 Mingyi Shi 2 Qifeng Chen 1 Hongbo Fu 3 Chiew-Lan Tai 1 1 The Hong Kong University of Science and Technology 2 The University of Hong Kong 3 The City University of Hong Kong WorkFlowInspirationA common approach to the problem of human action video alignment is to first estimate 2D or 3D human poses from two input videos, and then find the alignments by matching with features extracted from joint positions [48, 11], However, human poses still contain large variations in scale, bone length ratios, orientations, etc. Global orientation normalization by Procrustes alignment(普氏对齐) is hard to be applied to in-the-wild videos when the ground-truth 3D poses are not available. At first sight, a straightforward solution might be to compute joint angles from joint positions, and use raw joint angles [10] or their aggregations [35, 52] as features for matching. However, the joint angle features suffer from information loss by dropping the skeleton’s relational context, which has a proven significance in capturing pose discrimination [8, 40]. To address the limitations in position-based and angular based pose representations, we propose to use a normalized human pose, an intermediate pose representation that reflects the pose information with respect to joint rotations, and is parameterized by joint positions to preserve the relational context of body configurations, as shown in Figure 1(c). This normalized pose representation is enlightened by the recent works that use joint rotations as pose parameterizations for motion reconstruction [45] and pose sequence generation [53, 38]. They incorporate a deterministic forward kinematics (FK) layer in neural networks to convert the joint rotations into joint positions to avoid the joint rotation ambiguity problem in IK &gt;&gt;. Methods Pose NormalizationFor a video containing $T$ frames, we denote the $2D$ position of joint $n$ at frame $t$ as $x^n_t\\in R^2, t = 1, 2, \\cdots, T, n = 1, 2, \\cdots, N$, where $N$ is the total number of joints. The joint rotations, computed from the input $2D$ poses by $E_Q$, are represented as a unit quaternion &gt;&gt; for each joint $\\alpha_t^n \\in R^4$. Denote the $FK$ process as $$X = FK(s, \\alpha)$$ where bones in a skeleton $s$ are rotated according to a set of joint rotations $\\alpha$, resulting in the 3D joint positions $X$ of the skeleton. In order to train $E_Q$, we apply the joint rotations $\\alpha_t^n$ computed from 2D poses $x^n_t$ on two types of skeletons: the condition skeleton $S$, to facilitate the learning of pose normalization; the $3D$ source skeletons of the video subjects st computed from the ground-truth $3D$ poses, to assist the training of $E_Q$. Reconstruction BranchApplying $\\alpha_t^n$ on the $3D$ source skeletons $s_t$ results in reconstructed $3D$ poses $X^t_s$, which can be directly supervised by the groundtruth $3D$ poses $X_t^n$ using the reconstruction loss: $$\\mathcal{L_{recon}} = \\sum_{t_n} ||FK(s_t, \\alpha_t^n) - X_t^n||^2$$ Cycle Reconstruction BranchThe design of the cycle reconstruction branch is based on the observation that, since the condition skeleton has the same poses as the poses of the subject in the video frames, its projections should yield the same 3D joint rotations as the 3D joint rotations produced by the original input 2D poses. However, applying rotations $\\alpha_t^n$ on the condition skeleton results in new $3D$ poses $\\bar{X_S}$ without paired groundtruth for supervised training. Thus, we adopt this cycle reconstruction that projects the $3D$ condition skeleton pose into $2D$ pose, and then compute $3D$ joint rotations from the projected $2D$ poses. 本质上来讲，就是经过这样一通操作，condition skeleton motion is the same with source skeleton motion though they may have different motion data, and the neural network $E_Q$ will become a model to infer correct condition skeleton rotation motion instead of source skeleton rotation motion. The joint rotation consistency loss is computed as the differences between joint rotations from the input 2D poses and from the projected skeleton poses: $$\\mathcal{L_{jrc}} = \\sum_{t,n} ||\\alpha_t^n - \\hat{\\alpha_t^n}||^2$$ The cycle reconstruction loss is:$$\\mathcal{L_{cycle}} = \\sum_{t,n} ||FK(s_t, \\hat{\\alpha_t^n}) - X_t^n||^2$$Besides the above losses, we also adopted the foot contact loss $\\mathcal{L_{fc}}$, which is commonly used in 3D pose estimations to reduce the skating effect [46]. The total loss function for training is:$$\\mathcal{L} = \\mathcal{L_{recon}} + \\phi\\mathcal{L_{cycle}} + \\beta\\mathcal{L_{jrc}} + \\lambda\\mathcal{L_{fc}}$$ For inference, only the regressor $E_Q$ is used to compute the $3D$ joint rotations from $2D$ poses to be applied onto the condition skeleton. Now we can understand the key idea of this paper: We design a neural network that learns to normalize human poses in videos. Specifically, the pose normalization network takes in $2D$ poses and estimate joint rotations, which are then applied by $FK$ on a pre-defined $3D$ skeleton with unified fixed bone lengths (called condition skeleton as in [53]). The joint rotations of the subjects’ poses in videos are thus converted into the joint positions of the condition skeleton to normalize the poses. In this way, the difference in joint positions of the condition skeleton is caused only by the difference in joint rotations. Since the normalized poses are not paired with ground-truth poses for training, our network adopted a cycle consistency training strategy (Section 3.2). With joint rotations, the poses can also be easily unified to the same global orientation by specifying the root joint rotation. Finally, the pose features are learned from the normalized $3D$ poses by metric learning. The resulting pose features are high-level human pose representations and can be directly compared by the Euclidean distance. Pose EmbeddingAfter the poses in the video frames are normalized into unified bone lengths and viewpoints, we use metric learning to map the poses to a pose embedding space to extract highlevel pose features. 深度学习的任务就是把高维原始数据（图像，句子）映射到低维流形，使得高维的原始数据被映射到低维流形之后变得可分，而这个映射就叫做Embedding, 或者这样理解, Embedding就是从原始数据提取出来的Feature，也就是那个通过神经网络映射之后的低维向量。 Ablation StudyLimitations it extracts features from a complete normalized pose.(一个完整的归一化的姿态，也就是说不能有关节缺失) Future work includes modeling the normalized pose with partial observations. A potential solution that worth exploring is to adopt a probabilistic modeling with kinematics constraints [46, 23] as priors for the FK layer in the network, such that the missing joint positions would be filled by satisfying both kinematicspriors and non-missing joint positions. it requires ground-truth 3D joint positions in training.（需要在训练阶段有3d关节点的位置做ground-truth） Adopting weakly-supervised settings, such as using ordinal depths of joint pairs [36, 42] might enable training on in-the-wild datasets. Classic Reference[2]Andreas Aristidou, Daniel Cohen-Or, Jessica K Hodgins, Yiorgos Chrysanthou, and Ariel Shamir. Deep motifs and motion signatures. ACM Transactions on Graphics (TOG), 37(6):1–13, 2018. 3, 5 [10]Myung Geol Choi, Kyungyong Yang, Takeo Igarashi, Jun Mitani, and Jehee Lee. Retrieval and visualization of human motion data via stick figures. In Computer Graphics Forum, volume 31, pages 2057–2065. Wiley Online Library, 2012. 2, 3, 6, 8 [28]Julieta Martinez, Rayat Hossain, Javier Romero, and James J Little. A simple yet effective baseline for 3d human pose estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2640–2649, 2017. 1, 3, 5, 6, 7, 8 [56]Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris N. Metaxas. Semantic graph convolutional networks for 3d human pose regression. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3425–3435, 2019. 5, 6, 7, 8 [48]Jennifer J Sun, Jiaping Zhao, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, and Ting Liu. View-invariant probabilistic embedding for human pose. In European Conference on Computer Vision, pages 53–70. Springer, 2020. 1, 3, 6, 8 [24]Muhammed Kocabas, Nikos Athanasiou, and Michael J Black. Vibe: Video inference for human body pose and shape estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5253–5263, 2020. 3, 6, 8","link":"/Paper/Human-Pose/Paper-Normalized-Human-Pose-Features-for-Human-Action-Video-Alignment/"},{"title":"Paper-Sketch-based-Facial-Generation","text":"Keywords: 3D Facial Generation, Sketch Editing, Neural Network SketchFaceNeRF: Sketch-based Facial Generation and Editing in Neural Radiance Fields LIN GAO∗, Institute of Computing Technology, CAS and University of Chinese Academy of Sciences , ChinaFENG-LIN LIU, Institute of Computing Technology, CAS and University of Chinese Academy of Sciences, ChinaSHU-YU CHEN, Institute of Computing Technology, Chinese Academy of Sciences, ChinaKAIWEN JIANG, Institute of Computing Technology, CAS and Beijing Jiaotong University, ChinaCHUNPENG LI, Institute of Computing Technology, Chinese Academy of Sciences, ChinaYU-KUN LAI, School of Computer Science and Informatics, Cardiff University, UKHONGBO FU, School of Creative Media, City University of Hong Kong, China http://www.geometrylearning.com/SketchFaceNeRF/http://mlops.ccloud.conestore.cn:30010/sketchface/ WorkFlowInspirationChallengesThe Main ProcessInnovationLimitationCode Analysishttps://github.com/IGLICT/SketchFaceNeRF/","link":"/Paper/Sketch/Paper-Sketch-based-Facial-Generation/"},{"title":"Paper-StripMaker-Sketch-Consolidation","text":"Keywords: ClusterStrokes, Random Forest, Vector Sketch Consolidation StripMaker: Perception-driven Learned Vector Sketch Consolidation CHENXI LIU, University of British Columbia, CanadaTOSHIKI AOKI, University of Tokyo, JapanMIKHAIL BESSMELTSEV, Université de Montréal, CanadaALLA SHEFFER, University of British Columbia, Canada https://www.cs.ubc.ca/labs/imager/tr/2023/stripmaker/ WorkFlowInspirationThe Main Process Innovation avoid the need for an unsustainably large manually annotated learning corpus by leveraging observations about artist workflow and perceptual cues viewers employ when mentally consolidating sketches. Our method is the first to use a principled classification-based approach to vector sketch consolidation. LimitationOur performance is constrained by data scarcity which prevents greater reliance on context, due to overfitting concerns. MoreRandom ForestMore about Random Forest in PatternRecognition&gt;&gt; StrongRelatedPaperDave Pagurek van Mossel, Chenxi Liu, Nicholas Vining, Mikhail Bessmeltsev, and Alla Sheffer. 2021. StrokeStrip: Joint Parameterization and Fitting of Stroke Clusters. ACM Trans. Graph. 40, 4 (July 2021), 50:1–50:18 Peng Xu, Timothy M Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang, and Liang Wang. 2022. Deep learning for free-hand sketch: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence (2022) Jerry Yin, Chenxi Liu, Rebecca Lin, Nicholas Vining, Helge Rhodin, and Alla Sheffer. 2022. Detecting Viewer-Perceived Intended Vector Sketch Connectivity. ACM Transactions on Graphics 41 (2022). Issue 4. CodeSince the author doesn’t upload codes, so I just guess the rough code framwork as follows: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485//.h//used to get the likelyhood between different strips, so as to merge or splitclass RandomForestClassfier{//StrokeStrip parameterization[van Mossel et al. 2021].public: RandomForestClassfier(); virtual ~RandomForestClassfier(); //stroke and stroke virtual float getLikelihoodSS(Stroke&amp; a, Stroke&amp; b); //stroke and strip virtual float getLikelihoodSP(Stroke&amp; a, vector&lt;Stroke&gt;&amp; b); //strip and strip virtual float getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b);};class LocalClassifier : public RandomForestClassfier{public: LocalClassifier(); ~LocalClassifier(); //stroke and stroke virtual float getLikelihoodSS(Stroke&amp; a, Stroke&amp; b); //stroke and strip virtual float getLikelihoodSP(Stroke&amp; a, vector&lt;Stroke&gt;&amp; b); //strip and strip virtual float getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b);};class GlobalClassifier : public RandomForestClassfier{public: //stroke and stroke virtual float getLikelihoodSS(Stroke&amp; a, Stroke&amp; b); //stroke and strip virtual float getLikelihoodSP(Stroke&amp; a, vector&lt;Stroke&gt;&amp; b); //strip and strip virtual float getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b);};class Stroke{private: vector&lt;Point&gt; controlPoints; //each stroke is denoted by controlpoints, sorted by time //Time t; //the time this stroke is drawed by user vector&lt;Point&gt; samplePoints; int stripIndx; //which strip this stroke belongs topublic: bool getSamplePoints(float sampleRate); bool setStripIndx(int idx); int getStripIndx();};class VectorSketch{private: vector&lt;Stroke&gt; Strokes; //the vector Sketch consists of raw strokes vector&lt;vector&lt;Stroke&gt;&gt; Strips; //group the strokes to different strips int strokeWidth;public: bool Sample(); bool LocalConsolidation(); bool GlobalConsolidation(); bool IsCompatible(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b); bool Merge(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b); bool IsFormjectionByYin2022(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b); void FittingCurvesByVanMossel2021();};//main.cppint main(){ //Input, get the information of strokes VectorSketch vectorSketch; //Preprocess vectorSketch.Sample(); //Step1. temporal consolidation vectorSketch.LocalConsolidation(); //Step2. refinement vectorSketch.GlobalConsolidation(); //Compare Results Experiments..} Sample, LocalConsolidation and GlobalConsolidation funtions can be implemented as follows: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200//Stroke.cppbool Stroke::getSamplePoints(float sampleRate){ for (float t = 0; t &lt; 155; t += sampleRate) { Point p = Point(3 * t * t + 4 * (1 - t), 4 * t); //suppose the stroke function is like this samplePoints.emplace_back(p); }}bool Stroke::setStripIndx(int idx){ stripIndx = idx;}int Stroke::getStripIndx(){ int stripIndx;}//VectorSketch.cppbool VectorSketch::Sample(){ float sampleRate = 1.2 * strokeWidth; for (auto stroke : Strokes) { stroke.getSamplePoints(sampleRate); } //remove hook artifacts //....}bool IsCompatible(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ return false;}//merge strip b to strip abool Merge(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ int stripIdxOfa = a[0].getStripIndx(); for (auto bb : b) { //update the information of strokes of strip b, and update the elements of strip a bb.setStripIndx(stripIdxOfa); a.emplace_back(bb); } return true;}bool IsFormjectionByYin2022(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ //2022 Yin Algorithms //..}void FittingCurvesByVanMossel2021(){ //2021 Van Mossel Algorithms //...}bool VectorSketch::LocalConsolidation(){ auto Classfier = std::make_unique&lt;LocalClassifier&gt;(); int stripIdx = -1; for (int i = 0; i &lt; Strokes.size(); i++) { Stroke curStroke = Strokes[i]; Stroke previousStroke = Strokes[i - 1]; vector&lt;Stroke&gt; previousStrip = Strips[previousStroke.getStripIndx()]; float probability = Classfier-&gt;getLikelihoodSP(curStroke, previousStrip); //curStroke belongs to the sub-strip the previousStroke belongs to if (probability &gt; 0.5) { //update curstroke information Strokes[i].setStripIndx(stripIdx); //update the previous strip previousStrip.emplace_back(curStroke); //proceed to the next stroke continue; } else { //save curStroke as a new sub-strip Strokes[i].setStripIndx(++stripIdx); Strips[stripIdx].emplace_back(curStroke); //previousStrip has more than one stroke, decide if it can be merged with other strips. if (previousStrip.size() &gt; 1) { int p_max = 0; int idx_max = -1; for (int j = 0; j &lt; Strips.size(); j++) { //only call the classifier if two sub-strips are deemed compatible if (IsCompatible(Strips[j], previousStrip)) { float p = Classfier-&gt;getLikelihoodPP(Strips[j], previousStrip); if (p &gt; p_max) { p_max = p; idx_max = j; } } } //merge the most similar two strips to one strip Merge(Strips[idx_max], previousStrip); Strips[stripIdx].clear(); --stripIdx; previousStrip.clear(); } } } //after the iteration above, the whole strokes are grouped into different strips, as the differnt-color curves in the figure. //fitting curves to these strips FittingCurvesByVanMossel2021();}bool VectorSketch::GlobalConsolidation(){ auto Classfier = std::make_unique&lt;GlobalClassifier&gt;(); //strip reevaluation int n = Strips.size(); int StripIdx = n; for (int i = 0; i &lt; n; i++) { vector&lt;Stroke&gt; strip = Strips[i]; //find the least likelihood two seed strokes in this strip int min_likelihood = INT_MAX; vector&lt;Stroke&gt; seedStrokes(5); //Stroke seed_s1, seed_s2; for (int k = 0; k &lt; strip.size(); k++) { for (int l = k + 1; l &lt; strip.size(); l++) { Stroke s1 = strip[k]; Stroke s2 = strip[l]; float p = Classfier-&gt;getLikelihoodSS(s1, s2); if (p &lt; min_likelihood) { min_likelihood = p; seedStrokes[0] = s1; seedStrokes[1] = s2; } } } //thie likelihood is high enougth, don't change this strip if (min_likelihood &gt; 0.6) { continue; } else { //Reassign other strokes to sub-strip formed by seed_s1 or sub-strip formed by seed_s2 int newStripIdx = StripIdx; //seedStips.. vector&lt;vector&lt;Stroke&gt;&gt; seedStrips(5); seedStrokes[1].setStripIndx(newStripIdx); seedStrips[0].emplace_back(seedStrokes[0]); //seed_s1_strip seedStrips[1].emplace_back(seedStrokes[1]); //seed_s2_strip for (auto stroke : strip) { float p_max = 0; for (auto sub_strip : seedStrips) { float pi = Classfier-&gt;getLikelihoodSP(stroke, sub_strip); p_max = max(p_max, pi); } //this stroke doesn't belong to any seedStrip, so make this stroke as a new seed stroke if (p_max &lt; 0.5) { seedStrokes.emplace_back(stroke); stroke.setStripIndx(++newStripIdx); } else { //assign this stroke to seed_s1_strip or seed_s2_strip or ..., according to the likelihood //... } } //merge this seedStrips according the likelihood //update Strips, cause may produce new strips in step above } } //Connectivity Preservation classifier in Yin et al.[2022] for(auto strip : Strips) for (auto stripb : Strips) { if (IsFormjectionByYin2022(strip, strip)) { Merge(strip, strip); } } //fitting curves to these strips FittingCurvesByVanMossel2021();} The classifier can be implemented as follow: 12345678910111213141516//RandomForestClassfier.cppfloat RandomForestClassfier::getLikelihoodPP(vector&lt;Stroke&gt;&amp; a, vector&lt;Stroke&gt;&amp; b){ //call the python module, the trained model.. /* the classifier extract the features as follows: //Angles and Distance //Density //Narrows and Side-by-Side Extent //Evenness //1d Parameterization Distortion //Relative Precision */ return -1;}","link":"/Paper/Sketch/Paper-StripMaker-Sketch-Consolidation/"},{"title":"NumericalAnalysis-C9-Random-Numbers-and-Applications","text":"Keywords: Monte Carlo Simulation, Stochastic Differential Equations, Numerical methods for SDEs, Mathlab Random NumbersPseudo-random numbersExponential and normal random numbersMonte Carlo SimulationPower laws for Monte Carlo estimationQuasi-random numbersDiscrete and Continuous Brownian MotionRandom walksContinuous Brownian motionStochastic Differential EquationsAdding noise to differential equationsNumerical methods for SDEsReality Check 9: The Black–Scholes Formula","link":"/Math/Numerical-Analysis/NumericalAnalysis-C9-Random-Numbers-and-Applications/"},{"title":"Paper-StrokeStrip-Fitting-Stroke-Clusters","text":"Keywords: Curve Fitting to Strokes, Variational Optimization, Cluster Strokes, C++ https://www.cs.ubc.ca/labs/imager/tr/2021/StrokeStrip/ WorkFlowInspirationWe achieve this goal by leveraging insights about human perception of stroke clusters. Challengeswe do not know which points on different strokes that are adjacent in Euclidean space are perceived as WTS adjacent. our algorithm needs to compute these crosssections given only the raw strokes as input. We consequently face two interconnected challenges: we must compute optimal isovalues along each cluster cross-section while simultaneously computing the cross-sections themselves. Address the ChallengesWe robustly address both challenges at once by casting the computation of the parameterization and the underlying isoline crosssections as a constrained variational problem. We then solve it using a combined discrete-continuous optimization framework. Step1. determine the optimal orientations of parameterization gradients along each stroke（决定沿着每个笔画的最优梯度(参数化)朝向）, converting monotonicity constraints into more tractable inequality ones（把单调约束转化为更容易解决的不等式问题） Step2. jointly solve for cluster cross-sections and parameter values along them using a variational optimization framework（使用变分优化框架联合解决等值线和沿着等值线的参数值） The Main Process Some DefinitionsA strip is well described by its centerline $\\gamma(t) \\in R^2$, parameterized by its arclength $t \\in [0, L]$, and width $W(t)$ The two sides of a strip are then the two curves at the distance of $W(t)/2$ from the centerline in the normal direction, or more formally $\\gamma(t) \\pm \\frac{W(t)}{2}n(t)$. Simply put, it is a function $u(x)$ defined for all points in the strip, such that straight line segments $C(t) = \\lbrace \\gamma(t) + an(t)| a \\in [-\\frac{W(t)}{2}, \\frac{W(t)}{2}]\\rbrace$ are the parameterization’s isolines（等值线，和中心线正交的线）. $t = u(x), x ∈ C(t)$. centerline tangent $\\tau(t)$, about the centerline, we prioritize tangent alignment over centrality when computing the strip parameterizations and paths. thus, given the oriendted strokes $S = \\lbrace s_i, i = 1, \\cdots, N\\rbrace$, we are looking for a map(映射)$$u(x): U_iS_i \\mapsto [0;L]$$ For a non self-intersecting strip $S \\subset R^2$, its arc length parameterization $u: S \\rightarrow R$ is the minimizer of the following variational problem:$$\\underset{u}{min} \\int_0^L ||\\frac{1}{W(t)} \\int_{C(t)} \\nabla u(x)dx - \\tau(t)||^2dt$$ GRADIENT ORIENTATION InnovationLimitationMorearc-length parameterizationVariational PrincipleCode Analysishttps://github.com/davepagurek/StrokeStrip","link":"/Paper/Sketch/Paper-StrokeStrip-Fitting-Stroke-Clusters/"},{"title":"PatterRecognition-C10-Approximate-Inference","text":"Keywords: Variational Linear Regression, Variational Logistic Regression, Variational Inference, Python This is the Chapter10 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] Simply Understand Variational Inference &gt;&gt; Variational InferenceFunctional like entropy $H[p]$, which takes a probability distribution $p(x)$ as the input and returns the quantity:$$H[p] = \\int p(x) \\ln p(x) dx\\tag{10.1}$$as the output. We can the introduce the concept of a functional derivative, which expresses how the value of the functional changes in response to infinitesimal changes to the input function. Suppose we have a fully Bayesian model in which all parameters are given prior distributions. Our probabilistic model specifies the joint distribution $p(\\pmb{X},\\pmb{Z})$, and our goal is to find an approximation for the posterior distribution $p(\\pmb{Z}|\\pmb{X})$ as well as for the model evidence $p(X)$. As before, we can maximize the lower bound $\\mathcal{L}(q)$ by optimization with respect to the distribution $q(\\pmb{Z})$, which is equivalent to minimizing the $KL$ divergence. If we allow any possible choice for $q(\\pmb{Z})$, then the maximum of the lower bound occurs when the $KL$ divergence vanishes, which occurs when $q(\\pmb{Z})$ equals the posterior distribution $p(\\pmb{Z}|\\pmb{X})$. However, in practice, $p(\\pmb{Z}|\\pmb{X})$ is intractable. We therefore consider instead a restricted family of distributions $q(\\pmb{Z})$ and then seek the member of this family for which the $KL$ divergence is minimized. One way to restrict the family of approximating distributions is to use a parametric distribution $q(\\pmb{Z}|\\pmb{\\omega})$ governed by a set of parameters $\\omega$. The lower bound $L(q)$ then becomes a function of $\\omega$, and we can exploit standard nonlinear optimization techniques to determine the optimal values for the parameters. Factorized distributionsProperties of factorized approximationsExample: The univariate GaussianModel comparisonIllustration: Variational Mixture of GaussiansVariational distributionVariational lower boundPredictive densityDetermining the number of componentsInduced factorizationsVariational Linear RegressionVariational distributionPredictive distributionLower boundExponential Family DistributionsLower boundLocal Variational MethodsVariational Logistic RegressionVariational posterior distributionOptimizing the variational parametersInference of hyperparametersExpectation PropagationExample: The clutter problemExpectation propagation on graphshttps://zhuanlan.zhihu.com/p/59615785?utm_id=0","link":"/MachineLearning/PatternRecognition/PatterRecognition-C10-Approximate-Inference/"},{"title":"PatterRecognition-C12-Continuous-Latent-Variables","text":"Keywords: Conventional PCA, Probabilistic PCA, Nonlinear Latent Variable Models, Python This is the Chapter12 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] First Understanding PCA in Algebra &gt;&gt; Principal Component AnalysisThere are two commonly used definitions of PCA that give rise to the same algorithm: Maximum variance and Minimum-error. Maximum variance formulation$\\pmb{x} \\in R^D$, consider the projection onto a one-dimensional space (M = 1), We can define the direction of this space using a $D$-dimensional vector $\\pmb{u_1}$, which for convenience (and without loss of generality) we shall choose to be a unit vector so that $\\pmb{u_1^T u_1} = 1$. the mean of samples$$\\bar{\\pmb{x}} = \\frac{1}{N} \\sum_{n=1}^N \\pmb{x_n}\\tag{12.1}$$the variance of the projected data is given by$$\\frac{1}{N} \\sum_{n=1}^N \\lbrace \\pmb{u_1^Tx_n - u_1^T\\bar{x}}\\rbrace^2 = \\pmb{u_1^TSu_1}\\tag{12.2}$$where $\\pmb{S}$ is the data covariance matrix defined by$$\\pmb{S} = \\frac{1}{N} \\sum_{n=1}^N (\\pmb{x_n - \\bar{x}})(\\pmb{x_n - \\bar{x}})^T\\tag{12.3}$$use Lagrange multiplier and make an unconstrained maximization of$$\\pmb{u_1^TSu_1} + \\lambda _1 (1 - pmb{u_1^Tu_1})\\tag{12.4}$$we get$$\\pmb{Su_1} = \\lambda _1 \\pmb{u_1}$$so the variance will be a maximum when we set $\\pmb{u_1}$ equal to the eigenvector having the largest eigenvalue $\\lambda _1$. This eigenvector is known as the first principal component. To summarize, principal component analysis involves evaluating the mean $\\bar{\\pmb{x}}$ and the covariance matrix $\\pmb{S}$ of the data set and then finding the $M$ eigenvectors of S corresponding to the $M$ largest eigenvalues. Minimum-error formulationwe introduce a complete orthonormal set of $D$-dimensional basis vectors $\\lbrace \\pmb{u_i} \\rbrace$ where $i = 1, \\cdots , D$ that satisfy $$\\pmb{u_i^Tu_j} = \\sigma_{ij}\\tag{12.7}$$Because this basis is complete, each data point can be represented exactly by a linear combination of the basis vectors$$\\begin{aligned}\\pmb{x_n} &amp;= \\sum_{i=1}^D \\alpha_{ni} \\pmb{u_i} \\\\&amp;= \\sum_{i=1}^D (\\pmb{x_n^T u_i}) \\pmb{u_i}\\end{aligned}\\tag{12.8}$$ This simply corresponds to a rotation of the coordinate system to a new system defined by the $\\lbrace \\pmb{u_i} \\rbrace$, and the original $D$ components $\\lbrace x_{n1}, \\cdots, x_{nD} \\rbrace$ are replaced by an equivalent set $\\lbrace \\alpha_{n1}, \\cdots, \\alpha_{nD} \\rbrace$. The $M$ -dimensional linear subspace can be represented, without loss of generality, by the first $M$ of the basis vectors, and so we approximate each data point $\\pmb{x_n}$ by $$\\widetilde{\\pmb{x_n}} = \\sum_{i=1}^M z_{ni}\\pmb{u_i} + \\sum_{i = M+1}^{D} b_i \\pmb{u_i}\\tag{12.10}$$we are going to minimize the distortion introduced by the reduction in dimensionality.$$J = \\frac{1}{N} \\sum_{n=1}^N ||\\pmb{x_n - \\widetilde{x_n}}||^2\\tag{12.11}$$get$$z_{nj} = \\pmb{x_n^Tu_j}\\tag{12.12}$$$$b_j = \\bar{\\pmb{x}}^T \\pmb{u_j}\\tag{12.13}$$ substitute (12.12), (12.13) to (12.11), we get$$\\pmb{x_n - \\widetilde{x_n}} = \\sum_{i = M+1}^D \\lbrace (\\pmb{x_n - \\bar{x}})^T \\pmb{u_i}\\rbrace \\pmb{u_i}\\tag{12.14}$$from which we see that the displacement vector from $\\pmb{x_n}$ to $\\widetilde{\\pmb{x_n}}$ lies in the space orthogonal to the principal subspace. This result accords with our intuition that, in order to minimize the average squared projection distance, we should choose the principal component subspace to pass through the mean of the data points and to be aligned with the directions of maximum variance. thus, we get distortion measure $J$ as$$J = \\frac{1}{N} \\sum_{n=1}^N \\sum_{i=M+1}^D (\\pmb{x_n^T u_i - \\bar{x}^T u_i})^2 = \\sum_{i=M+1}^D \\pmb{u_i^T S u_i}\\tag{12.15}$$ Applications of PCA For data compression. data pre-processing. data visualization. PCA for high-dimensional dataProbabilistic PCAThis reformulation of PCA, known as probabilistic peA, brings several advantages compared with conventional PCA. deal with missing values in the data set. trained using the EM algorithm, which is computationally efficient. forms the basis for a Bayesian treatment of PCA. can be used to model class-conditional densities. can be run generatively to provide samples from the distribution. represents a constrained form of the Gaussian distribution. We can formulate probabilistic PCA by first introducing an explicit latent variable $\\pmb{z}$ corresponding to the principal-component subspace. the prior distribution is$$p(\\pmb{z}) = N(\\pmb{z | 0, I})\\tag{12.31}$$ the conditional distribution is$$p(\\pmb{x|z}) = N(\\pmb{x}| \\pmb{Wz + \\mu}, \\sigma^2 \\pmb{I})\\tag{12.32}$$ the columns of $\\pmb{W}: D \\times M$ span a linear subspace within the data space that corresponds to the principal subspace. We can view the probabilistic PCA model from a generative viewpoint in which a sampled value of the observed variable is obtained by first choosing a value for the latent variable and then sampling the observed variable conditioned on this latent value. Specifically, the $D$-dimensional observed variable $\\pmb{x}$ is defined by a linear transformation of the $M$-dimensional latent variable $\\pmb{z}$ plus additive Gaussian ‘noise’, so that$$\\pmb{x} = \\pmb{Wz + \\mu + \\epsilon}\\tag{12.33}$$the marginal distribution of $p(\\pmb{x})$ is$$p(\\pmb{x}) = \\int p(\\pmb{x|z})p(\\pmb{z}) d\\pmb{z} = N(\\pmb{x | \\mu, C})\\tag{12.34}$$where the $D \\times D$ matrix $\\pmb{C}$ is$$\\pmb{C} = \\pmb{WW^T} + \\sigma^2 \\pmb{I}\\tag{12.36}$$from 2.116 &gt;&gt;, we get the posterior distribution $p(\\pmb{z|x})$,$$p(\\pmb{z|x}) = N(\\pmb{z} | \\pmb{M^{-1} W^T (x - \\mu)}, \\sigma^{2}\\pmb{M})\\tag{12.42}$$where the $M \\times M$ matrix $\\pmb{M}$ is defined by$$\\pmb{M} = \\pmb{W^TW} + \\sigma^2 \\pmb{I}$$ Maximum likelihood PCAThe corresponding log likelihood function is given by$$\\begin{aligned}\\ln p(\\pmb{X} | \\pmb{\\mu, W}, \\sigma^2) &amp;= \\sum_{n=1}^N \\ln p(\\pmb{x_n} | \\pmb{W, \\mu}, \\sigma^2) \\\\&amp;= -\\frac{ND}{2} \\ln(2\\pi) - \\frac{N}{2} \\ln |\\pmb{C}| - \\frac{1}{2} \\sum_{n=1}^N (\\pmb{x_n - \\mu})^T \\pmb{C}^{-1} (\\pmb{x_n - \\mu}) \\\\&amp;= -\\frac{N}{2} \\lbrace D \\ln(2\\pi) + \\ln |\\pmb{C}| + Tr(\\pmb{C^{-1}S})\\rbrace\\end{aligned}\\tag{12.44}$$ maximum log likelihood, we get$$\\pmb{W_{ML}} = \\pmb{U_{M}} (\\pmb{L_M} - \\sigma^2 \\pmb{I})^{1/2} \\pmb{R}\\tag{12.45}$$where, $\\pmb{U_M}$ is a $D \\times M$ matrix whose columns are given by any subset (of size $M$) of the eigenvectors of the data covariance matrix $\\pmb{S}$, the $M \\times M$ diagonal matrix $\\pmb{L_M}$ has elements given by the corresponding eigenvalues $\\lambda_i$, and $\\pmb{R}$ is an arbitrary $M \\times M$ orthogonal matrix.$$\\sigma_{ML}^2 = \\frac{1}{D - M} \\sum_{i = M + 1}^D \\lambda_i\\tag{12.46}$$so that $\\sigma_{ML}^2$ is the average variance associated with the discarded dimensions. Because $\\pmb{R}$ is orthogonal, it can be interpreted as a rotation matrix in the $M \\times M$ latent space. For the particular case of $\\pmb{R} = \\pmb{I}$, we see that the columns of $\\pmb{W}$ are the principal component eigenvectors scaled by the variance parameters $\\lambda_i - \\sigma^2$.The interpretation of these scaling factors is clear once we recognize that for a convolution of independent Gaussian distributions (in this case the latent space distribution and the noise model) the variances are additive.Thus the variance $\\lambda_i$ in the direction of an eigenvector $\\pmb{u_i}$ is composed of the sum of a contribution $\\lambda_i - \\sigma^2$ from the projection of the unit-variance latent space distribution into data space through the corresponding column of $\\pmb{W}$, plus an isotropic contribution of variance $\\sigma^2$ which is added in all directions by the noise model. EM algorithm for PCAIn spaces of high dimensionality, there may be computational advantages in using an iterative EM procedure rather than working directly with the sample covariance matrix. This EM procedure can also be extended to the factor analysis model, for which there is no closed-form solution. Finally, it allows missing data to be handled in a principled way. the complete-data log likelihood function takes the form$$\\ln p(\\pmb{X,Z} | \\pmb{\\mu, W}, \\sigma^2) = \\sum_{n=1}^N \\lbrace \\ln p(\\pmb{x_n} | \\pmb{z_n}) + \\ln p(\\pmb{z_n}) \\rbrace\\tag{12.52}$$the $Q$ function(expectation of log likelihood function) is$$\\begin{aligned}E[\\ln p(\\pmb{X,Z} | \\pmb{\\mu, W}, \\sigma^2)] &amp;= -\\sum_{n=1}^N \\lbrace \\frac{D}{2} \\ln(2\\pi\\sigma^2) + \\frac{1}{2} Tr(E[\\pmb{z_nz_n^T}])\\\\&amp;+ \\frac{1}{2\\sigma^2} ||\\pmb{x_n - \\mu}||^2 - \\frac{1}{\\sigma^2} E[\\pmb{z_n}]^T \\pmb{W}^T (\\pmb{x_n - \\mu}) \\\\&amp;+ \\frac{1}{2\\sigma^2} Tr(E[\\pmb{z_nz_n^T}] \\pmb{W^TW})\\rbrace\\end{aligned}$$where$$E[\\pmb{z_n}] = \\pmb{M^{-1}W^T(x_n - \\bar{x})}\\tag{12.54}$$$$E[\\pmb{z_nz_n^T}] = \\sigma^2 \\pmb{M}^{-1} + E[\\pmb{z_n}]E[\\pmb{z_n}]^T\\tag{12.55}$$Now in the M step, we get Note that this EM algorithm can be implemented in an on-line form in which each $D$-dimensional data point is read in and processed and then discarded before the next data point is considered. Bayesian PCAWe want to infer the reduced dimention $M$ by our model, not be given by users. So here comes Bayesian PCA, we use evidence approximation. Recall the definiton, In Bayesian statistics, the evidence (also known as the marginal likelihood) is the probability of observing the data given a particular model, averaged over all possible parameter values. It serves as a normalizing constant when updating the prior beliefs to obtain the posterior distribution. The evidence is crucial for comparing different models and selecting the one that best explains the observed data.The evidence for a model $M$ given the data $D$ is denoted as $p(D|M)$ and calculated as:$$p(D|M) = \\int p(D|\\theta, M) p(\\theta|M) d\\theta$$where$p(D|\\theta, M)$ is the likelihood, representing the probability of the data given the model parameters.$p(\\theta | M)$ is the prior, representing the prior distribution over the model parameters. So here, we assume the prior of $\\pmb{W}$ is Gaussian,$$p(\\pmb{W} | \\pmb{\\alpha}) = \\prod_{i=1}^M (\\frac{\\alpha_i}{2\\pi})^{D/2} exp \\lbrace -\\frac{1}{2} \\alpha_i \\pmb{w_i^T w_i}\\rbrace\\tag{12.60}$$The values of $\\alpha_i$ are re-estimated during training by maximizing the log marginal likelihood given by$$p(\\pmb{X}| \\pmb{\\alpha, \\mu}, \\sigma^2) = \\int \\underbrace{p(\\pmb{X} | \\pmb{W, \\mu}, \\sigma^2)}_{12.43} p(\\pmb{W} | \\pmb{\\alpha}) d\\pmb{W}\\tag{12.61}$$ Because this integration is intractable, we make use of the Laplace approximation. If we assume that the posterior distribution is sharply peaked, as will occur for sufficiently large data sets, then the re-estimation equations obtained by maximizing the marginal likelihood with respect to $\\alpha_i$ take the simple form $$\\alpha_i^{new} = \\frac{D}{\\pmb{w_i^Tw_i}}\\tag{12.62}$$which follows from (3.98) &gt;&gt;, These reestimations are interleaved with the EM algorithm updates for determining $\\pmb{W}$ and $\\sigma^2$, the only change is to the M. Step equation for $\\pmb{W}$, Factor analysisFactor analysis is a linear-Gaussian latent variable model that is closely related to probabilistic PCA. Kernel PCAKernel Principal Component Analysis (Kernel PCA) is an extension of the classical Principal Component Analysis (PCA) technique that allows for the nonlinear transformation of data by using kernel functions. Kernel PCA is particularly useful for capturing complex, nonlinear relationships in the data. The choice of the kernel function and its parameters is crucial and may depend on the specific characteristics of the data. Nonlinear Latent Variable Modelsthe issues of nonlinearity and non-Gaussianity are related because a general probability density can be obtained from a simple fixed reference density, such as a Gaussian, by making a nonlinear change of variables. Independent component analysisindependent component analysis: the observed variables are related linearly to the latent variables, but for which the latent distribution is non-Gaussian. Consider a situation in which two people are talking at the same time, and we record their voices using two microphones. If we ignore effects such as time delay and echoes, then the signals received by the microphones at any point in time will be given by linear combinations of the amplitudes of the two voices. The coefficients of this linear combination will be constant, and if we can infer their values from sample data, then we can invert the mixing process (assuming it is nonsingular) and thereby obtain two clean signals each of which contains the voice of just one person. This type of problem is sometimes addressed using the following approach (MacKay, 2003) in which we ignore the temporal nature of the signals and treat the successive samples as i.i.d. We consider a generative model in which there are two latent variables corresponding to the unobserved speech signal amplitudes, and there are two observed variables given by the signal values at the microphones. The latent variables have a joint distribution that factorizes as,$$p(\\pmb{z}) = \\prod_{j=1}^M p(z_j)\\tag{12.89}$$and the observed variables are given by a linear combination of the latent variables. Given a data set of observations, The log likelihood can be maximized using gradient-based optimization giving rise to a particular version of independent component analysis. The success of this approach requires that the latent variables have non-Gaussian distributions. Why? Recall that in probabilistic PCA (and in factor analysis) the latent-space distribution is given by a zero-mean isotropic Gaussian.The model therefore cannot distinguish between two different choices for the latent variables where these differ simply by a rotation in latent space.This can be verified directly by noting that the marginal density (12.35), and hence the likelihood function, is unchanged if we make the transformation $\\pmb{W} \\rightarrow \\pmb{WR}$ where $\\pmb{R}$ is an orthogonal matrix satisfying $\\pmb{RR^T} = \\pmb{I}$, because the matrix $\\pmb{C}$ given by (12.36) is itself invariant.$$p(\\pmb{x}) = N(\\pmb{x | \\mu, C})\\tag{12.35}$$$$\\pmb{C} = \\pmb{WW^T} + \\sigma^2 \\pmb{I}\\tag{12.36}$$ Another way to see why a Gaussian latent variable distribution in a linear model is insufficient to find independent components is to note that the principal components represent a rotation of the coordinate system in data space such as to diagonalize the covariance matrix, so that the data distribution in the new coordinates is then uncorrelated. Although zero correlation is a necessary condition for independence it is not, however, sufficient. In practice, a common choice for the latent-variable distribution is given by$$p(z_j) = \\frac{1}{\\pi \\cosh(z_j)} = \\frac{1}{\\pi(e^{z_j} + e^{-z_j})}\\tag{12.90}$$ Autoassociative neural networks Modelling nonlinear manifolds","link":"/MachineLearning/PatternRecognition/PatterRecognition-C12-Continuous-Latent-Variables/"},{"title":"PatterRecognition-C13-Sequential-Data","text":"Keywords: Hidden Markov Models, Python This is the Chapter13 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] Markov Modelswe can use the product rule to express the joint distribution for a sequence of observations in the form$$p(\\pmb{x_1}, \\cdots, \\pmb{x_N}) = \\prod_{n=1}^N p(\\pmb{x_n} | \\pmb{x_1}, \\cdots, \\pmb{x_{n-1}})\\tag{13.1}$$the joint distribution for a sequence of observations under first-order Markov chain in the form$$p(\\pmb{x_1}, \\cdots, \\pmb{x_N}) = p(\\pmb{x_1}) \\prod_{n=2}^N p(\\pmb{x_n} | \\pmb{x_{n-1}})\\tag{13.2}$$the joint distribution for a sequence of observations under second-order Markov chain in the form$$p(\\pmb{x_1}, \\cdots, \\pmb{x_N}) = p(\\pmb{x_1}) p(\\pmb{x_2} | \\pmb{x_1}) \\prod_{n=3}^N p(\\pmb{x_n} | \\pmb{x_{n-1}}, \\pmb{x_{n-2}})\\tag{13.4}$$The joint distribution for Hidden Markov Model or linear dynamical system is given by$$p(\\pmb{x_1}, \\cdots, \\pmb{x_N}, \\pmb{z_1}, \\cdots, \\pmb{z_N}) = p(\\pmb{z_1}) [\\prod_{n=2}^N p(\\pmb{z_n | z_{n-1}})] \\prod_{n=1}^N p(\\pmb{x_n | z_n})\\tag{13.6}$$ Hidden Markov Models HMM can also be interpreted as an extension of a mixture model in which the choice of mixture component for each observation is not selected independently but depends on the choice of component for the previous observation. the conditional distribution of latent variable is$$p(\\pmb{z_n} | \\pmb{z_{n-1}}, \\pmb{A}) = \\prod_{k=1}^K prod_{j=1}^K A_{jk}^{z_{n-1,j} z_{nk}}\\tag{13.7}$$Because the latent variables are $K$-dimensional binary variables, this conditional distribution corresponds to a table of numbers that we denote by $\\pmb{A}$, the elements of which are known as transition probabilities. The initial latent node $\\pmb{z_1}$ is special in that it does not have a parent node$$p(\\pmb{z_1} | \\pmb{\\pi}) = \\prod_{k=1}^K \\pi_k^{z_1k}\\tag{13.8}$$where, $\\sum_k \\pi_k = 1$ the emission probabilities in the form$$p(\\pmb{x_n} | \\pmb{z_n}, \\pmb{\\phi}) = \\prod_{k=1}^K p(\\pmb{x_n} | \\phi_k )^{z_{nk}}\\tag{13.9}$$The joint probability distribution over both latent and observed variables is then given by$$p(\\pmb{X,Z} | \\pmb{\\theta}) = p(\\pmb{z_1} | \\pmb{\\pi}) [\\prod_{n=1}^N p(\\pmb{z_n} | \\pmb{z_{n-1}}, \\pmb{A})] \\prod_{m=1}^N \\underbrace{p(\\pmb{x_m} | \\pmb{z_m}, \\pmb{\\phi})}_{emission-probabilities}\\tag{13.10}$$where, $\\pmb{X} = \\lbrace \\pmb{x_1}, \\cdots, \\pmb{x_N}\\rbrace$, $\\pmb{Z} = \\lbrace \\pmb{z_1}, \\cdots, \\pmb{z_N}\\rbrace$, and $\\pmb{\\theta} = \\lbrace \\pmb{\\pi, A, \\phi}\\rbrace$ denotes the set of parameters governing the model. Many applications of hidden Markov models, for example speech recognition, or on-line character recognition, make use of left-to-right architectures. As an illustration of the left-to-right hidden Markov model, we consider an example involving handwritten digits. Here we train a hidden Markov model on a subset of data comprising 45 examples of the digit ‘2’. There are $K = 16$ states, each of which can generate a line segment of fixed length having one of 16 possible angles, and so the emission distribution is simply a $16 \\times 16$ table of probabilities associated with the allowed angle values for each state index value. Transition probabilities are all set to zero except for those that keep the state index $k$ the same or that increment it by 1, and the model parameters are optimized using 25 iterations of EM. We can gain some insight into the resulting model by running it generatively, as shown in Figure 13.11. Maximum likelihood for the HMMIf we have observed a data set $\\pmb{X} = \\lbrace \\pmb{x_1}, \\cdots, \\pmb{x_N} \\rbrace$, we can determine the parameters of an HMM using maximum likelihood. The likelihood function is obtained from the joint distribution (13.10) by marginalizing over the latent variables$$p(\\pmb{X} | \\pmb{\\theta}) = \\sum_{\\pmb{Z}} p(\\pmb{X, Z} | \\pmb{\\theta})\\tag{13.11}$$Direct maximization of the likelihood function will therefore lead to complex expressions with no closed-form solutions, as was the case for simple mixture models(recall that a mixture model for i.i.d. data is a special case of the HMM). We therefore turn to the expectation maximization algorithm to find an efficient framework for maximizing the likelihood function in hidden Markov models—EM Algorithm. EM Algorithm for HMM is now as follows: choose an initial value $\\pmb{\\theta}^{old}$ for the model parameters. E. Step: take these parameter values and find the posterior distribution of the latent variables$$p(\\pmb{Z}|\\pmb{X}, \\pmb{\\theta}^{old})$$ use this posterior distribution to evaluate the expectation of the logarithm of the complete-data likelihood function$$Q(\\pmb{\\theta, \\theta^{old}}) = \\sum_{Z} p(\\pmb{Z}|\\pmb{X}, \\pmb{\\theta}^{old}) \\ln p(\\pmb{X,Z} | \\pmb{\\theta})\\tag{13.12}$$ here, We shall use $\\gamma(\\pmb{z_n})$ to denote the marginal posterior distribution of a latent variable zn, and $\\xi(\\pmb{z_{n−1}}, \\pmb{z_n})$ to denote the joint posterior distribution of two successive latent variables, so that$$\\gamma(\\pmb{z_n}) = p(\\pmb{z_n} | \\pmb{X}, \\pmb{\\theta}^{old})\\tag{13.13}$$$$\\xi(\\pmb{z_{n-1}, z_n}) = p(\\pmb{z_{n-1}, z_n} | \\pmb{X}, \\pmb{\\theta}^{old})\\tag{13.14}$$ Because the expectation of a binary random variable is just the probability that it takes the value 1, we have$$\\gamma(z_{nk}) = E[z_{nk}] = \\sum_{\\pmb{z}} \\gamma(\\pmb{z}) z_{nk}\\tag{13.15}$$$$\\xi(z_{n-1,j}, z_{nk}) = E[z_{n-1,j}, z_{nk}] = \\sum_{\\pmb{z}} \\gamma(\\pmb{z})z_{n-1,j}, z_{nk}\\tag{13.16}$$thus,$$Q(\\pmb{\\theta, \\theta^{old}}) = \\sum_{k=1}^K \\gamma(z_{1k}) \\ln \\pi_k + \\sum_{n=2}^N \\sum_{j=1}^K \\sum_{k=1}^K \\xi(z_{n-1,j}, z_{nk}) \\ln A_{jk} + \\sum_{n=1}^N \\sum_{k=1}^K \\gamma(z_{nk}) \\ln p(\\pmb{x_n}|\\pmb{\\phi_k})\\tag{13.17}$$ M. Step: maximize the function $Q(\\pmb{\\theta}, \\pmb{\\theta}^{old})$ with respect to $\\pmb{\\theta} = \\lbrace \\pmb{\\pi, A, \\phi} \\rbrace$ in which we treat $\\gamma(\\pmb{z_n})$ and $p(\\pmb{z_{n-1}, z_n} | \\pmb{X}, \\pmb{\\theta}^{old})$ as constant. 3.1. Maximization with respect to $\\pmb{π}$ and $\\pmb{A}$ is easily achieved using appropriate Lagrange multipliers with the results$$\\pi_k = \\frac{\\gamma(z_{1k})}{\\sum_{j=1}^K \\gamma(z_{1j})}\\tag{13.18}$$$$A_{jk} = \\frac{\\sum_{n=2}^N \\xi(z_{n-1,j}, z_{nk})}{\\sum_{l=1}^K \\sum_{n=2}^N \\xi(z_{n-1,j}, z_{nl})}\\tag{13.19}$$ 3.2. maximize the function with respect to $\\phi_k$, suppose Gaussian emission densities we have $p(\\pmb{x}|\\phi_k) = N(\\pmb{x} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})$$$\\pmb{\\mu_k} = \\frac{\\sum_{n=1}^N \\gamma(z_{nk})\\pmb{x_n}}{\\sum_{n=1}^N \\gamma(z_{nk})}\\tag{13.20}$$$$\\pmb{\\Sigma_k} = \\frac{\\sum_{n=1}^N \\gamma(z_{nk})(\\pmb{x_n - \\mu_k})(\\pmb{x_n - \\mu_k})^T}{\\sum_{n=1}^N \\gamma(z_{nk})}\\tag{13.21}$$ The forward-backward algorithmNext we seek an efficient procedure for evaluating the quantities $\\gamma(z_{nk})$ and $\\xi(z_{n-1,j}, z_{nk})$, corresponding to the E step of the EM algorithm. Here, we use alpha-beta algorithm, which is a variant of forward-backward algorithm or Baum-Welch algorithm. Let us summarize the steps required to train a hidden Markov model using the EM algorithm. make an initial selection of the parameters $\\pmb{\\theta}^{old}$, where $\\pmb{\\theta} = (\\pmb{\\pi, A, \\phi})$. 1.1. The $\\pmb{A}$ and $\\pmb{\\pi}$ parameters are often initialized either uniformly or randomly from a uniform distribution (respecting their non-negativity and summation constraints). 1.2. Initialization of the parameters $\\pmb{\\phi}$ will depend on the form of the distribution. For instance in the case of Gaussians, the parameters $\\pmb{\\mu_k}$ might be initialized by applying the $K$-means algorithm to the data, and $\\pmb{\\Sigma_k}$ might be initialized to the covariance matrix of the corresponding $K$ means cluster. 1.3. Run both the forward $\\alpha$ recursion and the backward $\\beta$ recursion and use the results to evaluate $\\gamma(\\pmb{z_n})$ and $\\xi(\\pmb{z_{n−1}}, \\pmb{z_n})$. M.Step in last Section. We then continue to alternate between E and M steps until some convergence criterion is satisfied, for instance when the change in the likelihood function is below some threshold. The sum-product algorithm for the HMMScaling factorsThe Viterbi algorithmExtensions of the hidden Markov modelLinear Dynamical SystemsInference in LDSLearning in LDSExtensions of LDSParticle filters","link":"/MachineLearning/PatternRecognition/PatterRecognition-C13-Sequential-Data/"},{"title":"PatterRecognition-C7-Sparse-Kernel-Machines","text":"Keywords: SVM, RVM, Sparse Kernel technique, Python This is the Chapter7 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] One of the significant limitations of many such algorithms is that the kernel function $k(\\pmb{x_n}, \\pmb{x_m})$ must be evaluated for all possible pairs xn and xm of training points, which can be computationally infeasible during training and can lead to excessive computation times when making predictions for new data points. In this chapter we shall look at kernel-based algorithms that have sparse solutions, so that predictions for new inputs depend only on the kernel function evaluated at a subset of the training data points. Maximum Margin ClassifiersOverlapping class distributionsRelation to logistic regressionMulticlass SVMsSVMs for regressionComputational learning theoryRelevance Vector MachinesRVM for regressionAnalysis of sparsityRVM for classification","link":"/MachineLearning/PatternRecognition/PatterRecognition-C7-Sparse-Kernel-Machines/"},{"title":"PatterRecognition-C14-Combining-Models","text":"Keywords: Boosting, Conditional Mixture Models(linear regression, logistic regression), EM Algorithm This is the Chapter14 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. Bayesian Model AveragingDifference between Bayesian Model Averaging and Model Combination Methods. in Bayesian model averaging, the whole data set is generated by a single model(e.g. this single model is a mixture of Gaussians). $$p(X) = \\sum_{h=1}^H p(\\pmb{X} | h) p(h)\\tag{14.6}$$The interpretation of this summation over $h$ is that just one model is responsible for generating the whole data set, and the probability distribution over $h$ simply reflects our uncertainty as to which model that is.As the size of the data set increases, this uncertainty reduces, and the posterior probabilities $p(h|\\pmb{X})$ become increasingly focussed on just one of the models. Model Combination methods, e.g. mixture of Gaussians $$p(\\pmb{x}) = \\sum_{k=1}^K \\pi_k N(\\pmb{x} | \\pmb{\\mu_k}, \\Sigma_k)$$ $$p(\\pmb{X}) = \\prod_{n=1}^N p(\\pmb{x_n}) = \\prod_{n=1}^N [\\sum_{z_n} p(\\pmb{x_n}, \\pmb{z_n})]\\tag{14.5}$$The model contains a binary latent variable $\\pmb{z}$ that indicates which component of the mixture is responsible for generating the corresponding data point. CommitteesFor instance, we might train $L$ different models and then make predictions using the average of the predictions made by each model. Such combinations of models are sometimes called committees. Why we need committees?Recall the Bias-Variance Decomposition &gt;&gt; in Section 3.Such a procedure can be motivated from a frequentist perspective by considering the trade-off between bias and variance, which decomposes the error due to a model into the bias component that arises from differences between the model and the true function to be predicted, and the variance component that represents the sensitivity of the model to the individual data points.when we trained multiple polynomials using the sinusoidal data, and then averaged the resulting functions, the contribution arising from the variance term tended to cancel, leading to improved predictions. When we averaged a set of low-bias models (corresponding to higher order polynomials), we obtained accurate predictions for the underlying sinusoidal function from which the data were generated. But this “average method” has been proven not good enough, in order to achieve more significant improvements, we turn to a more sophisticated technique for building committees, known as boosting. BoostingThe principal difference between boosting and the committee methods such as bagging discussed above, is that the base classifiers are trained in sequence, and each base classifier is trained using a weighted form of the data set in which the weighting coefficient associated with each data point depends on the performance of the previous classifiers. In particular, points that are misclassified by one of the base classifiers are given greater weight when used to train the next classifier in the sequence. Once all the classifiers have been trained, their predictions are then combined through a weighted majority voting scheme, as illustrated schematically in Figure 14.1. Minimizing exponential errorFriedman et al. (2000) gave a different and very simple interpretation of boosting in terms of the sequential minimization of an exponential error function. Consider the exponential error function defined by$$E = \\sum_{n=1}^N exp \\lbrace -t_n f_m(\\pmb{x_n})\\rbrace\\tag{14.20}$$where$$f_m(\\pmb{x}) = \\frac{1}{2} \\sum_{l=1}^m a_l y_l(\\pmb{x})\\tag{14.21}$$Our goal is to minimize $E$ with respect to both the weighting coefficients $\\alpha_l$ and the parameters of the base classifiers $y_l(\\pmb{x})$. we shall suppose that the base classifiers $y_1(x), \\cdots , y_{m−1}(x)$ are fixed, as are their coefficients $\\alpha_1, \\cdots , \\alpha_{m-1}$, and so we are minimizing only with respect to $\\alpha_m$ and $y_m(x)$.$$\\begin{aligned}E &amp;= \\sum_{n=1}^N exp \\lbrace -t_nf_{m-1}(\\pmb{x_n}) - \\frac{1}{2} t_n \\alpha_m y_m(\\pmb{x_n})\\rbrace \\\\&amp;= \\sum_{n=1}^N w_n^{m} exp \\lbrace - \\frac{1}{2} t_n \\alpha_m y_m(\\pmb{x_n})\\rbrace\\end{aligned}\\tag{14.22}$$where$$w_n^{m} = exp \\lbrace -t_nf_{m-1}(\\pmb{x_n}) \\rbrace$$can be viewed as constants because we are optimizing only $\\alpha_m$ and $y_m(x)$. If we denote by $T_m$ the set of data points that are correctly classified by $y_m(\\pmb{x})$, and if we denote the remaining misclassified points by $M_m$, then we can in turn rewrite the error function in the form … Anyway, the formula following just proves that the AdaBoost method is a sequential minimization of exp error. Error functions for boostingTree-based ModelsFor any new input x, we determine which region it falls into by starting at the top of the tree at the root node and following a path down to a specific leaf node according to the decision criteria at each node. Note that such decision trees are not probabilistic graphical models. Within each region, there is a separate model to predict the target variable. Consider first a regression problem in which the goal is to predict a single target variable $t$ from a $D$-dimensional vector $\\pmb{x} = (x_1, \\cdots , x_D)^T$ of input variables. The training data consists of input vectors {\\pmb{x_1}, \\cdots , \\pmb{x_N}} along with the corresponding continuous labels ${t_1, \\cdots , t_N}$. If the partitioning of the input space is given, and we minimize the sum-of-squares error function, then the optimal value of the predictive variable within any given region is just given by the average of the values of tn for those data points that fall in that region. … However, in practice it is found that the particular tree structure that is learned is very sensitive to the details of the data set, so that a small change to the training data can result in a very different set of splits (Hastie et al., 2001). Conditional Mixture ModelsMixtures of linear regression modelsHere we consider a simple example corresponding to a mixture of linear regression models, which represents a straightforward extension of the Gaussian mixture model discussed in Section 9.2 to the case of conditional Gaussian distributions. the mixture distribution (conditional Gaussian distributions) can be written as$$p(\\pmb{t} | \\pmb{\\theta}) = \\sum_{k=1}^K \\pi_k N(t | \\pmb{w_k}^T\\pmb{\\phi}, \\beta^{-1})\\tag{14.34}$$ The log likelihood function for this model, given a data set of observations $\\lbrace \\phi_n, t_n\\rbrace$ takes the form$$\\ln p(\\pmb{t} | \\pmb{\\theta}) = \\sum_{n=1}^N \\ln (\\sum_{k=1}^K \\pi_k N(t_n | \\pmb{w_k}^T \\phi_n, \\beta^{-1}))\\tag{14.35}$$ The complete-data log likelihood function then takes the form$$\\ln p(\\pmb{t, Z} | \\pmb{\\theta}) = \\sum_{n=1}^N \\sum_{k=1}^K z_{nk}\\ln \\lbrace \\pi_k N(t_n | \\pmb{w_k}^T \\phi_n, \\beta^{-1}) \\rbrace$$ EM Algorithm for this formula is now as follows: choose an initial value $\\pmb{\\theta}^{old}$ for the model parameters. E. Step: use current $\\pmb{\\theta}$ to evaluate the posterior probabilities, or responsibilities, of each component $k$ for every data point $n$ given by $$ \\begin{aligned} \\gamma_{nk} &amp;= E[z_{nk}]\\\\ &amp;= p(k | \\phi_n, \\pmb{\\theta}^{old})\\\\ &amp;= \\frac{\\pi_k N(t_n | \\pmb{w_k}^T\\phi_n, \\beta^{-1})}{\\sum_j \\pi_j N(t_n | \\pmb{w_j}^T\\phi_n, \\beta^{-1})} \\end{aligned} \\tag{14.37} $$ use current responsibilities to determine the expectation, with respect to the posterior distribution $p(\\pmb{Z}|\\pmb{t}, \\pmb{\\theta}^{old})$, of the complete-data log likelihood $$ \\begin{aligned} Q(\\pmb{\\theta}, \\pmb{\\theta}^{old}) &amp;= E_{\\pmb{Z}}[\\ln p(\\pmb{t, Z} | \\pmb{\\theta})]\\\\ &amp;= \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk} \\lbrace \\ln \\pi_k + \\ln N(t_n | \\pmb{w_k}^T \\phi_n, \\beta^{-1})\\rbrace \\end{aligned} \\tag{14.37} $$ M. Step: 3.1. maximize the function $Q(\\pmb{\\theta}, \\pmb{\\theta}^{old})$ with respect to $\\pi_k$ $$ \\pi_k = \\frac{1}{N} \\sum_{n=1}^{N} \\gamma_{nk} \\tag{14.38} $$ 3.2. maximization with respect to the parameter vector $\\pmb{w_k}$ of the $k$ th linear regression model $$ Q(\\pmb{\\theta}, \\pmb{\\theta}^{old}) = \\sum_{n=1}^N \\gamma_{nk} \\lbrace -\\frac{\\beta}{2} (t_n - \\pmb{w_k}^T\\phi_n)^2 \\rbrace + const \\tag{14.39} $$ where the constant term includes the contributions from other weight vectors $\\pmb{w_j}$ for $j \\neq k$. Setting the derivative of (14.39) with respect to wk equal to zero gives $$ \\begin{aligned} 0 &amp;= \\sum_{n=1}^N \\gamma_{nk}(t_n - \\pmb{w_k}^T\\phi_n)\\phi_n\\\\ &amp;= \\pmb{\\Phi^T R_k (t - \\Phi w_k)} \\end{aligned} \\tag{14.40} $$ where $$ \\pmb{R_k} = diag(\\gamma_{nk}) $$ solving for $\\pmb{w_k}$, we get $$ \\pmb{w_k} = (\\pmb{\\Phi^T R_k \\Phi})^{-1} \\pmb{\\Phi^T R_k t} \\tag{14.42} $$ This represents a set of modified normal equations corresponding to the weighted least squares problem, of the same form as (4.99) found in the context of logistic regression. $$ \\pmb{w}^{(new)} = \\pmb{(\\Phi^T R \\Phi)^{-1} \\Phi^T R z} \\tag{4.99} $$ iterative reweighted least squares,IRLS &gt;&gt; 3.3. maximization with respect to the $\\beta$ $$ Q(\\pmb{\\theta}, \\pmb{\\theta}^{old}) = \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk} \\lbrace \\frac{1}{2} \\ln \\beta - \\frac{\\beta}{2}(t_n - \\pmb{w_k}^T \\phi_n)^2 \\rbrace \\tag{14.43} $$ Setting the derivative with respect to $\\beta$ equal to zero, $$ \\frac{1}{\\beta} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk} (t_n - \\pmb{w_k}^T \\phi_n)^2 \\tag{14.44} $$ However, the mixture model also assigns significant probability mass to regions where there is no data because its predictive distribution is bimodal for all values of $x$. This problem can be resolved by extending the model to allow the mixture coefficients themselves to be functions of $x$, leading to models such as the mixture density networks discussed in Section 5.6, and hierarchical mixture of experts discussed in Section 14.5.3 Mixtures of logistic models The conditional distribution of the target variable, for a probabilistic mixture of $K$ logistic regression models, is given by$$p(t | \\phi, \\pmb{\\theta}) = \\sum_{k=1}^K \\pi_k y_k^t [1 - y_k]^{1-t}\\tag{14.45}$$ corresponding likelihood function is then given by$$p(\\pmb{t} | \\pmb{\\theta}) = \\prod_{n=1}^N (\\sum_{k=1}^K \\pi_k y_{nk}^{t_n} [1 - y_{nk}]^{1 - t_n})\\tag{14.46}$$ complete-data likelihood function is then given by$$p(\\pmb{t, Z} | \\pmb{\\theta}) = \\prod_{n=1}^N \\prod_{k=1}^K \\lbrace \\pi_k y_{nk}^{t_n} [1-y_{nk}]^{1-t_n} \\rbrace ^{z_{nk}}\\tag{14.47}$$where $\\pmb{Z}$ is the matrix of latent variables with elements $z_{nk}$. E. Step$$\\begin{aligned}\\gamma_{nk} &amp;= E[z_{nk}]\\\\&amp;= p(k | \\phi_n, \\pmb{\\theta}^{old})\\\\&amp;= \\frac{\\pi_k y_{nk}^{t_n} [1 - y_{nk}]^{1 - t_n}}{\\sum_j \\pi_j y_{nj}^{t_n} [1 - y_{nj}]^{1 - t_n}}\\end{aligned}\\tag{14.48}$$$$\\begin{aligned}Q(\\pmb{\\theta}, \\pmb{\\theta}^{old}) &amp;= E_{\\pmb{Z}}[\\ln p(\\pmb{t, Z} | \\pmb{\\theta})]\\\\&amp;= \\sum_{n=1}^N \\sum_{k=1}^K \\gamma_{nk} \\lbrace \\ln \\pi_k + t_n \\ln y_{nk} + (1-t_n) \\ln(1-y_{nk})\\rbrace\\end{aligned}\\tag{14.49}$$ M. Step $$\\pi_k = \\frac{1}{N} \\sum_{n=1}^N \\gamma_{nk}\\tag{14.50}$$ Note that the M step does not have a closed-form solution and must be solved iteratively using, for instance, the iterative reweighted least square(IRLS) algorithm. The gradient and the Hessian for the vector $\\pmb{w_k}$ are given by$$\\nabla_k Q = \\sum_{n=1}^N \\gamma_{nk} (t_n - y_{nk}) \\Phi_n\\tag{14.51}$$$$\\pmb{H_k} = - \\nabla_k \\nabla_k Q = \\sum_{n=1}^N \\gamma_{nk}y_{nk}(1-y_{nk})\\phi_n \\phi_n^T\\tag{14.52}$$ Mixtures of expertsmixtures of linear regression models and mixtures of logistic models extend the flexibility of linear models to include more complex (e.g., multimodal) predictive distributions, they are still very limited. mixture of experts models, the mixing coefficients themselves to be functions of the input variable,$$p(\\pmb{t} | \\pmb{x}) = \\sum_{k=1}^K \\pi_k(\\pmb{x}) p_k(\\pmb{t} | \\pmb{x})\\tag{14.53}$$ $\\pi_k(\\pmb{x})$ is called gating functions.$p_k(\\pmb{t} | \\pmb{x})$ is called experts. The notion behind the terminology is that different components can model the distribution in different regions of input space (they are ‘experts’ at making predictions in their own regions), and the gating functions determine which components are dominant in which region. Such a model still has significant limitations due to the use of linear models for the gating and expert functions. A much more flexible model is obtained by using a multilevel gating function to give the hierarchical mixture of experts, or HME model. It can again be trained efficiently by maximum likelihood using an EM algorithm with IRLS in the M step. Recall mixture density network &gt;&gt;, they are not the same. By contrast, the advantage of the mixture density network approach is that the component densities and the mixing coefficients share the hidden units of the neural network. Furthermore, in the mixture density network, the splits of the input space are further relaxed compared to the hierarchical mixture of experts in that they are not only soft, and not constrained to be axis aligned, but they can also be nonlinear.","link":"/MachineLearning/PatternRecognition/PatterRecognition-C14-Combining-Models/"},{"title":"PatterRecognition-C3-LinearModels-for-Regression","text":"Keywords: Least Squares, Bayesian Linear Regression, Evidence Approximation, Bias-Variance Decomposition, Python This is the Chapter3 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] Linear Basis Function Models$$y(\\pmb{x}, \\pmb{w}) = w_0 + w_1 x_1 + \\cdots + w_Dx_D\\tag{3.1}$$where, $\\pmb{x} = (x_1, \\cdots, x_D)^T$. $$y(\\pmb{x}, \\pmb{w}) = w_0 + \\sum_{j=1}^{M-1}w_j\\phi_j(\\pmb{x})\\tag{3.2}$$where, $\\phi_j(\\pmb{x})$ is called bisis functions, the parameter $w_0$ is called as a bias parameter. $$y(\\pmb{x,w}) = \\sum_{j=0}^{M-1}w_j\\phi_j(\\pmb{x}) = \\pmb{w^T}\\phi(\\pmb{x})\\tag{3.3}$$where, $\\phi_0{(\\pmb{x})} = 1$. The polynomial regression considered in Chapter 1 is a particular example of this model in which there is a single input variable $x$, and the basis functions take the form of powers of $x$ so that $\\phi_j(x) = x^j$. ‘Gaussian’ basis functions:$$\\phi_j(x) = exp\\lbrace -\\frac{(x-\\mu_j)^2}{2s^2} \\rbrace\\tag{3.4}$$where the $\\mu_j$ govern the locations of the basis functions in input space, and the parameter $s$ governs their spatial scale. sigmoidal basis function:$$\\phi_j(x) = \\sigma(\\frac{x-\\mu_j}{s})\\tag{3.5}$$where, $\\sigma(a)$ is the logistic sigmoid function defined by$$\\sigma(a) = \\frac{1}{1 + exp(-a)}\\tag{3.6}$$Similar function$$tanh(a) = 2\\sigma(a)-1$$ Maximum likelihood and least squaresStill the polynomial curve fitting problem in Chapter 1. As before, we assume that the target variable $t$ is given by a deterministic function $y(x,\\pmb{w})$ with additive Gaussian noise so that$$t = y(x, \\pmb{w}) + \\epsilon\\tag{3.7}$$where $\\epsilon$ is a zero mean Gaussian random variable with precision (inverse variance) $\\beta$. Thus we can write$$p(t|x,\\pmb{w}, \\beta) = \\mathcal{N}(t|y(x,\\pmb{w}), \\beta^{-1})\\tag{3.8}$$ if we assume a squared loss function, then the optimal prediction, for a new value of $x$, will be given by the conditional mean of the target variable.$$E[t|x] = \\int tp(t|x) dt = y(x,\\pmb{w})\\tag{3.8}$$ Now consider a data set of inputs $X = \\lbrace x_1, \\cdots, x_N \\rbrace$ with corresponding target values T = $\\lbrace t_1, \\cdots, t_N \\rbrace$. The likelihood function $$p(T|X,\\pmb{w},\\beta) = \\prod_{n=1}^{N}\\mathcal{N}(t_n|\\pmb{w^T}\\phi(x_n), \\beta^{-1})\\tag{3.10}$$ Thus, $$\\begin{aligned}\\ln p(T|X, \\pmb{w}, \\beta) &amp;= \\ln p(T|\\pmb{w}, \\beta)\\\\&amp;=\\sum_{n=1}^N\\ln \\mathcal{N}(t_n|\\pmb{w^T}\\phi(x_n),\\beta^{-1})\\\\&amp;=\\frac{N}{2}\\ln \\beta - \\frac{N}{2}\\ln(2\\pi) - \\beta E_D(\\pmb{w})\\end{aligned}\\tag{3.11}$$where the sum-of-squares error function is defined by$$E_D(\\pmb{w}) = \\frac{1}{2} \\sum_{n=1}^{N}\\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace^2\\tag{3.12}$$ The gradient of the log likelihood function (3.11) takes the form$$\\nabla \\ln p(T|\\pmb{w},\\beta) = \\sum_{n=1}^N \\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace \\phi(x_n)^T\\tag{3.13}$$Setting this gradient to zero gives$$0 = \\sum_{n=1}^{N} t_n \\phi(x_n)^T - \\pmb{w^T}(\\sum_{n=1}^{N}\\phi(x_n)\\phi(x_n)^T)\\tag{3.14}$$Solving for $\\pmb{w}$ we obtain$$\\pmb{w}_{ML} = (\\Phi^T\\Phi)^{-1}\\Phi^T T\\tag{3.15}$$which are known as the normal equations for the least squares problem. Here $\\Phi$ is an $N \\times M$ matrix, called the design matrix, whose elements are given by $\\Phi_{nj} = \\phi_j(x_n)$, so that$$\\Phi =\\begin{bmatrix} \\phi_0(x_1) &amp; \\phi_1(x_1) &amp; \\cdots &amp; \\phi_{M-1}(x_1) \\\\ \\phi_0(x_2) &amp; \\phi_1(x_2) &amp; \\cdots &amp; \\phi_{M-1}(x_2) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ \\phi_0(x_N) &amp; \\phi_1(x_N) &amp; \\cdots &amp; \\phi_{M-1}(x_N)\\end{bmatrix}$$ Least Squares in Algebra &gt;&gt; Least Squares in Numerical Analysis &gt;&gt; Moore-Penrose pseudo-inverse of the matrix $\\Phi$ is:$$\\Phi^\\dagger = (\\Phi^T\\Phi)^{-1}\\Phi^T$$ Now back to $w_0$, the bias parameter. the error function (3.12) $$\\begin{aligned} E_D(\\pmb{w}) &amp;= \\frac{1}{2} \\sum_{n=1}^{N}\\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace^2 \\\\ &amp;= \\frac{1}{2} \\sum_{n=1}^{N}\\lbrace t_n - w_0 -\\sum_{j=1}^{M-1} w_j\\phi_j(x_n)\\rbrace^2\\end{aligned}\\tag{3.18}$$Setting the derivative with respect to $w_0$ equal to zero, and solving for $w_0$, we obtain$$w_0 = \\bar{t} - \\sum_{j=1}^{M-1}w_j\\bar{\\phi_j}\\tag{3.19}$$where,$$\\bar{t} = \\frac{1}{N}\\sum_{n=1}^{N} t_n\\\\\\bar{\\phi_j} = \\frac{1}{N}\\sum_{n=1}^{N}\\phi_j(x_n)\\tag{3.20}$$Thus the bias $w_0$ compensates for the difference between the averages (over the training set) of the target values and the weighted sum of the averages of the basis function values. We can also maximize the log likelihood function (3.11) with respect to the noise precision parameter $\\beta$, giving$$\\frac{1}{\\beta_{ML}} = \\frac{1}{N} \\sum_{n=1}^{N} \\lbrace t_n - \\pmb{w_{ML}^T} \\phi(x_n)\\rbrace^2\\tag{3.21}$$so we see that the inverse of the noise precision is given by the residual variance of the target values around the regression function. Geometry of least squares Note that $\\psi_j$ corresponds to the $j^{th}$ column of $\\Phi$, whereas $\\phi(x_n)$ corresponds to the $n^{th}$ row of $\\Phi$. We define $y$ to be an $N$-dimensional vector whose $n^{th}$ element is given by $y(x_n,\\pmb{w})$, where $n = 1, . . . , N$. Sequential learningBatch techniques, such as the maximum likelihood solution (3.15), which involve processing the entire training set in one go, can be computationally costly for large data sets. it may be worthwhile to use sequential algorithms, also known as on-line algorithms, in which the data points are considered one at a time, and the model parameters updated after each such presentation. We can obtain a sequential learning algorithm by applying the technique of stochastic gradient descent(随机梯度下降), also known as sequential gradient descent, as follows. If the error function comprises a sum over data points $E = \\sum_n E_n$, then after presentation of pattern $n$, the stochastic gradient descent algorithm updates the parameter vector $\\pmb{w}$ using$$\\pmb{w}^{\\tau + 1} = \\pmb{w}^\\tau - \\eta \\nabla E_n\\tag{3.22}$$where, $\\tau$ denotes the iteration number, and $\\eta$ is a learning rate parameter. For the case of the sum-of-squares error function (3.12), this gives$$\\pmb{w}^{\\tau + 1} = \\pmb{w}^\\tau + \\eta (t_n - \\pmb{w}^{(\\tau)T} \\phi_n)\\phi_n\\tag{3.23}$$where $\\phi_n = \\phi(x_n)$. This is known as least-mean-squares or the LMS algorithm. Regularized least squares To control over-fitting, we introduced a regularization term to an error function:$$E_D(\\pmb{w}) + \\lambda E_W(\\pmb{w}) =\\frac{1}{2}\\sum_{n=1}^{N}\\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace^2 + \\frac{\\lambda}{2}\\pmb{w^T}\\pmb{w}\\tag{3.27}$$ This particular choice of regularizer is known in the machine learning literature as weight decay because in sequential learning algorithms, it encourages weight values to decay towards zero, unless supported by the data. A more general regularizer is sometimes used, for which the regularized error takes the form $$\\frac{1}{2}\\sum_{n=1}^{N}\\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{M}|w_j|^q\\tag{3.29}$$where $q = 2$ corresponds to the quadratic regularizer (3.27). Figure 3.3 shows contours of the regularization function for different values of $q$. minimizing (3.29) is equivalent to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint$$\\sum_{j=1}^{M}|w_j|^q \\leq \\eta\\tag{3.30}$$for an appropriate value of the parameter $\\eta$, where the two approaches can be related using Lagrange multipliers. Multiple outputs$$\\pmb{y(x,w)} = \\pmb{W^T} \\phi(\\pmb{x})\\tag{3.31}$$Here, $\\pmb{t}$ or $\\pmb{y(x,w)}$ is not a single target variable, but a $K$-dimensional vector. $\\pmb{W}$ is an $M \\times K$ matrix of parameters. $\\phi(\\pmb{x})$ is an $M$-dimensional column vector with elements $\\phi_j(x)$, $\\phi_0(x) = 1$ as before. Suppose we take the conditional distribution of the target vector to be an isotropic Gaussian of the form$$p(\\pmb{t}|\\pmb{x}, \\pmb{W}, \\beta) = \\mathcal{N}(\\pmb{t}|\\pmb{W^T\\phi(x)}, \\beta^{-1}I)\\tag{3.32}$$ If we have a set of observations $\\pmb{t_1}, \\cdots, \\pmb{t_N}$, we can combine these into a matrix $\\pmb{T}$ of size $N \\times K$ such that the $n^{th}$ row is given by $\\pmb{t^T_n}$. The Bias-Variance DecompositionAs we have seen in earlier chapters, the phenomenon of over-fitting is really an unfortunate property of maximum likelihood and does not arise when we marginalize over parameters in a Bayesian setting. Here we generate $100$ data sets, each containing $N = 25$ data points, independently from the sinusoidal curve $h(x) = \\sin (2\\pi x)$. The data sets are indexed by $l = 1, \\cdots, L$, where $L = 100$, and for each data set $D^{(l)}$ we fit a model with 24 Gaussian basis functions by minimizing the regularized error function (3.27) to give a prediction function $y^{(l)}(x)$ as shown in Figure 3.5. $$E_D(\\pmb{w}) + \\lambda E_W(\\pmb{w}) =\\frac{1}{2}\\sum_{n=1}^{N}\\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace^2 + \\frac{\\lambda}{2}\\pmb{w^T}\\pmb{w}\\tag{3.27}$$ The top row corresponds to a large value of the regularization coefficient $\\lambda$ that gives low variance (because the red curves in the left plot look similar) but high bias (because the two curves in the right plot are very different).Conversely on the bottom row, for which $\\lambda$ is small, there is large variance (shown by the high variability between the red curves in the left plot) but low bias (shown by the good fit between the average model fit and the original sinusoidal function). Now we consider a frequentist viewpoint of the model complexity issue, known as the bias-variance trade-off. For regression problem, we considered various loss functions each of which leads to a corresponding optimal prediction once we are given the conditional distribution(called predictive distribution in Chapter 1) $p(t|x)$. A popular choice is the squared loss function, for which the optimal prediction is given by the conditional expectation, which we denote by $h(x)$ and which is given by $$h(x) = E[t|x] = \\int t p(t|x) dt\\tag{3.36}$$ the expected squared loss can be written in the form $$E[L] = \\int \\lbrace y(x) - h(x)\\rbrace ^2 p(x) dx + \\int \\lbrace h(x) - t\\rbrace^2 p(x,t) dx dt\\tag{3.37}$$ For any given data set $D$, we can run our learning algorithm and obtain a prediction function $y(x;D)$.Consider the integrand of the first term in (3.37), which for a particular data set $D$ takes the form $$\\lbrace y(x; D) - h(x)\\rbrace^2\\tag{3.38}$$ We now take the expectation of this expression with respect to $D$ and note that the final term will vanish, giving $$E_D[\\lbrace y(x;D) - h(x)\\rbrace^2] = \\underbrace{\\lbrace E_D[y(x;D)] - h(x) \\rbrace ^2}_{bias^2} +$$ $$\\underbrace{E_D[\\lbrace y(x;D) - E_D[y(x;D)] \\rbrace^2]}_{variance}\\tag{3.40}$$ So far, we have considered a single input value $x$. If we substitute this expansion back into (3.37)(代入展开式到3.37), we obtain the following decomposition of the expected squared loss(平方期望损失) $$expected \\space loss = (bias)^2 + variance + noise\\tag{3.41}$$where, $$(bias)^2 = \\int \\lbrace E_D{y(x;D)} - h(x)\\rbrace ^2 p(x) dx\\tag{3.42}$$ $$variance = \\int E_D[\\lbrace y(x;D - E_D[y(x;D)]\\rbrace^2)] p(x) dx\\tag{3.43}$$ $$noise = \\int \\lbrace h(x) - t\\rbrace^2 p(x,t) dx dt$$ The model with the optimal predictive capability is the one that leads to the best balance between bias and variance. Back to the fitting problem, the average prediction is estimated from $$\\bar{y}(x) = \\frac{1}{L} \\sum_{l=1}^L y^{(l)} (x)\\tag{3.45}$$and the integrated squared bias and integrated variance are then given by$$(bias)^2 = \\frac{1}{N} \\sum_{n=1}^{N} \\lbrace \\bar{y}(x_n) - h(x_n)\\rbrace^2\\tag{3.46}$$ $$variance = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{L} \\sum_{l=1}^L \\lbrace y^{(l)}(x_n) - \\bar y (x_n) \\rbrace^2\\tag{3.47}$$ Although the bias-variance decomposition may provide some interesting insights into the model complexity issue from a frequentist perspective, it is of limited practical value, because the bias-variance decomposition is based on averages with respect to ensembles of data sets, whereas in practice we have only the single observed data set.If we had a large number of independent training sets of a given size, we would be better off combining them into a single large training set, which of course would reduce the level of over-fitting for a given model complexity.Given these limitations, we turn in the next section to a Bayesian treatment of linear basis function models, which not only provides powerful insights into the issues of over-fitting but which also leads to practical techniques for addressing the question model complexity. Bayesian Linear RegressionWe now turn to a Bayesian treatment of linear regression, which will avoid the over-fitting problem of maximum likelihood, and which will also lead to automatic methods of determining model complexity using the training data alone. Parameter distributionwe shall treat the noise precision parameter $β$ as a known constant, thus, equation (3.10) becomes$$p(T|X,\\pmb{w},\\beta) \\Leftrightarrow p(T|\\pmb{w})\\\\= \\prod_{n=1}^{N}\\mathcal{N}(t_n|\\pmb{w^T}\\phi(x_n), \\beta^{-1})$$this function is a exponential of a quadratic function of $\\pmb{w}$. The corresponding conjugate prior is therefore given by a Gaussian distribution of the form$$p(\\pmb{w}) = \\mathcal{N}(\\pmb{w}|\\pmb{m_0},\\pmb{S_0})\\tag{3.48}$$having mean $\\pmb{m_0}$ and covariance $\\pmb{S_0}$. Next we compute the posterior distribution, which is proportional to the product of the likelihood function and the prior.$$p(\\pmb{w}|T) \\propto p(T|\\pmb{w}) p(\\pmb{w})$$thus,$$p(\\pmb{w}|T) = \\mathcal{N}(\\pmb{w}|\\pmb{m_N},\\pmb{S_N})\\tag{3.49}$$where,$$\\pmb{m_N }= \\pmb{S_N}(\\pmb{S_0^{-1}m_0} + \\beta \\pmb{\\Phi^TT})\\tag{3.50}$$$$\\pmb{S_N^{-1}} = \\pmb{S_0^{-1}} + \\beta \\pmb{\\Phi^T\\Phi}\\tag{3.51}$$ For simplicity, we consider a zero-mean isotropic Gaussian governed by a single precision parameter $\\alpha$ so that$$p(\\pmb{w}|\\alpha) = \\mathcal{N}(\\pmb{w}|\\pmb{0}, \\alpha^{-1}\\pmb{I})\\tag{3.52}$$and the corresponding posterior distribution over $\\pmb{w}$ is then given by (3.49) with$$\\pmb{m_N} = \\beta \\pmb{S_N}\\pmb{\\Phi^T}T\\tag{3.53}$$$$\\pmb{S_N^{-1}} = \\alpha \\pmb{I} + \\beta \\pmb{\\Phi^T\\Phi}\\tag{3.54}$$ For example: We can illustrate Bayesian learning in a linear basis function model, as well as the sequential update of a posterior distribution, using a simple example involving straight-line fitting.$$y(x,\\pmb{w}) = w_0 + w_1x$$ We generate points data by function $f(x,\\pmb{a}) = a_0 + a_1 x =-0.3 + 0.5 x$, using $x_n$ from uniform distribution $\\mathcal{U}(x | -1, 1)$, then evaluating $f(x_n, \\pmb{a})$, and finally adding Gaussian noise with standard deviation of $0.2$ to obtain the target values $t_n$. Our goal is to recover the values of $a_0$ and $a_1$ from such data. Solution: Predictive distributionIn practice, we are not usually interested in the value of $\\pmb{w}$ itself but rather in making predictions of $t$ for new values of $x$. This requires that we evaluate the predictive distribution defined by $$p(t|T,\\alpha, \\beta) = \\int p(t|\\pmb{w}, \\beta) p(w|T,\\alpha, \\beta) d\\pmb{w}\\tag{3.57}$$The conditional distribution $p(t|x, \\pmb{w}, \\beta)$ of the target variable is given by (3.8), and the posterior weight distribution is given by (3.49). $$p(t|x,T,\\alpha, \\beta) = \\mathcal{N}(t|\\pmb{m_N^T\\phi(x)}, \\sigma^2_N(x))\\tag{3.58}$$ where the variance $\\sigma^2_N(x)$ of the predictive distribution is given by$$\\sigma^2_N(x) = \\frac{1}{\\beta} + \\phi(x)^T\\pmb{S_N}\\phi(x)\\tag{3.59}$$ The first term in (3.59) represents the noise on the data whereas the second term reflects the uncertainty associated with the parameters $\\pmb{w}$. Because the noise process and the distribution of $\\pmb{w}$ are independent Gaussians, their variances are additive. we fit a model comprising a linear combination of Gaussian basis functions to data sets of various sizes and then look at the corresponding posterior distributions. Here the green curves correspond to the function $\\sin(2\\pi x)$ from which the data points were generated (with the addition of Gaussian noise). Data sets of size $N = 1, N = 2, N = 4$, and $N = 25$ are shown in the four plots by the blue circles. For each plot, the red curve shows the mean of the corresponding Gaussian predictive distribution, and the red shaded region spans one standard deviation either side of the mean. Note that the predictive uncertainty depends on $x$ and is smallest in the neighbourhood of the data points. Also note that the level of uncertainty decreases as more data points are observed. The plots in Figure 3.8 only show the point-wise predictive variance as a function of $x$. In order to gain insight into the covariance between the predictions at different values of $x$, we can draw samples from the posterior distribution over $\\pmb{w}$, and then plot the corresponding functions $y(x,\\pmb{w})$, as shown in Figure 3.9. If we used localized basis functions such as Gaussians, then in regions away from the basis function centres, the contribution from the second term in the predictive variance (3.59) will go to zero, leaving only the noise contribution $\\frac{1}{\\beta}$. Thus, the model becomes very confident in its predictions when extrapolating outside the region occupied by the basis functions, which is generally an undesirable behaviour. This problem can be avoided by adopting an alternative Bayesian approach to regression known as a Gaussian process. Equivalent kernelIf we substitute (3.53) into the expression (3.3), we see that the predictive mean can be written in the form $$\\pmb{m_N} = \\beta \\pmb{S_N}\\pmb{\\Phi^T}T\\tag{3.53}$$ $$y(\\pmb{x,w}) = \\sum_{j=0}^{M-1}w_j\\phi_j(\\pmb{x}) = \\pmb{w^T}\\phi(\\pmb{x})\\tag{3.3}$$ $$\\begin{aligned}y(x, m_N) &amp;= m_N^T \\phi(x)\\\\&amp;= \\beta \\phi(x)^TS_N \\Phi^T T \\\\&amp;= \\sum_{n=1}^N \\beta \\phi(x)^T S_N \\phi(x_n) t_n\\end{aligned}\\tag{3.60}$$where $S_N$ is defined by (3.51). $$\\pmb{S_N^{-1}} = \\pmb{S_0^{-1}} + \\beta \\pmb{\\Phi^T\\Phi}\\tag{3.51}$$Thus the mean of the predictive distribution at a point $x$ is given by a linear combination of the training set target variables $t_n$, so that we can write $$y(x, m_N) = \\sum_{n=1}^N k(x,x_n) t_n\\tag{3.61}$$where the function$$k(x,x’) = \\beta \\phi(x)^T S_N \\phi(x’)\\tag{3.62}$$is known as the smoother matrix or the equivalent kernel. The equivalent kernel is illustrated for the case of Gaussian basis functions in Figure 3.10 in which the kernel functions $k(x, x’)$ have been plotted as a function of $x’$ for three different values of $x$.We see that they are localized around $x$, and so the mean of the predictive distribution at $x$, given by $y(x,m_N)$, is obtained by forming a weighted combination of the target values in which data points close to $x$ are given higher weight than points further removed from $x$. Further insight into the role of the equivalent kernel can be obtained by considering the covariance between $y(x)$ and $y(x’)$, which is given by$$\\begin{aligned}cov[y(x), y(x’)] &amp;= cov[phi(x)^T w, w^T phi(x’)] \\\\&amp;= \\phi(x)^T S_N \\phi(x’) \\\\&amp;= \\beta^{-1}k(x,x’)\\end{aligned}\\tag{3.63}$$From the form of the equivalent kernel, we see that the predictive mean at nearby points will be highly correlated, whereas for more distant pairs of points the correlation will be smaller. The formulation of linear regression in terms of a kernel function suggests an alternative approach to regression as follows. Instead of introducing a set of basis functions, which implicitly determines an equivalent kernel, we can instead define a localized kernel directly and use this to make predictions for new input vectors $x$, given the observed training set. This leads to a practical framework for regression (and classification) called Gaussian processes. Bayesian Model ComparisonAs we shall see, the over-fitting associated with maximum likelihood can be avoided by marginalizing (summing or integrating) over the model parameters instead of making point estimates of their values. Suppose we wish to compare a set of $L$ models $\\lbrace M_i\\rbrace$, where $i = 1, \\cdots, L$. Here a model refers to a probability distributionover the observed data $D$. We shall suppose that the data is generated from one of these models but we are uncertain which one. Our uncertainty is expressed through a prior probability distribution $p(M_i)$. Given a training set $D$, we then wish to evaluate the posterior distribution $$p(M_i | D) \\propto p(M_i) \\underbrace{p(D|M_i)}_{model-evidence}\\tag{3.66}$$model-evidence expresses the preference shown by the data for different models, it is also called the marginal likelihood because it can be viewed as a likelihood function over the space of models, in which the parameters have been marginalized out. The ratio of model evidences $p(D|M_i)/p(D|M_j)$ for two models is known as a Bayes factor. The predictive distribution is given by,$$p(t|x, D) = \\sum_{i=1}^L p(t|x, M_i, D) p(M_i| D)\\tag{3.67}$$ This is an example of a mixture distribution in which the overall predictive distribution is obtained by averaging the predictive distributions $p(t|x,M_i,D)$ of individual models, weighted by the posterior probabilities $p(M_i|D)$ of those models. For a model governed by a set of parameters $\\pmb{w}$, the model evidence is given, from the sum and product rules of probability, by $$\\underbrace{p(D|M_i)}_{model-evidence} = \\int p(D|\\pmb{w}, M_i) p(\\pmb{w}|M_i) d\\pmb{w}\\tag{3.68}$$ From a sampling perspective, the marginal likelihood can be viewed as the probability of generating the data set $D$ from a model whose parameters are sampled at random from the prior. It is also interesting to note that the evidence is precisely the normalizing term that appears in the denominator in Bayes’ theorem when evaluating the posterior distribution over parameters because$$p(\\pmb{w} | D, M_i) = \\frac{p(D|\\pmb{w}, M_i) p(\\pmb{w} | M_i)}{p(D | M_i)}\\tag{3.69}$$ Consider first the case of a model having a single parameter $w$. The posterior distribution over parameters is proportional to$p(D|w)p(w)$, where we omit the dependence on the model $M_i$ to keep the notation uncluttered. If we assume that the posterior distribution is sharply peaked around the most probable value $w_{MAP}$, with width $\\Delta w_{posterior}$, then we can approximate the integral by the value of the integrand at its maximum times the width of the peak. If we further assume that the prior is flat with width $\\Delta w_{prior}$ so that $p(w) = 1/\\Delta w_{prior}$, then we have $$p(D) = \\int p(D|w) p(w) dw \\simeq p(D|w_{MAP}) \\frac{\\Delta w_{posterior}}{\\Delta w_{prior}}\\tag{3.70}$$so taking logs we obtain $$\\ln p(D) \\simeq \\ln p(D|w_{MAP}) + \\ln \\frac{\\Delta w_{posterior}}{\\Delta w_{prior}}\\tag{3.71}$$ The Evidence ApproximationHere we discuss an approximation in which we set the hyperparameters to specific values determined by maximizing the marginal likelihood function obtained by first integrating over the parameters $\\pmb{w}$. This framework is known in the statistics literature as empirical Bayes. If we introduce hyperpriors over $\\alpha$ and $\\beta$, the predictive distribution is obtained by marginalizing over $\\pmb{w}$, $\\alpha$ and $\\beta$ so that $$p(t|T) = \\int \\int \\int p(t | \\pmb{w}, \\beta) p(\\pmb{w} | T, \\alpha, \\beta) p(\\alpha, \\beta | T) d\\pmb{w} d\\alpha d\\beta\\tag{3.74}$$ where $p(t | \\pmb{w}, \\beta)$ is given by$$p(t|x,\\pmb{w}, \\beta) = \\mathcal{N}(t|y(x,\\pmb{w}), \\beta^{-1})\\tag{3.8}$$$p(\\pmb{w} | T, \\alpha, \\beta)$ is given by$$p(\\pmb{w}|T) = \\mathcal{N}(\\pmb{w}|\\pmb{m_N},\\pmb{S_N})\\tag{3.49}$$ If the posterior distribution $p(\\alpha, \\beta | T)$ is sharply peaked around values $\\hat{\\alpha}$ and $\\hat{\\beta}$, then the predictive distribution is obtained simply by marginalizing over $\\pmb{w}$ in which $\\alpha$ and $\\beta$ are fixed to the values $\\hat{\\alpha}$ and $\\hat{\\beta}$, so that$$p(t|T) \\simeq p(t|T,\\hat{\\alpha}, \\hat{\\beta}) = \\int p(t | \\pmb{w}, \\hat{\\beta}) p(\\pmb{w} | T, \\hat{\\alpha}, \\hat{\\beta}) d\\pmb{w}\\tag{3.75}$$From Bayes’ theorem, the posterior distribution for $\\alpha$ and $\\beta$ is given by$$p(\\alpha, \\beta | T) \\propto p(T | \\alpha, \\beta) p(\\alpha, \\beta)\\tag{3.76}$$If the prior is relatively flat, then in the evidence framework the values of $\\hat{\\alpha}$ and $\\hat{\\beta}$ are obtained by maximizing the marginal likelihood function $p(T | \\alpha, \\beta)$. Evaluation of the evidence functionMaximizing the evidence functionEffective number of parametersLimitations of Fixed Basis Functions","link":"/MachineLearning/PatternRecognition/PatterRecognition-C3-LinearModels-for-Regression/"},{"title":"PatterRecognition-C6-Kernel-Methods","text":"Keywords: Gaussian processes, Radial Basis Function Networks, Laplace approximation, Python This is the Chapter6 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] Dual Representations(二元表示) Many linear parametric models can be re-cast into an equivalent ‘dual representation’ in which the predictions are also based on linear combinations of a kernel function evaluated at the training data points.There are numerous forms of kernel functions in common use, and we shall encounter several examples in this chapter. Many have the property of being a function only of the difference between the arguments, so that $k(\\pmb{x}, \\pmb{x’}) = k(\\pmb{x − x’})$, which are known as stationary kernels because they are invariant to translations in input space.A further specialization involves homogeneous kernels, also known as radial basis functions, which depend only on the magnitude of the distance (typically Euclidean) between the arguments so that $k(\\pmb{x}, \\pmb{x’}) = k(||\\pmb{x − x’}||)$. Here we consider a linear regression model whose parameters are determined by minimizing a regularized sum-of-squares error function given by $$J(\\pmb{w}) = \\frac{1}{2}\\sum_{n=1}^N \\lbrace \\pmb{w}^T\\phi(x_n) - t_n\\rbrace^2 + \\frac{\\lambda}{2} \\pmb{w^Tw}\\tag{6.2}$$ If we set the gradient of $J(\\pmb{w})$ with respect to $\\pmb{w}$ equal to zero, $$\\begin{aligned}\\pmb{w} &amp;= -\\frac{1}{\\lambda} \\sum_{n=1}^{N} \\lbrace \\pmb{w^T}\\phi(x_n) - t_n\\rbrace \\phi(x_n)\\\\&amp;= \\sum_{n=1}^N a_n\\phi(x_n) \\\\&amp;= \\pmb{\\Phi^Ta}\\end{aligned}\\tag{6.3}$$where $\\pmb{\\Phi}$ is the design matrix, whose $n^{th}$ row is given by $\\phi(x_n)^T$. Here the vector $\\pmb{a} = (a_1, \\cdots, a_N)^T$, and we have defined $$a_n = -\\frac{1}{\\lambda} \\lbrace \\pmb{w^T\\phi(x_n) - t_n}\\rbrace\\tag{6.4}$$ If we substitute $\\pmb{w = \\Phi^Ta}$ into $J(\\pmb{w})$, we obtain $$J(\\pmb{a}) = \\frac{1}{2}\\pmb{a^T\\Phi \\Phi^T \\Phi \\Phi^T a - a^T\\Phi \\Phi^T T + \\frac{1}{2} T^TT + \\frac{\\lambda}{2}a^T \\Phi \\Phi^Ta}\\tag{6.5}$$We now define the Gram matrix $\\pmb{K = \\Phi \\Phi^T}$, which is an $N × N$ symmetric matrix with elements$$K_{nm} = \\phi(x_n)^T \\phi(x_m) = k(x_n, x_m)\\tag{6.6}$$the kernel function is$$k(x,x’) = \\phi(x)^T \\phi(x’)\\tag{6.1}$$ In terms of the Gram matrix, the sum-of-squares error function can be written as$$J(\\pmb{a}) = \\frac{1}{2}\\pmb{a^T KK a - a^T K T + \\frac{1}{2} T^TT + \\frac{\\lambda}{2}a^T K a}\\tag{6.7}$$Setting the gradient of $J(\\pmb{a})$ with respect to a to zero, we obtain the following solution$$\\pmb{a} = (\\pmb{K} + \\lambda \\pmb{I_N})^{-1}T\\tag{6.8}$$If we substitute this back into the linear regression model, we obtain the following prediction for a new input $x$$$\\begin{aligned}y(x) &amp;= \\pmb{w^T}\\phi(x) \\\\&amp;= \\pmb{a^T}\\Phi \\phi(x) \\\\&amp;= k(x)^T (\\pmb{K} + \\lambda \\pmb{I_N})^{-1}T\\end{aligned}\\tag{6.9}$$ Thus we see that the dual formulation allows the solution to the least-squares problem to be expressed entirely in terms of the kernel function $k(x, x’)$. We can therefore work directly in terms of kernels and avoid the explicit introduction of the feature vector $\\phi(x)$, which allows us implicitly to use feature spaces of high, even infinite, dimensionality. Constructing KernelsOne approach is to choose a feature space mapping $\\phi(x)$ and then use this to find the corresponding kernel, as is illustrated in Figure 6.1. Here the kernel function is defined for a one-dimensional input space by$$k(x,x’) = \\phi(x)^T \\phi(x’) = \\sum_{i=1}^M \\phi_i(x) \\phi_i(x’)\\tag{6.10}$$where $\\phi_i(x)$ are the basis functions. For example, consider a kernel function given by$$k(\\pmb{x,z}) = (\\pmb{x^Tz})^2\\tag{6.11}$$If we take the particular case of a two-dimensional input space $\\pmb{x} = (x_1, x_2)$ we can expand out the terms and thereby identify the corresponding nonlinear feature mapping$$\\begin{aligned}k(\\pmb{x,z}) &amp;= (\\pmb{x^Tz})^2\\\\&amp;=(x_1 z_1 + x_2 z_2)^2\\\\&amp;= (x_1^2, \\sqrt{2}x_1x_2, x_2^2)(z_1^2, \\sqrt{2}z_1z_2, z_2^2)^T\\\\&amp;= \\phi(\\pmb{x})^T\\phi(\\pmb{z})\\end{aligned}\\tag{6.12}$$ Equipped with these properties, we can now embark on the construction of more complex kernels appropriate to specific applications. We saw that the simple polynomial kernel $k(\\pmb{x, x’}) = (\\pmb{x^Tx’})^2$ contains only terms of degree two.Similarly, $k(\\pmb{x, x’}) = (\\pmb{x^Tx’})^M$ contains all monomials of order $M$. For instance, if $\\pmb{x}$ and $\\pmb{x’}$ are two images, then the kernel represents a particular weighted sum of all possible products of $M$ pixels in the first image with $M$ pixels in the second image. Another commonly used kernel takes the form is often called a ‘Gaussian’ kernel,$$k(\\pmb{x, x’}) = exp(-||\\pmb{x-x’}||^2 / 2\\sigma^2)\\tag{6.23}$$ Since,$$||\\pmb{x - x’}||^2 = \\pmb{x^Tx} + (\\pmb{x’^Tx’}) - 2\\pmb{x^Tx’}\\tag{6.24}$$then,$$k(\\pmb{x, x’}) = exp(-\\pmb{x^Tx}/2\\sigma^2)exp(\\pmb{x^Tx’}/\\sigma^2)exp(-\\pmb{x’^Tx’}/2\\sigma^2)\\tag{6.25}$$define $\\kappa(\\pmb{x,x’}) = \\pmb{x^Tx’}$,then$$k(\\pmb{x, x’}) = exp\\lbrace -\\frac{1}{2\\sigma^2} (\\kappa(\\pmb{x,x}) + \\kappa(\\pmb{x’,x’}) - 2\\kappa(\\pmb{x,x’}))\\rbrace\\tag{6.26}$$ One powerful approach to the construction of kernels starts from a probabilistic generative model (Haussler, 1999), which allows us to apply generative models in a discriminative setting. Given a generative model $p(\\pmb{x})$ we can define a kernel by$$k(\\pmb{x,x’}) = p(\\pmb{x})p(\\pmb{x’})\\tag{6.28}$$This is clearly a valid kernel function because we can interpret it as an inner product &gt;&gt; in the one-dimensional feature space defined by the mapping $p(\\pmb{x})$. By (6.13), (6.17), we extend the class of kernels with positive weighting coefficients $p(i)$,$$k(\\pmb{x,x’}) = \\sum_i p(\\pmb{x}|i)p(\\pmb{x’}|i)p(i)\\tag{6.29}$$This is equivalent, up to an overall multiplicative constant, to a mixture distribution in which the components factorize, with the index $i$ playing the role of a ‘latent’ variable(潜在变量). we can also consider kernels of the form$$k(\\pmb{x,x’}) = \\int p(\\pmb{x|z})p(\\pmb{x’|z})p(\\pmb{z}) d\\pmb{z}\\tag{6.30}$$where $z$ is a continuous latent variable. Now suppose that our data consists of ordered sequences of length $L$ so that an observation is given by $X = {x_1, \\cdots , x_L}$. A popular generative model for sequences is the hidden Markov model, which expresses the distribution $p(X)$ as a marginalization over a corresponding sequence of hidden states $Z = {z_1, \\cdots , z_L}$. We can use this approach to define a kernel function measuring the similarity of two sequences $X$ and $X’$ by extending the mixture representation (6.29) to give$$k(X,X’) = \\sum_Z p(X|Z)p(X’|Z)p(Z)\\tag{6.31}$$ so that both observed sequences are generated by the same hidden sequence $Z$. This model can easily be extended to allow sequences of differing length to be compared. Radial Basis Function Networksradial basis functions have the property that each basis function depends only on the radial distance (typically Euclidean) from a centre $\\pmb{\\mu}_j$ , so that $\\phi_j(\\pmb{x}) = h(||\\pmb{x - \\mu_j}||)$. Nadaraya-Watson modelRecall the prediction of a linear regression model for a new input $\\pmb{x}$ takes the form of a linear combination of the training set target values with coefficients given by the ‘equivalent kernel’ (3.62) where the equivalent kernel satisfies the summation constraint (3.64). equivalent-kernel &gt;&gt; Gaussian ProcessesIn the Gaussian process viewpoint, we dispense with the parametric model and instead define a prior probability distribution over functions directly. Linear regression revisitedRecall what we learned from linear regression$$y(\\pmb{x}) = \\pmb{w}^T \\phi(\\pmb{x})\\tag{6.49}$$when we introduce a prior distribution over $\\pmb{w}$ given by an isotropic Gaussian of the form$$p(\\pmb{w}) = N(\\pmb{w} | \\pmb{0}, \\alpha^{-1}\\pmb{I})\\tag{6.50}$$ The probability distribution over $\\pmb{w}$ defined by (6.50) therefore induces a probability distribution over functions $y(\\pmb{x})$. $$\\pmb{y} = \\pmb{\\Phi w}\\tag{6.51}$$where $\\pmb{y}$ have elements $y_n = y(\\pmb{x_n})$ for $n = 1, \\cdots, N$, $\\pmb{\\Phi}$ is the design matrix with elements $\\Phi_{nk} = \\phi_k(\\pmb{x_n})$ $\\pmb{y}$ is itself Gaussian, the mean and covariance are$$E[\\pmb{y}] = \\pmb{\\Phi}E[\\pmb{w}] = \\pmb{0}\\tag{6.52}$$$$\\begin{aligned}cov[\\pmb{y}] &amp;= E[\\pmb{yy^T}]\\\\&amp;= \\pmb{\\Phi} E[\\pmb{ww^T}] \\pmb{\\Phi}^T \\\\&amp;= \\frac{1}{\\alpha} \\pmb{\\Phi\\Phi^T} \\\\&amp;= \\pmb{K}\\end{aligned}\\tag{6.53}$$ where $\\pmb{K}$ is the Gram matrix with elements$$K_{nm} = k(\\pmb{x_n}, \\pmb{x_m}) = \\frac{1}{\\alpha} \\phi(\\pmb{x_n})^T \\phi(\\pmb{x_m})\\tag{6.54}$$ and $k(\\pmb{x}, \\pmb{x’})$ is the kernel function. This model provides us with a particular example of a Gaussian process. In general, a Gaussian process is defined as a probability distribution over functions $y(\\pmb{x})$ such that the set of values of $y(\\pmb{x})$ evaluated at an arbitrary set of points $\\pmb{x_1}, \\cdots , \\pmb{x_N}$ jointly have a Gaussian distribution. Gaussian processes for regressionHere we shall consider noise processes that have a Gaussian distribution, so that$$p(t_n | y_n) = N(t_n | y_n, \\beta^{-1})\\tag{6.58}$$Because the noise is independent for each data point, the joint distribution of the target values $\\pmb{t} = (t_1, \\cdots, t_N)^T$ conditioned on the values $\\pmb{y} = (y_1, \\cdots, y_N)^T$ is given by an isotropic Gaussian of the form $$p(\\pmb{t} | \\pmb{y}) = N(\\pmb{t} | \\pmb{y}, \\beta^{-1}I_N)\\tag{6.59}$$ From the definition of a Gaussian process, the marginal distribution $p(\\pmb{y})$ is given by a Gaussian whose mean is zero and whose covariance is defined by a Gram matrix $\\pmb{K}$ so that$$p(\\pmb{y}) = N(\\pmb{y} | \\pmb{0}, \\pmb{K})\\tag{6.60}$$The kernel function that determines $\\pmb{K}$ is typically chosen to express the property that, for points $\\pmb{x_n}$ and $\\pmb{x_m}$ that are similar, the corresponding values $y(\\pmb{x_n})$ and $y(\\pmb{x_m})$ will be more strongly correlated than for dissimilar points. Here the notion of similarity will depend on the application.Using (2.115), we see that the marginal distribution of $\\pmb{t}$ is given by$$p(\\pmb{t}) = \\int p(\\pmb{t} | \\pmb{y}) p(\\pmb{y}) d\\pmb{y} = N(\\pmb{t} | \\pmb{0}, \\pmb{C})\\tag{6.61}$$where the covariance matrix $\\pmb{C}$ has elements$$C(\\pmb{x_n}, \\pmb{x_m}) = k(\\pmb{x_n}, \\pmb{x_m}) + \\beta^{-1}\\sigma_{nm}\\tag{6.62}$$ One widely used kernel function for Gaussian process regression is given by the exponential of a quadratic form,$$k(\\pmb{x_n}, \\pmb{x_m}) = \\theta_0 exp \\lbrace -\\frac{\\theta_1}{2} ||\\pmb{x_n} - \\pmb{x_m}||^2\\rbrace + \\theta_2 + \\theta_3 \\pmb{x_n}^T\\pmb{x_m}\\tag{6.63}$$ But remember, Our goal in regression, however, is to make predictions of the target variables for new inputs, given a set of training data. To find the conditional distribution $p(t_{N+1}|\\pmb{t})$, We then apply the results from Section 2.3.1 to obtain the required conditional distribution, as illustrated in Figure 6.7. the joint distribution over $t_1, \\cdots , t_{N+1}$ will be given by$$p(\\pmb{t_{N+1}}) = N(\\pmb{t_{N+1}} | \\pmb{0}, \\pmb{C_{N+1}})\\tag{6.64}$$…we get$$m(\\pmb{x_{N+1}}) = \\pmb{k^TC_N^{-1}t}\\tag{6.66}$$$$\\sigma^{\\pmb{x_{N+1}}} = c - \\pmb{k^TC_N^{-1}k}\\tag{6.67}$$ where vector $\\pmb{k}$ has elements $k(\\pmb{x_n}, \\pmb{x_{N+1}})$ for $n = 1, \\cdots , N$, scalar $c = k(\\pmb{x_{N+1}}, \\pmb{x_{N+1}}) + \\beta^{-1}$ These are the key results that define Gaussian process regression. An example of Gaussian process regression is shown in Figure 6.8. The only restriction on the kernel function is that the covariance matrix given by (6.62) must be positive definite. For such models, we can therefore obtain the predictive distribution either by taking a parameter space viewpoint and using the linear regression result or by taking a function space viewpoint and using the Gaussian process result. Learning the hyperparametersAutomatic relevance determinationGaussian processes for classificationLaplace approximationConnection to neural networks","link":"/MachineLearning/PatternRecognition/PatterRecognition-C6-Kernel-Methods/"},{"title":"PatterRecognition-C4-Linear-Models-For-Classification","text":"Keywords: Discriminant Functions, Logistic Regression, Bayesian Logistic Regression, Python This is the Chapter4 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] Discriminant Functions(判别函数)A discriminant is a function that takes an input vector $\\pmb{x}$ and assigns it to one of $K$ classes, denoted $\\mathcal{C}_k$. Linear discriminants, namely those for which the decision surfaces are hyperplanes. Two classesThe simplest representation of a linear discriminant function is obtained by taking a linear function of the input vector so that$$y(\\pmb{x}) = \\pmb{w^Tx} + w_0\\tag{4.4}$$where $\\pmb{w}$ is called a weight vector, and $w_0$ is a bias (not to be confused with bias in the statistical sense). The negative of the bias is sometimes called a threshold. if $\\pmb{x}$ is a point on the decision surface, then $y(\\pmb{x}) = 0$, and so the normal distance from the origin to the decision surface is given by$$\\frac{\\pmb{w^T}\\pmb{x}}{||\\pmb{w}||} = -\\frac{w_0}{||\\pmb{w}||}\\tag{4.5}$$$\\pmb{w}$ determines the orientation of the decision surface, bias parameter $w_0$ determines the location of the decision surface. You can understand from surface denotation view $z = f(x,y)$ in Calculus &gt;&gt; the decision surfaces are $D$-dimensional hyperplanes passing through the origin of the $D + 1$-dimensional expanded input space. Multiple classes From Figure 4.2, we can see the ambiguous regions, to avoid this difficulty, we consider a single $K$-class discriminant comprising $K$ linear functions of the form$$y_k(\\pmb{x}) = \\pmb{w_k^Tx} + w_{k0}\\tag{4.9}$$and then assigning a point $\\pmb{x}$ to class $\\mathcal{C}_k$ if $y_k(\\pmb{x}) &gt; y_j(\\pmb{x})$ for all $j \\neq k$. The decision boundary between class $\\mathcal{C}_k$ and class $\\mathcal{C}_j$ is therefore given by $y_k(\\pmb{x}) = y_j(\\pmb{x})$ and hence corresponds to a $(D − 1)$-dimensional hyperplane defined by $$(\\pmb{w_k - w_j})^T\\pmb{x} + (w_{k0} - w_{j0}) = 0\\tag{4.10}$$ Still imagine the surface $z = f(x,y)$, there are many surfaces in the $R^3$ space, when the two surfaces intersect with each other, means $z_k = z_j$. The decision regions of such a discriminant are always singly connected and convex. To see this, consider two points $x_A$ and $x_B$ both of which lie inside decision region $\\mathcal{R}_k$, as illustrated in Figure 4.3. Any point $\\hat{x}$ that lies on the line connecting $x_A$ and $x_B$ can be expressed in the form$$\\hat{x} = \\lambda x_A + (1-\\lambda)x_B, 0 \\leq \\lambda \\leq 1\\tag{4.11}$$From the linearity of the discriminant functions, it follows that$$y_k(\\hat{x}) = \\lambda y_k(x_A) + (1-\\lambda)y_k(x_B)\\tag{4.12}$$Because both $x_A$ and $x_B$ lie inside $\\mathcal{R}_k$, it follows that $y_k(x_A) &gt; y_j(x_A)$, and $y_k(x_B) &gt; y_j(x_B)$, for all $j \\neq k$, and hence $y_k(\\hat{x}) &gt; y_j(\\hat{x})$, and so $\\hat{x}$ also lies inside $\\mathcal{R}_k$. Thus $\\mathcal{R}_k$ is singly connected and convex. More About Convex in Algebra&gt;&gt; Least squares for classificationMore about Least Squares for Linear Regression &gt;&gt; Each class $\\mathcal{C_k}$ is described by its own linear model so that$$y_k(\\pmb{x}) = \\pmb{w^T_k x} + w_{k0}\\tag{4.13}$$where $k = 1, \\cdots , K$. We can conveniently group these together using vector notation so that$$\\pmb{y(x)} = \\pmb{\\widetilde{W}^T\\widetilde{x}}\\tag{4.14}$$ we obtain the solution for $\\widetilde{W}$ in the form$$\\widetilde{W} = (\\widetilde{X}^T\\widetilde{X})^{-1}\\widetilde{X}^TT\\tag{4.16}$$ The least-squares approach gives an exact closed-form solution for the discriminant function parameters. We have already seen that least-squares solutions lack robustness to outliers(对异常值缺乏鲁棒性), and this applies equally to the classification application, as illustrated in Figure 4.4. However, problems with least squares can be more severe than simply lack of robustness, as illustrated in Figure 4.5. The failure of least squares should not surprise us when we recall that it corresponds to maximum likelihood under the assumption of a Gaussian conditional distribution, whereas binary target vectors clearly have a distribution that is far from Gaussian. Fisher’s linear discriminantsuppose we take the $D$-dimensional input vector $\\pmb{x}$ and project it down to $1$-dimension using$$y = \\pmb{w^Tx}\\tag{4.20}$$In general, the projection onto one dimension leads to a considerable loss of information, and classes that are well separated in the original $D$-dimensional space may become strongly overlapping in $1$- dimension. consider a two-class problem in which there are $N_1$ points of class $\\mathcal{C_1}$ and $N_2$ points of class $\\mathcal{C_2}$, so that the mean vectors of the two classes are given by$$\\pmb{m_1} = \\frac{1}{N_1}\\sum_{n\\in \\mathcal{C_1}} \\pmb{x_n}\\\\\\pmb{m_2} = \\frac{1}{N_2}\\sum_{n\\in \\mathcal{C_2}} \\pmb{x_n}\\tag{4.21}$$The simplest measure of the separation of the classes, when projected onto $\\pmb{w}$, is the separation of the projected class means. This suggests that we might choose $\\pmb{w}$ so as to maximize$$m_2 - m_1 = \\pmb{w^T}(\\pmb{m_2 - m_1})\\tag{4.22}$$where,$$m_k = \\pmb{w^Tm_k}\\tag{4.23}$$is the mean of the projected data from class $\\mathcal{C_k}$. Using a Lagrange multiplier &gt;&gt; to perform the constrained maximization, the constrained condition is $\\sum_{i} w_i^2 = 1$, we then find that$$\\pmb{w} \\propto \\pmb{(m_2 − m_1)}$$ The within-class variance of the transformed data from class $\\mathcal{C_k}$ is therefore given by$$s_k^2 = \\sum_{n \\in \\mathcal{C_k}} (y_n - m_k)^2, y_n = \\pmb{w^Tx_n}\\tag{4.24}$$ total within-class variance for the whole data set is$$s_1^2 + s_2^2$$ The Fisher criterion is defined to be the ratio of the between-class variance to the within-class variance and is given by$$J(\\pmb{w}) = \\frac{(m_2 - m_1)^2}{s_1^2 + s_2^2}\\tag{4.25}$$By equation (4.20), (4.23), (4.24), we get$$J(\\pmb{w}) = \\frac{\\pmb{w^TS_Bw}}{\\pmb{w^TS_Ww}}\\tag{4.26}$$where $\\pmb{S_B}$ is the between-class covariance matrix and is given by$$\\pmb{S_B} = (\\pmb{m_2 - m_1})(\\pmb{m_2 - m_1})^T\\tag{4.27}$$$\\pmb{S_W}$ is the total within-class covariance matrix, given by$$\\pmb{S_W} = \\sum_{n \\in \\mathcal{C_1}}\\pmb{(x_n - m_1)(x_n - m_1)^T} + \\sum_{n \\in \\mathcal{C_2}}\\pmb{(x_n - m_2)(x_n - m_2)^T}\\tag{4.28}$$Differentiating (4.26) with respect to $\\pmb{w}$, we find that $J(\\pmb{w})$ is maximized when$$\\pmb{(w^TS_Bw)S_Ww} = \\pmb{(w^TS_Ww)S_Bw}\\tag{4.29}$$we get$$\\pmb{w} \\propto \\pmb{S_W^{-1}(m_2-m_1)}\\tag{4.30}$$if the within-class covariance is isotropic, so that $\\pmb{S_W}$ is proportional to the unit matrix, we find that $\\pmb{w}$ is proportional to the difference of the class means, as discussed above. Relation to least squaresFisher’s discriminant for multiple classes$$J(\\pmb{w}) = Tr \\lbrace (\\pmb{WS_WW^T})^{-1} (\\pmb{WS_BW^T})\\rbrace\\tag{4.51}$$ The perceptron algorithmThe general of perceptron algorithm is $$y(\\pmb{x}) = f(\\pmb{w^T}\\phi(\\pmb{x}))\\tag{4.52}$$where the nonlinear activation function $f(\\cdot)$ is given by a step function of the form$$f(a) =\\begin{cases} +1, a \\geq 0\\\\ -1, a &lt; 0\\end{cases}\\tag{4.53}$$it is more convenient to use target values $t = +1$ for class $\\mathcal{C_1}$ and $t = −1$ for class $\\mathcal{C_2}$, which matches the choice of activation function. We consider an alternative error function known as the perceptron criterion:$$E_P(\\pmb{w}) = - \\sum_{n \\in \\mathcal{M}} \\pmb{w^T}\\phi_n t_n\\tag{4.54}$$where $\\mathcal{M}$ denotes the set of all misclassified patterns. We now apply the stochastic gradient descent algorithm to this error function. The change in the weight vector $\\pmb{w}$ is then given by$$\\pmb{w}^{(\\tau + 1)} = \\pmb{w}^{(\\tau)} - \\eta \\nabla E_p(\\pmb{w}) = \\pmb{w}^{(\\tau)} + \\eta \\phi_n t_n\\tag{4.55}$$ Furthermore, for data sets that are not linearly separable, the perceptron learning algorithm will never converge. Probabilistic Generative ModelsHere we shall adopt a generative approach in which we model the class-conditional densities $p(\\pmb{x} | C_k)$, as well as the class priors $p(C_k)$, and then use these to compute posterior probabilities $p(C_k|\\pmb{x})$ through Bayes’ theorem. Consider first of all the case of two classes. The posterior probability for class $C_1$ can be written as $$p(C_1 | \\pmb{x}) = \\frac{p(\\pmb{x} | C_1) p(C_1)}{p(\\pmb{x} | C_1) p(C_1) + p(\\pmb{x} | C_2) p(C_2)} = \\frac{1}{1 + exp(-a)} = \\sigma(a)\\tag{4.57}$$ where$$a = \\ln \\frac{p(\\pmb{x} | C_1) p(C_1)}{p(\\pmb{x} | C_2) p(C_2)}\\tag{4.58}$$ For the case of $K &gt; 2$ classes, we have $$p(C_k | \\pmb{x}) = \\frac{p(\\pmb{x} | C_k) p(C_k)}{\\sum_j p(\\pmb{x} | C_j) p(C_j)} = \\frac{exp(a_k)}{\\sum_j exp(a_j)}\\tag{4.62}$$ The normalized exponential is also known as the softmax function, as it represents a smoothed version of the ‘max’ function because, if $a_k &gt;&gt; a_j$ for all $j \\neq k$, then $p(C_k|\\pmb{x}) \\simeq 1$, and $p(C_j|\\pmb{x}) \\simeq 0$. Continuous inputsAssume the class-conditional densities are Gaussian and then explore the resulting form for the posterior probabilities. The density for class $C_k$ is given by$$p(\\pmb{x} | C_k) = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}} exp \\lbrace -\\frac{1}{2} (\\pmb{x - \\mu_k})^T \\Sigma^{-1} \\pmb{x - \\mu_k}\\rbrace\\tag{4.64}$$From (4.57) and (4.58), we have $$p(C_1 | \\pmb{x}) = \\sigma(\\pmb{w^T x} + w_0)\\tag{4.65}$$where we have defined$$\\pmb{w} = \\Sigma^{-1} (\\pmb{\\mu_1 - \\mu_2})\\tag{4.66}$$$$w_0 = -\\frac{1}{2} \\pmb{\\mu_1}^T \\Sigma^{-1} \\pmb{\\mu_1} + \\frac{1}{2} \\pmb{\\mu_2}^T \\Sigma^{-1} \\pmb{\\mu_2} + \\ln \\frac{p(C_1)}{p(C_2)}\\tag{4.67}$$ For the general case of K classes we have, from (4.62) and (4.63), $$a_k(\\pmb{x}) = \\pmb{w_k}^T\\pmb{x} + w_{k0}\\tag{4.68}$$where we have defined$$\\pmb{w} = \\Sigma^{-1} (\\pmb{\\mu_k})\\tag{4.69}$$$$w_0 = -\\frac{1}{2} \\pmb{\\mu_k}^T \\Sigma^{-1} \\pmb{\\mu_k} + \\ln p(C_k)\\tag{4.70}$$ Maximum likelihood solution Once we have specified a parametric functional form for the class-conditional densities $p(\\pmb{x}|C_k)$, we can then determine the values of the parameters, together with the prior class probabilities $p(C_k)$, using maximum likelihood. This requires a data set comprising observations of $x$ along with their corresponding class labels.就是说现在还是最大似然函数那一套：先建模，比如用高斯去参数化model，然后点估计，最大化似然函数得到参数 $w_{ML}$ Consider first the case of two classes, each having a Gaussian class-conditional density with a shared covariance matrix, and suppose we have a data set $\\lbrace x_n, t_n \\rbrace$ where $n = 1, \\cdots , N$. Here $t_n = 1$ denotes class $C_1$ and $t_n = 0$ denotes class $C_2$. We denote the prior class probability $p(C_1) = \\pi$, so that $p(C_2) = 1−\\pi$. For a data point $x_n$ from class $C_1$, we have $t_n = 1$ and hence $$p(x_n, C_1) = p(C_1)p(x_n | C_1) = \\pi N(x_n | \\mu_1, \\Sigma)$$Thus the likelihood function is given by$$p(T|\\pi, \\mu_1, \\mu_2, \\Sigma) = \\prod_{n=1}^{N} [\\pi N(x_n | \\mu_1, \\Sigma)]^{t_n}[(1-\\pi)N(x_n | \\mu_2, \\Sigma)]^{1-t_n}\\tag{4.71}$$maximum the log of likelihood function we get,$$\\pi = \\frac{N_1}{N_1 + N_2}\\tag{4.73}$$where $N_1$ denotes the total number of data points in class $C_1$, and $N_2$ denotes the total number of data points in class $C_2$.$$\\mu_1 = \\frac{1}{N_1} \\sum_{n=1}^N t_n x_n\\tag{4.75}$$which is simply the mean of all the input vectors $x_n$ assigned to class $C_1$. Similar,$$\\mu_2 = \\frac{1}{N_2} \\sum_{n=1}^N (1 - t_n) x_n\\tag{4.76}$$Also, we have$$\\Sigma = S = \\frac{N_1}{N} S_1 + \\frac{N_2}{N} S_2\\tag{4.78}$$where$$S_1 = \\frac{1}{N_1} \\sum_{n \\in C_1} (x_n - \\mu_1) (x_n - \\mu_1)^T\\tag{4.79}$$$$S_1 = \\frac{1}{N_2} \\sum_{n \\in C_2} (x_n - \\mu_2) (x_n - \\mu_2)^T\\tag{4.80}$$ Discrete featuresExponential familyProbabilistic Discriminative ModelsFixed basis functions Logistic regression In our discussion of generative approaches before, we saw that under rather general assumptions, the posterior probability of class $C_1$ can be written as a logistic sigmoid acting on a linear function of the feature vector $\\phi$ so that$$p(C_1 | \\phi) = y(\\phi) = \\sigma(\\pmb{w^T} \\phi)\\tag{4.87}$$Here $\\sigma(·)$ is the logistic sigmoid function defined by (4.59). In the terminology of statistics, this model is known as logistic regression, although it should be emphasized that this is a model for classification rather than regression.$$\\sigma(a) = \\frac{1}{1 + exp(-a)}\\tag{4.59}$$ We now use maximum likelihood to determine the parameters of the logistic regression model. For a dataset $\\lbrace \\phi_n, t_n\\rbrace$, where $t_n \\in \\lbrace 0,1\\rbrace$, and $\\phi_n = \\phi(x_n)$, with $n = 1, \\cdots, N$. The likelyhood function is$$p(T|\\pmb{w}) = \\prod_{n=1}^{N} y_n^{t_n} \\lbrace 1 - y_n\\rbrace^{1-t_n}\\tag{4.89}$$where$$y_n = p(C_1|\\phi_n)$$As usual, crossentropy error function in the form$$\\begin{aligned}E(\\pmb{w}) &amp;= -\\ln p(T|\\pmb{w}) \\\\&amp;= -\\sum_{n=1}^{N} \\lbrace t_n \\ln y_n + (1-t_n)\\ln(1-y_n)\\rbrace\\end{aligned}\\tag{4.90}$$where$$y_n = \\sigma(a_n)$$and$$a_n = \\pmb{w}^T\\phi_n$$Taking the gradient of the error function with respect to $\\pmb{w}$, we obtain$$\\nabla E(\\pmb{w}) = \\sum_{n=1}^N (y_n - t_n)\\phi_n\\tag{4.91}$$ comparison with (3.13) shows that this takes precisely the same form as the gradient of the sum-of-squares error function for the linear regression model.$$\\nabla \\ln p(T|\\pmb{w},\\beta) = \\sum_{n=1}^N \\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace \\phi(x_n)^T\\tag{3.13}$$ Iterative reweighted least squares In the case of the linear regression models discussed in Chapter 3, the maximum likelihood solution, on the assumption of a Gaussian noise model, leads to a closed-form solution. This was a consequence of the quadratic dependence of the log likelihood function on the parameter vector $w$.For logistic regression, there is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid function.However, the departure from a quadratic form is not substantial. To be precise, the error function is concave, as we shall see shortly, and hence has a unique minimum.Furthermore, the error function can be minimized by an efficient iterative technique based on the Newton-Raphson iterative optimization scheme, which uses a local quadratic approximation to the log likelihood function.$$\\pmb{w}^{(new)} = \\pmb{w}^{(old)} - \\pmb{H}^{-1} \\nabla E(\\pmb{w})\\tag{4.92}$$ Here we compare logistic regression with linear regression. For Linear Regression $$E_D(\\pmb{w}) = \\frac{1}{2} \\sum_{n=1}^{N}\\lbrace t_n - \\pmb{w^T}\\phi(x_n)\\rbrace^2\\tag{3.12}$$ The gradient and Hessian of this error function are given by $$\\nabla E(\\pmb{w}) = \\sum_{n=1}^{N}(\\pmb{w}^T \\phi_n - t_n) \\phi_n = \\pmb{\\Phi^T\\Phi w - \\Phi^T T}\\tag{4.93}$$$$\\pmb{H} = \\nabla \\nabla E(\\pmb{w}) = \\sum_{n=1}^{N} \\phi_n \\phi_n^T = \\pmb{\\Phi^T\\Phi}\\tag{4.94}$$ The Newton-Raphson update then takes the form$$\\pmb{w}^{(new)} = \\pmb{(\\Phi^T \\Phi)^{-1} \\Phi T}$$ which we recognize as the standard least-squares solution. For Logistic Regression $$\\nabla E(\\pmb{w}) = \\sum_{n=1}^N (y_n - t_n)\\phi_n = \\pmb{\\Phi^T(y-t)}\\tag{4.96}$$$$\\pmb{H} = \\nabla \\nabla E(\\pmb{w}) = \\sum_{n=1}^N y_n(1-y_n)\\phi_n \\phi_n^T = \\pmb{\\Phi^TR\\Phi}\\tag{4.97}$$where$$R_{nn} = y_n(1-y_n)\\tag{4.98}$$The Newton-Raphson update then takes the form$$\\pmb{w}^{(new)} = \\pmb{(\\Phi^T R \\Phi)^{-1} \\Phi^T R z}\\tag{4.99}$$where$$\\pmb{z} = \\pmb{\\Phi w}^{(old)} - \\pmb{R^{-1}(y-T)}$$ Because the weighing matrix $\\pmb{R}$ is not constant but depends on the parameter vector $\\pmb{w}$, we must apply the normal equations iteratively, each time using the new weight vector $\\pmb{w}$ to compute a revised weighing matrix $\\pmb{R}$. For this reason, the algorithm is known as iterative reweighted least squares, or IRLS (Rubin, 1983). Multiclass logistic regressionalso called softmax regression. The same with two-class logistic regression. maximum likelihood function–&gt; get the error function–&gt; use Newton-Raphson iterative method –&gt; get unique minimum. Probit regressionCanonical link functionsThe Laplace ApproximationRecall Bayesian Linear Regression in Chapter3, the posterior distribution is still a Gaussian distribution, but it’s not the same with Bayesian Logistic Regression. In particular, we cannot integrate exactly over the parameter vector $\\pmb{w}$ since the posterior distribution is no longer Gaussian. It is therefore necessary to introduce some form of approximation. Consider first the case of a single continuous variable $z$, and suppose the distribution $p(z)$ is defined by$$p(z) = \\frac{1}{Z}f(z)\\tag{4.125}$$where $Z = \\int f(z) dz$ is the normalization coefficient. In the Laplace method the goal is to find a Gaussian approximation $q(z)$ which is centred on a mode of the distribution $p(z)$. The first step is to find a mode of $p(z)$, in other words a point $z_0$ such that $p’(z_0) = 0$, or equivalently $$\\frac{d f(z)}{dz} |_{z = z_0} = 0\\tag{4.126}$$ Now we know $z_0$ is a local maximum of the distribution, so if we make a Taylor expansion of $\\ln f(z)$ centered on the mode $z_0$, we get $$\\ln f(z) \\simeq \\ln f(z_0) - \\frac{1}{2}A(z - z_0)^2$$for M-dimensional space $\\pmb{z}$, we have$$\\ln f(\\pmb{z}) \\simeq \\ln f(\\pmb{z_0}) - \\frac{1}{2} \\pmb{(z - z_0)}^T \\pmb{A} (\\pmb{z - z_0})\\tag{4.131}$$ where $$A = -\\frac{d^2}{dz^2} \\ln f(z) | _{z = z_0}$$the $M \\times M$ Hessian Matrix $\\pmb{A}$ is$$\\pmb{A} = - \\nabla \\nabla \\ln f(\\pmb{z}) | _{\\pmb{z = z_0}}\\tag{4.132}$$ the first-order term does not appear because the first-order at $z_0$ is zero. Taking the exponential we obtain$$f(z) \\simeq f(z_0) exp \\lbrace -\\frac{A}{2}(z-z_0)^2 \\rbrace$$for vector space we get$$f(\\pmb{z}) \\simeq f(\\pmb{z_0}) exp \\lbrace -\\frac{1}{2} \\pmb{(z - z_0)}^T \\pmb{A} (\\pmb{z - z_0})\\rbrace\\tag{4.133}$$ We can then obtain a normalized distribution $q(z)$ by making use of the standard result for the normalization of a Gaussian, so that $$q(z) = (\\frac{A}{2\\pi})^{1/2} exp \\lbrace -\\frac{A}{2}(z-z_0)^2\\rbrace$$ using the standard result (2.43) for a normalized multivariate Gaussian, giving$$ \\mathcal{N}(\\pmb{x} | \\pmb{\\mu}, \\pmb{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} exp \\lbrace -\\frac{1}{2}(\\pmb{x - \\mu})^T \\pmb{\\Sigma}^{-1}(\\pmb{x-\\mu}) \\rbrace\\tag{2.43}$$ $$q(\\pmb{z}) = \\frac{|\\pmb{A}^{1/2}|}{(2\\pi)^{M/2}} exp \\lbrace -\\frac{1}{2} (\\pmb{z - z_0})^T \\pmb{A} (\\pmb{z - z_0})\\rbrace = N(\\pmb{z} | \\pmb{z_0, A^{-1}})\\tag{4.134}$$ This Gaussian distribution will be well defined provided its precision matrix, given by $\\pmb{A}$, is positive definite, which impliesthat the stationary point $\\pmb{z_0}$ must be a local maximum, not a minimum or a saddle point. (Recall Extrema in Calculus &gt;&gt;) Model comparison and BICAs well as approximating the distribution $p(\\pmb{z})$ we can also obtain an approximation to the normalization constant $Z$. Using the approximation (4.133) we have $$\\begin{aligned}Z &amp;= \\int f(\\pmb{z}) d\\pmb{z} \\\\&amp; \\simeq f(\\pmb{z_0}) \\int exp \\lbrace -\\frac{1}{2} \\pmb{(z - z_0)}^T \\pmb{A} (\\pmb{z - z_0})\\rbrace d\\pmb{z} \\\\&amp;= f(\\pmb{z_0}) \\frac{(2\\pi)^{M/2}}{|\\pmb{A}|^{1/2}}\\end{aligned}\\tag{4.135}$$ Bayesian Logistic RegressionLaplace approximationBecause we seek a Gaussian representation for the posterior distribution, it is natural to begin with a Gaussian prior, which we write in the general form$$p(\\pmb{w}) = N(\\pmb{w} | \\pmb{m_0, S_0})\\tag{4.140}$$ The posterior distribution over $\\pmb{w}$ is given by $$p(\\pmb{w | t}) \\propto p(\\pmb{w}) p(\\pmb{t | w})\\tag{4.141}$$where $\\pmb{t} = (t_1, \\cdots , \\pmb{t_N})^T$. (4.140) and (4.89), take the log of both sides, we get$$\\ln p(\\pmb{w|t}) = -\\frac{1}{2}(\\pmb{w - m_0})^T \\pmb{S_0}^{-1} (\\pmb{w - m_0}) + \\sum_{n=1}^{N} \\lbrace t_n \\ln y_n + (1 - t_n) \\ln (1-y_n)\\rbrace + const\\tag{4.142}$$ where $y_n = \\sigma(\\pmb{w}^T \\phi_n)$ To obtain a Gaussian approximation to the posterior distribution, we first maximize the posterior distribution to give the MAP (maximum posterior) solution $\\pmb{w_{MAP}}$, which defines the mean of the Gaussian.The covariance is then given by the inverse of the matrix of second derivatives of the negative log likelihood, which takes the form$$\\pmb{S_N} = - \\nabla \\nabla \\ln p(\\pmb{w|t}) = \\pmb{S_0}^{-1} + \\sum_{n=1}^{N} y_n(1-y_n)\\phi_n \\phi_n^T\\tag{4.143}$$The Gaussian approximation to the posterior distribution therefore takes the form$$q(\\pmb{w}) = N(\\pmb{w} | \\pmb{w_{MAP}, S_N})\\tag{4.144}$$ Predictive distributionThe predictive distribution for class $C_1$, is$$p(C_1 | \\phi, \\pmb{t}) = \\int p(C_1 | \\phi, \\pmb{w}) p(\\pmb{w|t}) d\\pmb{w} \\simeq \\int \\sigma(\\pmb{w}^T \\phi) q(\\pmb{w}) d\\pmb{w}\\tag{4.145}$$… we get$$p(C_1 | \\phi, \\pmb{t}) = \\sigma(\\kappa(\\sigma_a^2)\\mu_a)\\tag{4.155}$$where$$\\kappa (\\sigma^2) = (1 + \\pi \\sigma^2 / 8)^{-1/2}\\tag{4.154}$$$$\\mu_a = \\pmb{w}_{MAP}^T \\phi\\tag{4.149}$$$$\\sigma_a^2 = \\phi^T \\pmb{S_N} \\phi\\tag{4.150}$$","link":"/MachineLearning/PatternRecognition/PatterRecognition-C4-Linear-Models-For-Classification/"},{"title":"PatterRecognition-C1-Introduction","text":"Keywords: Bayes’Theorem, Polynomial Curve Fitting, Posterior Probability, Python This is the Chapter1 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] Cases such as the digit recognition example, in which the aim is to assign each input vector to one of a finite number of discrete categories, are called classification problems. If the desired output consists of one or more continuous variables, then the task is called regression. Example: Polynomial Curve Fitting(error minimization view)More about Polynomial Curve Fitting in Numerical Analysis &gt;&gt; $$y(x,\\pmb{w}) = w_0 + w_1 x + \\cdots + w_Mx^M = \\sum_{j=0}^{M}w_jx^j\\tag{1.1}$$Note that, although the polynomial function $y(x,\\pmb{w})$ is a nonlinear function of $x$, it is a linear function of the coefficients $\\pmb{w}$. Functions, such as the polynomial, which are linear in the unknown parameters have important properties and are called linear models and will be discussed extensively in Chapters 3 and 4. $$E(\\pmb{w}) = \\frac{1}{2}\\sum_{n=1}^{N}\\lbrace y(x_n, \\pmb{w}) - t_n\\rbrace ^ 2\\tag{1.2}$$ Because the error function is a quadratic function of the coefficients $\\pmb{w}$, its derivatives with respect to the coefficients will be linear in the elements of $\\pmb{w}$, and so the minimization of the error function has a unique solution, denoted by $\\pmb{w}^{\\ast}$, which can be found in closed form. $$E_{RMS} = \\sqrt{2E(\\pmb{w^{\\ast}})/N}\\tag{1.3}$$ the fitted curve oscillates wildly and gives a very poor representation of the function sin(2πx). This latter behaviour is known as over-fitting. We shall see that the least squares approach to finding the model parameters represents a specific case of maximum likelihood (discussed in Section 1.2.5), and that the over-fitting problem can be understood as Section 3.4 a general property of maximum likelihood. By adopting a Bayesian approach, the over-fitting problem can be avoided. One technique that is often used to control the over-fitting phenomenon in such cases is that of regularization, which involves adding a penalty term to the error function (1.2) in order to discourage the coefficients from reaching large values.$$\\tilde{E}{(\\pmb{w})} = \\frac{1}{2}\\sum_{n=1}^{N}\\lbrace y(x_n, \\pmb{w}) - t_n\\rbrace ^ 2 + \\frac{\\lambda}{2}||\\pmb{w}||^2\\tag{1.4}$$ by taking the available data and partitioning it into a training set, used to determine the coefficients $\\pmb{w}$, and a separate validation set, also called a hold-out set, used to optimize the model complexity (either $M$ or $\\lambda$). Probability Theorythe identity of the box that will be chosen is a random variable, which we shall denote by $B$. This random variable can take one of two possible values, namely $r$ (corresponding to the red box) or $b$ (corresponding to the blue box). the identity of the fruit is also a random variable and will be denoted by $F$. It can take either of the values $a$ (for apple) or $o$ (for orange). See Figure 1.10, two random variables $X$ and $Y$ (which could for instance be the Box and Fruit variables considered above).A total of N trials in which we sample both of the variables $X$ and $Y$. the joint probability of $X = x_i$ and $Y = y_j$ is$$p(X=x_i, Y = y_j) = \\frac{n_{ij}}{N}\\tag{1.5}$$Here we are implicitly considering the $limit N \\rightarrow \\infty$. the marginal probability is,$$\\begin{aligned} p(X = x_i) &amp;= \\frac{c_i}{N}\\\\ &amp;= \\sum_{j=1}^Lp(X=x_i,Y=y_j)\\end{aligned}\\tag{1.7}$$ the conditional probability is,$$p(Y=y_j | X= x_i) = \\frac{n_{ij}}{c_i}\\tag{1.8}$$ thus, $$\\begin{aligned}p(X = x_i, Y = y_j) &amp;= \\frac{n_{ij}}{N}\\\\ &amp;= \\frac{n_{ij}}{c_i} \\cdot \\frac{c_i}{N} \\\\ &amp;= p(Y=y_j | X= x_i) p(X = x_i)\\end{aligned}\\tag{1.9}$$ From the product rule, together with the symmetry property $p(X, Y) = p(Y,X)$, we immediately obtain the following relationship between conditional probabilities, which is called Bayes’theorem$$p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}\\tag{1.12}$$ Using the sum rule, the denominator in Bayes’ theorem can be expressed in terms of the quantities appearing in the numerator$$p(X) = \\sum_{Y}p(X|Y)p(Y)\\tag{1.13}$$ Now back to box and fruit, $$p(B=r) = \\frac{4}{10}, p(B=b) = \\frac{6}{10}$$$$p(F = a | B = r) = \\frac{1}{4}\\\\p(F = o | B = r) = \\frac{3}{4}\\\\p(F = a | B = b) = \\frac{3}{4}\\\\p(F = o | B = b) = \\frac{1}{4}$$ note that these probabilities are normalized so that$$p(F=a|B=r) + p(F=o|B=r) = 1\\\\p(F=a|B=b) + p(F=o|B=b) = 1$$ We can now use the sum and product rules of probability to evaluate the overall probability of choosing an apple$$\\begin{aligned} p(F = a) &amp;= p(F=a|B=r)p(B=r) + p(F=a|B=b)p(B=b)\\\\ &amp;= \\frac{1}{4} \\times \\frac{4}{10} + \\frac{3}{4} \\times \\frac{6}{10}\\\\ &amp;=\\frac{11}{20}\\\\p(F=o) &amp;=1-\\frac{11}{20} = \\frac{9}{20}\\end{aligned}\\tag{1.22}$$ We can solve the problem of reversing the conditional probability by using Bayes’ theorem to give $$\\begin{aligned}p(B=r|F=o) &amp;= \\frac{p(F=o|B=r)p(B=r)}{p(F=o)}\\\\&amp;= \\frac{3}{4}\\times \\frac{4}{10} \\times \\frac{20}{9}\\\\&amp;=\\frac{2}{3}\\end{aligned}\\tag{1.23}$$ If we had been asked which box had been chosen before being told the identity of the selected item of fruit, then the most complete information we have available is provided by the probability $p(B)$. We call this the prior probability because it is the probability available before we observe the identity of the fruit. Once we are told that the fruit is an orange, we can then use Bayes’ theorem to compute the probability $p(B|F)$, which we shall call the posterior probability because it is the probability obtained after we have observed $F$. Bayes’ theorem was used to convert a prior probability into a posterior probability by incorporating the evidence provided by the observed data. Probability densitiesIf the probability of a real-valued variable $x$ falling in the interval $(x, x + \\sigma x)$ is given by $p(x)\\sigma x$ for $\\sigma x \\rightarrow 0$, then $p(x)$ is called the probability density over $x$.$$p(x\\in(a,b)) = \\int_a^b p(x) dx\\tag{1.24}$$$$p(x) \\geq 0 \\tag{1.25}$$$$\\int_{-\\infty}^{\\infty} p(x)dx = 1 \\tag{1.26}$$$$P(z) = \\int_{-\\infty}^{z} p(x) dx\\tag{1.28}$$ Expectations and covariancesThe average value of some function $f(x)$ under a probability distribution $p(x)$ is called the expectation of $f(x)$ and will be denoted by $E[f]$. $$E[f] = \\sum_{x} p(x) f(x)\\tag{1.33}$$ $$E[f] = \\int p(x)f(x) dx\\tag{1.34}$$ The variance of $f(x)$ is$$\\begin{aligned} var[f] &amp;= E[(f(x) - E[f(x)])^2]\\\\ &amp;= E[f(x)^2] - E[f(x)]^2\\end{aligned}\\tag{1.38}$$and provides a measure of how much variability there is in $f(x)$ around its mean value $E[f(x)]$. conditional expectation for two-variable function $f(x,y)$: $$E_x[f|y] = \\sum_x p(x|y)f(x)\\tag{1.37}$$ For two random variables $x$ and $y$, the covariance is defined by $$\\begin{aligned} cov[x,y]&amp;= E_{x,y}[\\lbrace x - E(x)\\rbrace \\lbrace y - E(y)\\rbrace] \\\\ &amp;= E_{x,y}[xy] - E[x]E[y]\\end{aligned}\\tag{1.41}$$expresses the extent to which $x$ and $y$ vary together. If $x$ and $y$ are independent, then their covariance vanishes. In the case of two vectors of random variables $\\pmb{x}$ and $\\pmb{y}$, the covariance is a matrix(默认是列向量) $$\\begin{aligned} cov[\\pmb{x},\\pmb{y}]&amp;= E_{\\pmb{x,y}}[\\lbrace \\pmb{x} - E(\\pmb{x})\\rbrace \\lbrace \\pmb{y^T} - E(\\pmb{y^T})\\rbrace] \\\\ &amp;= E_{\\pmb{x,y}}[\\pmb{xy^T}] - E[\\pmb{x}]E[\\pmb{y^T}]\\end{aligned}\\tag{1.42}$$ Bayesian probabilitiesTwo ways to see probability: frequentist interpretation. view probabilities in terms of the frequencies of random, repeatable events. Bayesian view. probabilities provide a quantification of uncertainty.(不确定性的量化) we can adopt a similar approach (fruit in boxes) when making inferences about quantities such as the parameters $\\pmb{w}$ in the polynomial curve fitting example. We capture our assumptions about $\\pmb{w}$, before observing the data, in the form of a prior probability distribution $p(\\pmb{w})$. The effect of the observed data $D = {t_1, \\cdots, t_N}$ is expressed through the conditional probability $p(D|\\pmb{w})$ Bayes’ theorem, which takes the form$$p(\\pmb{w}|D) = \\frac{p(D|\\pmb{w})p(\\pmb{w})}{p(D)}\\tag{1.43}$$ then allows us to evaluate the uncertainty in $\\pmb{w}$ after we have observed $D$ in the form of the posterior probability $p(\\pmb{w}|D)$. The quantity $p(D|\\pmb{w})$ on the right-hand side of Bayes’ theorem is evaluated for the observed data set $D$ and can be viewed as a function of the parameter vector $\\pmb{w}$, in which case it is called the likelihood function. It expresses how probable the observed data set is for different settings of the parameter vector $\\pmb{w}$. $$posterior \\propto likelihood \\times prior\\tag{1.44}$$ Indeed, integrating both sides of (1.43) with respect to $\\pmb{w}$, we can express the denominator in Bayes’ theorem in terms of the prior distribution and the likelihood function$$p(D) = \\int p(D|\\pmb{w})p(\\pmb{w}) d\\pmb{w}\\tag{1.45}$$where, $\\int p(\\pmb{w}|D) d \\pmb{w} = 1$. The Gaussian distributionFor the case of a single real-valued variable $x$, the Gaussian distribution is defined by$$ \\mathcal{N}(x | \\mu, \\sigma^2) = \\frac{1}{(2\\pi \\sigma^2)^{\\frac{1}{2}}} exp\\lbrace -\\frac{1}{2\\sigma^2}(x-\\mu)^2\\rbrace\\tag{1.46}$$which is governed by two parameters: $\\mu$, called the mean, and $\\sigma^2$, called the variance, the reciprocal of the variance, written as $\\beta = \\frac{1}{\\sigma^2}$, is called the precision. $$\\int_{-\\infty}^{\\infty} \\mathcal{N}(x|\\mu, \\sigma^2)aligned dx = 1\\tag{1.48}$$ We are also interested in the Gaussian distribution defined over a $D$-dimensional vector $\\pmb{x}$ of continuous variables, which is given by$$ \\mathcal{N}(\\pmb{x} | \\pmb{\\mu}, \\pmb{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} exp \\lbrace -\\frac{1}{2}(\\pmb{x - \\mu})^T \\pmb{\\Sigma}^{-1}(\\pmb{x-\\mu}) \\rbrace\\tag{1.52}$$where $\\pmb{\\Sigma}$ is a $D \\times D$ matrix, called : covariance. Now suppose that we have a data set of observations $X = (x_1, \\cdots , x_N)^T$, representing $N$ observations of the scalar variable $x$. We shall suppose that the observations are drawn independently from a Gaussian distribution whose mean $\\mu$ and variance $\\sigma^2$ are unknown, and we would like to determine these parameters from the data set. Because our data set $X$ is i.i.d., we can therefore write the probability of the data set, given mean $\\mu$ and variance $\\sigma^2$, in the form$$p(X|\\mu, \\sigma^2) = \\prod_{n=1}^{N} \\mathcal{N}(x_n|\\mu, \\sigma^2)\\tag{1.53}$$ When viewed as a function of mean $\\mu$ and variance $\\sigma^2$, this is the likelihood function for the Gaussian and is interpreted diagrammatically in Figure 1.14. We shall determine values for the unknown parameters $\\mu$ and $\\sigma^2$ in the Gaussian by maximizing the likelihood function (1.53). it is convenient to maximize the logarithm of the likelihood function.$$\\ln p(X|\\mu, \\sigma^2) = -\\frac{1}{2\\sigma^2} \\sum_{n=1}^{N} (x_n - \\mu)^2 - \\frac{N}{2} \\ln \\sigma^2 - \\frac{N}{2}\\ln(2\\pi)\\tag{1.54}$$ Maximizing (1.54) with respect to $\\mu$, we get sample mean.$$\\mu_{ML} = \\frac{1}{N} \\sum_{n=1}^N x_n\\tag{1.55}$$ Maximizing (1.54) with respect to $\\sigma^2$, we get sample variance.$$\\sigma^2_{ML} = \\frac{1}{N}\\sum_{n=1}^N (x_n - \\mu_{ML})^2\\tag{1.56}$$ In particular, we shall show that the maximum likelihood approach systematically underestimates the variance of the distribution. The following estimate for the variance parameter is unbiased(无偏估计)$$\\widetilde{\\sigma^2} = \\frac{N}{N-1}\\sigma_{ML}^2 = \\frac{N}{N-1}\\sum_{n=1}^N (x_n - \\mu_{ML})^2\\tag{1.59}$$ Why underestimates? &gt;&gt; Curve fitting re-visited(probabilistic perspective) The goal in the curve fitting problem is to be able to make predictions for the target variable $t$ given some new value of the input variable $x$ on the basis of a set of training data comprising $N$ input values $X = (x_1, \\cdots , x_N)^T$ and their corresponding target values $T = (t_1, \\cdots , t_N)^T$. We can express our uncertainty over the value of the target variable using a probability distribution. For this purpose, we shall assume that, given the value of $x$, the corresponding value of $t$ has a Gaussian distribution with a mean equal to the value $y(x,\\pmb{w})$ of the polynomial curve given by (1.1). Thus we have(See Figure 1.16.)$$p(t|x,\\pmb{w}, \\beta) = \\mathcal{N}(t|y(x,\\pmb{w}), \\beta^{-1}) \\tag{1.60}$$ We now use the training data $\\lbrace X,T \\rbrace$ to determine the values of the unknown parameters $\\pmb{w}$ and $\\pmb{β}$ by maximum likelihood. If the data are assumed to be drawn independently from the distribution (1.60), then the likelihood function is given by$$p(T|X, \\pmb{w}, \\beta) = \\prod_{n=1}^N \\mathcal{N}(t_n | y(x_n, \\pmb{w}), \\beta^{-1})\\tag{1.61}$$ log function is convienient.$$\\ln p(T|X, \\pmb{w}, \\beta) = -\\frac{\\beta}{2}\\sum_{n=1}^N\\lbrace y(x_n, \\pmb{w}) - t_n \\rbrace^2 + \\frac{N}{2} \\ln \\beta - \\frac{N}{2} \\ln(2\\pi)\\tag{1.62}$$ maximum the (1.62) with respect to $\\mu$, We therefore see that maximizing likelihood is equivalent, so far as determining $\\pmb{w}$ is concerned, to minimizing the sum-of-squares error function defined by (1.2). maximum the (1.62) with respect to $\\beta$,$$\\frac{1}{\\beta_{ML}} = \\frac{1}{N}\\sum_{n=1}^{N} \\lbrace y(x_n, \\pmb{w_{ML}}) - t_n\\rbrace^2\\tag{1.63}$$ Having determined the parameters $\\pmb{w}$ and $\\pmb{β}$, we can now make predictions for new values of $x$. Because we now have a probabilistic model, these are expressed in terms of the predictive distribution that gives the probability distribution over $t$, rather than simply a point estimate, and is obtained by substituting the maximum likelihood parameters into (1.60) to give$$p(t|x, \\pmb{w_{ML}}, \\beta_{ML}) = \\mathcal{N}(t|y(x, \\pmb{w_{ML}}), \\beta_{ML}^{-1})\\tag{1.64}$$ Let us introduce a prior distribution over the polynomial coefficients $\\pmb{w}$. Let us consider a Gaussian distribution of the form$$p(\\pmb{w}|\\alpha) = \\mathcal{N}(\\pmb{w}| \\pmb{0}, \\alpha^{-1} \\pmb{I}) = (\\frac{\\alpha}{2\\pi})^{(M+1)/2} exp \\lbrace -\\frac{\\alpha}{2} \\pmb{w}^T\\pmb{w}\\rbrace\\tag{1.65}$$where $\\alpha$ is the precision of the distribution, and $M+1$ is the total number of elements in the vector $\\pmb{w}$ for an $M$th order polynomial. Variables such as $\\alpha$, which control the distribution of model parameters, are called hyperparameters. Using Bayes’ theorem, the posterior distribution for $\\pmb{w}$ is proportional to the product of the prior distribution and the likelihood function$$p(\\pmb{w}|X,T,\\alpha, \\beta) \\propto p(T|X,\\pmb{w}, \\beta) p(\\pmb{w}|\\alpha)\\tag{1.66}$$ We can now determine $\\pmb{w}$ by finding the most probable value of $\\pmb{w}$ given the data, in other words by maximizing the posterior distribution. This technique is called maximum posterior, or simply MAP. By(1.62), (1.65) and (1.66), we find that the maximum of the posterior is given by the minimum of$$\\frac{\\beta}{2} \\sum_{n=1}^{N}\\lbrace y(x_n, \\pmb{w}) - t_n\\rbrace^2 + \\frac{\\alpha}{2}\\pmb{w^T}\\pmb{w}\\tag{1.67}$$ Thus we see that maximizing the posterior distribution is equivalent to minimizing the regularized sum-of-squares error function encountered earlier in the form (1.4), with a regularization parameter given by $\\lambda = \\frac{\\alpha}{\\beta}$. Bayesian curve fitting(using integration)In a fully Bayesian approach, we should consistently apply the sum and product rules of probability, which requires, as we shall see shortly, that we integrate over all values of $\\pmb{w}$. In the curve fitting problem, we are given the training data $X$ and $T$, along with a new test point $x$, and our goal is to predict the value of $t$. We therefore wish to evaluate the predictive distribution $p(t|x, X, T)$. $$p(t|x,X,T) = \\int \\underbrace{p(t|x,\\pmb{w})}_{(1.60)}\\underbrace{p(\\pmb{w}|X,T)}_{post-distribution(1.66)}d\\pmb{w}\\tag{1.68}$$ The integration in (1.68) can also be performed analytically with the result that the predictive distribution is given by a Gaussian of the form$$p(t|x,X,T) = \\mathcal{N}(t|m(x),s^2(x))\\tag{1.69}$$where the mean and variance are given by$$m(x) = \\beta \\phi(x)^T S\\sum_{n=1}^{N} \\phi(x_n)t_n\\tag{1.70}$$$$s^2(x) = \\beta^{-1} + \\phi(x)^TS\\phi(x)\\tag{1.71}$$where, the matrix $S$ is given by$$S^{-1} = \\alpha \\pmb{I} + \\beta \\sum_{n=1}^{N} \\phi(x_n)\\phi(x)^T\\tag{1.72}$$where $\\pmb{I}$ is the unit matrix, and we have defined the vector $\\phi(x)$ with elements $\\phi_i(x) = x^i, for, i = 0, \\cdots , M$. Model Selection The Curse of Dimensionality Decision TheoryMinimizing the misclassification rateMinimizing the expected lossThe reject optionInference and decisionLoss functions for regressionInformation TheoryRelative entropy and mutual information","link":"/MachineLearning/PatternRecognition/PatterRecognition-C1-Introduction/"},{"title":"PatterRecognition-C2-Probability-Distributions","text":"Keywords: Gaussian Distribution, The Exponential Family, Python This is the Chapter2 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] One role for the distributions discussed in this chapter is to model the probability distribution $p(\\pmb{x})$ of a random variable $\\pmb{x}$, given a finite set $\\lbrace \\pmb{x_1}, \\cdots , \\pmb{x_N} \\rbrace$ of observations. This problem is known as density estimation. Binary VariablesWe begin by considering a single binary random variable $x \\in \\lbrace 0,1 \\rbrace$. For example, $x$ might describe the outcome of flipping a coin, with $x = 1$ representing ‘heads’, and x = 0 representing ‘tails’. The probability of $x = 1$ will be denoted by the parameter $\\mu$ so that$$p(x=1|\\mu) = \\mu\\tag{2.1}$$where $0 \\leq \\mu \\leq 1$, from which it follows that $p(x = 0|\\mu) = 1 − \\mu$. The probability distribution over $x$ can therefore be written in the form$$Bern(x | \\mu) = \\mu^x (1-\\mu)^{1-x}\\tag{2.2}$$which is known as the Bernoulli distribution 伯努利分布. It is easily verified that this distribution is normalized and that it has mean and variance given by$$E[x] = \\mu\\tag{2.3}$$$$var[x] = \\mu(1-\\mu)\\tag{2.4}$$ Suppose we have a data set $D = \\lbrace x_1, \\cdots , x_N \\rbrace$ of observed values of $x$, iid, so the likelyhood function is$$p(D|\\mu) = \\prod_{n=1}^{N} p(x_n|\\mu) = \\prod_{n=1}^{N} \\mu^{x_n} (1 - \\mu)^{1-x_n}\\tag{2.5}$$ $$\\ln p(D|\\mu) = \\sum_{n=1}^{N} \\ln p(x_n|\\mu) = \\sum_{n=1}^{N} \\lbrace x_n\\ln \\mu + (1-x_n)\\ln(1 - \\mu) \\rbrace\\tag{2.6}$$ If we set the derivative of $\\ln p(D|\\mu)$ with respect to $\\mu$ equal to zero, we obtain the maximum likelihood estimator$$\\mu_{ML} = \\frac{1}{N}\\sum_{n=1}^{N} x_n\\tag{2.7}$$which is also known as the sample mean.If we denote the number of observations of $x = 1$ (heads) within this data set by $m$, then we can write (2.7) in the form$$\\mu_{ML} = \\frac{m}{N}\\tag{2.8}$$ We can also work out the distribution of the number $m$ of observations of $x = 1$, given that the data set has size $N$. This is called the binomial distribution 二项分布,$$Bin(m|N,\\mu) = \\begin{pmatrix} N\\\\m\\end{pmatrix} \\mu^m (1-\\mu)^{N-m}\\tag{2.9}$$$$E[m] = \\sum_{m=1}^N m Bin(m|N,\\mu) = N\\mu\\tag{2.11}$$$$var[m] = N\\mu(1-\\mu)\\tag{2.12}$$ The beta distributionwe note that the likelihood function takes the form of the product of factors of the form $\\mu^{x} (1 - \\mu)^{1-x}$. If we choose a prior to be proportional to powers of $\\mu$ and $(1 − \\mu)$, then the posterior distribution, which is proportional to the product of the prior and the likelihood function, will have the same functional form as the prior. This property is called conjugacy 共轭. We therefore choose a prior, called the beta distribution, given by$$Beta(\\mu | a, b) = \\frac{\\Gamma (a+b)}{\\Gamma(a) \\Gamma(b)} \\mu^{a-1}(1-\\mu)^{b-1}\\tag{2.13}$$where $\\Gamma(x)$ is the gamma function defined as$$\\Gamma(t) = \\int_0^{+\\infty} x^{t-1}e^{-x}dx, t&gt;0$$ $$\\int_0^1 Beta(\\mu | a, b) d\\mu = 1\\tag{2.14}$$The mean and variance of the beta distribution are given by$$E[\\mu] = \\frac{a}{a+b}\\tag{2.15}$$$$var[\\mu] = \\frac{ab}{(a+b)^2 (a+b+1)}\\tag{2.16}$$By Bayes’s Therom and (2.13) and (2.9), this posterior distribution $$p(\\mu|m,l,a,b) \\propto \\mu^{m+a-1}(1-\\mu)^{l+b-1}, l = N-m\\tag{2.17}$$it is simply another beta distribution, and its normalization coefficient can therefore be obtained by comparison with (2.13) to give $$p(\\mu|m,l,a,b) = \\frac{\\Gamma (m+a+l+b)}{\\Gamma(m+a) \\Gamma(l+b)} \\mu^{m+a-1}(1-\\mu)^{l+b-1}, l = N-m\\tag{2.18}$$ we must evaluate the predictive distribution of $x$, given the observed data set $D$.$$\\begin{aligned}p(x=1|D) &amp;= \\int_0^1 p(x=1|\\mu)p(\\mu|D) d\\mu \\\\ &amp;= \\int_0^1 \\mu p(\\mu|D)d\\mu \\\\ &amp;= E[\\mu|D]\\\\\\underset{Together with (2.18),(2.15)}{\\longrightarrow}&amp;= \\frac{m+a}{m+a+l+b}\\end{aligned}\\tag{2.19}$$ 贝叶斯视角下的后验概率的均值，随着观察数据的增多，越来越趋向于最大似然数得到的结果。 Multinomial VariablesIf we have a variable that can take $K = 6$ states and a particular observation of the variable happens to correspond to the state where $x_3 = 1$, then $\\pmb{x}$ will be represented by $$\\pmb{x} = (0,0,1,0,0,0)^T$$ where, $\\sum_{k=1}^{K} x_k = 1$. If we denote the probability of $x_k = 1$ by the parameter $\\mu_k$, then the distribution of $\\pmb{x}$ is given$$p(\\pmb{x}|\\pmb{\\mu}) = \\prod_{k=1}^{K} \\mu_k^{x_k}\\tag{2.26}$$where, $\\pmb{\\mu} = (\\mu_1, \\cdots, \\mu_K)^T$ and, $\\sum_{k} \\mu_k = 1$ The distribution (2.26) can be regarded as a generalization of the Bernoulli distribution to more than two outcomes. It is easily seen that the distribution is normalized$$\\sum_{\\pmb{x}} p(\\pmb{x}|\\pmb{\\mu}) = \\sum_{k=1}^{K} \\mu_k = 1$$ and $$E[\\pmb{x}|\\pmb{\\mu}] = \\sum_{\\pmb{x}} p(\\pmb{x}|\\pmb{\\mu}) \\pmb{x} = \\pmb{\\mu}\\tag{2.28}$$ By maximum likelihood, we get $$\\mu_k^{ML} = \\frac{m_k}{N}\\tag{2.33}$$which is the fraction of the $N$ observations for which $x_k = 1$. The multinomial distribution is:$$Mult(m_1, m_2, \\cdots, m_K | \\pmb{\\mu}, N) = \\begin{pmatrix}N \\\\ m_1m_2\\cdots m_K\\end{pmatrix}\\prod_{k=1}^{K} \\mu_k^{m_k}\\tag{2.34}$$ The Dirichlet distributionBy inspection of the form of the multinomial distribution, we see that the conjugate prior is given by $$p(\\pmb{\\mu}|\\pmb{\\alpha}) \\propto \\prod_{k=1}^{K} \\mu_k ^{\\alpha_{k}-1}$$ the Dirichlet distribution is:$$Dir(\\pmb{\\mu}|\\pmb{\\alpha}) = \\frac{\\Gamma{(\\alpha_0)}}{\\Gamma{(\\alpha_1)} \\cdots \\Gamma{(\\alpha_K)}} \\prod_{k=1}^K \\mu_k^{\\alpha_k - 1}\\tag{2.38}$$ Multiplying the prior (2.38) by the likelihood function (2.34), we obtain the posterior distribution for the parameters ${\\mu_k}$ in the form$$p(\\pmb{\\mu}|D,\\pmb{alpha}) \\propto p(D|\\pmb{\\mu}) p(\\pmb{\\mu}|\\pmb{alpha}) \\propto \\prod_{k=1}^{K} \\mu_k^{\\alpha_k + m_k - 1}\\tag{2.40}$$We see that the posterior distribution again takes the form of a Dirichlet distribution, confirming that the Dirichlet is indeed a conjugate prior for the multinomial. The Gaussian DistributionFor a $D$-dimensional vector $\\pmb{x}$, the multivariate Gaussian distribution takes the form$$ \\mathcal{N}(\\pmb{x} | \\pmb{\\mu}, \\pmb{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} exp \\lbrace -\\frac{1}{2}(\\pmb{x - \\mu})^T \\pmb{\\Sigma}^{-1}(\\pmb{x-\\mu}) \\rbrace\\tag{2.43}$$ Gaussian distribution arises is when we consider the sum of multiple random variables. The central limit theorem (due to Laplace) tells us that, subject to certain mild conditions, the sum of a set of random variables, which is of course itself a random variable, has a distribution that becomes increasingly Gaussian as the number of terms in the sum increases (Walker, 1969). We begin by considering the geometrical form of the Gaussian distribution. The functional dependence of the Gaussian on $x$ is through the quadratic form (高斯函数对$x$的函数依赖性通过二次形式表现) $$\\Delta^2 = (\\pmb{x - \\mu})^T \\Sigma^{-1} (\\pmb{x - \\mu})\\tag{2.44}$$The quantity $\\Delta$ is called the Mahalanobis distance from $\\pmb{\\mu}$ to $\\pmb{x}$ and reduces to the Euclidean distance when $\\Sigma$ is the identity matrix. The Gaussian distribution will be constant on surfaces in x-space for which this quadratic form is constant. The covariance matrix $\\Sigma$ is a symmetric matrix, it can be expressed as an expansion in terms of its eigenvectors in the form$$\\Sigma = \\sum_{i=1}^{D} \\lambda_i \\pmb{u_i u_i^T}\\tag{2.48}$$ More about Symmetric matrix and EigenVectors &gt;&gt; $$\\Sigma^{-1} = \\sum_{i=1}^{D} \\frac{1}{\\lambda_i} \\pmb{u_i u_i^T}\\tag{2.49}$$Substituting (2.49) into (2.44), the quadratic form becomes$$\\Delta^2 = \\sum_{i=1}^{D} \\frac{y_i^2}{\\lambda_i}\\tag{2.50}$$where we have defined$$y_i = \\pmb{u_i}^T(\\pmb{x-\\mu})\\tag{2.51}$$ $\\pmb{x^TAx} = \\pmb{y^TDy}$ where, $\\pmb{x = Py}$ and $\\pmb{P}$ is a matrix of eigen vectors of $\\pmb{A}$. We can interpret ${y_i}$ as a new coordinate system defined by the orthonormal vectors $\\pmb{u_i}$ that are shifted and rotated with respect to the original $x_i$ coordinates. Forming the vector $\\pmb{y} = (y_1, \\cdots, y_D)^T$, we have$$\\pmb{y = U(x-\\mu)}\\tag{2.52}$$The quadratic form, and hence the Gaussian density, will be constant on surfaces for which (2.51) is constant. Now consider the form of the Gaussian distribution in the new coordinate system defined by the $y_i$.In going from the $\\pmb{x}$ to the $\\pmb{y}$ coordinate system, we have a Jacobian matrix $J$ with elements given by$$J_{ij} = \\frac{\\partial x_i}{\\partial y_j} = U_{ji}\\tag{2.53}$$thus$$|J|^2 = |U^T|^2 = 1\\tag{2.54}$$the determinant $|\\Sigma|$ of the covariance matrix can be written as the product of its eigenvalues, and hence$$|\\Sigma|^{1/2} = \\prod_{j=1}^{D}\\lambda_j^{1/2}\\tag{2.55}$$ Thus in the $y_j$ coordinate system, the Gaussian distribution takes the form$$p(\\pmb{y}) = p(\\pmb{x})|J| = \\prod_{j=1}^{D} \\frac{1}{(2\\pi\\lambda_j)^{1/2}} exp\\lbrace -\\frac{y_j^2}{2\\lambda_j} \\rbrace\\tag{2.56}$$which is the product of $D$ independent univariate Gaussian distributions. ($D$ is the dimension of the vector, like $D = 2$, $\\pmb{x} = (x_1, x_2)$) The eigenvectors therefore define a new set of shifted and rotated coordinates with respect to which the joint probability distribution factorizes into a product of independent distributions. The integral of the distribution in the $\\pmb{y}$ coordinate system is then$$\\int p(\\pmb{y}) d\\pmb{y} = \\prod_{j=1}^{D} \\int_{-\\infty}^{\\infty} \\frac{1}{(2\\pi\\lambda_j)^{1/2}} exp\\lbrace -\\frac{y_j^2}{2\\lambda_j} \\rbrace dy_j = 1\\tag{2.57}$$where we have used the result (1.48) for the normalization of the univariate Gaussian. This confirms that the multivariate Gaussian (2.43) is indeed normalized.$$\\int_{-\\infty}^{\\infty} \\mathcal{N}(x|\\mu, \\sigma^2) dx = 1\\tag{1.48}$$ We now look at the moments of the Gaussian distribution and thereby provide an interpretation of the parameters $\\pmb{\\mu}$ and $\\Sigma$. https://en.wikipedia.org/wiki/Moment_(mathematics) The expectation of $\\pmb{x}$ under the Gaussian distribution is given by$$\\begin{aligned}E[\\pmb{x}] &amp;= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} \\int exp \\lbrace -\\frac{1}{2}(\\pmb{x - \\mu})^T \\pmb{\\Sigma}^{-1}(\\pmb{x-\\mu}) \\rbrace \\pmb{x} d\\pmb{x}\\\\&amp;= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} \\int exp \\lbrace -\\frac{1}{2}\\pmb{z}^T \\pmb{\\Sigma}^{-1}\\pmb{z} \\rbrace (\\pmb{z+\\mu}) d\\pmb{z}\\\\&amp;= \\pmb{\\mu}\\end{aligned}\\tag{2.58}$$and so we refer to $\\pmb{\\mu}$ as the mean of the Gaussian distribution. We now consider second order moments of the Gaussian.(variance) In the univariate case,$$E[x^2] = \\int_{-\\infty}^{\\infty} N(x|\\mu, \\sigma^2) x^2 dx = \\mu^2 + \\sigma^2\\tag{1.50}$$$$var[x] = E[x^2] - E[x]^2 = \\sigma^2\\tag{1.51}$$ thus, For the multivariate Gaussian$$\\begin{aligned}E[\\pmb{xx^T}] &amp;= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} \\int exp \\lbrace -\\frac{1}{2}(\\pmb{x - \\mu})^T \\pmb{\\Sigma}^{-1}(\\pmb{x-\\mu}) \\rbrace \\pmb{xx^T} d\\pmb{x}\\\\&amp;= \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} \\int exp \\lbrace -\\frac{1}{2}\\pmb{z}^T \\pmb{\\Sigma}^{-1}\\pmb{z} \\rbrace (\\pmb{z+\\mu})(\\pmb{z+\\mu})^T d\\pmb{z}\\\\&amp;= \\pmb{\\mu\\mu^T} + \\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\pmb{\\Sigma}|^{1/2}} \\int exp \\lbrace -\\frac{1}{2}\\pmb{z}^T \\pmb{\\Sigma}^{-1}\\pmb{z} \\rbrace \\pmb{z}\\pmb{z}^T d\\pmb{z}\\\\&amp;= \\pmb{\\mu\\mu^T} + \\sum_{i=1}^{D} \\pmb{u_iu_i^T}\\lambda_i\\\\&amp;= \\pmb{\\mu\\mu^T} + \\Sigma\\end{aligned}\\tag{2.61}$$where,$$\\begin{aligned}\\pmb{z} &amp;= \\pmb{x-\\mu}\\\\&amp;= \\sum_{j=1}^{D} y_j \\pmb{u_j}\\end{aligned}\\tag{2.60}$$thus,$$cov[\\pmb{x}] = \\Sigma\\tag{2.64}$$ Gaussian distribution has some sever limitations: to many parameters, computation expensive. intrinsically unimodal(单峰), unable to provide a good approximation to multimodal distributions. We will see later that the introduction of latent variables, also called hidden variables or unobserved variables, allows both of these problems to be addressed. Conditional Gaussian distributionsSuppose $\\pmb{x}$ is a $D$-dimensional vector with Gaussian distribution $N(\\pmb{x}|\\pmb{\\mu}, \\Sigma)$ and that we partition $\\pmb{x}$ into two disjoint subsets $\\pmb{x}_a$ and $\\pmb{x}_b$. Without loss of generality, we can take $\\pmb{x}_a$ to form the first $M$ components of $\\pmb{x}$, with $\\pmb{x}_b$ comprising the remaining $D−M$ components, so that $$\\pmb{x} = \\begin{pmatrix} \\pmb{x_a} \\\\ \\pmb{x_b}\\end{pmatrix}\\tag{2.65}$$ corresponding partitions of the mean vector $\\pmb{\\mu}$ and covariance matrix $\\Sigma$ given by$$\\pmb{\\mu} = \\begin{pmatrix} \\pmb{\\mu_a} \\\\ \\pmb{\\mu_b}\\end{pmatrix}\\tag{2.66}$$$$\\Sigma = \\begin{bmatrix} \\Sigma_{aa} &amp; \\Sigma_{ab} \\\\ \\Sigma_{ba} &amp; \\Sigma_{bb}\\end{bmatrix}\\tag{2.67}$$where, $\\Sigma^T = \\Sigma$. The precision matrix $\\Lambda = \\Sigma^{-1}$. Conditional distribution $p(\\pmb{x_a|x_b})$ will be Gaussian:$$\\pmb{\\mu_{a|b}} = \\pmb{\\mu_a} + \\Sigma_{ab}\\Sigma_{bb}^{-1}(\\pmb{x_b}-\\pmb{\\mu_b})\\tag{2.81}$$$$\\Sigma_{a|b} = \\Sigma_{aa} - \\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba}\\tag{2.82}$$ Note that the mean of the conditional distribution $p(\\pmb{x_a|x_b})$, given by (2.81), is a linear function of $\\pmb{x_b}$ and that the covariance, given by (2.82), is independent of $\\pmb{x_a}$. This represents an example of a linear-Gaussian model. Marginal Gaussian distributionsNow we turn to a discussion of the marginal distribution given by$$p(\\pmb{x_a}) = \\int p(\\pmb{x_a, x_b}) d\\pmb{x_b}\\tag{2.83}$$which, as we shall see, is also Gaussian. Bayes’ theorem for Gaussian variablesHere we shall suppose that we are given a Gaussian marginal distribution $p(\\pmb{x})$ and a Gaussian conditional distribution $p(\\pmb{y}|\\pmb{x})$ in which $p(\\pmb{y}|\\pmb{x})$ has a mean that is a linear function of $\\pmb{x}$, and a covariance which is independent of $\\pmb{x}$. Maximum likelihood for the GaussianGiven a data set $\\pmb{X} = (\\pmb{x_1}, \\cdots , \\pmb{x_N})^T$ in which the observations $\\lbrace \\pmb{x_n} \\rbrace$ are assumed to be drawn independently from a multivariate Gaussian distribution, we can estimate the parameters of the distribution by maximum likelihood, and get$$\\pmb{\\mu_{ML}} = \\frac{1}{N} \\sum_{n=1}^{N}\\pmb{x_n}$$$$\\pmb{\\widetilde{\\Sigma}} = \\frac{1}{N-1} \\sum_{n=1}^{N}(\\pmb{x_n}-\\pmb{\\mu_{ML}})(\\pmb{x_n}-\\pmb{\\mu_{ML}})^T$$ Sequential estimationSequential methods allow data points to be processed one at a time and then discarded and are important for on-line applications, and also where large data sets are involved so that batch processing of all data points at once is infeasible. Bayesian inference for the GaussianThe maximum likelihood framework gave point estimates for the parameters $\\pmb{\\mu}$ and $\\pmb{\\Sigma}$. Now we develop a Bayesian treatment by introducing prior distributions over these parameters. We shall suppose that the variance $\\sigma^2$ is known, and we consider the task of inferring the mean $\\mu$ given a set of $N$ observations $\\pmb{X} = \\lbrace x_1, \\cdots , x_N \\rbrace $. The likelihood function, that is the probability of the observed data given $\\mu$, viewed as a function of $\\mu$, is given by$$\\begin{aligned}p(\\pmb{X}|\\mu) &amp;= \\prod_{n=1}^{N} p(x_n|\\mu) \\\\&amp;= \\frac{1}{(2\\pi\\sigma^2)^{N/2}} exp \\lbrace -\\frac{1}{2\\sigma^2} \\sum_{n=1}^{N} (x_n - \\mu)^2\\rbrace\\end{aligned}\\tag{2.137}$$ the posterior distribution is given by,$$p(\\mu|\\pmb{X}) \\propto p(\\pmb{X}|\\mu)p(\\mu)\\tag{2.139}$$ where$$p(\\mu) = N(\\mu|mu_0, \\sigma_0^2)\\tag{2.138}$$ thus, we get$$\\mu_N = \\frac{\\sigma^2}{N\\sigma_0^2 + \\sigma^2}\\mu_0 + \\frac{N\\sigma_0^2}{N\\sigma_0^2+\\sigma^2}\\mu_{ML}\\tag{2.141}$$$$\\frac{1}{\\sigma_N^2} = \\frac{1}{\\sigma_0^2} + \\frac{N}{\\sigma^2}\\tag{2.142}$$ In fact, the Bayesian paradigm leads very naturally to a sequential view of the inference problem. To see this in the context of the inference of the mean of a Gaussian, we write the posterior distribution with the contribution from the final data point $\\pmb{x_N}$ separated out so that $$p(\\pmb{\\mu}|D) \\propto [ p(\\pmb{\\mu}) \\prod_{n=1}^{N-1} p(\\pmb{x_n}|\\pmb{\\mu}) ] p(\\pmb{x_N}|\\pmb{\\mu})$$ The term in square brackets is (up to a normalization coefficient) just the posterior distribution after observing $N − 1$ data points. We see that this can be viewed as a prior distribution, which is combined using Bayes’ theorem with the likelihood function associated with data point $\\pmb{x_N}$ to arrive at the posterior distribution after observing $N$ data points. This sequential view of Bayesian inference is very general and applies to any problem in which the observed data are assumed to be independent and identically distributed. So far, we have assumed that the variance of the Gaussian distribution over the data is known and our goal is to infer the mean. Now let us suppose that the mean is known and we wish to infer the variance. Let $\\lambda \\equiv \\frac{1}{\\sigma^2}$, then, $$\\begin{aligned}p(\\pmb{X}|\\lambda) &amp;= \\prod_{n=1}^{N} N(x_n | \\mu, \\lambda^{-1}) \\\\&amp;\\propto \\lambda^{N/2} exp \\lbrace -\\frac{\\lambda}{2} \\sum_{n=1}^{N} (x_n - \\mu)^2\\rbrace\\end{aligned}\\tag{2.145}$$ the posterior distribution we get is$$p(\\lambda|\\pmb{X}) \\propto \\lambda^{a_0 - 1} \\lambda^{N/2} exp \\lbrace -b_0\\lambda - \\frac{\\lambda}{2} \\sum_{n=1}^{N} (x_n - \\mu)^2 \\rbrace\\tag{2.149}$$where the prior distribution of $\\lambda$ is$$Gam(\\lambda|a,b) = \\frac{1}{\\Gamma(a)} b^a \\lambda^{a-1} exp(-b\\lambda)\\tag{2.146}$$ we get$$a_N = a_0 + \\frac{N}{2}\\tag{2.150}$$ $$b_N = b_0 + \\frac{1}{2} \\sum_{n=1}^{N} (x_n - \\mu)^2 = b_0 + \\frac{N}{2} \\sigma^2_{ML}\\tag{2.151}$$ Student’s t-distributionIf we have a univariate Gaussian $N(x|\\mu, \\tau)$ together with a Gamma prior $Gam(\\tau | a, b)$ and we integrate out the precision, we obtain the marginal distribution of $x$ in the form $$\\begin{aligned}p(x | \\mu, a, b) &amp;= \\int_0^\\infty N(x | \\mu, \\tau^{-1}) Gam(\\tau | a, b) d\\tau \\\\&amp;= \\frac{b^a}{\\Gamma(a)} (\\frac{1}{2\\pi})^{1/2} [b+\\frac{(x-\\mu)^2}{2}]^{-a-1/2} \\Gamma(a+1/2)\\end{aligned}\\tag{2.158}$$ By convention we define new parameters given by $ν = 2a$ and $\\lambda = a / b$, in terms of which the distribution $p(x|\\mu, a, b)$ takes the form $$St(x|\\mu, \\lambda, v) = \\frac{\\Gamma (v / 2 + 1 / 2)}{\\Gamma (v/2)} (\\frac{\\lambda}{\\pi v})^{1/2} [1 + \\frac{\\lambda (x - \\mu)^2}{v}]^{-v/2-1/2}\\tag{2.159}$$ which is known as Student’s t-distribution. The parameter $\\lambda$ is sometimes called the precision of the t-distribution, even though it is not in general equal to the inverse of the variance. The parameter $v$ is called the degrees of freedom, and its effect is illustrated in Figure 2.15. From (2.158), we see that Student’s t-distribution is obtained by adding up an infinite number of Gaussian distributions having the same mean but different precisions.This gives the tdistribution an important property called robustness, which means that it is much less sensitive than the Gaussian to the presence of a few data points which are outliers. The robustness of the t-distribution is illustrated in Figure 2.16, which compares the maximum likelihood solutions for a Gaussian and a t-distribution $$St(\\pmb{x}|\\pmb{\\mu, \\Lambda}, v) = \\frac{\\Gamma(D/2+v/2)}{\\Gamma(v/2)} \\frac{|\\Lambda|^{1/2}}{(\\pi v)^{D/2}} [1+\\frac{\\Delta^2}{v}]^{-D/2-v/2}\\tag{2.162}$$where$$\\Delta^2 = (\\pmb{x-\\mu}^T)\\Lambda(\\pmb{x-\\mu})\\tag{2.163}$$ This is the multivariate form of Student’s t-distribution and satisfies the following properties$$E[\\pmb{x}] = \\pmb{\\mu}, v &gt; 1\\\\cov[\\pmb{x}] = \\frac{v}{(v-2)}\\Lambda^{-1}, v &gt; 2\\\\mode[\\pmb{x}] = \\pmb{\\mu}$$ Periodic variablesMixtures of GaussiansWe therefore consider a superposition of K Gaussian densities of the form$$p(\\pmb{x}) = \\sum_{k=1}^{K} \\pi_k N(\\pmb{x}|\\pmb{\\mu_K, \\Sigma_k})\\tag{2.188}$$where$$\\sum_{k=1}^K \\pi_k = 1\\tag{2.189}$$which is called a mixture of Gaussians. From the sum and product rules, the marginal density is given by$$p(\\pmb{x}) = \\sum_{k=1}^{K} p(k) p(\\pmb{x}|k)\\tag{2.191}$$ we can view $\\pi_k = p(k)$ as the prior probability of picking the $k^{th}$ component, and the density $N(\\pmb{x}|\\pmb{\\mu_k}, \\Sigma_k) = p(\\pmb{x}|k)$ as the probability of $\\pmb{x}$ conditioned on $k$. An important role is played by the posterior probabilities $p(k|\\pmb{x})$, which are also known as responsibilities. From Bayes’ theorem it is given by $$\\begin{aligned}\\gamma_k(\\pmb{x}) &amp;\\equiv p(k|\\pmb{x}) \\\\&amp;= \\frac{p(k)p(\\pmb{x}|k)}{\\sum_l p(l)p(\\pmb{x}|l)}\\\\&amp;= \\frac{\\pi_k N(\\pmb{x}|\\pmb{\\mu_k}, \\Sigma_k)}{\\sum_l \\pi_l N(\\pmb{x}|\\pmb{\\mu_l}, \\Sigma_l)}\\end{aligned}\\tag{2.192}$$ As a result, the maximum likelihood solution for the parameters no longer has a closed-form analytical solution. Alternatively we can employ a powerful framework called expectation maximization, The Exponential FamilyThe exponential family of distributions over $\\pmb{x}$, given parameters $\\eta$, is defined to be the set of distributions of the form $$p(\\pmb{x}|\\pmb{\\eta}) = h(\\pmb{x})g(\\pmb{\\eta}) exp \\lbrace \\pmb{\\eta^T} u(\\pmb{x}) \\rbrace\\tag{2.194}$$Here $\\pmb{\\eta}$ are called the natural parameters of the distribution, and $u(\\pmb{x})$ is some function of $\\pmb{x}$. The function $g(\\pmb{\\eta})$ can be interpreted as the coefficient that ensures that the distribution is normalized and therefore satisfies$$g(\\pmb{\\eta}) \\int h(\\pmb{x}) exp \\lbrace \\pmb{\\eta^T} u(\\pmb{x}) \\rbrace d\\pmb{x} = 1\\tag{2.159}$$ Maximum likelihood and sufficient statisticsTaking the gradient of both sides of (2.195) with respect to $\\pmb{\\eta}$, we have $$\\nabla g(\\pmb{\\eta}) \\int h(\\pmb{x}) exp \\lbrace \\pmb{\\eta}^T u(\\pmb{x}) \\rbrace d\\pmb{x} + g(\\pmb{\\eta}) \\int h(\\pmb{x}) exp \\lbrace \\pmb{\\eta}^T u(\\pmb{x}) \\rbrace u(\\pmb{x}) d\\pmb{x} = 0\\tag{2.224}$$ thus, we get $$-\\nabla \\ln g(\\pmb{\\eta}) = E[u(\\pmb{x})]\\tag{2.226}$$ Now consider a set of independent identically distributed data denoted by $\\pmb{X} = \\lbrace \\pmb{x_1}, \\cdots, \\pmb{x_n} \\rbrace$, for which the likelihood function is given by $$p(\\pmb{X}|\\pmb{\\eta}) = (\\prod_{n=1}^{N} h(\\pmb{x_n})) g(\\pmb{\\eta})^N exp \\lbrace \\pmb{\\eta}^T \\sum_{n=1}^{N} u(\\pmb{x_n})\\rbrace$$ Setting the gradient of $\\ln p(\\pmb{X}|\\pmb{\\eta})$ with respect to $\\pmb{\\eta}$ to zero, we get the following condition to be satisfied by the maximum likelihood estimator $\\pmb{\\eta}_{ML}$ $$-\\nabla \\ln g(\\pmb{\\eta_{ML}}) = \\frac{1}{N} \\sum_{n=1}^{N} u(\\pmb{x_n})\\tag{2.228}$$ We see that the solution for the maximum likelihood estimator depends on the data only through $\\sum_n u(\\pmb{x_n})$, which is therefore called the sufficient statistic of the distribution (2.194). We do not need to store the entire data set itself but only the value of the sufficient statistic. For the Bernoulli distribution, for example, the function $u(\\pmb{x})$ is given just by $\\pmb{x}$ and so we need only keep the sum of the data points $\\lbrace \\pmb{x_n} \\rbrace $, whereas for the Gaussian $u(\\pmb{x}) = (\\pmb{x}, \\pmb{x^2})^T$, and so we should keep both the sum of $\\lbrace \\pmb{x_n} \\rbrace $ and the sum of $\\lbrace \\pmb{x_n^2} \\rbrace $. If we consider the limit $N \\rightarrow \\infty$, then the right-hand side of (2.228) becomes $E[u(\\pmb{x})]$, and so by comparing with (2.226) we see that in this limit $\\pmb{\\eta_{ML}}$ will equal the true value $\\pmb{\\eta}$. Conjugate priorsFor any member of the exponential family (2.194), there exists a conjugate prior that can be written in the form $$p(\\pmb{\\eta}|\\pmb{\\chi}, v) = f(\\pmb{\\chi}, v) g(\\pmb{\\eta})^v exp \\lbrace v \\pmb{\\eta}^T \\pmb{\\chi} \\rbrace\\tag{2.229}$$the posterior distribution is like$$p(\\pmb{\\eta}|\\pmb{X}, \\pmb{\\chi}, v) \\propto g(\\pmb{\\eta})^{v+N} exp \\lbrace \\pmb{\\eta}^T (\\sum_{n=1}^N u(\\pmb{x_n}) + v\\pmb{\\chi})\\rbrace$$ Noninformative priorsNonparametric MethodsThroughout this chapter, we have focussed on the use of probability distributions having specific functional forms governed by a small number of parameters whose values are to be determined from a data set. This is called the parametric approach to density modelling. An important limitation of this approach is that the chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. For instance, if the process that generates the data is multimodal, then this aspect of the distribution can never be captured by a Gaussian, which is necessarily unimodal. we turn now to a discussion of two widely used nonparametric techniques for density estimation, kernel estimators and nearest neighbours. Kernel density estimatorskernel density estimators: $$p(\\pmb{x}) = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{h^D} k(\\frac{\\pmb{x} - \\pmb{x_n}}{h})\\tag{2.249}$$We can obtain a smoother density model if we choose a smoother kernel function, and a common choice is the Gaussian, which gives rise to the following kernel density model$$p(\\pmb{x}) = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{(2\\pi h^2)^{1/2}} exp \\lbrace -\\frac{||\\pmb{x}-\\pmb{x_n}||^2}{2h^2} \\rbrace\\tag{2.250}$$ Nearest-neighbour methods","link":"/MachineLearning/PatternRecognition/PatterRecognition-C2-Probability-Distributions/"},{"title":"PatterRecognition-C11-Sampling-Methods","text":"Keywords: Markov Chain Monte Carlo, Python This is the Chapter11 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] Basic Sampling AlgorithmsStandard distributionsRejection samplingAdaptive rejection samplingImportance samplingSampling-importance-resamplingSampling and the EM algorithmMarkov Chain Monte CarloMarkov chainsThe Metropolis-Hastings algorithmGibbs SamplingSlice SamplingThe Hybrid Monte Carlo AlgorithmDynamical systemsHybrid Monte CarloEstimating the Partition Function","link":"/MachineLearning/PatternRecognition/PatterRecognition-C11-Sampling-Methods/"},{"title":"PatterRecognition-C8-Graphical-Models","text":"Keywords: Bayesian Networks, Markov Random Fields, Inference, Python This is the Chapter8 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] We shall find it highly advantageous to augment the analysis using diagrammatic representations of probability distributions, called probabilistic graphical models. These offer several useful properties: They provide a simple way to visualize the structure of a probabilistic model and can be used to design and motivate new models.(直观的看到概率模型的结构，可以推动设计新的模型) Insights into the properties of the model, including conditional independence properties, can be obtained by inspection of the graph.(直观的看到概率模型的属性，包括条件独立等) Complex computations, required to perform inference and learning in sophisticated models, can be expressed in terms of graphical manipulations, in which underlying mathematical expressions are carried along implicitly.(通过图计算，可以表达复杂的数学计算，推断学习等过程) Bayesian NetworksConsider first an arbitrary joint distribution $p(a, b, c)$ over three variables $a, b$, and $c$. By application of the product rule of probability (1.11), we can write the joint distribution in the form$$\\begin{aligned}p(a,b,c) &amp;= p(c|a,b)p(a,b)\\\\&amp;= p(c|a,b)p(b|a)p(a)\\end{aligned}\\tag{8.2}$$ By repeated application of the product rule of probability, this joint distribution can be written as a product of conditional distributions, one for each of the variables$$p(x_1, \\cdots, x_K) = p(x_k|x_1,\\cdots, x_{K-1})\\cdots p(x_2|x_1)p(x_1)\\tag{8.3}$$We say that this graph is fully connected because there is a link between every pair of nodes. See Figure8.2, the joint distribution of all 7 variables is therefore given by$$p(x_1)p(x_2)p(x_3)p(x_4|x_1, x_2, x_3)p(x_5|x_1, x_3)p(x_6|x_4)p(x_7|x_4, x_5)\\tag{8.4}$$Thus, for a graph with $K$ nodes, the joint distribution is given by$$p(\\pmb{x}) = \\prod_{k=1}^{K} p(x_k | pa_k)\\tag{8.5}$$where $pa_k$ denotes the set of parents of $x_k$, and $\\pmb{x} = \\lbrace x_1, \\cdots , x_K \\rbrace$. This key equation expresses the factorization properties of the joint distribution for a directed graphical model. Example: Polynomial regressionWe consider the Bayesian polynomial regression model &gt;&gt;. Random Variables in this model: the vector of polynomial coefficients $\\pmb{w}$, the observed data $T = (t_1, \\cdots , t_N)^T$. Parameters in this model: the input data $X = (x_1, \\cdots , x_N)^T$, the noise variance $\\sigma^2$, the hyperparameter $\\alpha$ representing the precision of the Gaussian prior over $\\pmb{w}$. Note that, the parameters definition here is a little different from before. We usually say $\\pmb{w}$ is a parameter and $T$ is the observation/target data in Chaper 1. we see that the joint distribution is given by the product of the prior $p(\\pmb{w})$ and $N$ conditional distributions $p(t_n|\\pmb{w})$ for $n = 1, \\cdots , N$ so that$$p(T,\\pmb{w}) = p(\\pmb{w})\\prod_{n=1}^N p(t_n|\\pmb{w})\\Longleftrightarrowp(T,\\pmb{w}|X,\\alpha,\\sigma^2) = p(\\pmb{w}|\\alpha) \\prod_{n=1}^N p(t_n|\\pmb{w}, x_n, \\sigma^2)\\tag{8.6}$$Correspondingly, we can make $X$ and $\\alpha$ explicit in the graphical representation. Note that the value of $\\pmb{w}$ is not observed, and so $\\pmb{w}$ is an example of a latent variable, also known as a hidden variable. Having observed the values $\\lbrace t_n \\rbrace$ we can, if desired, evaluate the posterior distribution of the polynomial coefficients $\\pmb{w}$ as discussed before. For the moment, we note that this involves a straightforward application of Bayes’ theorem &gt;&gt;$$p(\\pmb{w}|T) \\propto p(\\pmb{w})\\prod_{n=1}^N p(t_n|\\pmb{w})\\tag{8.7}$$ Suppose we are given a new input value $\\hat{x}$ and we wish to find the corresponding probability distribution for $\\hat{t}$ conditioned on the observed data. the corresponding joint distribution of all of the random variables in this model, conditioned on the deterministic parameters, is then given by$$p(\\hat{t}, T, \\pmb{w} | \\hat{x}, X, \\alpha, \\sigma^2) = \\left[ \\prod_{n=1}^{N}p(t_n|x_n, \\pmb{w}, \\sigma^2) \\right] p(\\pmb{w}|\\alpha) p(\\hat{t}|\\hat{x}, \\pmb{w}, \\sigma^2)\\tag{8.8}$$The required predictive distribution for $\\hat{t}$ is then obtained, from the sum rule of probability, by integrating out the model parameters $\\pmb{w}$ so that$$p(\\hat{t}|\\hat{x}, X,T, \\alpha, \\sigma^2) \\propto \\int p(\\hat{t}, T, \\pmb{w} | \\hat{x}, X, \\alpha, \\sigma^2) d\\pmb{w}$$where we are implicitly setting the random variables in $T$ to the specific values observed in the data set. Generative modelsConsider a joint distribution $p(x_1, \\cdots, x_K)$ over $K$ variables that factorizes according to (8.5) corresponding to a directed acyclic graph. Our goal is to draw a sample $\\hat{x_1}, \\cdots, \\hat{x_k}$ from the joint distribution. we start with the lowest-numbered node and draw a sample from the distribution $p(x_1)$, which we call $\\hat{x_1}$. then work through each of the nodes in order, so that for node $n$ we draw a sample from the conditional distribution $p(x_n|pa_n)$ in which the parent variables have been set to their sampled values. Once we have sampled from the final variable $x_K$, we will have achieved our objective of obtaining a sample from the joint distribution. We can interpret such models as expressing the processes by which the observed data arose. For instance, consider an object recognition task in which each observed data point corresponds to an image (comprising a vector of pixel intensities) of one of the objects. In this case, the latent variables might have an interpretation as the position and orientation of the object. Given a particular observed image, our goal is to find the posterior distribution over objects, in which we integrate over all possible positions and orientations. We can represent this problem using a graphical model of the form show in Figure 8.8. The graphical model captures the causal process (Pearl, 1988) by which the observed data was generated. For this reason, such models are often called generative models. By contrast, the polynomial regression model described by Figure 8.5 is not generative because there is no probability distribution associated with the input variable $x$, and so it is not possible to generate synthetic data points from this model. We could make it generative by introducing a suitable prior distribution $p(x)$, at the expense of a more complex model. Discrete variablesFirst to know Exponential Family &gt;&gt; The probability distribution $p(x|\\mu)$ for a single discrete variable $x$ having $K$ possible states (using the 1-of-K representation) is given by$$p(x|\\mu) = \\prod_{k=1}^K \\mu_k^{x_k}\\tag{8.9}$$and is governed by the parameters $\\mu = (\\mu_1, \\cdots , \\mu_K)^T$. Linear-Gaussian modelsConditional IndependenceThree example graphsD-separationMarkov Random FieldsConditional independence propertiesFactorization propertiesIllustration: Image de-noisingRelation to directed graphsInference in Graphical ModelsInference on a chainTreesFactor graphsThe sum-product algorithmThe max-sum algorithmExact inference in general graphsLoopy belief propagationLearning the graph structure","link":"/MachineLearning/PatternRecognition/PatterRecognition-C8-Graphical-Models/"},{"title":"Rendering-First-Met-With-RayTracing","text":"Keywords: RayTracing, Radiometry, Monte Carlo Methods, BRDF Hey guys,speaking of RayTracing,I have to tell u that I’m really confused at the first time–What the hell is RayTracing?! Well u know I’m not the kind of person who is addicted to games, all I know about Graphics begins with the course UnityGame in my first year of Graduate,before that I am the one who doesn’t even know what pixel is! So not to mention those much more advanced technologies such as RayTracing! I assume readers are the same with me! You just learned the really really basic graphics—！ coordinates transformation,！ uv mapping,！ something about graphicspipeline,！ unity games,(ok I admit that I’m a little irritable at present, cuz a bug cannot be fixed for a few days)suddenly a voice told you time to learn RayTracing, and it seems a mess with the existing knowledges in your head. Don’t worry, learn with me. This article is going to be very very long, it’s about RayTracing,Acceleration,BRDF,PathTracing,Calculus,Global illumination,Unity,VScode etc, there will be lots pics and formulas. If you are patient enough to read through it, I promise you can learn something, but before this article, please scan the two articles:Lighting-Shading-Texture,Graphicspipeline,cuz we have lots things to link up. RAY-TRACINGWhy RayTracing?AccelerationRADIOMETRYRadiometry is closely related to calculus. If you know nothing about Calculus, please go to learn it and later come back, at least have the conception in your head. Though I’ll begin with mathematics first. Spherical CoordinatesWe all know Cartesian coordinates, cuz we define the points and vectors in Cartesian coordinate system since high school. Actually,we can also use Spherical coordinates to define them. In spherical coordinate system,the position of a point or direction and length of a vector are defined by two angles(denoted $\\theta$ and $\\phi$) and a radial distance($r$). The angle $\\theta$ is called polar angle and is measured from the fixed zenith direction.The zenith direction in relation to which this polar angle will be measured is the y-axis. To go from spherical coordinates to cartesian coordinates,we can use the following equation: $$x = r sin(\\theta)cos(\\phi)$$$$y = r cos(\\theta)$$$$z = r sin(\\theta)sin(\\phi)\\tag{spherical2cartesian}$$ Also to go from cartesian coordinates to spherical coordinates,we can use the following equation: $$r = \\sqrt{x^2+y^2+z^2}$$$$\\theta = cos^{-1}(\\frac{y}{r})$$$$\\phi = tan^{-1}(\\frac{z}{x})\\tag{catesian2spherical}$$ Differential and Integral CalculusCalculus includes differential and integral. Since this is really basic math,I’ll just make a rough review. The first fundamental theorem of calculus: $$F = \\int f(x)dx\\tag{1}$$ The second fundamental theorem of calculus: $$\\int_{a}^{b}f(x)dx = F(b)-F(a)\\tag{2}$$ The above is about functions defined in one dimension($f(x)$ takes one variable only, $x$). How about the functions in two dimension,even three dimension? So the integral of the function in the above pic is :$$\\int_{ax}^{bx}\\int_{ay}^{by}f(x,y)dxdy\\tag{2d integral}$$ So the integral of the function in the above pic is :$$\\rho = \\int_{ax}^{bx}\\int_{ay}^{by}\\int_{az}^{bz}f(x,y,z)dxdydz\\tag{3d integral}$$$\\rho$ means the density of the volume. RadiometryPreviously on the RayTracing part, the Whitted style ray tracing cannot give us correct results. That’s because the shading part only used empirical model. Blinn-phong Model cannot give us correct resutls, it’s only an empirical model, not based on physics, there’s lots materials blinn-phong cannot present. We want to simulate much more materials in our real life on computer, which means we need the PBS(physically based shading),and Radiometry is the basic concept of PBS. To begin with radiometry, we have new terms to remember: Radiant flux intensity irradiance radiance Radiant Energy and Flux(Power)We all know what Energy and Power mean in physics. They are almost the same in CG. Radiant energy is the energy of electromagnetic radiation. It is measured in units of joules, and denoted by the symbol: $$Q[J = Joule]$$ Radiant flux(power) is the energy emitted,reflected,transmitted or received, per unit time. $$\\Phi = \\frac{dQ}{dt} [W = Watt][lm = lumen]\\tag{flux}$$ Before go to intensity,irradiance,radiance,here’s a pic. Radiant IntensityRadiant intensity is the power per unit solid angle emitted by a point light source. $$I(\\omega) = \\frac{d\\Phi}{d\\omega} [\\frac{W}{sr}][\\frac{lm}{sr}=cd=candela]\\tag{intensity}$$ Angle:ration of subtended arc length on circle to radius $\\theta = \\frac{l}{r}$ Circle has $2\\pi$ radians Solid angle:ration of subtended area on sphere to radius squared $\\Omega = \\frac{A}{r^2}$ Sphere has $4\\pi$ steradians $$dA = (rd\\theta)(rsin\\theta d\\phi) = r^2 sin\\theta d\\theta d\\phi$$$$d\\omega = \\frac{dA}{r^2} = sin\\theta d\\theta d\\phi \\tag{solid angles}$$ IrradianceIrradiance is the power per(perpendicular/projected)unit area incident on a surface point. $$E(x) = \\frac{d\\Phi(x)}{dA} [\\frac{W}{m^2}][\\frac{lm}{m^2} = lux]\\tag{irradiance}$$ Do you still rememeber that in the Lighting-Shading-Texture article, we mentioned the empirical model Blinn phong model, and there’s diffuse component, when we calculate the diffuse component, we just assume the point’s worldposition,the point’s worldnormal etc. Actually,we assume this point is not really a point but a very small surface which we call differential area, denoted as $dA$.Now,what we actually consider as well is the amount of light falling on the surface of this very small area around P.Light that falls at P is not reduced to a single light ray since we are not interested in singular point but the small region $dA$ around that point. Light that falls on this region is contained within a small volume perpendicular to P. In the empirical model, the diffuse component is calculated like this:$$c_{diff} = c_{light} \\cdot m_{diff} \\cdot max(0,n \\cdot l)\\tag{blinn-phong-diffuse}$$as before, n is the surface normal and l is a unit vector that points towards the light source. The factor $m_{diff}$ is the material’s diffuse color, which is the value that most people think of when they think of the “color” of an object. The diffuse material color often comes from a texture map. The diffuse color of the light source is $c_{light}$. 1fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); The diffuse component obeys Lambert’s Cosine Law,here the Irradiance also obeys Lambert’s Cosine Law: RadianceRadiance(luminance) is the power emitted,reflected, transmitted or received by a surface, per unit solid angle, per projected unit area.Radiance is the fundamental field quantity that describes the distribution of light in an environment Radiance is the quantity associated with a ray Rendering is all about computing radiance $$L(p,\\omega) = \\frac{d^2\\Phi(p,\\omega)}{d\\omega dAcos\\theta} [\\frac{W}{sr m^2}][\\frac{cd}{m^2} = \\frac{lm}{sr m^2} = nit]\\tag{radiance}$$ Incident Radiance Incident radiance is the irradiance per unit solid angle arriving at the surface Exiting Radiance Exiting surface radiance is the intensity per unit projected area leaving the surface. Those concepts are really hard to remember, but don’t feel frustrated. Keep going. Read more. If you cannot recall the concepts, turn back, read again. BRDFFinally BRDF. Actualy the Radiometry step is for BRDF.As mentioned above what Phong essentially used to simulate the appearance of shiny materials is a function. This function (which includes a specular and a diffuse compute). This function contains a certain number of parameters such as n that can be tweaked to change the appearance of the material, but more importantly, it actually depends on two variables, the incident light direction (which is used to compute both the diffuse and specular component) and the view direction (which is used to compute the specular component only). We could essential write this function as: $$f_R(\\omega_o,\\omega_i)$$Where $\\omega_o$ and $\\omega_i$ are the angle between the surface normal (N) and the view direction (V) and the surface normal and the light direction (I) respectively.The subscript o stands for outgoing. In computer graphics, this function is given the fancy name of Bidirectional Reflectance Distribution Function or in short BRDF. A BRDF is nothing else than a function that returns the amount of light reflected in the view direction for a given incident light direction: $$BRDF(\\omega_o,\\omega_i)$$ In the following pic,BRDF represents how much light is reflected into each outgoing direction $\\omega_r$ from each incoming direction: $$f_r(\\omega_i \\rightarrow \\omega_r) = \\frac{dL_r(\\omega_r)}{dE_i(\\omega_i)} = \\frac{dL_r(\\omega_r)}{L_i(\\omega_i)cos\\theta_i d\\omega_i}\\tag{BRDF}$$ One thing that I want to mention, why camera can see the objects? It’s because when one light beam hits the surface, it will reflect in all directions among the half hemisphere, but only the reflected light enter the viewer’s eye make sense. So we need to calculate the output reflected light. So here come’s The Reflection Equation. $$L_r(p,\\omega_r) = \\int_{H^2}f_r(p,\\omega_i \\rightarrow \\omega_r)L_i(p,\\omega_i)cos\\theta_i d\\omega_i\\tag{The-Reflection-Equation}$$ Please make sure you understand the meaning of each symbol in the above formulas. Here come’s The Rendering Equation. $$L_o(p,\\omega_o) = L_e(p,\\omega_o) + \\int_{\\Omega^+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot\\omega_i)d\\omega_i\\tag{The-Rendering-Equation}$$ $L_e(p,\\omega_o)$ means the object’point p emit the emissive light to the $\\omega_o$ direction(remeber the $\\omega_o$ direction should be the view direction), and $\\int_{\\Omega^+}L_i(p,\\omega_i)f_r(p,\\omega_i,\\omega_o)(n\\cdot\\omega_i)d\\omega_i$ means the reflected light from $L_i(p,\\omega_i)$. Summary global illuminationPATH TRACINGAppendixMonte Carlo MethodsBefore we start, one question, do u have the background in Probability and Statistics Theory? If the answer is no, please go to learn something and later come back. What is Monte Carlo?Assume that we want to know the average height of all adults in one city, the most common method is to sample some adults and calculate the average height of them to approximate the true result. Here’s the formula: $$Approximation(Average(X)) = \\frac{1}{N}\\sum_{n=1}^{N}x_{n}$$ We generally denote random variables with upper case letters,the height of a population would be called a random variable,so the letter X is used.The formula can be read as,the approximation of the average value of the random variable X(the height of the adult population of the given country),is equal to sum of the height of N adults randomly chosen from that population(the samples),divided by the number N(the sample size). This in essence, is called a Monte Carlo approximation. In statistics,the average of the random variable X is called an expectation and is written E(X). So To summarize, Monte Carlo approximation (which is one of the MC methods) is a technique to approximate the expectation of random variables, using samples. Monte Carlo raytracing References: [1]GAMES [2]scratchapixel [3]3D Math Primer for Graphics and Game Development 2nd Edition. [4]Fundamentals of Computer Graphics and 3rd Edition. [5]Unity+Shader入门精要 [6]Unity3d Mannual [7]VSCode Document [8]基于物理着色：BRDF [9]如何正确理解 BRDF","link":"/Graphics/Rendering/Rendering-First-Met-With-RayTracing/"},{"title":"address-opereator","text":"Keywords: address This is for the readers who have basic c++ background. Those days I have noticed that there’s char ‘&amp;’ in front of function. So decided to record it. I suggest an online c++ compiler : (You can test your simple programs on it.) 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;memory&gt;using namespace std;int&amp; fun(int &amp;a){ cout &lt;&lt; \"aa \" &lt;&lt; &amp;a &lt;&lt;endl; return a;}int main(){ int b; cout &lt;&lt; \"b: \" &lt;&lt; &amp;b &lt;&lt;endl; b = 4; cout &lt;&lt; \"fun: \"&lt;&lt; &amp;fun(b) &lt;&lt; endl; int a; cout &lt;&lt; \"a: \" &lt;&lt; &amp;a &lt;&lt; endl; a = fun(b); cout &lt;&lt; a &lt;&lt; endl; cout &lt;&lt; &amp;a &lt;&lt; endl;} The result is : 1234567b: 0x7ffe5ec356b8fun: aa 0x7ffe5ec356b80x7ffe5ec356b8a: 0x7ffe5ec356bcaa 0x7ffe5ec356b840x7ffe5ec356bc To be honest, &amp; and * are too hard in c++.","link":"/CS/Advanced-CPP/STL-Address-Opereator/"},{"title":"PatterRecognition-C5-Neural-Networks","text":"Keywords: Gradient descent optimization, Error backpropagation, Hessian Matrix, Jacobian Matrix, Regularization, Mixture Density Network, Bayesian Neural Network, Python This is the Chapter5 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] In Linear Models for Regression &gt;&gt; and Linear Models for Classification &gt;&gt; that comprised linear combinations of fixed basis functions. We saw that such models have useful analytical and computational properties but that their practical applicability was limited by the curse of dimensionality. In order to apply such models to largescale problems, it is necessary to adapt the basis functions to the data. Feed-forward Network FunctionsThe general form of linear combinations of fixed nonlinear basis functions $\\phi_j(x)$ is$$y(\\pmb{x,w}) = f(\\sum_{j=1}^M w_j \\phi_j(x))\\tag{5.1}$$Our goal is to extend this model by making the basis functions $\\phi_j(x)$ depend on parameters and then to allow these parameters to be adjusted, along with the coefficients $\\lbrace w_j \\rbrace$, during training. We can give the overall network function that, for sigmoidal output unit activation functions, takes the form$$y_k(x,\\pmb{w}) = \\sigma(\\sum_{j=0}^{M} w_{kj}^{(2)}h(\\sum_{i=0}^{D}w_{ji}^{(1)}x_i))\\tag{5.9}$$A key difference compared to the perceptron, however, is that the neural network uses continuous sigmoidal nonlinearities in the hidden units, whereas the perceptron uses step-function nonlinearities. This means that the neural network function is differentiable with respect to the network parameters, and this property will play a central role in network training. Network TrainingIn summary, there is a natural choice of both output unit activation function and matching error function, according to the type of problem being solved. For regression. we use linear outputs and a sum-of-squares error.$$E(\\pmb{w}) = \\frac{1}{2}\\sum_{n=1}^{N}\\lbrace y(x_n, \\pmb{w}) - t_n\\rbrace^2\\tag{5.14}$$ for (multiple independent) binary classifications. we use logistic sigmoid outputs and a cross-entropy error function.$$y = \\sigma(a) = \\frac{1}{1 + exp(-a)}\\tag{5.19}$$$$E(\\pmb{w}) = -\\sum_{n=1}^{N}\\lbrace t_n\\ln y_n + (1-t_n)\\ln(1-y_n)\\rbrace, t_n = 0,1\\tag{5.21}$$If we have $K$ separate binary classifications to perform, then we can use a network having $K$ outputs each of which has a logistic sigmoid activation function. Associated with each output is a binary class label $t_k \\in {0, 1}$, where $k = 1, . . . , K$.$$E(\\pmb{w}) = -\\sum_{n=1}^{N}\\sum_{k=1}^{K} \\lbrace t_{nk}\\ln y_{nk} + (1 - t_{nk})\\ln (1-y_{nk})\\rbrace\\tag{5.23}$$ for multiclass classification. we use softmax outputs with the corresponding multiclass cross-entropy error function.$$y_k(x,\\pmb{w}) = \\frac{exp(a_k(\\pmb{x, w}))}{\\sum_j exp(a_j(\\pmb{x,w}))}\\tag{5.25}$$$$E(\\pmb{w}) = -\\sum_{n=1}^{N}\\sum_{k=1}^{K} t_{nk} \\ln y_k(x_n, \\pmb{w})\\tag{5.24}$$ For classification problems involving two classes. we can use a single logistic sigmoid output, or alternatively we can use a network with two outputs having a softmax output activation function. Parameter optimization if we make a small step in weight space from $\\pmb{w}$ to $\\pmb{w + \\sigma w}$ then the change in the error function is $\\sigma E \\simeq \\sigma \\pmb{w^T} \\nabla E(\\pmb{w})$, where the vector $\\nabla E(\\pmb{w})$ points in the direction of greatest rate of increase of the error function. However, the error function typically has a highly nonlinear dependence on the weights and bias parameters, and so there will be many points in weight space at which the gradient vanishes (or is numerically very small).$$\\nabla E(\\pmb{w}) = 0 \\tag{5.26}$$ For a successful application of neural networks, it may not be necessary to find the global minimum (and in general it will not be known whether the global minimum has been found) but it may be necessary to compare several local minima in order to find a sufficiently good solution. Because there is clearly no hope of finding an analytical solution to the equation $\\nabla E(\\pmb{w}) = 0$ we resort to iterative numerical procedures. Most techniques involve choosing some initial value $\\pmb{w^{(0)}}$ for the weight vector and then moving through weight space in a succession of steps of the form$$\\pmb{w}^{(\\tau + 1)} = \\pmb{w}^{(\\tau)} + \\Delta \\pmb{w^{(\\tau)}}\\tag{5.27}$$ More about Gradient in Calculus &gt;&gt; Local quadratic approximationConsider the Taylor expansion &gt;&gt; of $E(\\pmb{w})$ around some point $\\pmb{\\hat{w}}$ in weight space$$E(\\pmb{w}) \\simeq E(\\pmb{\\hat{w}}) + (\\pmb{w - \\pmb{\\hat{w}}})^T \\pmb{b} + \\frac{1}{2}(\\pmb{w - \\pmb{\\hat{w}}})^T \\pmb{H}(\\pmb{w - \\pmb{\\hat{w}}})\\tag{5.28}$$where cubic and higher terms have been omitted. Here $\\pmb{b}$ is defined to be the gradient of $E$ evaluated at $\\pmb{\\hat{w}}$$$\\pmb{b} = \\nabla E|_{\\pmb{w} = \\pmb{\\hat{w}}}\\tag{5.29}$$ and the Hessian matrix $\\pmb{H} = \\nabla \\nabla E$ has elements($\\pmb{H}$ is symmetric) $$(\\pmb{H})_{ij} = \\frac{\\partial E}{\\partial w_i \\partial w_j} |_{\\pmb{w = \\hat{w}}}\\tag{5.30}$$From (5.28), the corresponding local approximation to the gradient is given by$$\\nabla E \\simeq \\pmb{b} + \\pmb{H}(\\pmb{w - \\hat{w}})\\tag{5.31}$$ Consider the particular case of a local quadratic approximation around a point $\\pmb{w^{\\ast}}$ that is a minimum of the error function. In this case there is no linear term, because $\\nabla E = 0$ at $\\pmb{w^{\\ast}}$, and (5.28) becomes$$E(\\pmb{w}) = E(\\pmb{w\\ast}) + \\frac{1}{2}(\\pmb{w-w^{\\ast}})^T\\pmb{H}(\\pmb{w-w^{\\ast}})\\tag{5.32}$$where the Hessian $\\pmb{H}$ is evaluated at $\\pmb{w^{\\ast}}$. Consider the eigenvalue equation for Hessian Matrix: $$\\pmb{Hu_i} = \\lambda_i \\pmb{u_i}\\tag{5.33}$$where the eigenvectors $\\pmb{u_i}$ form a complete orthonormal set so that$$\\pmb{u_i^Tu_j} = \\sigma_{ij}\\tag{5.34}$$We now expand $(\\pmb{w − w^{\\ast}})$ as a linear combination of the eigenvectors in the form$$\\pmb{w-w^{\\ast}} = \\sum_i \\alpha_i \\pmb{u_i}\\tag{5.35}$$ Recall $x = Py$, $x^T A x = y^T D y$, columns of $P$ are the eigen vectors of $A$, $D$ is a diagonal matrix with eigen values. This can be regarded as a transformation of the coordinate system in which the origin is translated to the point $\\pmb{w^{\\ast}}$, and the axes are rotated to align with the eigenvectors. Substituting (5.35) into (5.32), and using (5.33) and (5.34), allows the error function to be written in the form$$E(\\pmb{w}) = E(\\pmb{w^{\\ast}}) + \\frac{1}{2}\\sum_i \\lambda_i \\alpha_i^2\\tag{5.36}$$A matrix $\\pmb{H}$ is said to be positive definite if, and only if,$$\\pmb{v^THv} &gt; 0, for-all-\\pmb{v}\\tag{5.37}$$Because the eigenvectors $\\lbrace \\pmb{u_i} \\rbrace$ form a complete set, an arbitrary vector $\\pmb{v}$ can be written in the form $$\\pmb{v} = \\sum_i c_i \\pmb{u_i}\\tag{5.38}$$ From (5.33) and (5.34), we then have$$\\pmb{v^THv} = \\sum_i c_i^2 \\lambda_i\\tag{5.39}$$ and so $\\pmb{H}$ will be positive definite if, and only if, all of its eigenvalues are positive. For a one-dimensional weight space, a stationary point $\\pmb{w^{\\ast}}$ will be a minimum if$$\\frac{\\partial^2 E}{\\partial \\pmb{w^2}}|_{\\pmb{w^{\\ast}}} &gt; 0\\tag{5.40}$$The corresponding result in $D$-dimensions is that the Hessian matrix, evaluated at $\\pmb{w^{\\ast}}$, should be positive definite. More about Eigenvalues in Algebra &gt;&gt; More about Positive Definite in Algebra &gt;&gt; Use of gradient informationGradient descent optimizationThe simplest approach to using gradient information is to choose the weight update in (5.27) to comprise a small step in the direction of the negative gradient, so that $$\\pmb{w}^{\\tau + 1} = \\pmb{w}^{(\\tau)} - \\eta \\nabla E(\\pmb{w}^{(\\tau)})\\tag{5.41}$$where the parameter $\\eta &gt; 0$ is known as the learning rate. For batch optimization (Techniques that use the whole data set at once are called batch methods), there are more efficient methods, such as conjugate gradients and quasi-Newton methods, which are much more robust and much faster than simple gradient descent.Unlike gradient descent, these algorithms have the property that the error function always decreases at each iteration unless the weight vector has arrived at a local or global minimum. There is, however, an on-line version of gradient descent that has proved useful in practice for training neural networks on large data sets. On-line gradient descent, also known as sequential gradient descent or stochastic gradient descent, makes an update to the weight vector based on one data point at a time, so that $$\\pmb{w}^{\\tau + 1} = \\pmb{w}^{\\tau} - \\eta \\nabla E_n(\\pmb{w}^{(\\tau)})\\tag{5.43}$$ One advantage of on-line methods compared to batch methods is that the former handle redundancy in the data much more efficiently. Another property of on-line gradient descent is the possibility of escaping from local minima, since a stationary point with respect to the error function for the whole data set will generally not be a stationary point for each data point individually. Error BackpropagationEvaluation of error-function derivativesConsider first a simple linear model in which the outputs $y_k$ are linear combinations of the input variables $x_i$ so that$$y_k = \\sum_i w_{ki}x_i\\tag{5.45}$$together with an error function that, for a particular input pattern $n$, takes the form$$E_n = \\frac{1}{2}\\sum_k (y_{nk} - t_{nk})^2, y_{nk} = y_k(x_n, \\pmb{w})\\tag{5.46}$$The gradient of this error function with respect to a weight $w_{ji}$ is given by$$\\frac{\\partial E_n}{\\partial w_{ji}} = (y_{nj}-t_{nj})x_{ni}\\tag{5.47}$$which can be interpreted as a ‘local’ computation involving the product of an ‘error signal’ $y_{nj} − t_{nj}$ associated with the output end of the link $w_{ji}$ and the variable $x_{ni}$ associated with the input end of the link. In a general feed-forward network, each unit computes a weighted sum of its inputs of the form$$a_j = \\sum_i w_{ji}z_i\\tag{5.48}$$where $z_i$ is the activation of a unit, or input, that sends a connection to unit $j$, and $w_{ji}$ is the weight associated with that connection. The sum in (5.48) is transformed by a nonlinear activation function $h(\\cdot)$ to give the activation $z_j$ of unit $j$ in the form$$z_j = h(a_j)\\tag{5.49}$$We can therefore apply the chain rule &gt;&gt; for partial derivatives to give$$\\frac{\\partial E_n}{\\partial w_{ji}} = \\frac{\\partial E_n}{\\partial a_j} \\frac{\\partial a_j}{\\partial w_{ji}}\\tag{5.50}$$let $\\delta_j$ be errors,$$\\delta_j \\equiv \\frac{\\partial E_n}{\\partial a_j}\\tag{5.51}$$By equation(5.48), we get$$\\frac{\\partial a_j}{\\partial w_{ji}} = z_i\\tag{5.52}$$Substituting (5.51) and (5.52) into(5.50), we obtain$$\\frac{\\partial E_n}{\\partial w_{ji}} = \\delta_j z_i\\tag{5.53}$$Equation (5.53) has the same form with equation(5.47), right? As we have seen already, for the output units, we have$$\\delta_k = y_k - t_k\\tag{5.54}$$To evaluate the $\\sigma$’s for hidden units, we again make use of the chain rule for partial derivatives,$$\\delta_j \\equiv \\frac{\\partial E_n}{\\partial a_j} = \\sum_k \\frac{\\partial E_n}{\\partial a_k} \\frac{\\partial a_k}{\\partial a_j}\\tag{5.55}$$where the sum runs over all units $k$ to which unit $j$ sends connections. If we now substitute the definition of $\\sigma$ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the following backpropagation formula$$\\delta_j = h’(a_j)\\sum_k w_{kj} \\delta_k\\tag{5.56}$$which tells us that the value of $\\sigma$ for a particular hidden unit can be obtained by propagating the $\\sigma$’s backwards from units higher up in the network, as illustrated in Figure 5.7. A simple exampleConsider a two-layer network of the form illustrated in Figure 5.1, together with a sum-of-squares error, in which the output units have linear activation functions, so that $y_k = a_k$, while the hidden units have logistic sigmoid activation functions given by $$h(a) \\equiv tanh(a)\\tag{5.58}$$where$$tanh(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}}\\tag{5.59}$$ this activation function’s derivative is,$$h(a) = 1 - h(a)^2\\tag{5.60}$$ for pattern n the sum-of-squares error function is given by $$E_n = \\frac{1}{2} \\sum_{k=1}^K (y_k - t_k)^2\\tag{5.61}$$ where $y_k$ is the activation of output unit $k$, and $t_k$ is the corresponding target, for a particular input pattern $x_n$. For each pattern in the training set in turn, we first perform a forward propagation using$$a_j = \\sum_{i = 0}^D w_{ji}^{(1)} x_i\\tag{5.62}$$ $$z_j = tanh(a_j)\\tag{5.63}$$ $$y_k = \\sum_{j=1}^M w_{kj}^{(2)} z_j\\tag{5.64}$$ Next we compute the $\\sigma$’s for each output unit using $$\\sigma_k = y_k - t_k\\tag{5.65}$$Then we backpropagate these to obtain $\\sigma s$ for the hidden units using$$\\sigma_j = (1 - z_j^2) \\sum_{k=1}^K w_{kj} \\sigma_k\\tag{5.66}$$Finally, the derivatives with respect to the first-layer and second-layer weights are given by$$\\frac{\\partial E_n}{\\partial w_{ji}^{(1)}} = \\sigma_j x_i$$$$\\frac{\\partial E_n}{\\partial w_{kj}^{(2)}} = \\sigma_k z_j\\tag{5.67}$$ Efficiency of backpropagationThe Jacobian matrixThe technique of backpropagation can also be applied to the calculation of other derivatives. Here we consider the evaluation of the Jacobian matrix, whose elements are given by the derivatives of the network outputs with respect to the inputs$$J_{ki} \\equiv \\frac{\\partial y_k}{\\partial x_i}\\tag{5.70}$$Jacobian matrices play a useful role in systems built from a number of distinct modules, as illustrated in Figure 5.8. Suppose we wish to minimize an error function $E$ with respect to the parameter $w$ in Figure 5.8. The derivative of the error function is given by$$\\frac{\\partial E}{\\partial w} = \\sum_{k,j} \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial z_j}\\frac{\\partial z_j}{\\partial w}\\tag{5.71}$$ In general, the network mapping represented by a trained neural network will be nonlinear, and so the elements of the Jacobian matrix will not be constants but will depend on the particular input vector used. $$\\begin{aligned}J_{ki} &amp;= \\frac{\\partial y_k}{\\partial x_i}\\\\&amp;= \\sum_j \\frac{\\partial y_k}{\\partial a_j}\\frac{\\partial a_j}{\\partial x_i}\\\\&amp;= \\sum_j w_{ji}\\frac{\\partial y_k}{\\partial a_j}\\end{aligned}\\tag{5.73}$$ We now write down a recursive backpropagation formula to determine the derivatives $\\frac{\\partial y_k}{\\partial a_j}$$$\\begin{aligned}\\frac{\\partial y_k}{\\partial a_j} &amp;= \\sum_l \\frac{\\partial y_k}{\\partial a_l} \\frac{\\partial a_l}{\\partial a_j}\\\\&amp;= h’(a_j)\\sum_l w_{lj}\\frac{\\partial y_k}{\\partial a_l}\\end{aligned}\\tag{5.74}$$ Jacobian matrix in Numberical Analysis &gt;&gt; The Hessian MatrixThe Hessian plays an important role in many aspects of neural computing, including the following: Several nonlinear optimization algorithms used for training neural networks are based on considerations of the second-order properties of the error surface, which are controlled by the Hessian matrix (Bishop and Nabney, 2008). The Hessian forms the basis of a fast procedure for re-training a feed-forward network following a small change in the training data (Bishop, 1991). The inverse of the Hessian has been used to identify the least significant weights in a network as part of network ‘pruning’ algorithms (Le Cun et al., 1990). The Hessian plays a central role in the Laplace approximation for a Bayesian neural network (see Section 5.7). Its inverse is used to determine the predictive distribution for a trained network, its eigenvalues determine the values of hyperparameters, and its determinant is used to evaluate the model evidence. Diagonal approximationFrom (5.48), the diagonal elements of the Hessian, for pattern $n$, can be written$$\\frac{\\partial^2 E_n}{\\partial w^2_{ji}} = \\frac{\\partial^2 E_n}{\\partial a^2_j} z^2_i\\tag{5.79}$$recursively using the chain rule of differential calculus to give a backpropagation equation of the form$$\\frac{\\partial^2 E_n}{\\partial a^2_j} = h’(a_j)^2 \\sum_k \\sum_{k’} w_{kj}w_{k’j} \\frac{\\partial^2 E_n}{\\partial a_k \\partial a_{k’}} + h’’(a_j) \\sum_k w_{kj} \\frac{\\partial^2 E_n}{\\partial a_k}\\tag{5.80}$$If we now neglect off-diagonal elements in the second-derivative terms,$$\\frac{\\partial^2 E_n}{\\partial a^2_j} = h’(a_j)^2 \\sum_k w^2_{kj} \\frac{\\partial^2 E_n}{\\partial^2 a_k} + h’’(a_j) \\sum_k w_{kj} \\frac{\\partial E_n}{\\partial a_k}\\tag{5.81}$$The major roblem with diagonal approximations, however, is that in practice the Hessian is typically found to be strongly nondiagonal, and so these approximations, which are driven mainly be computational convenience, must be treated with care. Outer product approximationWhen neural networks are applied to regression problems, it is common to use a sum-of-squares error function of the form$$E = \\frac{1}{2} \\sum_{n=1}^N(y_n - t_n)^2\\tag{5.82}$$We can then write the Hessian matrix in the form$$\\pmb{H} = \\nabla \\nabla E = \\sum_{n=1}^{N} \\nabla y_n \\nabla y_n + \\sum_{n=1}^N (y_n - t_n)\\nabla \\nabla y_n\\tag{5.83}$$By neglecting the second term in (5.83), we arrive at the Levenberg–Marquardt approximation or outer product approximation (because the Hessian matrix is built up from a sum of outer products of vectors), given by$$\\pmb{H} \\simeq \\sum_{n=1}^N \\pmb{b_n b_n^T}, \\pmb{b_n} = \\nabla y_n = \\nabla a_n\\tag{5.84}$$Evaluation of the outer product approximation for the Hessian is straightforward as it only involves first derivatives of the error function,which can be evaluated efficiently in $O(W)$ steps using standard backpropagation. It is important to emphasize that this approximation is only likely to be valid for a network that has been trained appropriately. Inverse HessianFirst we write the outer-product approximation in matrix notation as$$\\pmb{H_N} = \\sum_{n=1}^N \\pmb{b_n b_n^T}\\tag{5.86}$$where $\\pmb{b_n} \\equiv ∇_{\\pmb{w}}a_n$ is the contribution to the gradient of the output unit activation arising from data point $n$. Suppose we have already obtained the inverse Hessian using the first $L$ data points. By separating off the contribution from data point $L + 1$, we obtain $$\\pmb{H} _{L+1} = \\pmb{H} _L + \\pmb{b} _{L+1} \\pmb{b^T} _{L+1}\\tag{5.87}$$ In order to evaluate the inverse of the Hessian, we now consider the matrix identity $$(\\pmb{M+vv^T})^{-1} = \\pmb{M}^{-1}- \\frac{(\\pmb{M^{-1}v})(\\pmb{v^TM^{-1}})}{1+\\pmb{v^TM^{-1}v}}\\tag{5.88}$$ If we now identify $\\pmb{H_{L}}$ with $\\pmb{M}$ and $\\pmb{b}_{L+1}$ with $\\pmb{v}$, we obtain $$\\pmb{H_{L+1}}^{-1} = \\pmb{H_L}^{-1}- \\frac{(\\pmb{H_L^{-1}b_{L+1}})(\\pmb{b_{L+1}^TH_L^{-1}})}{1+\\pmb{b_{L+1}^TH_L^{-1}b_{L+1}}}\\tag{5.89}$$In this way, data points are sequentially absorbed until $L+1 = N$ and the whole data set has been processed. In particular, quasi-Newton nonlinear optimization algorithms gradually build up an approximation to the inverse of the Hessian during training. Finite differencesExact evaluation of the HessianFast multiplication by the HessianFor many applications of the Hessian, the quantity of interest is not the Hessian matrix $\\pmb{H}$ itself but the product of $\\pmb{H}$ with some vector $\\pmb{v}$. Regularization in Neural NetworksNote that $M$ controls the number of parameters (weights and biases) in the network, and so we might expect that in a maximum likelihood setting there will be an optimum value of $M$ that gives the best generalization performance, corresponding to the optimum balance between under-fitting and over-fitting. Consistent Gaussian priorsRemember in Chapter1, we see that an alternative approach is to choose a relatively large value for $M$ and then to control complexity by the addition of a regularization term to the error function. The simplest regularizer is the quadratic, giving a regularized error of the form $$\\widetilde{E}(\\pmb{w}) = E(\\pmb{w}) + \\frac{\\lambda}{2}\\pmb{w^Tw}\\tag{5.112}$$This regularizer is also known as weight decay. One limitation of this form is inconsistent with certain scaling properties of network mappings. Consider a multilayer perceptron network having two layers of weights and linear output units. The activations of the hidden units in the first hidden layer take the form $$z_j = h(\\sum_i w_{ji}x_i + w_{j0})\\tag{5.113}$$ while the activations of the output units are given by $$y_k = \\sum_j w_{kj}z_j + w_{k0}\\tag{5.114}$$ Suppose we perform a linear transformation of the input data of the form $$x_i \\rightarrow \\widetilde{x_i} = ax_i + b\\tag{5.115}$$ $$y_k \\rightarrow \\widetilde{y_k} = cy_k + d\\tag{5.118}$$also, we can perform a corresponding linear transformation of output data and the weights parameters. If we train one network using the original data and one network using data for which the input and/or target variables are transformed by one of the above linear transformations, then consistency requires that we should obtain equivalent networks that differ only by the linear transformation of the weights as given. Any regularizer should be consistent with this property. These require that the regularizer should be invariant to re-scaling of the weights and to shifts of the biases. Such a regularizer is given by$$\\frac{\\lambda_1}{2} \\sum_{w \\in \\mathcal{W_1}} w^2 + \\frac{\\lambda_2}{2} \\sum_{w \\in \\mathcal{W_2}} w^2\\tag{5.121}$$where $\\mathcal{W_1}$ denotes the set of weights in the first layer, $\\mathcal{W_2}$ denotes the set of weights in the second layer, and biases are excluded from the summations. This regularizer will remain unchanged under the weight transformations provided the regularization parameters are re-scaled using $$\\lambda_1 \\rightarrow a^{1/2}\\lambda_1\\\\\\lambda_2 \\rightarrow c^{-1/2}\\lambda_2$$ The regularizer (5.121) corresponds to a prior of the form $$p(\\pmb{w}|\\alpha_1, \\alpha_2) \\propto exp(-\\frac{\\alpha_1}{2}\\sum_{w \\in \\mathcal{W_1}} w^2 - \\frac{\\alpha_2}{2}\\sum_{w \\in \\mathcal{W_2}} w^2)\\tag{5.122}$$ We can illustrate the effect of the resulting four hyperparameters by drawing samples from the prior and plotting the corresponding network functions, as shown in Figure 5.11. More generally, we can consider priors in which the weights are divided into any number of groups $\\mathcal{W_k}$ so that $$p(\\pmb{w}) \\propto exp(-\\frac{1}{2}\\sum_k \\alpha_k ||\\pmb{w}||_k^2)\\tag{5.123}$$ where, $$||\\pmb{w}|| _k^2 = \\sum_{j \\in \\mathcal{W_k}} w_j^2\\tag{5.124}$$ Early stoppingAn alternative to regularization as a way of controlling the effective complexity of a network is the procedure of early stopping. InvariancesFor example, in the classification of objects in two-dimensional images, such as handwritten digits, a particular object should be assigned the same classification irrespective of its position within the image (translation invariance) or of its size (scale invariance). Such transformations produce significant changes in the raw data, expressed in terms of the intensities at each of the pixels in the image, and yet should give rise to the same output from the classification system. We therefore seek alternative approaches for encouraging an adaptive model to exhibit the required invariances. These can broadly be divided into four categories: The training set is augmented using replicas of the training patterns, transformed according to the desired invariances. For instance, in our digit recognition example, we could make multiple copies of each example in which the digit is shifted to a different position in each image. A regularization term is added to the error function that penalizes changes in the model output when the input is transformed. This leads to the technique of tangent propagation, discussed in Section 5.5.4. Invariance is built into the pre-processing by extracting features that are invariant under the required transformations. Any subsequent regression or classification system that uses such features as inputs will necessarily also respect these invariances. The final option is to build the invariance properties into the structure of a neural network (or into the definition of a kernel function in the case of techniques such as the relevance vector machine). One way to achieve this is through the use of local receptive fields and shared weights, as discussed in the context of convolutional neural networks in Section 5.5.6. Tangent propagationSuppose the transformation is governed by a single parameter $\\xi$ (which might be rotation angle for instance). Then the subspace $\\mathcal{M}$ swept out by $x_n$ will be one-dimensional, and will be parameterized by $\\xi$. Let the vector that results from acting on $x_n$ by this transformation be denoted by $s(x_n, \\xi)$, which is defined so that $s(x_n, 0) = x$. Then the tangent to the curve $M$ is given by the directional derivative &gt;&gt; $\\tau = \\frac{\\partial s}{\\partial \\xi}$, and the tangent vector at the point $x_n$ is given by $$\\tau_n = \\frac{\\partial s(x_n, \\xi)}{\\partial \\xi}|_{\\xi = 0}\\tag{5.125}$$ Image a function $z = f(x,y)$, the directional derivative is $D_{\\pmb{u}}f|P_0$, is the slope of the trace curve on the surface at point $P_0$. The derivative of output $k$ with respect to $\\xi$ is given by$$\\frac{\\partial y_k}{\\partial \\xi} |_{\\xi = 0} = \\sum_{i=1}^D \\frac{\\partial y_k}{\\partial x_i}\\frac{\\partial x_i}{\\partial \\xi}|_{\\xi = 0} = \\sum_{i=1}^D J_{ki}\\tau_i\\tag{5.126}$$where $J_{ki}$ is the $(k, i)$ element of the Jacobian matrix $J$. The result (5.126) can be used to modify the standard error function, so as to encourage local invariance in the neighbourhood of the data points, by the addition to the original error function $E$ of a regularization function $\\Omega$ to give a total error function of the form $$\\widetilde{E} = E + \\lambda \\Omega\\tag{5.127}$$where $\\lambda$ is a regularization coefficient and $$\\begin{aligned}\\Omega &amp;= \\frac{1}{2} \\sum_n \\sum_k (\\frac{\\partial y_{nk}}{\\partial \\xi}|_{\\xi = 0})^2 \\\\&amp;= \\frac{1}{2} \\sum_n \\sum_k (\\sum_{i=1}^D J_{nki}\\tau _{ni})^2\\end{aligned}\\tag{5.128}$$The regularization function will be zero when the network mapping function is invariant under the transformation in the neighbourhood of each pattern vector, and the value of the parameter $\\lambda$ determines the balance between fitting the training data and learning the invariance property. A related technique, called tangent distance, can be used to build invariance properties into distance-based methods such as nearest-neighbour classifiers (Simardet al., 1993). Training with transformed dataEncourage invariance of a model to a set of transformations is to expand the training set using transformed versions of the originalinput patterns, this approach is closely related to the technique of tangent propagation. Convolutional networksAnother approach to creating models that are invariant to certain transformation of the inputs is to build the invariance properties into the structure of a neural network. This is the basis for the convolutional neural network (Le Cun et al., 1989; LeCun et al., 1998), which has been widely applied to image data. First Understanding CNN &gt;&gt; Soft weight sharingRecall that the simple weight decay regularizer, given in (5.112), can be viewed as the negative log of a Gaussian prior distribution over the weights. We can encourage the weight values to form several groups, rather than just one group, by considering instead a probability distribution that is a mixture of Gaussians. The centres and variances of the Gaussian components, as well as the mixing coefficients, will be considered as adjustable parameters to be determined as part of the learning process. Thus, we have a probability density of the form $$p(\\pmb{w}) = \\prod_i p(w_i)\\tag{5.136}$$where$$p(w_i) = \\sum_{j=1}^M \\pi_j N(w_i | \\mu_j, \\sigma_j^2)\\tag{5.137}$$Taking the negative logarithm then leads to a regularization function of the form$$\\Omega (\\pmb{w}) = - \\sum_i \\ln (\\sum_{j=1}^M \\pi_j N(w_j|\\mu_j, \\sigma_j^2))\\tag{5.138}$$The total error function is then given by$$\\widetilde{E}(\\pmb{w}) = E(\\pmb{w}) + \\lambda \\Omega (\\pmb{w})\\tag{5.139}$$ Mixture Density NetworksThe goal of supervised learning is to model a conditional distribution $p(T|X)$, which for many simple regression problems is chosen to be Gaussian.（maximum likelihood function, posterior distribution, predictive distribution…） However, practical machine learning problems can often have significantly non-Gaussian distributions. These can arise, for example, with inverse problems in which the distribution can be multimodal(多模态), in which case the Gaussian assumption can lead to very poor predictions. For example, Data for this problem is generated by sampling a variable $x$ uniformly over the interval $(0, 1)$, to give a set of values ${x_n}$, and the corresponding target values tn are obtained by computing the function $x_n + 0.3 \\sin(2\\pi x_n)$ and then adding uniform noise over the interval $(−0.1, 0.1)$. The inverse problem is then obtained by keeping the same data points but exchanging the roles of $x$ and $t$. Least squares corresponds to maximum likelihood under a Gaussian assumption. We see that this leads to a very poor model for the highly non-Gaussian inverse problem. We therefore seek a general framework for modelling conditional probability distributions. This can be achieved by using a mixture model for $p(\\pmb{t|x})$ in which both the mixing coefficients as well as the component densities are flexible functions of the input vector $\\pmb{x}$, giving rise to the mixture density network. For any given value of $\\pmb{x}$, the mixture model provides a general formalism for modelling an arbitrary conditional density function $p(\\pmb{t|x})$. Provided we consider a sufficiently flexible network, we then have a framework for approximating arbitrary conditional distributions. Here we shall develop the model explicitly for Gaussian components, so that$$p(\\pmb{t|x}) = \\sum_{k=1}^K \\pi_k(\\pmb{x}) N(\\pmb{t}|\\pmb{\\mu_k}, \\sigma_k^2(\\pmb{x}))\\tag{5.148}$$ The neural network in Figure 5.20 can, for example, be a two-layer network having sigmoidal (‘tanh’) hidden units. If there are $L$(e.g. L = 1, only Gaussian) components in the mixture model (5.148), and if $t$ has $K$(e.g. K = 3, 3 gaussian distribution mixture) components. Then the network will have $L$ output unit activations denoted by $a_k^{\\pi}$ that determine the mixing coefficients $\\pi_k(x)$, K outputs denoted by $a_k^{\\sigma}$ that determine the kernel widths $\\sigma_k(x)$, and $L \\times K$ outputs denoted by $a_{kj}^{\\mu}$ that determine the components $\\mu_{kj}(x)$ of the kernel centres $\\mu_k(x)$. ??? The mixing coefficients must satisfy the constraints$$\\sum_{k=1}^K \\pi_k(x) = 1\\tag{5.149}$$which can be achieved using a set of softmax outputs$$\\pi_k(x) = \\frac{exp (a_k^\\pi)}{\\sum_{l=1}^K exp(a_l^\\pi)}\\tag{5.150}$$Similarly, the variances must satisfy $\\sigma_k^2(x) \\geq 0$ and so can be represented in terms of the exponentials of the corresponding network activations using$$\\sigma_k(x) = exp(a_k^{\\sigma})\\tag{5.151}$$Finally, because the means $\\mu_k(x)$ have real components, they can be represented directly by the network output activations$$\\mu_{kj}(x) = a_{kj}^{\\mu}$$ Bayesian Neural NetworksPosterior parameter distributionConsider the problem of predicting a single continuous target variable $t$ from a vector $\\pmb{x}$ of inputs (the extension to multiple targets is straightforward). We shall suppose that the conditional distribution $p(t|\\pmb{x})$ is Gaussian, with an $x$-dependent mean given by the output of a neural network model $y(\\pmb{x},\\pmb{w})$, and with precision (inverse variance) $\\beta$. We can find a Gaussian approximation to the posterior distribution by using the Laplace approximation &gt;&gt; . $$p(\\pmb{w} | D, \\alpha, \\beta) \\propto p(\\pmb{w} | \\alpha) p(D | \\pmb{w}, \\beta)\\tag{5.164}$$ $$q(\\pmb{w} | D) = N( \\pmb{w} | \\pmb{w_{MAP}}, \\pmb{A}^{-1})\\tag{5.167}$$ However, even with the Gaussian approximation to the posterior, this integration is still analytically intractable due to the nonlinearity of the network function $y(\\pmb{x}, \\pmb{w})$ as a function of $\\pmb{w}$.$$p(t|\\pmb{x}, D) = \\int p(t|\\pmb{x, w}) q(\\pmb{w}|D) d\\pmb{w}\\tag{5.168}$$ To make progress, we now assume that the posterior distribution has small variance compared with the characteristic scales of $\\pmb{w}$ over which $y(\\pmb{x}, \\pmb{w})$ is varying. This allows us to make a Taylor series expansion of the network function around $\\pmb{w_{MAP}}$ and retain only the linear terms $$y(\\pmb{x}, \\pmb{w}) \\simeq y(\\pmb{x}, \\pmb{w_{MAP}}) + \\pmb{g^T(w - w_{MAP})}\\tag{5.169}$$where we have defined$$\\pmb{g} = \\nabla _w y(\\pmb{x, w})|_wMAP\\tag{5.170}$$ With this approximation, we now have a linear-Gaussian model with a Gaussian distribution for $p(\\pmb{w})$ and a Gaussian for $p(t|\\pmb{w})$ whose mean is a linear function of $\\pmb{w}$ of the form $$p(t | \\pmb{x},\\pmb{w}, \\beta) \\simeq N(t | y(\\pmb{x, w_{MAP}} + \\pmb{g^T(w-w_{MAP})}), \\beta^{-1})\\tag{5.171}$$We can therefore make use of the general result (2.115) for the marginal $p(t)$ to give2.115 &gt;&gt; $$p(t | \\pmb{x}, D, \\alpha, \\beta) = N(t | y(\\pmb{x} , \\pmb{w_{MAP}}), \\sigma^2(\\pmb{x}))\\tag{5.172}$$where$$\\sigma^2{\\pmb{x}} = \\beta^{-1} + \\pmb{g^TA^{-1}g}\\tag{5.173}$$ Hyperparameter optimizationSo far, we have assumed that the hyperparameters $\\alpha$ and $\\beta$ are fixed and known. We can make use of the evidence framework, discussed in Section 3.5, together with the Gaussian approximation to the posterior obtained using the Laplace approximation, to obtain a practical procedure for choosing the values of such hyperparameters. Bayesian neural networks for classification","link":"/MachineLearning/PatternRecognition/PatterRecognition-C5-Neural-Networks/"},{"title":"PatterRecognition-C9-Mixture-Models-and-EM","text":"Keywords: K-means Clustering, Mixtures of Gaussians, EM Algorithm, Python This is the Chapter9 ReadingNotes from book Bishop-Pattern-Recognition-and-Machine-Learning-2006. [Code_Python] K-means ClusteringFor each data point $\\pmb{x_n}$, we introduce a corresponding set of binary indicator variables $r_{nk} \\in \\left\\{ 0, 1 \\right\\}$, where $k = 1, \\cdots , K$ describing which of the $K$ clusters the data point $\\pmb{x_n}$ is assigned to, so that if data point $\\pmb{x_n}$ is assigned to cluster $k$ then $r_{nk} = 1$, and $r_{nj} = 0$ for $j \\neq k$. This is known as the $1-of-K$ coding scheme. We can then define an objective function, sometimes called a distortion measure, given by$$J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} ||\\pmb{x_n} - \\pmb{\\mu_k}||^2\\tag{9.1}$$which represents the sum of the squares of the distances of each data point to its assigned vector $\\pmb{\\mu_k}$.Our goal is to find values for the $\\left\\{ r_{nk} \\right\\}$ and the $\\left\\{ \\pmb{\\mu_{nk}} \\right\\}$ so as to minimize $J$. Image segmentation and compressionMixtures of GaussiansFirst Understanding Mixtures of Gaussians &gt;&gt; We now turn to a formulation of Gaussian mixtures in terms of discrete latent variables. Gaussian mixture distribution can be written as a linear superposition of Gaussians in the form,$$p(\\pmb{x}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\pmb{x|\\mu_k}, \\pmb{\\Sigma_k})\\tag{9.7}$$ Proof: Let us introduce a $K$-dimensional binary random variable $\\pmb{z}$ having a $1$-of-$K$ representation in which a particular element $z_k$ is equal to $1$ and all other elements are equal to $0$. The marginal distribution over $\\pmb{z}$ is specified in terms of the mixing coefficients $\\pi_k$, such that$$p(z_k = 1) = \\pi_k$$where the parameters $\\left\\{ \\pi_k \\right\\}$ must satisfy$$0 \\leq \\pi_k \\leq 1\\tag{9.8}$$together with$$\\sum_{k=1}^K \\pi_k = 1\\tag{9.9}$$in order to be valid probabilities. We shall define the joint distribution $p(\\pmb{x}, \\pmb{z})$ in terms of a marginal distribution $p(\\pmb{z})$ and a conditional distribution $p(\\pmb{x}|\\pmb{z})$, corresponding to the graphical model in Figure 9.4. More About $\\pmb{\\Sigma}$ Meaning &gt;&gt; Because $\\pmb{z}$ uses a $1$-of-$K$ representation, we can also write this distribution in the form$$p(\\pmb{z}) = \\prod_{k=1}^K \\pi_k^{z_k}\\tag{9.10}$$Similarly, the conditional distribution of $\\pmb{x}$ given a particular value for $\\pmb{z}$ is a Gaussian$$p(\\pmb{x}|z_k = 1) = \\mathcal{N}(\\pmb{x}|\\pmb{\\mu_k}, \\pmb{\\Sigma_k})$$$$p(\\pmb{x}|\\pmb{z}) = \\prod_{k=1}^K \\mathcal{N}(\\pmb{x}| \\pmb{\\mu_k}, \\pmb{\\Sigma_k})^{z_k}\\tag{9.11}$$the marginal distribution of $\\pmb{x}$ is then obtained by summing the joint distribution over all possible states of $\\pmb{z}$ to give$$\\begin{aligned}p(\\pmb{x}) &amp;= \\sum_{\\pmb{z}} p(\\pmb{z}) p(\\pmb{x}|\\pmb{z}) \\\\&amp;= \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\pmb{x} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})\\end{aligned}\\tag{9.12}$$Thus the marginal distribution of $\\pmb{x}$ is a Gaussian mixture of the form (9.7). because we have represented the marginal distribution in the form $p(\\pmb{x}) = \\sum_{\\pmb{z}} p(\\pmb{x}, \\pmb{z})$, it follows that for every observed data point $\\pmb{x_n}$ there is a corresponding latent variable $\\pmb{z_n}$. We shall use $\\gamma(z_k)$ to denote $p(z_k = 1| \\pmb{x})$, whose value can be found using Bayes’ theorem$$\\begin{aligned}\\gamma(z_k) &amp; \\equiv p(z_k = 1 | \\pmb{x})\\\\&amp;= \\frac{p(z_k = 1) p(\\pmb{x}|z_k=1)}{\\sum_{j=1}^K p(z_j = 1)p(\\pmb{x} | z_j = 1)} \\\\&amp;= \\frac{\\pi_k \\mathcal{N}(\\pmb{x} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})}{\\sum_{j=1}^K \\pi_j \\mathcal{N}(\\pmb{x} | \\pmb{\\mu_j}, \\pmb{\\Sigma_j})}\\end{aligned}\\tag{9.13}$$prior probability of $z_k = 1$: $\\pi_k$ posterior probability: $\\gamma(z_k)$ once we have observed $\\pmb{x}$. $\\gamma(z_k)$ can also be viewed as the responsibility that component $k$ takes for ‘explaining’ the observation $\\pmb{x}$. Maximum likelihood$\\pmb{X}$: $N \\times D$ matrix, in which the $n^{th}$ row is given by $\\pmb{x_n}^T$. $\\pmb{Z}$: $N \\times K$ matrix, in which the $n^{th}$ row is given by $\\pmb{z_n}^T$. If we assume that the data points are drawn independently from the distribution, then we can express the Gaussian mixture model for this i.i.d. data set using the graphical representation shown in Figure 9.6. From (9.7) the log of the likelihood function is given by $$\\ln p(\\pmb{X} | \\pmb{\\pi}, \\pmb{\\mu}, \\pmb{\\Sigma}) = \\sum_{n=1}^{N} \\ln \\left\\{ \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k}) \\right\\}\\tag{9.14}$$ Consider a Gaussian mixture whose components have covariance matrices given by $\\pmb{\\Sigma_k} = \\sigma_k^2 \\pmb{I}$, where $\\pmb{I}$ is the unit matrix. Suppose that one of the components of the mixture model, let us say the $j^{th}$ component, has its mean $\\pmb{\\mu_j}$ exactly equal to one of the data points so that $\\pmb{\\mu_j} = \\pmb{x_n}$ for some value of $n$. This data point will then contribute a term in the likelihood function of the form$$\\mathcal{N}(\\pmb{x_n} | \\pmb{x_n}, \\sigma_j^2 \\pmb{I}) = \\frac{1}{(2\\pi)^{\\frac{1}{2}}} \\frac{1}{\\sigma_j}\\tag{9.15}$$ If we consider the limit $\\sigma_j \\rightarrow 0$, then we see that this term goes to infinity and so the log likelihood function will also go to infinity.Thus the maximization of the log likelihood function is not a well posed problem because such singularities will always be present and will occur whenever one of the Gaussian components ‘collapses’ onto a specific data point. 如果单个高斯函数折叠到一个数据点上，它将对由其他数据点产生的似然函数产生乘性因子，这些因子将以指数形式快速趋于零，从而使总体似然函数趋于零而不是无穷。然而，一旦我们在混合中有(至少)两个成分，其中一个成分可能具有有限的方差，因此为所有数据点分配有限的概率，而另一个成分可以缩小到一个特定的数据点，从而为对数似然提供越来越大的附加值。 EM for Gaussian mixtures(without latent view)An elegant and powerful method for finding maximum likelihood solutions formodels with latent variables is called the expectation-maximization algorithm, or EMalgorithm (Dempster et al., 1977; McLachlan and Krishnan, 1997). Setting the derivatives of $\\ln p(\\pmb{X}|\\pmb{\\pi}, \\pmb{\\mu}, \\pmb{\\Sigma})$ in (9.14) with respect to the means $\\pmb{\\mu_k}$ of the Gaussian components to zero, we obtain$$0 = -\\sum_{n=1}^N \\underbrace{\\frac{\\pi_k \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})}{\\sum_j \\pi_j \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_j}, \\pmb{\\Sigma_j})}}_{\\gamma(z_{nk})} \\pmb{\\Sigma_k}(\\pmb{x_n} - \\pmb{\\mu_k})\\tag{9.16}$$Multiplying by $\\pmb{\\Sigma_k^{-1}}$ (which we assume to be nonsingular) and rearranging we obtain$$\\pmb{\\mu_k} = \\frac{1}{N_k} \\sum_{n=1}^{N} \\gamma(z_{nk})\\pmb{x_n}\\tag{9.17}$$where,$$N_k = \\sum_{n=1}^{N} \\gamma(z_{nk})\\tag{9.18}$$ $N_k$ : effective number of points assigned to cluster $k$.$\\pmb{\\mu_k}$: take a weighted mean of all of the points in the data set, in which the weighting factor for data point $\\pmb{x_n}$ is given by the posterior probability $\\gamma(z_{nk})$ that component $k$ was responsible for generating $\\pmb{x_n}$. Similar, we maximize $\\ln p(\\pmb{X}|\\pmb{\\pi}, \\pmb{\\mu}, \\pmb{\\Sigma})$ with respect to the mixing coefficients $\\pmb{\\Sigma_k}$, $$\\pmb{\\Sigma_k} = \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) (\\pmb{x_n} - \\pmb{\\mu_k})(\\pmb{x_n} - \\pmb{\\mu_k})^T\\tag{9.19}$$ Similar, we maximize $\\ln p(\\pmb{X}|\\pmb{\\pi}, \\pmb{\\mu}, \\pmb{\\Sigma})$ with respect to the mixing coefficients $\\pi_k$, Here we must take account of the constraint (9.9), which requires the mixing coefficients to sum to one. $$\\begin{cases}\\ln p(\\pmb{X} | \\pmb{\\pi}, \\pmb{\\mu}, \\pmb{\\Sigma}) = \\sum_{n=1}^{N} \\ln \\left\\{ \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k}) \\right\\} \\\\\\sum_{k=1}^K \\pi_k = 1\\end{cases}$$ This can be achieved using a Lagrange multiplier &gt;&gt; and maximizing the following quantity $$\\ln p(\\pmb{X} | \\pmb{\\pi}, \\pmb{\\mu}, \\pmb{\\Sigma}) + \\lambda (\\sum_{k=1}^K \\pi_k - 1)$$which gives$$0 = \\sum_{n=1}^{N} \\frac{\\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})}{\\sum_j \\pi_j \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_j}, \\pmb{\\Sigma_j})} + \\lambda\\tag{9.21}$$we get$$\\pi_k = \\frac{N_k}{N}\\tag{9.22}$$so that the mixing coefficient for the $k$th component is given by the average responsibility which that component takes for explaining the data points. It is worth emphasizing that the results (9.17), (9.19), and (9.22) do not constitute a closed-form solution for the parameters of the mixture model because the responsibilities $\\gamma(z_{nk})$ depend on those parameters in a complex way through (9.13). Iterative EM algorithm: We first choose some initial values for the means, covariances, and mixing coefficients. Then we alternate between the following two updates that we shall call the E step and the M step. In the expectation step, or E step, we use the current values for the parameters to evaluate the posterior probabilities, or responsibilities, given by (9.13). We then use these probabilities in the maximization step, or M step, to re-estimate the means, covariances, and mixing coefficients using the results (9.17), (9.19), and (9.22). As with gradient-based approaches for maximizing the log likelihood, techniques must be employed to avoid singularities of the likelihood function in which a Gaussian component collapses onto a particular data point.It should be emphasized that there will generally be multiple local maxima of the log likelihood function, and that EM is not guaranteed to find the largest of these maxima. An Alternative View of EMThe goal of the EM algorithm is to find maximum likelihood solutions for models having latent variables. The set of all model parameters is denoted by $\\pmb{\\theta}$, and so the log likelihood function is given by $$\\ln p(\\pmb{X}|\\pmb{\\theta}) = \\ln \\left\\{ \\sum_{\\pmb{Z}} p(\\pmb{X,Z|\\theta}) \\right\\}\\tag{9.29}$$Note that our discussion will apply equally well to continuous latent variables simply by replacing the sum over $\\pmb{Z}$ with an integral. Now suppose that, for each observation in $\\pmb{X}$, we were told the corresponding value of the latent variable $\\pmb{Z}$. We shall call $\\left\\{ \\pmb{X}, \\pmb{Z} \\right\\}$ the complete data set, and we shall refer to the actual observed data $\\pmb{X}$ as incomplete, In practice, however, we are not given the complete data set $\\left\\{ \\pmb{X}, \\pmb{Z} \\right\\}$, but only the incomplete data $\\pmb{X}$. The likelihood function for the complete data set simply takes the form $\\ln p(\\pmb{X},\\pmb{Z}|\\pmb{\\theta})$. Our state of knowledge of the values of the latent variables in $\\pmb{Z}$ is given only by the posterior distribution $p(\\pmb{Z}| \\pmb{X}, \\pmb{\\theta})$. Because we cannot use the complete-data log likelihood, we consider instead its expected value under the posterior distribution of the latent variable, which corresponds (as we shall see) to the $E$ step of the $EM$ algorithm. In the E step, we use the current parameter values $\\pmb{\\theta^{old}}$ to find the posterior distribution of the latent variables given by $p(\\pmb{Z}|\\pmb{X}, \\pmb{\\theta^{old}})$. We then use this posterior distribution to find the expectation of the complete-data log likelihood evaluated for some general parameter value $\\pmb{\\theta}$. This expectation, denoted $E$, is given by$$E(\\pmb{\\theta, \\theta^{old}}) = \\sum_{\\pmb{Z}} p(\\pmb{Z} | \\pmb{X}, \\pmb{\\theta^{old}}) \\ln p(\\pmb{X}, \\pmb{Z} | \\pmb{\\theta})\\tag{9.30}$$In the M step, we determine the revised parameter estimate $\\pmb{\\theta^{new}}$ by maximizing this function$$\\pmb{\\theta^{new}} = \\underset{\\theta}{argmax} E (\\pmb{\\theta, \\theta^{old}})\\tag{9.31}$$ Gaussian mixtures revisited(with latent view)Now consider the problem of maximizing the likelihood for the complete data set $\\left\\{ \\pmb{X}, \\pmb{Z} \\right\\}$. From (9.10) and (9.11), this likelihood function takes the form$$p(\\pmb{X}, \\pmb{Z} | \\pmb{\\mu}, \\pmb{\\Sigma}, \\pmb{\\pi}) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\pi_k^{z_{nk}} \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})^{z_{nk}}\\tag{9.35}$$where $z_{nk}$ denotes the $k^{th}$ component of $\\pmb{z}_n$. Taking the logarithm, we obtain $$\\ln p(\\pmb{X}, \\pmb{Z} | \\pmb{\\mu}, \\pmb{\\Sigma}, \\pmb{\\pi}) = \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} \\left\\{ \\ln \\pi_k + \\ln \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k}) \\right\\}\\tag{9.36}$$ by Lagrange multiplier as before,$$\\pi_k = \\frac{1}{N} \\sum_{n=1}^{N} z_{nk}\\tag{9.37}$$ Using (9.10) and (9.11) together with Bayes’ theorem, we see that this posterior distribution takes the form $$p(\\pmb{Z}| \\pmb{X}, \\pmb{\\mu}, \\pmb{\\Sigma}, \\pmb{\\pi}) \\propto \\prod_{n=1}^{N} \\prod_{k=1}^{K} \\left[\\pi_k \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})\\right]^{z_{nk}}\\tag{9.38}$$The expected value of the indicator variable $z_{nk}$ under this posterior distribution is then given by$$E[z_{nk}] = \\frac{\\sum_{z_{nk}}z_{nk} [\\pi_k \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})]^{z_{nk}}}{\\sum_{z_{nj}} [\\pi_j \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_j}, \\pmb{\\Sigma_j})]^{z_{nj}}}=\\frac{\\pi_k \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k})}{\\sum_{j=1}^{K}\\pi_j \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_j}, \\pmb{\\Sigma_j})} = \\gamma(z_{nk})\\tag{9.39}$$ The expected value of the complete-data log likelihood function is therefore given by$$E_{\\pmb{Z}} [\\ln p(\\pmb{X}, \\pmb{Z}| \\pmb{\\mu}, \\pmb{\\Sigma}, \\pmb{\\pi})] = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\gamma(z_{nk}) \\left\\{ \\ln \\pi_k + \\ln \\mathcal{N}(\\pmb{x_n} | \\pmb{\\mu_k}, \\pmb{\\Sigma_k}) \\right\\}\\tag{9.40}$$ We can now proceed as follows.E step : First we choose some initial values for the parameters $\\pmb{\\mu^{old}}$, $\\pmb{\\Sigma^{old}}$ and $\\pmb{\\pi^{old}}$, and use these to evaluate the responsibilities.M step : We then keep the responsibilities fixed and maximize (9.40) with respect to $\\pmb{\\mu}$, $\\pmb{\\Sigma}$ and $\\pmb{\\pi}$.This leads to closed form solutions for $\\pmb{\\mu^{new}}$, $\\pmb{\\Sigma^{new}}$ and $\\pmb{\\pi^{new}}$ given by (9.17), (9.19), and (9.22) as before.This is precisely the EM algorithm for Gaussian mixtures as derived earlier. Relation to K-meansWhereas the K-means algorithm performs a hard assignment of data points to clusters, in which each data point is associated uniquely with one cluster, the EM algorithm makes a soft assignment based on the posterior probabilities. In fact, we can derive the K-means algorithm as a particular limit of EM for Gaussian mixtures as follows. Mixtures of Bernoulli distributionsFirst Understanding Bernoulli distributions &gt;&gt; EM for Bayesian linear regressionThe EM Algorithm in GeneralFirst Understanding $KL$ Divergence &gt;&gt; The expectation maximization algorithm, or EM algorithm, is a general technique for finding maximum likelihood solutions for probabilistic models having latent variables (Dempster et al., 1977; McLachlan and Krishnan, 1997). Our goal is to maximize the likelihood function that is given by$$p(\\pmb{X}|\\pmb{\\theta}) = \\sum_{\\pmb{Z}} p(\\pmb{X}, \\pmb{Z}|\\pmb{\\theta})\\tag{9.69}$$We shall suppose that direct optimization of $p(\\pmb{X}|\\pmb{\\theta})$ is difficult, but that optimization of the complete-data likelihood function $p(\\pmb{X}, \\pmb{Z}|\\pmb{\\theta})$ is significantly easier. We introduce a distribution $q(\\pmb{Z})$ defined over the latent variables, and we observe that, for any choice of $q(\\pmb{Z})$, the following decomposition holds$$\\ln p(\\pmb{X}|\\pmb{\\theta}) = \\mathcal{L}(q,\\pmb{\\theta}) + KL(q||p)\\tag{9.70}$$where$$\\mathcal{L}(q,\\pmb{\\theta}) = \\sum_{\\pmb{Z}} q(\\pmb{Z}) \\ln \\left\\{ \\frac{p(\\pmb{X}, \\pmb{Z} | \\pmb{\\theta})}{q(\\pmb{Z})} \\right\\}\\tag{9.71}$$$$KL(q||p) = - \\sum_{\\pmb{Z}} q(\\pmb{Z}) \\ln \\left\\{ \\frac{p(\\pmb{Z}|\\pmb{X}, \\pmb{\\theta})}{q(\\pmb{Z})} \\right\\}\\tag{9.72}$$Note that $\\mathcal{L}(q, \\pmb{\\theta})$ is a functional of the distribution $q(\\pmb{Z})$, and a function of the parameters $\\pmb{\\theta}$. Proof of (9.70):","link":"/MachineLearning/PatternRecognition/PatterRecognition-C9-Mixture-Models-and-EM/"},{"title":"something-from-vector-implementation","text":"Keywords: STL This article is for the readers who have a basic background of math, graphics, c++. Although I have written the code about Vector2,Vector3 by c++, but as time goes by, all the principles were forgotten. So I decided to write something about the simple program to get much more strong memory.If you search the implementation of vector in github, you can find lots examples. Here’s an example vector.hpp, actually it’s too hard for me to read. Today’s vector.hpp is about the really basic syntax in c++ such as operator overloading, friends, inline etc. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#pragma once#include &lt;cmath&gt;#include &lt;iostream&gt;class Vector3f{public: Vector3f() : x(0) , y(0) , z(0) {} Vector3f(float xx) : x(xx) , y(xx) , z(xx) {} Vector3f(float xx, float yy, float zz) : x(xx) , y(yy) , z(zz) {} Vector3f operator*(const float&amp; r) const { return Vector3f(x * r, y * r, z * r); } Vector3f operator/(const float&amp; r) const { return Vector3f(x / r, y / r, z / r); } Vector3f operator*(const Vector3f&amp; v) const { return Vector3f(x * v.x, y * v.y, z * v.z); } Vector3f operator-(const Vector3f&amp; v) const { return Vector3f(x - v.x, y - v.y, z - v.z); } Vector3f operator+(const Vector3f&amp; v) const { return Vector3f(x + v.x, y + v.y, z + v.z); } Vector3f operator-() const { return Vector3f(-x, -y, -z); } Vector3f&amp; operator+=(const Vector3f&amp; v) { x += v.x, y += v.y, z += v.z; return *this; } friend Vector3f operator*(const float&amp; r, const Vector3f&amp; v) { return Vector3f(v.x * r, v.y * r, v.z * r); } friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const Vector3f&amp; v) { return os &lt;&lt; v.x &lt;&lt; \", \" &lt;&lt; v.y &lt;&lt; \", \" &lt;&lt; v.z; } float x, y, z;};class Vector2f{public: Vector2f() : x(0) , y(0) {} Vector2f(float xx) : x(xx) , y(xx) {} Vector2f(float xx, float yy) : x(xx) , y(yy) {} Vector2f operator*(const float&amp; r) const { return Vector2f(x * r, y * r); } Vector2f operator+(const Vector2f&amp; v) const { return Vector2f(x + v.x, y + v.y); } float x, y;};inline Vector3f lerp(const Vector3f&amp; a, const Vector3f&amp; b, const float&amp; t){ return a * (1 - t) + b * t;}inline Vector3f normalize(const Vector3f&amp; v){ float mag2 = v.x * v.x + v.y * v.y + v.z * v.z; if (mag2 &gt; 0) { float invMag = 1 / sqrtf(mag2); return Vector3f(v.x * invMag, v.y * invMag, v.z * invMag); } return v;}inline float dotProduct(const Vector3f&amp; a, const Vector3f&amp; b){ return a.x * b.x + a.y * b.y + a.z * b.z;}inline Vector3f crossProduct(const Vector3f&amp; a, const Vector3f&amp; b){ return Vector3f(a.y * b.z - a.z * b.y, a.z * b.x - a.x * b.z, a.x * b.y - a.y * b.x);} Scan the program above, here’s some tips: Inline Functions Reference Variables Operator Overloading Friends Inline Functions Inline functions are a C++ enhancement designed to speed up programs. The primary distinction between normal functions and inline functions is not in how you code them but in how the C++ compiler incorporates them into a program.To understand the distinction between inline functions and normal functions,you need to peer more deeply into a program’s innards.Let’s do that now. How do normal function calls work?The final product of the compilation process is an executable program,which consists of a set of machine language instructions.When you start a program,the operating system loads these instructions into the computer’s memory so that each instruction has a particular memory address.The computer then goes through these instructions step-by-step.Sometimes,as when you have a loop or a branching statement,program execution skips over instructions,jumping backward or forward to a particular address. Normal function calls also involve having a program jump to another address (the function’s address) and then jump back when the function terminates.Let’s look at a typical implementation of that process in a little more detail.When a program reaches the function call instruction, the program stores the memory address of the instruction immediately following the function call,copies function arguments to the stack (a block of memory reserved for that purpose),jumps to the memory location that marks the beginning of the function,executes the function code (perhaps placing a return value in a register),and then jumps back to the instruction whose address it saved. Jumping back and forth and keeping track of where to jump means that there is an overhead in elapsed time to using functions. How do inline functions work?C++ inline functions provide an alternative.In an inline function,the compiled code is “in line” with the other code in the program.That is,the compiler replaces the function call with the corresponding function code.With inline code,the program doesn’t have to jump to another location to execute the code and then jump back. Inline functions thus run a little faster than regular functions,but they come with a memory penalty.If a program calls an inline function at ten separate locations,then the program winds up with ten copies of the function inserted into the code. When use inline?You should be selective about using inline functions.If the time needed to execute the function code is long compared to the time needed to handle the function call mechanism,then the time saved is a relatively small portion of the entire process.If the code execution time is short,then an inline call can save a large portion of the time used by the non-inline call.On the other hand,you are now saving a large portion of a relatively quick process,so the absolute time savings may not be that great unless the function is called frequently. Inline versus MacrosThe inline facility is an addition to C++. C uses the preprocessor #define statement to provide macros, which are crude implementations of inline code. For example, here’s a macro for squaring a number: 1#define SQUARE(X) X*X This works not by passing arguments but through text substitution, with the X acting as a symbolic label for the “argument”: 123a = SQUARE(5.0); is replaced by a = 5.0*5.0;b = SQUARE(4.5 + 7.5); is replaced by b = 4.5 + 7.5 * 4.5 + 7.5;d = SQUARE(c++); is replaced by d = c++*c++; Only the first example here works properly. You can improve matters with a liberal application of parentheses: 1#define SQUARE(X) ((X)*(X)) Still, the problem remains that macros don’t pass by value. Even with this new definition, SQUARE(c++) increments c twice, but the inline square() function evaluates c , passes that value to be squared, and then increments c once. The intent here is not to show you how to write C macros. Rather, it is to suggest that if you have been using C macros to perform function-like services, you should consider converting them to C++ inline functions. Reference VariablesThe main use for a reference variable is as a formal argument to a function.If you use a reference as an argument,the function works with the original data instead of with a copy.References provide a convenient alternative to pointers for processing large structures with a function,and they are essential for designing classes. Creating a reference variable‘&amp;’ in c++ has two functions: to indicate the address of a variable to declare references 12int rats;int &amp; rodents = rats; // makes rodents an alias for rats In this context, &amp; is not the address operator.Instead,it serves as part of the type identifier. Just as char * in a declaration means pointer-to-char , int &amp; means reference-to-int .The reference declaration allows you to use rats and rodents interchangeably;both refer to the same value and the same memory location. Difference between pointer and reference?123int rats = 101;int &amp; rodents = rats; // rodents a referenceint * prats = &amp;rats; // prats a pointer Then you could use the expressions rodents and *prats interchangeably with rats and use the expressions &amp;rodents and prats interchangeably with &amp;rats .From this standpoint,a reference looks a lot like a pointer in disguised notation in which the * dereferencing operator is understood implicitly.And,in fact,that’s more or less what a reference is.But there are differences besides those of notation.For one,it is necessary to initialize the reference when you declare it;you can’t declare the reference and then assign it a value later the way you can with a pointer: 123int rat;int &amp; rodent;rodent = rat; // No, you can't do this. You should initialize a reference variable when you declare it. A reference is rather like a const pointer;you have to initialize it when you create it, and when a reference pledges its allegiance to a particular variable,it sticks to its pledge. That is, int &amp; rodents = rats; (equals to) int * const pr = &amp; rats; References as Function Parameterscode from[1] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;iostream&gt;using namespace std;void swapr(int &amp; a, int &amp; b); // a, b are aliases for intsvoid swapp(int * p, int * q); // p, q are addresses of intsvoid swapv(int a, int b); // a, b are new variablesint main(){ int wallet1 = 300; int wallet2 = 350; cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; cout &lt;&lt; \"Using references to swap contents:\\n\"; swapr(wallet1, wallet2); // pass variables cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; cout &lt;&lt; \"Using pointers to swap contents again:\\n\"; swapp(&amp;wallet1, &amp;wallet2); // pass addresses of variables cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; cout &lt;&lt; \"Trying to use passing by value:\\n\"; swapv(wallet1, wallet2); // pass values of variables cout &lt;&lt; \"wallet1 = $\" &lt;&lt; wallet1; cout &lt;&lt; \" wallet2 = $\" &lt;&lt; wallet2 &lt;&lt; endl; return 0;}void swapr(int &amp; a, int &amp; b) // use references{ int temp; temp = a; // use a, b for values of variables a = b; b = temp;}void swapp(int * p, int * q) // use pointers{ int temp; temp = *p; // use *p, *q for values of variables *p = *q; *q = temp;}void swapv(int a, int b) // try using values{ int temp; temp = a; // use a, b for values of variables a = b; b = temp;} The result is : 1234567wallet1 = $300 wallet2 = $350Using references to swap contents:wallet1 = $350 wallet2 = $300Using pointers to swap contents again:wallet1 = $300 wallet2 = $350Trying to use passing by value:wallet1 = $300 wallet2 = $350 The reference and pointer methods both successfully swap the contents of the two wallets,whereas the passing by value method fails. Operator OverloadingOperator overloading is a technique for giving object operations a prettier look. Operator overloading is an example of C++ polymorphism. 123456int main(){ Vector3f v (2,3,4); Vector3f w (1,2,1); std::cout &lt;&lt; v + w &lt;&lt; std::endl;} FriendsAs you’ve seen,C++ controls access to the private portions of a class object.Usually,public class methods serve as the only access,but sometimes this restriction is too rigid to fit particular programming problems.In such cases,C++ provides another form of access:the friend.Friends come in three varieties: Friend functions Friend classes Friend member functions Why we need friends?Often,overloading a binary operator (that is,an operator with two arguments) for a class generates a need for friends Multiplying a Time object by a real number provides just such a situation,so let’s study that case. 1A = B * 2.75; Remember,the left operand is the invoking object. Translates to the following member function call: 1A = B.operator*(2.75); But what about the following statement? 1A = 2.75 * B; // cannot correspond to a member function Conceptually, 2.75 * B should be the same as B * 2.75 ,but the first expression cannot correspond to a member function because 2.75 is not a type Time object.Remember, the left operand is the invoking object,but 2.75 is not an object.So the compiler cannot replace the expression with a member function call. One way around this difficulty is to tell everyone (and to remember yourself) that you can only write B * 2.75 but never write 2.75 * B .This is a server-friendly,client-beware solution,and that’s not what OOP is about. However,there is another possibility—using a nonmember function.(Remember,most operators can be overloaded using either member or nonmember functions.) A nonmember function is not invoked by an object;instead,any values it uses,including objects,are explicit arguments.Thus,the compiler could match the expression 1Time operator*(double m, const Time &amp; t); Using a nonmember function solves the problem of getting the operands in the desired order (first double and then Time ),but it raises a new problem:Nonmember functions can’t directly access private data in a class.Well,at least ordinary nonmember functions lack access.But there is a special category of nonmember functions,called friends,that can access private members of a class. Creating friendsThe first step toward creating a friend function is to place a prototype in the class declaration and prefix the declaration with the keyword friend . Like the code we showed in the beginning. 1234friend Vector3f operator*(const float&amp; r, const Vector3f&amp; v){ return Vector3f(v.x * r, v.y * r, v.z * r);} This prototype has two implications: Although the operator*() function is declared in the class declaration,it is not a member function.So it isn’t invoked by using the membership operator. Although the operator*() function is not a member function,it has the same access rights as a member function. Are friends unfaithful to OOP?At first glance, it might seem that friends violate the OOP principle of data hiding because the friend mechanism allows nonmember functions to access private data. However, that’s an overly narrow view. Instead, you should think of friend functions as part of an extended interface for a class. For example, from a conceptual point of view, multiplying a double by a Time value is pretty much the same as multiplying a Time value by a double . That the first requires a friend function whereas the second can be done with a member function is the result of C++ syntax, not of a deep conceptual difference. By using both a friend function and a class method, you can express either operation with the same user interface. Also keep in mind that only a class declaration can decide which functions are friends, so the class declaration still controls which functions access private data. In short, class methods and friends are simply two different mechanisms for expressing a class interface. A common kind of friend: Overloading the &lt;&lt; OperatorFrom the code in the begining,we see that : 1234friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; os, const Vector3f&amp; v){ return os &lt;&lt; v.x &lt;&lt; \", \" &lt;&lt; v.y &lt;&lt; \", \" &lt;&lt; v.z;} In its most basic incarnation,the &lt;&lt; operator is one of C and C++’s bit manipulation operators;it shifts bits left in a value.But the ostream class overloads the operator,converting it into an output tool.Recall that cout is an ostream object and that it is smart enough to recognize all the basic C++ types.That’s because the ostream class declaration includes an overloaded operator&lt;&lt;() definition for each of the basic types.That is,one definition uses an int argument,one uses a double argument,and so on.So one way to teach cout to recognize a Vector3f object is to add a new function operator definition to the ostream class declaration.But it’s a dangerous idea to alter the iostream file and mess around with a standard interface. Instead,use the Vector3f class declaration to teach the Vector3f class how to use cout. So this is the basic syntax, anyway, there’s still lots deeper knowledge there… if I have met much more advanced code, I will write this article again. References:[1]C++ Primer Plus.","link":"/CS/Advanced-CPP/STL-Vector/"},{"title":"Rendering-Graphics-Pipeline-Overview","text":"Keywords: Rendering Pipeline, Deduce Projection Matrix, Rotation Matrix Just look at the middle part of Fig1,the working flow is followed. Step1.Setting up the sceneBefore we begin rendering,we must set several options that apply to the entire scene. For example,we need to set up the camera, to be more specifically,that means,pick a point of view in the scene from which to render it, and choose where on the screen to render it. We also need to select lighting and fog options, and prepare the depth buffer. If you have used Unity,then it is easy to understand, you put the camera in the proper place and set the lighting properties,also change the aspect ratio. Step2.Visibility determinationOnce we have a camera in place,we must then decide which objects in the scene are visible. In unity this means that you can tick the box on the Inspector panel to determine the object visible or not. Step3.Setting object-level rendering statesEach object may have its own rendering options. We must install these options into the rendering context before rendering any primitives associated with the object. The most basic property associated with an object is material that describes the surface propertis of the object. In unity,the material defines how the surface should be rendered,by including referencse to the texutres it uses,tiling information,color tints and so on. The avaliable options for a material depend on which shader the material is using. Step4.Geometry generation/deliveryThe geometry is actually submitted to the rendering API.Typically,the data is delivered in the form of triangles;either as individual triangles,or an indexed triangle mesh,triangle strip,or some other form. If you have heard about 3D Max or Maya,then you can get it.The artists create the model in the form of .obj file, we programmers load the model to the RAM, then we got the triangles data. You can achieve the obj_loader.h. Then you can get the triangles data like this: 123456789101112131415161718std::vector&lt;Triangle*&gt; TriangleList;objl::Loader Loader;bool loadout = Loader.LoadFile(obj_path);for(auto mesh:Loader.LoadedMeshes){ for(int i=0;i&lt;mesh.Vertices.size();i+=3) { Triangle* t = new Triangle(); for(int j=0;j&lt;3;j++) { t-&gt;setVertex(j,Vector4f(mesh.Vertices[i+j].Position.X,mesh.Vertices[i+j].Position.Y,mesh.Vertices[i+j].Position.Z,1.0)); t-&gt;setNormal(j,Vector3f(mesh.Vertices[i+j].Normal.X,mesh.Vertices[i+j].Normal.Y,mesh.Vertices[i+j].Normal.Z)); t-&gt;setTexCoord(j,Vector2f(mesh.Vertices[i+j].TextureCoordinate.X, mesh.Vertices[i+j].TextureCoordinate.Y)); } TriangleList.push_back(t); }}draw(TriangleList); In unity, this is done by the powerful engine. Step5.Vertex-level operationsOnce we have the geometry in some triangulated format,a number of various operations are performed at the vertex level. The most important operation is the transformation of vertex positions from modeling space into camera space/clip space. In unity, this operation is performed by a user-supplied microprogram called vertex shader. Like this: 1234567891011121314151617181920212223struct a2v{ float4 vertex : POSITION; float3 normal : NORMAL; float4 tangent : TANGENT; float4 texcoord : TEXCOORD0;};struct v2f{ float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float3 lightDir: TEXCOORD1; float3 viewDir : TEXCOORD2;};v2f vert(a2v v){ v2f o; //transform the vertex positions from modeling space into clip space o.pos = UnityObjectToClipPos(v.vertex); //... return o;} Though Unity has encapsulated the transformation function for us,there exists lots things to write.  We all know that the models that artists give us is in the model space,then how to transform them to the world space/camera space(view space)/clip space/screen space? How to deduce the matrixs(mvp)? What is the coordinates difference among OpenGL,DirectX and Unity? I will describe those in the Appendix :) Actually,the details have confused me for a long time,if you have the same feeling,don’t worry.Just go ahead. After we transformed the triangles to the camera space, any portion of a triangle outside the view frustum is removed, by the process known as clipping. Here the mvp matrix have ended. Once we have a clipped polygon in 3D clip space, we then project the vertices of that polygon,mapping them to 2D screen-space coordinates of the output window, here the viewport matrix is used. Step6.RasterizationOnce we have a clipped polygon in screen space,it is rasterized. Rasterization refers to the process of selecting which pixels on the screen should be drawn for a particular triangle; interpolating texture coordinates, colors, and lighting values that were computed at the vetex level across the face for each pixel; and passing these down to the next stage for pixel(fragment) shading. The pseudo-code is as follows: 123456789101112131415161718192021222324252627...got the TriangleListfor (const auto&amp; t:TriangleList){ //...mvp //...viewport rasterize(t);}rasterize(){ //...get the triangle bounding box for(x = x_min; x &lt; x_max+1; x++&gt; { for(y = y_min; y &lt; y_max+1; y++) { //...if the pixel(x,y) is the triangle t //...interpolate the depth buffer/color/normal/texcoords/shadingcoords. if(depth_buffer &lt; z_buffer[]) means visible { //...compute the color(texture/lighting...)(pixel shading) setpixel(x,y,color); } } }} Attention: in the code above, why we need the shadingcoords. That’s because, variable x,y is in the screen space, but the shading process should be done in the world space/view space/clip space. In unity, rasterization is mostly done by the powerful engine, but we can control the process of viewport to adjust the game to different resolution platforms and control the shader part to get more amazing effects. Step7.Pixel(fragment) shadingWe compute a color for the pixel,a process known as shading. The innocuous phrase “compute a color” is the heart of computer graphics! In unity, we write the fragment shader to compute the pixel colors under different lighting models. code-snippet as follows,from [3]. 1234567891011121314151617181920212223fixed4 frag(v2f i) : SV_Target { fixed3 tangentLightDir = normalize(i.lightDir); fixed3 tangentViewDir = normalize(i.viewDir); // Get the texel in the normal map fixed4 packedNormal = tex2D(_BumpMap, i.uv.zw); fixed3 tangentNormal; // If the texture is not marked as \"Normal map\" //tangentNormal.xy = (packedNormal.xy * 2 - 1) * _BumpScale; //tangentNormal.z = sqrt(1.0 - saturate(dot(tangentNormal.xy, tangentNormal.xy))); // Or mark the texture as \"Normal map\", and use the built-in funciton tangentNormal = UnpackNormal(packedNormal); tangentNormal.xy *= _BumpScale; tangentNormal.z = sqrt(1.0 - saturate(dot(tangentNormal.xy, tangentNormal.xy))); fixed3 albedo = tex2D(_MainTex, i.uv).rgb * _Color.rgb; fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(tangentNormal, tangentLightDir)); fixed3 halfDir = normalize(tangentLightDir + tangentViewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(tangentNormal, halfDir)), _Gloss); return fixed4(ambient + diffuse + specular, 1.0);} From the code we can see that the fragment shader computes the ambient,diffuse,specular,the MainTex &amp; BumpMap texture controls the coefficients value of lighting model formulas. The lighting model is much more than you see. There are many physical formulas. But they are not hard to understand. You can get the details from the reference books[1][2][3]. Step8.Blending and OutputFinally! At the bottom of the render pipeline, we have produced a color,opacity, and depth value. The depth value is tested against the depth buffer for per-pixel visibility determination to ensure that an object farther away from the camera doesn’t obscure one closer to the camera. Pixels with an opacity that is too low are rejected, and the output color is then combined with the previous color in the frame buffer in a process known as alpha blending. SUMMARYOK! Now the 8 steps have all been listed. You may want to overview the rough processes. The pseudocode summarizes the simplified rendering pipeline outlined above, from[1]. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// First , figure how to view the scenesetupTheCamera ( ) ;// Clear the zbufferclearZBuffer ( ) ;// Setup environmental lighting and fogsetGlobalLightingAndFog ( ) ;// get a l i s t of objects that are potentially visiblepotentiallyVisibleObjectList = highLevelVisibilityDetermination ( scene ) ;// Render everything we found to be potentially visiblefor ( all objects in potentiallyVisibleObjectList ) { // Perform lower−level VSD using bounding volume test i f (! object . isBoundingVolumeVisible ( ) ) continue ; // Fetch or procedurally generate the geometry triMesh = object . getGeometry ( ) // Clip and render the faces for ( each triangle in the geometry ) { // Transform the vertices to clip space , and perform // vertex−level calculations (run the vertex shader ) clipSpaceTriangle = transformAndLighting ( triangle ) ; // Clip the triangle to the view volume clippedTriangle = clipToViewVolume ( clipSpaceTriangle ) ; i f ( clippedTriangle . isEmpty ( ) ) continue ; // Project the triangle onto screen space screenSpaceTriangle = clippedTriangle . projectToScreenSpace ( ) ; // Is the triangle backfacing ? i f ( screenSpaceTriangle . isBackFacing ( ) ) continue ; // Rasterize the triangle for ( each pixel in the triangle ) { // Scissor the pixel here ( i f triangle was // not completely clipped to the frustum ) i f ( pixel is off−screen ) continue ; // Interpolate color , zbuffer value , // and texture mapping coords // The pixel shader takes interpolated values // and computes a color and alpha value color = shadePixel ( ) ; // Perform zbuffering i f (! zbufferTest ( ) ) continue ; // Alpha test to ignore pixels that are ”too // transparent” i f (! alphaTest ( ) ) continue ; // Write to the frame buffer and zbuffer writePixel ( color , interpolatedZ ) ; // Move on to the next pixel in this triangle } // Move on to the next triangle in this object }// Move on to the next potentially visible object} AppendixSince we referred to Coordinates Transformation in Step5. I guess you may not very clear about the internal matrixs and the workflow. Come on baby! Time to overcome the difficulties! Model,World,Camera Space,Clip SpaceThe geometry of an object is initially described in object space,which is a coordinate space local to the object. The information described usually consisits of vertex positions and surface normals. Object space = Model space = Local spaceForm the model space,the vertices are transformed into world space. The transformation from modeling space to world space is often called model transform. Typically,lighting for the scene is specified in world space,but it doesn’t matter what coordinate space is used to perform the lighting calculations provided that the geometry and the lights can be expressed in the same space. So it is not weird that you see lighting calculations in the world space,or view space,or tangent space,or clip space in unity shader file. From world space,vertices are transformed into camera sapce. Camera space is a 3D coordinate space in which the origin is at the center of projection,one is axis parallel to the direction the camera is facing(perpendicullar to the projection plane),one axis is the intersection of the top and bottom clip planes,and the other axis is the intersection of the left and right clip planes. Camera space = View space = Eye spaceHere we should be alert to the difference between left-handed world and right-handed world,as shown in Fig2.In the left-handed world,the most common convention is to point +z in the direction that the camera is facing,with +x and +y pointing “right” and “up”.This is fairly intuitive,as shown in Fig3.The typical right-handed convention is to have -z point in the direction that the camera is facing. From camera space,vertices are transformed once again into clip space. The matrix that transforms vertices from camera space into clip space is called the clip matrix. clip space = canonical view volume space clip matrix = projection matrixMore Learning on Transformation and Matrix &gt;&gt; References:[1]3D Math Primer for Graphics and Game Development 2nd Edition.[2]Fundamentals of Computer Graphics and 3rd Edition.[3]Unity+Shader入门精要[4]Unity3d Mannual[5]GAMES","link":"/Graphics/Rendering/Rendering-Graphics-Pipeline/"},{"title":"Variation-First-Understanding-of-Variation","text":"Keywords: Newbie, Variation This is the translation of this article(partly) https://zhuanlan.zhihu.com/p/139018146. Basic TheorySuppose we have two points $(a,p), (b,q)$, any curves joints them satisfies the bouding condition as follows: $$y(a) = p, y(b) = q\\tag{1}$$ Now we consider the definite Integral: $$I = \\int_a^b f(y,y’)dx\\tag{2}$$ More about Why $f(y,y’)$ &gt;&gt; $f(y,y’)$ is a function about $y(x)$ and it’s first-order derivative $y’(x)$, we expect a specific $y(x)$ lets $I$ has extrema. Recall what we have leanred about extrema in common function ? More about Extrema &gt;&gt; So $I$ is a functional（泛函） to $y(x)$ —- $y(x)$ changes, so as $I$, when $y(x) = ?$, I reaches the extrema. if y(x) has any small changes $\\delta y(x)$, we call it the variation of $y(x)$. (In basic calculus, we denote the samll changes of $x$ is $\\Delta x$ or $dx$) So the changes of $f(y,y’)$ is:$$\\delta f = \\frac{\\partial f}{\\partial y} \\delta y + \\frac{\\partial f}{\\partial y’}\\delta y’\\tag{3}$$ More about Partial Calculus &gt;&gt; The corresponding change of $I$ is : $$\\begin{aligned}\\delta I &amp;= \\int_a^b (\\delta f) dx \\\\&amp;= \\int_a^b [\\frac{\\partial f}{\\partial y} \\delta y + \\frac{\\partial f}{\\partial y’}\\delta y’] dx \\\\&amp;= \\int_a^b \\frac{\\partial f}{\\partial y} \\delta y dx + \\int_a^b \\frac{\\partial f}{\\partial y’}\\delta y’ dx\\end{aligned}\\tag{4}$$ the second term of equation(4) can be written as$$\\frac{\\partial f}{\\partial y’}\\delta y’ = \\frac{\\partial f}{\\partial y’} \\frac{d(\\delta y)}{dx}$$so, we are going to simplify equation(4), we firstly integrate the second term: $$\\begin{aligned}\\int_a^b \\frac{\\partial f}{\\partial y’}\\delta y’ dx &amp;= \\int_a^b \\frac{\\partial f}{\\partial y’} \\frac{d(\\delta y)}{dx} dx\\\\&amp;= \\int_a^b \\frac{\\partial f}{\\partial y’} d(\\delta y)\\\\&amp;= \\left. \\frac{\\partial f}{\\partial y’} \\delta y \\right|_a^b - \\int_a^b \\delta y d(\\frac{\\partial f}{\\partial y’})\\\\&amp;= \\left. \\frac{\\partial f}{\\partial y’} \\delta y \\right|_a^b - \\int_a^b \\delta y \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’}) dx\\\\\\end{aligned}\\tag{4.2}$$ More about Integral Formulas &gt;&gt; Since the function $y(x)$ has constant bouding value, which is $y(a) = p, y(b) = q$, so that$$\\delta y(a) = 0, \\delta y(b) = 0$$ thus, the first term in Equation(4.2), $\\left. \\frac{\\partial f}{\\partial y’} \\delta y \\right|_a^b = 0$. Substitue (4.2) to (4), we get $$\\begin{aligned}\\delta I &amp;= \\int_a^b \\frac{\\partial f}{\\partial y} \\delta y dx - \\int_a^b \\delta y \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’}) dx \\\\&amp;= \\int_a^b \\left[ \\frac{\\partial f}{\\partial y} - \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’})\\right] \\delta y(x) dx\\end{aligned}\\tag{5}$$ If $I$ has any extrama, then for any $\\delta y(x)$ satisfying the bounding contitions, there must have $\\delta I = 0$, which means: $$\\frac{\\partial f}{\\partial y} - \\frac{d}{dx}(\\frac{\\partial f}{\\partial y’}) = 0\\tag{6}$$ This is Euler-Lagrange Equation, which is the basic Theorem in Variation. With it, we can find the extremal funtion $y(x)$. Of couse, Equation(6) is a second-order Differential function. More about Second-Order Differential Functions &gt;&gt; ExamplesThe shortest path between two pointsGive two points in the xy-plane, which is the shortest path(curve) that joints them? Solution: Example2Example3","link":"/Math/Variation/Variation-First-Understanding-of-Variation/"},{"title":"Rendering-Lighting-Shading-Texture","text":"Keywords: Surface Shading, Texture mapping How to Understand Shading?How to understand shading? Just like everything does in the real world, all the scenes we see are under different ligths : the sun, the lamp, the flash light etc. Shading is kind of a technique that has a significant relationship with lights and will present the colorful drawings to us at last. Surface shading means the surface is ‘painted’ with lights, it’s a process of applying a material to an object. The Standard Local Lighting ModelI bet that you have heard the term: BRDF(Bidirectional Reflectance Distribution Function) which seems to have inextricable linkes to the Standard Lighting Model. Yes,BRDF will appear in PBS(Physically Based Shading) which is used to make more realistic modeling presenting the reactions between lights and materials. As for Standard Local Lighting Model, they can be considered as simplified versions of PBS, they are empirical models, but they are easier to understand. The Standard Lighting Equation OverviewThe Standard Lighting Model only cares about direct light (direct reflection).Lights are special entities without any corresponding geometry,and are simulated as if the light were emitting from a sight point. The rendering equation is an equation for the radiance outgoing from a point in any particular direction,the only outgoing direction that mattered in those days were the directions that pointed to the eye. Why say that? Because you know the real world doesn’t work like this where the light may reflect for dozens of times and the process is really complicated, the cost is also a luxury that could not yet be afforded. The basic idea is to classify coming into the eye into four distinct categories, each of which has a unique method for calculating its contribution. The four categories are : Emissive contribution,denoted as $c_{emis}$. It tells the amount of radiance emitted directly from the surface in the given direction. Note that without global illumination techniques,these surfaces do not actually light up anything(except themselves). Specular contribution,denoted as $c_{spc}$. It accounts for light incident directly from the light source that is scattered preferentially in the direction of a perfect “mirror bounce”. Diffuse contribution,denoted as $c_{diff}$. It accounts for light incident directly from the light source that is scattered in every direction evenly. Ambient contribution,denoted as $c_{amb}$. It is a fudge factor to account for all indirect light. The Ambient and Emmisive ComponentsTo model light that is reflected more than one time before it enters the eye,we can use a very crude approximation known as “ambient light”. The ambient portion of the lighting equation depends only on the properties of materials and an ambient lighting value,which is often a global value used for the entire scene.$$c_{amb} = g_{amb} \\cdot m_{amb}\\tag{1}$$The factor $m_{amb}$ is the material’s “ambient color”. This is almost always the same as the diffuse color (which is often defined using texture map). The other factor,$g_{amb}$,is the ambient light value. Somtimes a ray of light travels directly from the light source to the eye,without striking any surface in between. The standard lighting equation accounts for such rays by assigning a material an emissive color. For example,when we render the surface of the light bulb,this surface will probably appear very bright,even if there’s no other light in the scene,because the light bulb is emitting light. In many situations,the emissive contribution doesn’t depend on environmental factor; it is simply the emissive color of the material.$$c_{emis} = m_{emis}\\tag{2}$$ The Diffuse ComponentFor diffuse lighting, the location of the viewer is not relevant,since the reflections are scattered randomly, and no matter where we position the camera,it is equally likely that a ray will be sent our way. But the direction if incidence l,which is dictated by the position of the light source relative to the surface, is important. Diffuse lighting obeys Lambert’s law: the intensity of the reflected light is proportional to the cosine of the angle between the surface normal and the rays of light. We calculate the diffuse component according to Lambert’s Law:$$c_{diff} = c_{light} \\cdot m_{diff} \\cdot max(0,n \\cdot l)\\tag{3}$$as before, n is the surface normal and l is a unit vector that points towards the light source. The factor $m_{diff}$ is the material’s diffuse color, which is the value that most people think of when they think of the “color” of an object. The diffuse material color often comes from a texture map. The diffuse color of the light source is $c_{light}$. On thing needs attention, that is the max(), because we need to prevent the dot result of normal and light negative, we use $max(0,n \\cdot l)$, so the object won’t be lighted by the rays from it’s back. The Specular ComponentThe specular component is what gives surfaces a “shiny” appearance. If you don’t understand what a specular is, think about the professional term in animes: Now let’s see how the standard model calculates the specular contribution. For convenience,we assume that all of these vectors are unit vectors. n is the local outward-pointing surface normal v points towards the viewer. l points towards the light source. r is the reflection vector, which is the direction of a “perfect mirror bounce.” It’s the result of reflecting l about n. $\\theta$ is the angle between r and v. Of the four vectors, you can see the reflection vector can be computed by The Phong Model for specular reflection is :$$c_{spec} = c_{light} \\cdot m_{spec} \\cdot max(0,v \\cdot r)^{m_{gls}}\\tag{4}$$$$r = 2(n \\cdot l)n-l$$ $m_{gls}$ means the glossiness of the material,also known as the Phong exponent, specular exponent, or just as the material shininess. This controls how wide the “hotspot” is - a smaller $m_{gls}$ produces a larger, more gradual falloff from the hotspot,and a larger $m_{gls}$ produces a tight hotspot with sharp falloff. $m_{spec}$ is related to “shininess”, it represents the material’s specular color. While $m_{gls}$ controls the size of the hotspot, $m_{spec}$ controls its intensity and color. $c_{light}$ is essentially the “color” of the light, which contains both its color and intensity. But!!We usually use Blinn Phong Model instead of Phong Model. The Blinn phong model can be faster to implement in hardware than the Phong model, if the viewer and light source are far enough away from the object to be considered a constant,since then h is a constant and only needs to be computed once. But when v or l may not be considered constant, the Phong model calculation might be faster.The Blinn Phong Model for specualr reflection is :$$c_{spec} = c_{light} \\cdot m_{spec} \\cdot max(0,n \\cdot h)^{m_{gls}}\\tag{4}$$$$h = \\frac{v + l}{|v + l|}$$ In real coding, vector in the above (1)(2)(3)(4) should be unit vector Limitations of the Standard ModelWhy learn about this ancient history? First, it isn’t exactly ancient history, it’s alive and well. Second,the current local lighting model is one that content creators can understand and use. A final reason to learn the standard lighting model is becausemany newer models bear similarities to the standard model, and you cannotknow when to use more advanced lighting models without understandingthe old standard. Since Blinn Phong Model contains all the components above,so we call the it Blinn-Phong. Actually, there are several important physical phenomena not properly captured by the Blinn-Phong model. Such as Fresnel reflectance. (:) We’ll discuss this PBS part in the Appendix). Flat &amp; Gouraud ShadingThis part is about the Shading Frequencies. Are you confused? If not, it’s impossible. Because I’m confused at the first time learning and the second time learning. But now, I got it. So come with me. On modern shader-based hardware, lighting calculations are usually done on a per-pixel basis. By this we mean that for each pixel, we determine a surface normal (whether by interpolating the vertex normal across the face or by fetching it from a bump map), and then we perform the full lighting equation using this surface normal. This is per-pixel lighting, and the technique of interpolating vertex normals across the face is sometimes called Phong shading, not to be confused with the Phong calculation for specular reflection. The alternative to Phong shading is to perform the lighting equation less frequently (per face, or per vertex). These two techniques are known as flat shading and Gouraud shading, respectively. Flat shading is almost never used in practice except in software rendering. This is because most modern methods of sending geometry efficiently to the hardware do not provide any face-level data whatsoever. Gouraud shading, in contrast, still has some limited use on some platforms. Some important general principles can be gleaned from studying these methods, so let’s examine their results. Phong shading ≠ Phong Reflection Model ≠ Blinn Phong Reflection Model The table below can list differences among them. per-pixel lighting per-vertex lighting per-face lighting Phong shading Gouraud shading Flat shading Interpolate normal vectors across each triangle Interpolate colors from vertices across triangle Triangle face is flat — one normal vector Compute full shading model at each pixel Each vertex has a normal vector Not good for smooth surfaces # Mostly used is phong shading Talk is cheap, show me the code. Here’s a code (from[3]) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667//per-pixel lightingShader \"Unity Shaders Book/Chapter 6/Blinn-Phong Use Built-in Functions\" { Properties { _Diffuse (\"Diffuse\", Color) = (1, 1, 1, 1) _Specular (\"Specular\", Color) = (1, 1, 1, 1) _Gloss (\"Gloss\", Range(1.0, 500)) = 20 } SubShader { Pass { Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float4 worldPos : TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); // Use the build-in funtion to compute the normal in world space o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex); return o; } fixed4 frag(v2f i) : SV_Target { fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; fixed3 worldNormal = normalize(i.worldNormal); // Use the build-in funtion to compute the light direction in world space // Remember to normalize the result fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); // Use the build-in funtion to compute the view direction in world space // Remember to normalize the result fixed3 viewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); return fixed4(ambient + diffuse + specular, 1.0); } ENDCG } } FallBack \"Specular\"} The result is : Light SourcesIf you have used Unity, you won’t forget different kinds of lights. Standard Abstract Light Types A point light source represents light that emanates from a single point outward in all directions. Point lights are also called omni lights (short for “omnidirectional”) or spherical lights.A point light has a position and color, which controls not only the hue of the light, but also its intensity. Point lights can be used to represent many common light sources, such as light bulbs, lamps, fires, and so forth. point light = omni light = spherical light A spot light is used to represent light from a specific location in a specific direction. These are used for lights such as flashlights, headlights, and of course, spot lights~ As for A conical spot light, it has a circular “bottom”, the width of the cone is defined by a falloff angle(not to be confused with the falloff distance). Also, there is an inner angle that measures the size of the hotspot. A directional light represents light emanating from a point in space sufficiently far away that all the rays of light involved in lighting the scene (or at least the object we are currently considering) can be considered as parallel. Directional lights usually do not have a position, at least as far as lighting calculations are concerned, and they usually do not attenuate. Like the sun and moon in our real world. Directional light = parallel light = distant light An area light is only useful in bake. So we don’t talke about it here. Here’s intuitional effects among the lights. Light AttenuationIn the real world, the intensity of a light is inversely proportional to the square of the distance between the light and the object, as$$\\frac{i_1}{i_2} = (\\frac{d_2}{d_1})^2\\tag{1}$$where i is the radiant flux (the radiant power per unit area) and d is the distance. This part will be mentioned again in the RayTracing article, it’s about Radiometry. Here you just need to know that the final amount of emitted light is obtained by multiplying the light color by its intensity: light amount = light color * light intensity Actually,I haven’t used light-falloff in my coding. Also this blog is for the novices, so let’s continue with a simple practice and finish this part. Just rememeber that this is the very primary part. Talk is cheap, show me the code.(frome[3]) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161Shader \"Unity Shaders Book/Chapter 9/Forward Rendering\" { Properties { _Diffuse (\"Diffuse\", Color) = (1, 1, 1, 1) _Specular (\"Specular\", Color) = (1, 1, 1, 1) _Gloss (\"Gloss\", Range(8.0, 256)) = 20 } SubShader { Tags { \"RenderType\"=\"Opaque\" } Pass { // Pass for ambient light &amp; first pixel light (directional light) Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM // Apparently need to add this declaration //该指令可以保证我们的shader中使用的光照衰减等光照变量可以被正确赋值 #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag(v2f i) : SV_Target { fixed3 worldNormal = normalize(i.worldNormal); fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); fixed atten = 1.0; return fixed4(ambient + (diffuse + specular) * atten, 1.0); } ENDCG } Pass { // Pass for other pixel lights Tags { \"LightMode\"=\"ForwardAdd\" } Blend One One CGPROGRAM // Apparently need to add this declaration //该指令保证我们在additional pass 中访问到正确的光照变量 #pragma multi_compile_fwdadd #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" #include \"AutoLight.cginc\" fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag(v2f i) : SV_Target { fixed3 worldNormal = normalize(i.worldNormal); //如果当前处理的光源类型是平行光,因为平行光没有固定的位置 #ifdef USING_DIRECTIONAL_LIGHT fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); //如果是点光源或者聚光灯，他们的位置是 #else fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos.xyz); #endif fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); //如果是平行光，衰减值为1 #ifdef USING_DIRECTIONAL_LIGHT fixed atten = 1.0; #else //if point light,transform the vertex position from world spact to light space //sample the texture to get the Attenuation value //如果是点光源，把顶点坐标从世界空间到光线空间 #if defined (POINT) float3 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)).xyz; fixed atten = tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; //if spot light #elif defined (SPOT) float4 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)); fixed atten = (lightCoord.z &gt; 0) * tex2D(_LightTexture0, lightCoord.xy / lightCoord.w + 0.5).w * tex2D(_LightTextureB0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; #else fixed atten = 1.0; #endif #endif return fixed4((diffuse + specular) * atten, 1.0); } ENDCG } } FallBack \"Specular\"} If you want to know the rendering order,you can use the Frame Debug, this tool is really useful. I think as the study goes further, this part will be mentioned again. Also this is my learning curve, maybe it matches you too. So go on with my articles. Texture MappingFinally, here comes the texture mapping!I am already gearing up and eager to try. Cuz I really want to overview the shadow mapping and opacity blend again. And there are too many things cannot learn forward without the knowledge of texture. What is a Texture?There is much more to the appearance of an object than its shape. Different objects are different colors and have different patterns on their surface. One simple yet powerful way to capture these qualities is through texture mapping. A texture map is a bitmap image that is “pasted” to the surface of an object. bitmap image is pixel-image, on the contrary, vector-image So a texture map is just a regular bitmap that is applied onto the surface of a model. Exactly how does this work? The key idea is that, at each point on the surface of the mesh, we can obtain texture-mapping coordinates, which define the 2D location in the texture map that corresponds to this 3D location. Traditionally, these coordinates are assigned the variables (u,v), where u is the horizontal coordinate and v is the vertical coordinate; thus texture-mapping coordinates are often called UV coordinates or simply UVs. On thing needs attention : The origin is in the upper left-hand corner of the image, which is the DirectX-style convention, or in the lower left-hand corner, the OpenGL conventions.In unity, the powerful engine has solved the problem for us, unity use uniform left-hand corner as OpenGL. Although bitmaps come in different sizes, UV coordinates are normalized such that the mapping space ranges from 0 to 1 over the entire width(u) or height (v) of the image, rather than depending on the image dimensions. We typically compute or assign UV coordinates only at the vertex level, and the UV coordinates at an arbitratry interior position on a face are obtained through interpolation (:) See in Appendix) So the pseudo-code of UV mapping should be: 12345//c++for each rasterized screen sample (x,y): //sample (x,y)-usually a pixel's center (u,v) = evaluate texture coordinate at (x,y) //using barycentric coordinates texcolor = texture.sample(u,v); set sample’s color to texcolor; //usually the diffuse albedo Kd(recall the Blinn-Phong reflectance model) Texture MagnificationUV coordinates outside of the range [0,1] are allowed, and in fact are quite useful. Such coordinates are interpreted in a variety of ways. The most common addressing modes (Wrap Mode) are repeat (also known as tile or wrap) and clamp. When repeating is used, the integer portion is discarded and only the fractional portion is used, causing the texture to repeat. Under clamping, when a coordinate outside the range [0,1] is used to access a bitmap, it is clamped in range. This has the effect of streaking the edge pixels of the bitmap outwards. The mesh in both cases is identical: a single polygon with four vertices. And the meshes have identical UV coordinates. The only difference is how coordinates outside the [0,1] range are interpreted. See Fig17. If you have used Unity, this is not strange to you. See the example below(from[3]). The shader code on the materail of the Quad is : 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354Shader \"Unity Shaders Book/Chapter 7/Texture Properties\" { Properties { _MainTex (\"Main Tex\", 2D) = \"white\" {} } SubShader { Pass { Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" sampler2D _MainTex; float4 _MainTex_ST; struct a2v { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; }; struct v2f { float4 position : SV_POSITION; float2 uv : TEXCOORD0; }; v2f vert(a2v v) { v2f o; // Transform the vertex from object space to projection space o.position = UnityObjectToClipPos(v.vertex); //注意这里的v是模型空间中就定义好的纹理坐标 //v.texcoord应该是0-1之间,如果在面板上更改缩放值和偏移量，就会改动o.uv不在0-1之间 //o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; o.uv = TRANSFORM_TEX(v.texcoord, _MainTex); return o; } fixed4 frag(v2f i) : SV_Target { //根据uv值对纹理进行采样 fixed4 c = tex2D(_MainTex, i.uv); return fixed4(c.rgb, 1.0); } ENDCG } } FallBack \"Diffuse\"} I think you have noticed the code //o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; Look at the gif below, _MainTex_ST.xy means Tiling, _MainTex_ST.zw means offset. Also, I think you have noticed that there is Mipmap &amp; FilterMode properties in the panel of Fig17-1, what’s the meaning of these? You see that in Unity, the png is 512*512, it matches the Quad just in time. What if the texture(png) is too small? It’s easy to imagine, that you have an image, but the object is too giant, you need some methods to let the texture ‘pasted’ on the object surface without low resolution/distortion. Here I want to infer Bilinear Interpolation (:) see it in the Appendix) Then What if the texture(png) is too large? Here comes Mipmap (This part is a little hard for me. so Jump over it and later back..) Different Types Texture MappingThere are too many types of texture mapping. Bump MappingBump mapping is a general term that can refer to at least two different methods of controlling the surface normal per texel. A height map is a grayscale map, in which the intensity indicates the local “elevation” of the surface. Lighter colors indicate portions of the surface that are “bumped out,” and darker colors are areas where the surface is “bumped in.” Height maps are attractive because they are very easy to author, but they are not ideal for real-time purposes because the normal is not directly available; instead, it must be calculated from the intensity gradient. (We wil talk about it in Displacement Mapping) A bump map, which is very common nowadays, is Normal Mapping. Normal MappingIn a normal map, the coordinates of the surface normal are directly encoded in the map. How could a bump map save the surface normal of the object? Of course, the color. The most basic way is to encode x, y, and z in the red, green, and blue channels. Since the normal vector is bounded in [-1,1],and the color channel component is bounded in [0,1], so there should be a principle:$$pixel = \\frac{normal + 1}{2}$$Seems easy~ The bump map storse the normal vectors in model space in terms of pixels(rgb). Voila! If only it were that easy. Real-world objects exhibit a great deal of symmetry and self-similarity, and patterns are often repeated. For example, a box often has similar bumps and notches on more than one side. Because of this, it is currently a more efficient use of the same amount of memory (and artist time) to increase the resolution of the map and reuse the same normal map (or perhaps just portions of it) on multiple models (or perhaps just on multiple places in the same model). Of course, the same principle applies to any sort of texture map, not just normal maps. But normal maps are different in that they cannot be arbitrarily rotated or mirrored because they encode a vector. Imagine using the same normal map on all six sides of a cube. While shading a point on the surface of the cube, we will fetch a texel from the map and decode it into a 3D vector. A particular normal map texel on the top will produce a surface normal that points in the same direction as that same texel on the bottom of the cube, when they should be opposites! We need some other kind of information to tell us how to interpret the normal we get from the texture, and this extra bit of information is stored in the basis vectors. So there comes the Tangent Space.In tangent space, +z points out from the surface; the +z basis vector is actually just the surface normal n. The x basis vector is known as the tangent vector, which we’ll denote t, and it points in the direction of increasing t in texture space. Similarly, the y basis vector, known as the binormal and denoted here as b, corresponds to the direction of increasing b, although whether this motion is “up” or “down” in the texture space depends on the conventions for the origin in (t,b) space, which can differ, as we discussed earlier. The coordinates for the tangent and binormal are given in model space. And how to calculate basis vectors as the average of adjacent triangle normals?Here’s the formula &amp; code(from[1])We are given a triangle with vertex positions $p_0 = (x_0 ,y_0 ,z_0 ), p_1 = (x_1 ,y_1 ,z_1 ), and p_2 = (x_2 ,y_2 ,z_2),$ and at those vertices we have the UV coordinates $(u_0 ,v_0 ), (u_1 ,v_1 ), and (u_2 ,v_2 ).$$$q_1 = p_1 − p_0 , s_1 = u_1 − u_0 , t_1 = v_1 − v_0$$$$q_2 = p 2 − p_0 , s_2 = u_2 − u_0 , t_2 = v_2 − v_0.$$$$tangent = t_2q_1 - t_1q_2 , binormal = -s_2q_1 + s_1q_2$$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071//c++struct Vertex { Vector3 pos ; float u, v ; Vector3 normal ; Vector3 tangent ; float det ; // determinant of tangent transform. (−1 i f mirrored )};struct Triangle { int vertexIndex [3];};struct TriangleMesh { int vertexCount ; Vertex ∗vertexList ; int triangleCount ; Triangle ∗ triangleList ; void computeBasisVectors ( ) { // Note: we assume vertex normals are valid Vector3 ∗tempTangent = new Vector3 [ vertexCount ]; Vector3 ∗tempBinormal = new Vector3 [ vertexCount ]; // F i r s t clear out the accumulators for ( int i = 0 ; i &lt; vertexCount ; ++i ) { tempTangent [i].zero ( ) ; tempBinormal [i].zero ( ) ; } // Average in the basis vectors for each face // into i t s neighboring vertices for ( int i = 0 ; i &lt; triangleCount ; ++i ) { // Get shortcuts const Triangle &amp;tri = triangleList [ i ]; const Vertex &amp;v0 = vertexList [ tri.vertexIndex [0]]; const Vertex &amp;v1 = vertexList [ tri.vertexIndex [1]]; const Vertex &amp;v2 = vertexList [ tri.vertexIndex [2]]; // Compute intermediate values Vector3 q1 = v1.pos − v0.pos ; Vector3 q2 = v2.pos − v0.pos ; float s1 = v1.u − v0.u; float s2 = v2.u − v0.u; float t1 = v1.v − v0.v ; float t2 = v2.v − v0.v ; // Compute basis vectors for this triangle Vector3 tangent = t2∗q1 − t1∗q2; tangent.normalize ( ) ; Vector3 binormal = −s2∗q1 + s1∗q2; binormal.normalize ( ) ; // Add them into the running totals for neighboring verts for ( int j = 0 ; j &lt; 3 ; ++j ) { tempTangent [ tri.vertexIndex [ j ]] += tangent ; tempBinormal [ tri.vertexIndex [ j ]] += binormal ; } } // Now f i l l in the values into the vertices for ( int i = 0 ; i &lt; vertexCount ; ++i ) { Vertex &amp;v = vertexList [ i ]; Vector3 t = tempTangent [ i ]; // Ensure tangent is perpendicular to the normal. // (Gram−Schmit ) , then keep normalized version t −= v.normal ∗ dot (t,v.normal ) ; t.normalize ( ) ; v.tangent = t ; // Figure out i f we’ re mirrored if ( dot ( cross ( v.normal , t ) , tempBinormal [ i ]) &lt; 0.0 f ) { v.det = −1.0f ; // we’ re mirrored } else { v.det = +1.0 f ; // not mirrored } } // Clean up delete [] tempTangent ; delete [] tempBinormal ; }}; In unity, you can calculate the lighting model in the world space with bump textures.Here an example.(from[3]) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697Shader \"Unity Shaders Book/Chapter 7/Normal Map In World Space\" { Properties { _Color (\"Color Tint\", Color) = (1, 1, 1, 1) _MainTex (\"Main Tex\", 2D) = \"white\" {} _BumpMap (\"Normal Map\", 2D) = \"bump\" {} _BumpScale (\"Bump Scale\", Float) = 1.0 _Specular (\"Specular\", Color) = (1, 1, 1, 1) _Gloss (\"Gloss\", Range(8.0, 256)) = 20 } SubShader { Pass { Tags { \"LightMode\"=\"ForwardBase\" } CGPROGRAM #pragma vertex vert #pragma fragment frag #include \"Lighting.cginc\" fixed4 _Color; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpMap; float4 _BumpMap_ST; float _BumpScale; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; float4 tangent : TANGENT; float4 texcoord : TEXCOORD0; }; struct v2f { float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float4 TtoW0 : TEXCOORD1; float4 TtoW1 : TEXCOORD2; float4 TtoW2 : TEXCOORD3; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; o.uv.zw = v.texcoord.xy * _BumpMap_ST.xy + _BumpMap_ST.zw; float3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; fixed3 worldNormal = UnityObjectToWorldNormal(v.normal); fixed3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); fixed3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; // Compute the matrix that transform directions from tangent space to world space // Put the world position in w component for optimization o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); return o; } fixed4 frag(v2f i) : SV_Target { // Get the position in world space float3 worldPos = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); // Compute the light and view dir in world space fixed3 lightDir = normalize(UnityWorldSpaceLightDir(worldPos)); fixed3 viewDir = normalize(UnityWorldSpaceViewDir(worldPos)); // Get the normal in tangent space fixed3 bump = UnpackNormal(tex2D(_BumpMap, i.uv.zw)); bump.xy *= _BumpScale; bump.z = sqrt(1.0 - saturate(dot(bump.xy, bump.xy))); // Transform the narmal from tangent space to world space bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump))); fixed3 albedo = tex2D(_MainTex, i.uv).rgb * _Color.rgb; fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(bump, lightDir)); fixed3 halfDir = normalize(lightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(bump, halfDir)), _Gloss); return fixed4(ambient + diffuse + specular, 1.0); } ENDCG } } FallBack \"Specular\"} Displacement MappingA height map (or true displacement map) can be easily painted in Photoshop; Since normal map is clear , displacement is not hard for you.A displacement map actually changes the geometry using a texture. A common simplification is that the displacement will be in the direction of the surface normal. $$p\\prime = p + f(p)n.$$ Environment MappingOften we want to have a texture-mapped background and for objects to have specular reflections of that background. This can be accomplished using environment maps; There are many ways to store environment maps. Here is the most common method cube map. If you have used Unity, then you’ll be familiar with cube map, yes, the sky box~ In ideal cases, we want to generate the corresponding cube map for the objects of different positions in the scene. So the smart way is to write scripts. Here’s an example 123456789101112131415161718192021222324252627282930313233using UnityEngine;using UnityEditor;using System.Collections;public class RenderCubemapWizard : ScriptableWizard { public Transform renderFromPosition; public Cubemap cubemap; void OnWizardUpdate () { helpString = \"Select transform to render from and cubemap to render into\"; isValid = (renderFromPosition != null) &amp;&amp; (cubemap != null); } void OnWizardCreate () { // create temporary camera for rendering GameObject go = new GameObject( \"CubemapCamera\"); go.AddComponent&lt;Camera&gt;(); // place it on the object go.transform.position = renderFromPosition.position; // render into cubemap go.GetComponent&lt;Camera&gt;().RenderToCubemap(cubemap); // destroy temporary camera DestroyImmediate( go ); } [MenuItem(\"GameObject/Render into Cubemap\")] static void RenderCubemap () { ScriptableWizard.DisplayWizard&lt;RenderCubemapWizard&gt;( \"Render cubemap\", \"Render!\"); }} Shadow MapsHere comes the shadow map. Opacity Blending Appendix:PBSThis part will be explained in First-Met-With-RayTracing. InterpolationBefore learning CG, I couldn’t understand the term interpolation. Now it’s time write something about it.There are many interpolation methods, today I want to introduce a common method, called Barycentric Coordinates - used in Interpolation Across Triangles. If you have read the above the paragraphs carefully, you can see the barycentric coordinates method has appeared before. Why do we want interplate? Specify values at vertices Obtain smoothly varying values across triangles What do we want to interpolate? Texture coordinates, colors, normal vectors, … Barycentric Coordinates: Formulas $$\\alpha = \\frac{-(x-x_B)(y_C - y_B) + (y-y_B)(x_C-x_B)}{-(x_A-x_B)(y_C-y_B) + (y_A-y_B)(x_C-x_B)}\\tag{Barycentric Coordinates: Formulas}$$$$\\beta = \\frac{-(x-x_C)(y_A-y_C) + (y-y_C)(x_A-x_C)}{-(x_B-x_C)(y_A-y_C) + (y_B-y_C)(x_A-x_C)}$$$$\\gamma = 1 - \\alpha - \\beta$$ Using Barycentric Coordinates talk is cheap, show me the code 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//c++static std::tuple&lt;float, float, float&gt; computeBarycentric2D(float x, float y, const Vector4f* v){ float alpha = (x*(v[1].y() - v[2].y()) + (v[2].x() - v[1].x())*y + v[1].x()*v[2].y() - v[2].x()*v[1].y()) / (v[0].x()*(v[1].y() - v[2].y()) + (v[2].x() - v[1].x())*v[0].y() + v[1].x()*v[2].y() - v[2].x()*v[1].y()); float beta = (x*(v[2].y() - v[0].y()) + (v[0].x() - v[2].x())*y + v[2].x()*v[0].y() - v[0].x()*v[2].y()) / (v[1].x()*(v[2].y() - v[0].y()) + (v[0].x() - v[2].x())*v[1].y() + v[2].x()*v[0].y() - v[0].x()*v[2].y()); float gamma = (x*(v[0].y() - v[1].y()) + (v[1].x() - v[0].x())*y + v[0].x()*v[1].y() - v[1].x()*v[0].y()) / (v[2].x()*(v[0].y() - v[1].y()) + (v[1].x() - v[0].x())*v[2].y() + v[0].x()*v[1].y() - v[1].x()*v[0].y()); return {alpha,beta,gamma};}//we all know that color,vertex position,normal are Vector3fstatic Eigen::Vector3f interpolate(float alpha, float beta, float gamma, const Eigen::Vector3f&amp; vert1, const Eigen::Vector3f&amp; vert2, const Eigen::Vector3f&amp; vert3, float weight){ return (alpha * vert1 + beta * vert2 + gamma * vert3) / weight;}//uv coordinates are Vector2fstatic Eigen::Vector2f interpolate(float alpha, float beta, float gamma, const Eigen::Vector2f&amp; vert1, const Eigen::Vector2f&amp; vert2, const Eigen::Vector2f&amp; vert3, float weight){ auto u = (alpha * vert1[0] + beta * vert2[0] + gamma * vert3[0]); auto v = (alpha * vert1[1] + beta * vert2[1] + gamma * vert3[1]); u /= weight; v /= weight; return Eigen::Vector2f(u, v);}//here's the rasterization processvoid rasterization(Triangle &amp;t){ ...find the bounding box of t for(int x = int(x_min); x &lt; int(x_max)+1; x++) { for(int y = int(y_min); y&lt; int(y_max)+1;y++) { if(insideTriangle(float(x) + 0.5, float(y) + 0.5, t)) { //get alpha,beta,gamma auto[alpha, beta, gamma] = computeBarycentric2D(x, y, t.v); float Z = 1.0 / (alpha / v[0].w() + beta / v[1].w() + gamma / v[2].w()); //interpolate depth float zp = alpha * v[0].z() / v[0].w() + beta * v[1].z() / v[1].w() + gamma * v[2].z() / v[2].w(); zp *= Z; //if pass the depth test auto interpolated_color = interpolate(alpha,beta,gamma,t.color[0],t.color[1],t.color[2],1); auto interpolated_normal = interpolate(alpha,beta,gamma,t.normal[0],t.normal[1],t.normal[2],1); auto interpolated_texcoords = interpolate(alpha,beta,gamma,t.tex_coords[0],t.tex_coords[1],t.tex_coords[2],1); ... } } }} Bilinear InterpolationSince we mentioned bilinear interpolation in the texture magnificient part. So let’s go straight. Step1. We want to sample texture f(x,y) at red point, black points indicate texture sample locations. Step2. Take 4 nearest sample locations, with texture values as labeled. Step3. Calculate fractional offsets,(s,t) Step4. $$lerp(x,v_0,v_1) = v_0 + x(v_1 - v_0)\\tag{Linear interpolation (1D)}$$$$u_0 = lerp(s,u_{00},u_{10})$$$$u_1 = lerp(s,u_{01},u_{11})\\tag{Two helper lerps}$$$$f(x,y) = lerp(t,u_0,u_1)\\tag{Final vertical lerp, to get result}$$ talk is cheap, show me the code 1234567891011121314151617181920212223242526272829303132333435//c++/opencvEigen::Vector3f getColor(float u, float v) { auto u_img = u * (width-1); auto v_img = (1 - v) * (height-1); auto color = image_data.at&lt;cv::Vec3b&gt;(v_img, u_img); return Eigen::Vector3f(color[0], color[1], color[2]); } //if the texture image is low-pixels, then u_img &amp; v_img will not be int(ideally case). Eigen::Vector3f getColorBilinear(float u,float v) { auto u_img = u * (width-1); auto v_img = v * (height-1); Eigen::Vector2f u00(std::floor(u_img)*1.f,std::floor(v_img)*1.f); Eigen::Vector2f u10(std::ceil(u_img)*1.f,std::floor(v_img)*1.f); Eigen::Vector2f u01(std::floor(u_img)*1.f,std::ceil(v_img)*1.f); Eigen::Vector2f u11(std::ceil(u_img)*1.f,std::ceil(v_img)*1.f); float s = (u_img - u00.x()); float t = (v_img - u00.y()); Eigen::Vector3f u0 = lerp(s,getColor(u00.x()/width,u00.y()/height),getColor(u10.x()/width,u10.y()/height)); Eigen::Vector3f u1 = lerp(s,getColor(u01.x()/width,u01.y()/height),getColor(u11.x()/width,u11.y()/height)); Eigen::Vector3f color = lerp(t,u0,u1); return color; } Eigen::Vector3f lerp(float coefficient,Eigen::Vector3f a,Eigen::Vector3f b) { //return (coefficient * a + (1-coefficient) * b); return (a + coefficient*(b-a)); } For the code above, I have a few words to add: In opencv:Mat image;image.at&lt;&gt;(i,j) i–&gt;y j–&gt;xcolor order : BGRthe origin is at the upper-left corner Another one, Since have learned that one pixel can be seen as a little square. and the center of the pixel is (x + 0.5,y + 0.5); I tried this experiment using OpenCV, and found that : the result shows that they represent the same pixel. 12window.at&lt;cv::Vec3b&gt;(1,1)[1] = 255window.at&lt;cv::Vec3b&gt;(1,1.9)[1] = 255 References:[1]3D Math Primer for Graphics and Game Development 2nd Edition.[2]Fundamentals of Computer Graphics and 3rd Edition.[3]Unity+Shader入门精要[4]Unity3d Mannual[5]GAMES[6]scratchapixel","link":"/Graphics/Rendering/Rendering-Lighting-Shading-Texture/"},{"title":"Function-Template","text":"Keywords: FunctionTemplate This is for the readers who have basic c++ background. If you want to get the rough sketch of funtion template of c++, then there it is. I suggest an online c++ compiler : http://coliru.stacked-crooked.com/ (You can test your simple programs on it.) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include &lt;iostream&gt;#include &lt;functional&gt;#include &lt;array&gt;using namespace std;std::function&lt;int(int)&gt; Functional;int TestFunc(int a){ return a;}auto lambdaFn = [](float a)-&gt;float{ return a; };class Functor{ public: int operator()(int a){return a;}};class TestClass{ public: int ClassMember(int a) { return a; } static int StaticMember(int a) { return a; }};int main(){ Functional = TestFunc; int result = Functional(10); //10 cout &lt;&lt; \"TestFunc：\"&lt;&lt; result &lt;&lt; endl; Functional = lambdaFn; result = Functional(22.00); //22 cout &lt;&lt; \"Lambda：\"&lt;&lt; result &lt;&lt; endl; Functor testFunctor; Functional = testFunctor; result = Functional(30); //30 cout &lt;&lt; \"Functor：\"&lt;&lt; result &lt;&lt; endl; TestClass testObj; Functional = std::bind(&amp;TestClass::ClassMember, testObj, std::placeholders::_1); result = Functional(40); //40 cout &lt;&lt; \"TestClass：\"&lt;&lt; result &lt;&lt; endl; Functional = TestClass::StaticMember; result = Functional(50); //50 cout &lt;&lt; \"TestClass(static)：\"&lt;&lt; result &lt;&lt; endl; return 0;} function is a kind of wrapper,which provides a way to handle several funtion-like forms uniformly. If you know polymorphism, then this is easy to understand. Let’s see another code segment. 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;functional&gt;#include &lt;map&gt;using namespace std;int add(int x,int y){return x+y;}struct divide{ int operator()(int denominator,int divisor) { return denominator/divisor; }};int main(){ map&lt;char,function&lt;int(int,int)&gt;&gt; op = { {'+',add}, {'/',divide()}, {'-',[](int i,int j){return i-j;}} }; cout &lt;&lt; op['+'](1, 2) &lt;&lt; endl; //3 cout &lt;&lt; op['-'](1, 2) &lt;&lt; endl; //-1 cout &lt;&lt; op['/'](1, 2) &lt;&lt; endl; //0} Reference: https://www.cnblogs.com/reboost/p/11076511.html","link":"/CS/Advanced-CPP/STL-FunctionTemplate/"}],"tags":[{"name":"Linear Algebra","slug":"Linear-Algebra","link":"/tags/Linear-Algebra/"},{"name":"Animation","slug":"Animation","link":"/tags/Animation/"},{"name":"Algorithms","slug":"Algorithms","link":"/tags/Algorithms/"},{"name":"Calculus","slug":"Calculus","link":"/tags/Calculus/"},{"name":"GameProgramming","slug":"GameProgramming","link":"/tags/GameProgramming/"},{"name":"Geometry","slug":"Geometry","link":"/tags/Geometry/"},{"name":"Numerical Analysis","slug":"Numerical-Analysis","link":"/tags/Numerical-Analysis/"},{"name":"Motion Generation","slug":"Motion-Generation","link":"/tags/Motion-Generation/"},{"name":"Sketch","slug":"Sketch","link":"/tags/Sketch/"},{"name":"Motion Control","slug":"Motion-Control","link":"/tags/Motion-Control/"},{"name":"Human Pose","slug":"Human-Pose","link":"/tags/Human-Pose/"},{"name":"Sketh","slug":"Sketh","link":"/tags/Sketh/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"Rendering","slug":"Rendering","link":"/tags/Rendering/"},{"name":"CPP","slug":"CPP","link":"/tags/CPP/"},{"name":"Variation","slug":"Variation","link":"/tags/Variation/"}],"categories":[{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"Linear Algebra","slug":"Math/Linear-Algebra","link":"/categories/Math/Linear-Algebra/"},{"name":"Graphics","slug":"Graphics","link":"/categories/Graphics/"},{"name":"CS","slug":"CS","link":"/categories/CS/"},{"name":"Calculus","slug":"Math/Calculus","link":"/categories/Math/Calculus/"},{"name":"Animation","slug":"Graphics/Animation","link":"/categories/Graphics/Animation/"},{"name":"Algorithms","slug":"CS/Algorithms","link":"/categories/CS/Algorithms/"},{"name":"GameProgramming","slug":"GameProgramming","link":"/categories/GameProgramming/"},{"name":"Geometry","slug":"Graphics/Geometry","link":"/categories/Graphics/Geometry/"},{"name":"Numerical Analysis","slug":"Math/Numerical-Analysis","link":"/categories/Math/Numerical-Analysis/"},{"name":"Paper","slug":"Paper","link":"/categories/Paper/"},{"name":"System","slug":"GameProgramming/System","link":"/categories/GameProgramming/System/"},{"name":"Motion Generation","slug":"Paper/Motion-Generation","link":"/categories/Paper/Motion-Generation/"},{"name":"Sketch","slug":"Paper/Sketch","link":"/categories/Paper/Sketch/"},{"name":"Motion Control","slug":"Paper/Motion-Control","link":"/categories/Paper/Motion-Control/"},{"name":"Human Pose","slug":"Paper/Human-Pose","link":"/categories/Paper/Human-Pose/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/categories/MachineLearning/"},{"name":"PatternRecognition","slug":"MachineLearning/PatternRecognition","link":"/categories/MachineLearning/PatternRecognition/"},{"name":"Rendering","slug":"Graphics/Rendering","link":"/categories/Graphics/Rendering/"},{"name":"Advanced CPP","slug":"CS/Advanced-CPP","link":"/categories/CS/Advanced-CPP/"},{"name":"Variation","slug":"Math/Variation","link":"/categories/Math/Variation/"}]}